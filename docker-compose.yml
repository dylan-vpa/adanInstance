version: '3.8'

services:
  nginx:
    build: ./nginx
    ports:
      - "${PORT_HTTP:-80}:80"
      - "${PORT_HTTPS:-443}:443"
    volumes:
      - ./nginx/ssl:/etc/nginx/ssl
      - ./nginx/logs:/var/log/nginx
    depends_on:
      - moderador-api
    networks:
      - adan-network
    environment:
      - RUNPOD_PUBLIC_IP=${RUNPOD_PUBLIC_IP}
      - RUNPOD_POD_ID=${RUNPOD_POD_ID}

  vllm-server:
    build: ./vllm
    ports:
      - "${PORT_VLLM:-8000}:8000"
    volumes:
      - ./models:/app/models
      - ./vllm/logs:/app/logs
    environment:
      - MODEL_NAME=${MODEL_NAME:-mistralai/Mixtral-8x7B-Instruct-v0.1}
      - MODEL_PATH=/app/models
      - MAX_MODEL_LEN=4096
      - QUANTIZATION=awq
      - HOST=0.0.0.0
      - PORT=8000
      - WORKERS=1
      - MAX_BATCH_SIZE=32
      - MAX_BATCH_TOKENS=4096
      - GPU_MEMORY_UTILIZATION=0.9
      - TENSOR_PARALLEL_SIZE=1
      - RUNPOD_GPU_ID=${RUNPOD_GPU_ID}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    networks:
      - adan-network

  moderador-api:
    build: ./moderador-api
    ports:
      - "${PORT_API:-8003}:8003"
    volumes:
      - ./models:/app/models
      - ./fine-tuning:/app/fine-tuning
    environment:
      - VLLM_HOST=vllm-server
      - VLLM_PORT=8000
      - API_HOST=0.0.0.0
      - API_PORT=8003
      - HUGGING_FACE_TOKEN=${HUGGING_FACE_TOKEN}
      - RUNPOD_PUBLIC_IP=${RUNPOD_PUBLIC_IP}
    depends_on:
      - vllm-server
    networks:
      - adan-network

networks:
  adan-network:
    driver: bridge 