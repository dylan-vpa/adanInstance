{
  "title": "Deep Learning Ian Goodfellow, Yoshua Bengio & Aaron Courville",
  "source": "Libros/Deep Learning Ian Goodfellow, Yoshua Bengio & Aaron Courville.pdf",
  "content": "DeepLearningIanGoodfellowYoshuaBengioAaronCourville ContentsWebsiteviiAcknowledgmentsviiiNotationxi1Introduction11.1WhoShouldReadThisBook?. . . . . .. . . . . . . . .. . . . .81.2HistoricalTrendsinDeepLearning. . . . . . . . .. . . . . . . .11IAppliedMathandMachineLearningBasics292LinearAlgebra312.1Scalars,Vectors,MatricesandTensors. . . . . . . . .. . . . . .312.2MultiplyingMatricesandVectors. . . . . . . . .. . . . . . . ..342.3IdentityandInverseMatrices. . . . . . . . .. . . . . . . .. . .362.4LinearDependenceandSpan. . . . . . . . .. . . . . . . .. . .372.5Norms. . . . . . . . .. . . . . . . .. . . . . . . .. . . . . . . .392.6SpecialKindsofMatricesandVectors. . . . . . . . . . . . . . .402.7Eigendecomposition. . . . . . . . . .. . . . . . . .. . . . . . . .422.8SingularValueDecomposition. . . . . . . .. . . . . . . .. . . .442.9TheMoore-PenrosePseudoinverse. . . . . . . . .. . . . . . . ..452.10TheTraceOperator. . . . . . . . .. . . . . . . .. . . . . . . .462.11TheDeterminant. .. . . . . . . .. . . . . . . .. . . . . . . . .472.12Example:PrincipalComponentsAnalysis. . . . . . . . .. . . .483ProbabilityandInformationTheory533.1WhyProbability?. . . . .. . . . . . . . .. . . . . . . .. . . . .54i CONTENTS3.2RandomVariables. . . . .. . . . . . . .. . . . . . . . .. . . .563.3ProbabilityDistributions. . . . . . . . .. . . . . . . .. . . . . .563.4MarginalProbability. . . . . . . . .. . . . . . . . .. . . . . . .583.5ConditionalProbability. .. . . . . . . .. . . . . . . .. . . . .593.6TheChainRuleofConditionalProbabilities. . . . . . . . .. . .593.7IndependenceandConditionalIndependence. . . . . . . . .. . .603.8Expectation,VarianceandCovariance. . . . . . . . . .. . . . .603.9CommonProbabilityDistributions. . . . . . . . . . . . . . .. .623.10UsefulPropertiesofCommonFunctions. . .. . . . . . . . .. .673.11Bayes’Rule. . . . . . . . . .. . . . . . . .. . . . . . . .. . . .703.12TechnicalDetailsofContinuousVariables. . . . . .. . . . . . .713.13InformationTheory. . . . . . . . . .. . . . . . . .. . . . . . . .733.14StructuredProbabilisticModels. . . .. . . . . . . .. . . . . . .754NumericalComputation804.1OverﬂowandUnderﬂow. . . . . . . . .. . . . . . . .. . . . . .804.2PoorConditioning. . . . . . . . .. . . . . . . .. . . . . . . . .824.3Gradient-BasedOptimization. . . . . . .. . . . . . . .. . . . .824.4ConstrainedOptimization. . . . . . . . . . . . .. . . . . . . ..934.5Example:LinearLeastSquares. . . . . . .. . . . . . . . .. . .965MachineLearningBasics985.1LearningAlgorithms. . . . . . . . . . .. . . . . . . .. . . . . .995.2Capacity,OverﬁttingandUnderﬁtting. .. . . . . . . .. . . . .1105.3HyperparametersandValidationSets.. . . . . . . .. . . . . . .1205.4Estimators,BiasandVariance. . . . . .. . . . . . . .. . . . . .1225.5MaximumLikelihoodEstimation. . . . . .. . . . . . . . .. . .1315.6BayesianStatistics. . . . . . . . . . .. . . . . . . .. . . . . . .1355.7SupervisedLearningAlgorithms. . .. . . . . . . .. . . . . . . .1405.8UnsupervisedLearningAlgorithms. . . . . . . . . . . . . . .. .1465.9StochasticGradientDescent. . . .. . . . . . . . .. . . . . . . .1515.10BuildingaMachineLearningAlgorithm. . . . . . . . . . . . ..1535.11ChallengesMotivatingDeepLearning. . . . .. . . . . . . . .. .155IIDeepNetworks:ModernPractices1666DeepFeedforwardNetworks1686.1Example:LearningXOR.. . . . . . . . .. . . . . . . .. . . . .1716.2Gradient-BasedLearning.. . . . . . . .. . . . . . . .. . . . . .177ii CONTENTS6.3HiddenUnits. . . . . .. . . . . . . .. . . . . . . . .. . . . . .1916.4ArchitectureDesign. . . . . . . . .. . . . . . . .. . . . . . . ..1976.5Back-PropagationandOtherDiﬀerentiationAlgorithms. . . . .2046.6HistoricalNotes. . . . . . .. . . . . . . .. . . . . . . . .. . . .2247RegularizationforDeepLearning2287.1ParameterNormPenalties. . . . .. . . . . . . . .. . . . . . . .2307.2NormPenaltiesasConstrainedOptimization. . . . . . . .. . . .2377.3RegularizationandUnder-ConstrainedProblems. .. . . . . . .2397.4DatasetAugmentation. . . . . . . . . .. . . . . . . . .. . . . .2407.5NoiseRobustness. . . . . . . . .. . . . . . . .. . . . . . . .. .2427.6Semi-SupervisedLearning. . . . . . . . . . . . . . . .. . . . . .2437.7Multi-TaskLearning. . . . . . . . . . . . . .. . . . . . . . .. .2447.8EarlyStopping. . . . . . . . .. . . . . . . .. . . . . . . .. . .2467.9ParameterTyingandParameterSharing . . . . . . . . . . . . . .2537.10SparseRepresentations. . . . . . . . .. . . . . . . .. . . . . . .2547.11BaggingandOtherEnsembleMethods.. . . . . . . . .. . . . .2567.12Dropout. . . . . . . .. . . . . . . . .. . . . . . . .. . . . . . .2587.13AdversarialTraining. . . . . . . .. . . . . . . . .. . . . . . . .2687.14TangentDistance,TangentProp,andManifoldTangentClassiﬁer2708OptimizationforTrainingDeepModels2748.1HowLearningDiﬀersfromPureOptimization. . . . . . . . . . .2758.2ChallengesinNeuralNetworkOptimization. . . . .. . . . . . .2828.3BasicAlgorithms. . . . . . . . . . . . .. . . . . . . .. . . . . .2948.4ParameterInitializationStrategies.. . . . . . . . .. . . . . . .3018.5AlgorithmswithAdaptiveLearningRates. . . . . . .. . . . . .3068.6ApproximateSecond-OrderMethods. . . .. . . . . . . . .. . .3108.7OptimizationStrategiesandMeta-Algorithms. . . . .. . . . . .3179ConvolutionalNetworks3309.1TheConvolutionOperation. . . . . . . . . . . . . . . .. . . . .3319.2Motivation. .. . . . . . . .. . . . . . . . .. . . . . . . .. . . .3359.3Pooling. . . . . . . . . . . . .. . . . . . . .. . . . . . . . .. . .3399.4ConvolutionandPoolingasanInﬁnitelyStrongPrior. .. . . . .3459.5VariantsoftheBasicConvolutionFunction. . . . . . . . . . . .3479.6StructuredOutputs.. . . . . . . .. . . . . . . . .. . . . . . . .3589.7DataTypes. . . . . .. . . . . . . .. . . . . . . .. . . . . . . .3609.8EﬃcientConvolutionAlgorithms. . . . . . . .. . . . . . . .. .3629.9RandomorUnsupervisedFeatures. . . . . . . .. . . . . . . ..363iii CONTENTS9.10TheNeuroscientiﬁcBasisforConvolutionalNetworks. . . . . ..3649.11ConvolutionalNetworksandtheHistoryofDeepLearning. . . .37110 SequenceModeling:RecurrentandRecursiveNets37310.1UnfoldingComputationalGraphs. . . . . . . . . . . . .. . . . .37510.2RecurrentNeuralNetworks. . .. . . . . . . . .. . . . . . . ..37810.3BidirectionalRNNs . . . . . . . . . . . . . .. . . . . . . . .. . .39410.4Encoder-DecoderSequence-to-SequenceArchitectures. . . . . ..39610.5DeepRecurrentNetworks. . . . . . . .. . . . . . . . .. . . . .39810.6RecursiveNeuralNetworks. . . . .. . . . . . . . .. . . . . . . .40010.7TheChallengeofLong-TermDependencies. . . . . . . . . .. . .40110.8EchoStateNetworks. . . . . . . . . .. . . . . . . . .. . . . . .40410.9LeakyUnitsandOtherStrategiesforMultipleTimeScales. . ..40610.10 TheLongShort-TermMemoryandOtherGatedRNNs. .. . . .40810.11 OptimizationforLong-TermDependencies. . . . . . . .. . . . .41310.12 ExplicitMemory. . . . . . . . . .. . . . . . . . .. . . . . . . .41611 PracticalMethodology42111.1PerformanceMetrics. . . . . . . . . .. . . . . . . .. . . . . . .42211.2DefaultBaselineModels. . . . . . . .. . . . . . . .. . . . . . .42511.3DeterminingWhethertoGatherMoreData. . . . . . . . . . . .42611.4SelectingHyperparameters. . . . . . . . .. . . . . . . .. . . . .42711.5DebuggingStrategies. . . . .. . . . . . . .. . . . . . . . .. . .43611.6Example:Multi-DigitNumberRecognition. . . . .. . . . . . . .44012 Applications44312.1Large-ScaleDeepLearning.. . . . . . . .. . . . . . . .. . . . .44312.2ComputerVision. . . . . . . . .. . . . . . . .. . . . . . . .. .45212.3SpeechRecognition . . . . . .. . . . . . . .. . . . . . . . .. . .45812.4NaturalLanguageProcessing. . .. . . . . . . .. . . . . . . ..46112.5OtherApplications. . . . . . . . .. . . . . . . .. . . . . . . ..478IIIDeepLearningResearch48613 LinearFactorModels48913.1ProbabilisticPCAandFactorAnalysis. . . . . . .. . . . . . . .49013.2IndependentComponentAnalysis(ICA). . . . . . . . . . . .. .49113.3SlowFeatureAnalysis. . . . . .. . . . . . . . .. . . . . . . ..49313.4SparseCoding. . . . . .. . . . . . . .. . . . . . . . .. . . . . .496iv CONTENTS13.5ManifoldInterpretationofPCA. . . . . . . . . . . . . . . . .. .49914 Autoencoders50214.1UndercompleteAutoencoders. . . . . . . . . .. . . . . . . .. .50314.2RegularizedAutoencoders. . . . . . . . .. . . . . . . .. . . . .50414.3RepresentationalPower,LayerSizeandDepth. . . . . .. . . . .50814.4StochasticEncodersandDecoders. . . . . . . . . . .. . . . . . .50914.5DenoisingAutoencoders. .. . . . . . . .. . . . . . . . .. . . .51014.6LearningManifoldswithAutoencoders. . . . . .. . . . . . . . .51514.7ContractiveAutoencoders.. . . . . . . .. . . . . . . .. . . . .52114.8PredictiveSparseDecomposition. . . . . . . .. . . . . . . . ..52314.9ApplicationsofAutoencoders. . . . .. . . . . . . . .. . . . . .52415 RepresentationLearning52615.1GreedyLayer-WiseUnsupervisedPretraining. . . . . .. . . . .52815.2TransferLearningandDomainAdaptation. . . .. . . . . . . ..53615.3Semi-SupervisedDisentanglingofCausalFactors. . . . .. . . .54115.4DistributedRepresentation. . . . . . . . . . . .. . . . . . . . ..54615.5ExponentialGainsfromDepth. . . . . . . . . .. . . . . . . ..55315.6ProvidingCluestoDiscoverUnderlyingCauses. . . .. . . . . .55416 StructuredProbabilisticModelsforDeepLearning55816.1TheChallengeofUnstructuredModeling.. . . . . . . .. . . . .55916.2UsingGraphstoDescribeModelStructure. .. . . . . . . .. . .56316.3SamplingfromGraphicalModels. . .. . . . . . . .. . . . . . .58016.4AdvantagesofStructuredModeling .. . . . . . . . .. . . . . . .58216.5LearningaboutDependencies. . . .. . . . . . . .. . . . . . . .58216.6InferenceandApproximateInference. . . . . . . . .. . . . . . .58416.7TheDeepLearningApproachtoStructuredProbabilisticModels58517 MonteCarloMethods59017.1SamplingandMonteCarloMethods. . . . . . . .. . . . . . . .59017.2ImportanceSampling. . . . . . . . . . . .. . . . . . . .. . . . .59217.3MarkovChainMonteCarloMethods. . . . .. . . . . . . .. . .59517.4GibbsSampling . . . . . . .. . . . . . . . .. . . . . . . .. . . .59917.5TheChallengeofMixingbetweenSeparatedModes. . . . . . ..59918 ConfrontingthePartitionFunction60518.1TheLog-LikelihoodGradient.. . . . . . . .. . . . . . . . .. .60618.2StochasticMaximumLikelihoodandContrastiveDivergence. . .607v CONTENTS18.3Pseudolikelihood. . . . . . . . . . .. . . . . . . . .. . . . . . .61518.4ScoreMatchingandRatioMatching. . . . . . . .. . . . . . . .61718.5DenoisingScoreMatching. . . . . . . . .. . . . . . . .. . . . .61918.6Noise-ContrastiveEstimation. . . . .. . . . . . . .. . . . . . .62018.7EstimatingthePartitionFunction. . . . . . . . . . .. . . . . . .62319 ApproximateInference63119.1InferenceasOptimization.. . . . . . . . .. . . . . . . .. . . .63319.2ExpectationMaximization. .. . . . . . . .. . . . . . . . .. . .63419.3MAPInferenceandSparseCoding.. . . . . . . . .. . . . . . .63519.4VariationalInferenceandLearning. . . . . . . . . . . . . . .. .63819.5LearnedApproximateInference. . .. . . . . . . . .. . . . . . .65120 DeepGenerativeModels65420.1BoltzmannMachines. . . . . . . . . . .. . . . . . . . .. . . . .65420.2RestrictedBoltzmannMachines. . . . . . .. . . . . . . . .. . .65620.3DeepBeliefNetworks.. . . . . . . .. . . . . . . .. . . . . . . .66020.4DeepBoltzmannMachines. . . . . . . . . .. . . . . . . .. . . .66320.5BoltzmannMachinesforReal-ValuedData. . . . . . . . .. . . .67620.6ConvolutionalBoltzmannMachines . . . . . . . . . . . . . . . ..68320.7BoltzmannMachinesforStructuredorSequentialOutputs. . . .68520.8OtherBoltzmannMachines. . . . .. . . . . . . .. . . . . . . .68620.9Back-PropagationthroughRandomOperations. . . . . .. . . .68720.10 DirectedGenerativeNets. . . . . . . . . . . .. . . . . . . . .. .69220.11 DrawingSamplesfromAutoencoders. . . . .. . . . . . . . .. .71120.12 GenerativeStochasticNetworks. . .. . . . . . . .. . . . . . . .71420.13 OtherGenerationSchemes. . . . . . . . . . . . .. . . . . . . . .71620.14 EvaluatingGenerativeModels . . . . . . . . . . . . .. . . . . . .71720.15 Conclusion. . . . . . . .. . . . . . . . .. . . . . . . .. . . . . .720Bibliography721Index777 vi Websitewww.deeplearningbook.orgThisbookisaccompaniedbytheabovewebsite.Thewebsiteprovidesavarietyofsupplementarymaterial,includingexercises,lectureslides,correctionsofmistakes,andotherresourcesthatshouldbeusefultobothreadersandinstructors. vii AcknowledgmentsThisbookwouldnothavebeenpossiblewithoutthecontributionsofmanypeople.Wewouldliketothankthosewhocommentedonourproposalforthebookandhelpedplanitscontentsandorganization:GuillaumeAlain,KyunghyunCho,ÇağlarGülçehre,DavidKrueger,HugoLarochelle,RazvanPascanuandThomasRohée.Wewouldliketothankthepeoplewhooﬀeredfeedbackonthecontentofthebookitself.Someoﬀeredfeedbackonmanychapters:MartínAbadi,GuillaumeAlain,IonAndroutsopoulos,FredBertsch,OlexaBilaniuk,UfukCanBiçici,MatkoBošnjak,JohnBoersma,GregBrockman,AlexandredeBrébisson,PierreLucCarrier,SarathChandar,PawelChilinski,MarkDaoust,OlegDashevskii,LaurentDinh,StephanDreseitl,JimFan,MiaoFan,MeireFortunato,FrédéricFrancis,Nando deFreitas,Çağlar Gülçehre, Jurgen VanGael,JavierAlonso García,JonathanHunt,GopiJeyaram,ChingizKabytayev,LukaszKaiser,VarunKanade,AsifullahKhan,AkielKhan,JohnKing,DiederikP.Kingma,YannLeCun,RudolfMathey,MatíasMattamala,AbhinavMaurya,KevinMurphy,OlegMürk,RomanNovak,AugustusQ.Odena,SimonPavlik,KarlPichotta,EddiePierce,KariPulli,RousselRahman,TapaniRaiko,AnuragRanjan,JohannesRoith,MihaelaRosca,HalisSak, CésarSalgado,GrigorySapunov,YoshinoriSasaki, MikeSchuster,JulianSerban,NirShabat,KenShirriﬀ,AndreSimpelo,ScottStanley,DavidSussillo,IlyaSutskever,CarlesGeladaSáez,GrahamTaylor,ValentinTolmer,MassimilianoTomassoli,AnTran,ShubhenduTrivedi,AlexeyUmnov,VincentVanhoucke,MarcoVisentini-Scarzanella,MartinVita,DavidWarde-Farley,DustinWebb,KelvinXu,WeiXue,KeYang,LiYao,ZygmuntZającandOzanÇağlayan.Wewouldalsoliketothankthosewhoprovideduswithusefulfeedbackonindividualchapters:•Notation:ZhangYuanhang.•Chapter,:YusufAkgul,SebastienBratieres,SamiraEbrahimi,1Introductionviii CONTENTSCharlieGorichanaz,BrendanLoudermilk,EricMorris,CosminPârvulescuandAlfredoSolano.•Chapter,:AmjadAlmahairi,NikolaBanić,KevinBennett,2LinearAlgebraPhilippeCastonguay,OscarChang,EricFosler-Lussier,AndreyKhalyavin,SergeyOreshkov,IstvánPetrás,DennisPrangle,ThomasRohée,GitanjaliGulveSehgal,ColbyToland,AlessandroVitaleandBobWelland.•Chapter,:JohnPhilipAnderson,Kai3ProbabilityandInformationTheoryArulkumaran,VincentDumoulin,RuiFa,StephanGouws,ArtemOboturov,AnttiRasmus,AlexeySurkovandVolkerTresp.•Chapter , :Tran LamAnIan Fischer andHu4NumericalComputationYuhuang.•Chapter,:DzmitryBahdanau,JustinDomingue,5MachineLearningBasicsNikhilGarg,MakotoOtsuka,BobPepin,PhilipPopien,EmmanuelRayner,PeterShepard,Kee-BongSong,ZhengSunandAndyWu.•Chapter,6DeepFeedforwardNetworks:UrielBerdugo,FabrizioBottarel,ElizabethBurl,IshanDurugkar,JeﬀHlywa,JongWookKim,DavidKruegerandAdityaKumarPraharaj.•Chapter,:MortenKolbæk,KshitijLauria,7RegularizationforDeepLearningInkyuLee,SunilMohan,HaiPhongPhanandJoshuaSalisbury.•Chapter,8OptimizationforTrainingDeepModels:MarcelAckermann,PeterArmitage,RowelAtienza,AndrewBrock,TeganMaharaj,JamesMartens,KashifRasul,KlausStroblandNicholasTurner.•Chapter,9ConvolutionalNetworks:MartínArjovsky,EugeneBrevdo,Kon-stantinDivilov,EricJensen,MehdiMirza,AlexPaino,MarjorieSayer,RyanStoutandWentaoWu.•Chapter,10SequenceModeling:RecurrentandRecursiveNets:GökçenEraslan,StevenHickson,RazvanPascanu,LorenzovonRitter,RuiRodrigues,DmitriySerdyuk,DongyuShiandKaiyuYang.•Chapter,:DanielBeckstein.11PracticalMethodology•Chapter,:GeorgeDahl,VladimirNekrasovandRibana12ApplicationsRoscher.•Chapter,13LinearFactorModels:JayanthKoushik.ix CONTENTS•Chapter,:KunalGhosh.15RepresentationLearning•Chapter,: MinhLê16StructuredProbabilisticModelsforDeepLearningandAntonVarfolom.•Chapter,18ConfrontingthePartitionFunction:SamBowman.•Chapter,:YujiaBao.19ApproximateInference•Chapter,20DeepGenerativeModels:NicolasChapados,DanielGalvez,WenmingMa,FadyMedhat,ShakirMohamedandGrégoireMontavon.•Bibliography:LukasMichelbacherandLeslieN.Smith.Wealsowanttothankthosewhoallowedustoreproduceimages,ﬁguresordatafromtheirpublications.Weindicatetheircontributionsintheﬁgurecaptionsthroughoutthetext.WewouldliketothankLuWangforwritingpdf2htmlEX,whichweusedtomakethewebversionofthebook,andforoﬀeringsupporttoimprovethequalityoftheresultingHTML.We would liketothank Ian’swifeDaniela FloriGoodfellowforpatientlysupportingIanduringthewritingofthebookaswellasforhelpwithproofreading.WewouldliketothanktheGoogleBrainteamforprovidinganintellectualenvironmentwhereIancoulddevoteatremendousamountoftimetowritingthisbookandreceivefeedbackandguidancefromcolleagues.WewouldespeciallyliketothankIan’sformermanager,GregCorrado,andhiscurrentmanager,SamyBengio,fortheirsupportofthisproject.Finally,wewouldliketothankGeoﬀreyHintonforencouragementwhenwritingwasdiﬃcult. x NotationThissectionprovidesaconcisereferencedescribingthenotationusedthroughoutthisbook.Ifyouareunfamiliarwithanyofthecorrespondingmathematicalconcepts,wedescribemostoftheseideasinchapters2–4.NumbersandArraysaAscalar(integerorreal)aAvectorAAmatrixAAtensorInIdentitymatrixwithrowsandcolumnsnnIIdentitymatrixwithdimensionalityimpliedbycontexte()iStandardbasisvector[0,...,0,1,0,...,0]witha1atpositionidiag()aAsquare,diagonalmatrixwithdiagonalentriesgivenbyaaAscalarrandomvariableaAvector-valuedrandomvariableAAmatrix-valuedrandomvariablexi CONTENTSSetsandGraphsAAsetRThesetofrealnumbers{}01,Thesetcontaining0and1{}01,,...,nThesetofallintegersbetweenand0n[]a,bTherealintervalincludingandab(]a,bTherealintervalexcludingbutincludingabAB\\Setsubtraction,i.e., thesetcontainingtheele-mentsofthatarenotinABGAgraphPaG(xi)TheparentsofxiinGIndexingaiElementiofvectora,withindexingstartingat1a−iAllelementsofvectorexceptforelementaiAi,jElementofmatrixi,jAAi,:RowofmatrixiAA:,iColumnofmatrixiAAi,j,kElementofa3-Dtensor()i,j,kAA::,,i2-Dsliceofa3-DtensoraiElementoftherandomvectoriaLinearAlgebraOperationsATransposeofmatrixAA+Moore-PenrosepseudoinverseofAABElement-wise(Hadamard)productofandABdet()ADeterminantofAxii CONTENTSCalculusdydxDerivativeofwithrespecttoyx∂y∂xPartialderivativeofwithrespecttoyx∇xyGradientofwithrespecttoyx∇XyMatrixderivativesofwithrespecttoyX∇XyTensorcontainingderivativesofywithrespecttoX∂f∂xJacobianmatrixJ∈Rmn×off: Rn→Rm∇2xfff()(xorH)()xTheHessianmatrixofatinputpointxfd()xxDeﬁniteintegralovertheentiredomainofxSfd()xxxDeﬁniteintegralwithrespecttooverthesetSProbabilityandInformationTheoryabTherandomvariablesaandbareindependent⊥abcTheyareconditionallyindependentgivenc⊥|P()aAprobabilitydistributionoveradiscretevariablep()aAprobabilitydistributionoveracontinuousvari-able,oroveravariablewhosetypehasnotbeenspeciﬁedaRandomvariableahasdistribution∼PPEx∼P[()]()()()fxorEfxExpectationoffxwithrespecttoPxVar(())fxVarianceofunderxfx()P()Cov(()())fx,gxCovarianceofandunderxfx()gx()P()H()xShannonentropyoftherandomvariablexDKL()PQKullback-LeiblerdivergenceofPandQN(;)xµ,ΣGaussiandistributionoverxwithmeanµandcovarianceΣxiii CONTENTSFunctionsff: AB→ThefunctionwithdomainandrangeABfgfg◦Compositionofthefunctionsandf(;)xθAfunctionofxparametrizedbyθ. (Sometimeswewritef(x)andomittheargumentθtolightennotation)logxxNaturallogarithmofσx()Logisticsigmoid,11+exp()−xζxx()log(1+exp(Softplus,))||||xpLpnormofx||||xL2normofxx+Positivepartof,i.e.,xmax(0),x1conditionis1iftheconditionistrue,0otherwiseSometimesweuseafunctionfwhoseargumentisascalarbutapplyittoavector,matrix,ortensor:f(x),f(X),orf(X).Thisdenotestheapplicationofftothearrayelement-wise.Forexample,ifC=σ(X),thenCi,j,k=σ(Xi,j,k)forallvalidvaluesof,and.ijkDatasetsandDistributionspdataThedatageneratingdistributionˆpdataTheempiricaldistributiondeﬁnedbythetrainingsetXAsetoftrainingexamplesx()iThe-thexample(input)fromadatasetiy()iory()iThetargetassociatedwithx()iforsupervisedlearn-ingXThemn×matrixwithinputexamplex()iinrowXi,:xiv Chapter1IntroductionInventorshavelongdreamedofcreatingmachinesthatthink.ThisdesiredatesbacktoatleastthetimeofancientGreece.ThemythicalﬁguresPygmalion,Daedalus,andHephaestusmayallbeinterpretedaslegendaryinventors,andGalatea,Talos,andPandoramayallberegardedasartiﬁciallife(,OvidandMartin2004Sparkes1996Tandy1997;,;,).Whenprogrammablecomputerswereﬁrstconceived,peoplewonderedwhethersuchmachinesmightbecomeintelligent,overahundredyearsbeforeonewasbuilt(Lovelace1842,).Today,artiﬁcialintelligence(AI)isathrivingﬁeldwithmanypracticalapplicationsandactiveresearchtopics.Welooktointelligentsoftwaretoautomateroutinelabor,understandspeechorimages,makediagnosesinmedicineandsupportbasicscientiﬁcresearch.Intheearlydaysofartiﬁcialintelligence,theﬁeldrapidlytackledandsolvedproblemsthatareintellectuallydiﬃcultforhumanbeingsbutrelativelystraight-forwardforcomputers—problemsthatcanbedescribedbyalistofformal,math-ematicalrules. Thetruechallengetoartiﬁcialintelligenceprovedtobesolvingthetasksthatareeasyforpeopletoperformbuthardforpeopletodescribeformally—problemsthatwesolveintuitively,thatfeelautomatic,likerecognizingspokenwordsorfacesinimages.Thisbookisaboutasolutiontothesemoreintuitiveproblems.Thissolutionistoallowcomputerstolearnfromexperienceandunderstandtheworldintermsofahierarchyofconcepts,witheachconceptdeﬁnedintermsofitsrelationtosimplerconcepts.Bygatheringknowledgefromexperience,thisapproachavoidstheneedforhumanoperatorstoformallyspecifyalloftheknowledgethatthecomputerneeds.Thehierarchyofconceptsallowsthecomputertolearncomplicatedconceptsbybuildingthemoutofsimplerones.Ifwedrawagraphshowinghowthese1 CHAPTER1.INTRODUCTIONconceptsarebuiltontopofeachother,thegraphisdeep,withmanylayers.Forthisreason,wecallthisapproachtoAI.deeplearningManyoftheearlysuccessesofAItookplaceinrelativelysterileandformalenvironmentsanddidnotrequirecomputerstohavemuchknowledgeabouttheworld.Forexample,IBM’sDeepBluechess-playingsystemdefeatedworldchampionGarryKasparovin1997(,).ChessisofcourseaverysimpleHsu2002world,containingonlysixty-fourlocationsandthirty-twopiecesthatcanmoveinonlyrigidlycircumscribedways.Devisingasuccessfulchessstrategyis atremendousaccomplishment, butthechallengeisnotduetothediﬃcultyofdescribingthesetofchesspiecesandallowablemovestothecomputer.Chesscanbecompletelydescribedbyaverybrieflistofcompletelyformalrules,easilyprovidedaheadoftimebytheprogrammer.Ironically,abstractandformaltasksthatareamongthemostdiﬃcultmentalundertakingsforahumanbeingareamongtheeasiestforacomputer.Computershavelongbeenabletodefeateventhebesthumanchessplayer,butareonlyrecentlymatchingsomeoftheabilitiesofaveragehumanbeingstorecognizeobjectsorspeech.Aperson’severydayliferequiresanimmenseamountofknowledgeabouttheworld.Muchofthisknowledgeissubjectiveandintuitive,andthereforediﬃculttoarticulateinaformalway.Computersneedtocapturethissameknowledgeinordertobehaveinanintelligentway.Oneofthekeychallengesinartiﬁcialintelligenceishowtogetthisinformalknowledgeintoacomputer.Severalartiﬁcialintelligenceprojectshavesoughttohard-codeknowledgeabouttheworldinformallanguages.Acomputercanreasonaboutstatementsintheseformallanguagesautomaticallyusinglogicalinferencerules.Thisisknownastheknowledgebaseapproachtoartiﬁcialintelligence.Noneoftheseprojectshasledtoamajorsuccess.OneofthemostfamoussuchprojectsisCyc(,LenatandGuha1989).CycisaninferenceengineandadatabaseofstatementsinalanguagecalledCycL.Thesestatementsareenteredbyastaﬀofhumansupervisors.Itisanunwieldyprocess.Peoplestruggletodeviseformalruleswithenoughcomplexitytoaccuratelydescribetheworld.Forexample,CycfailedtounderstandastoryaboutapersonnamedFredshavinginthemorning(,).ItsinferenceLinde1992enginedetectedaninconsistencyinthestory: itknewthatpeopledonothaveelectricalparts,butbecauseFredwasholdinganelectricrazor,itbelievedtheentity“FredWhileShaving”containedelectricalparts.ItthereforeaskedwhetherFredwasstillapersonwhilehewasshaving.Thediﬃcultiesfacedbysystemsrelyingonhard-codedknowledgesuggestthatAIsystemsneedtheabilitytoacquiretheirownknowledge,byextractingpatternsfromrawdata.Thiscapabilityisknownasmachinelearning.The2 CHAPTER1.INTRODUCTIONintroductionofmachinelearningallowedcomputerstotackleproblemsinvolvingknowledgeoftherealworldandmakedecisionsthatappearsubjective.Asimplemachinelearningalgorithmcalledlogisticregressioncandeterminewhethertorecommendcesareandelivery(Mor-Yosef1990etal.,).AsimplemachinelearningalgorithmcallednaiveBayescanseparatelegitimatee-mailfromspame-mail.Theperformanceofthesesimplemachinelearningalgorithmsdependsheavilyontherepresentationofthedatatheyaregiven.Forexample,whenlogisticregressionisusedtorecommendcesareandelivery,theAIsystemdoesnotexaminethepatientdirectly.Instead,thedoctortellsthesystemseveralpiecesofrelevantinformation,suchasthepresenceorabsenceofauterinescar.Eachpieceofinformationincludedintherepresentationofthepatientisknownasafeature.Logisticregressionlearnshoweachofthesefeaturesofthepatientcorrelateswithvariousoutcomes.However,itcannotinﬂuencethewaythatthefeaturesaredeﬁnedinanyway. IflogisticregressionwasgivenanMRIscanofthepatient,ratherthanthedoctor’sformalizedreport,itwouldnotbeabletomakeusefulpredictions.IndividualpixelsinanMRIscanhavenegligiblecorrelationwithanycomplicationsthatmightoccurduringdelivery.Thisdependenceonrepresentationsisageneralphenomenonthatappearsthroughoutcomputerscienceandevendailylife.Incomputerscience,opera-tionssuchassearchingacollectionofdatacanproceedexponentiallyfasterifthecollectionisstructuredandindexedintelligently.PeoplecaneasilyperformarithmeticonArabicnumerals,butﬁndarithmeticonRomannumeralsmuchmoretime-consuming.Itisnotsurprisingthatthechoiceofrepresentationhasanenormouseﬀectontheperformanceofmachinelearningalgorithms.Forasimplevisualexample,seeﬁgure.1.1Manyartiﬁcialintelligencetaskscanbesolvedbydesigningtherightsetoffeaturestoextractforthattask,thenprovidingthesefeaturestoasimplemachinelearningalgorithm.Forexample,ausefulfeatureforspeakeridentiﬁcationfromsoundisanestimateofthesizeofspeaker’svocaltract.Itthereforegivesastrongclueastowhetherthespeakerisaman,woman,orchild.However,formanytasks,itisdiﬃculttoknowwhatfeaturesshouldbeextracted.Forexample,supposethatwewouldliketowriteaprogramtodetectcarsinphotographs.Weknowthatcarshavewheels,sowemightliketousethepresenceofawheelasafeature.Unfortunately,itisdiﬃculttodescribeexactlywhatawheellookslikeintermsofpixelvalues.Awheelhasasimplegeometricshapebutitsimagemaybecomplicatedbyshadowsfallingonthewheel,thesunglaringoﬀthemetalpartsofthewheel,thefenderofthecaroranobjectintheforegroundobscuringpartofthewheel,andsoon.3 CHAPTER1.INTRODUCTION   Figure1.1:Exampleofdiﬀerentrepresentations:supposewewanttoseparatetwocategoriesofdatabydrawingalinebetweentheminascatterplot.Intheplotontheleft,werepresentsomedatausingCartesiancoordinates,andthetaskisimpossible.Intheplotontheright,werepresentthedatawithpolarcoordinatesandthetaskbecomessimpletosolvewithaverticalline.FigureproducedincollaborationwithDavidWarde-Farley.Onesolutiontothisproblemistousemachinelearningtodiscovernotonlythemappingfromrepresentationtooutputbutalsotherepresentationitself.Thisapproachisknownasrepresentationlearning. Learnedrepresentationsoftenresultinmuchbetterperformancethancanbeobtainedwithhand-designedrepresentations.TheyalsoallowAIsystemstorapidlyadapttonewtasks,withminimalhumanintervention.Arepresentationlearningalgorithmcandiscoveragoodsetoffeaturesforasimpletaskinminutes,oracomplextaskinhourstomonths.Manuallydesigningfeaturesforacomplextaskrequiresagreatdealofhumantimeandeﬀort;itcantakedecadesforanentirecommunityofresearchers.Thequintessentialexampleofarepresentationlearningalgorithmistheau-toencoder.Anautoencoderisthecombinationofanencoderfunctionthatconvertstheinputdataintoadiﬀerentrepresentation,andadecoderfunctionthatconvertsthenewrepresentationbackintotheoriginalformat.Autoencodersaretrainedtopreserveasmuchinformationaspossiblewhenaninputisrunthroughtheencoderandthenthedecoder,butarealsotrainedtomakethenewrepresentationhavevariousniceproperties.Diﬀerentkindsofautoencodersaimtoachievediﬀerentkindsofproperties.Whendesigningfeaturesoralgorithmsforlearningfeatures,ourgoalisusuallytoseparatethefactorsofvariationthatexplaintheobserveddata.Inthiscontext,weusetheword“factors”simplytorefertoseparatesourcesofinﬂuence;thefactorsareusuallynotcombinedbymultiplication.Suchfactorsareoftennot4 CHAPTER1.INTRODUCTIONquantitiesthataredirectlyobserved.Instead,theymayexisteitherasunobservedobjectsorunobservedforcesinthephysicalworldthataﬀectobservablequantities.Theymayalsoexistasconstructsinthehumanmindthatprovideusefulsimplifyingexplanationsorinferredcausesoftheobserveddata.Theycanbethoughtofasconceptsorabstractionsthathelpusmakesenseoftherichvariabilityinthedata.Whenanalyzingaspeechrecording,thefactorsofvariationincludethespeaker’sage,theirsex,theiraccentandthewordsthattheyarespeaking.Whenanalyzinganimageofacar,thefactorsofvariationincludethepositionofthecar,itscolor,andtheangleandbrightnessofthesun.Amajorsourceofdiﬃcultyinmanyreal-worldartiﬁcialintelligenceapplicationsisthatmanyofthefactorsofvariationinﬂuenceeverysinglepieceofdataweareabletoobserve.Theindividualpixelsinanimageofaredcarmightbeveryclosetoblackatnight.Theshapeofthecar’ssilhouettedependsontheviewingangle.Mostapplicationsrequireustothefactorsofvariationanddiscardthedisentangleonesthatwedonotcareabout.Ofcourse,itcanbeverydiﬃculttoextractsuchhigh-level,abstractfeaturesfromrawdata.Manyofthesefactorsofvariation,suchasaspeaker’saccent,canbeidentiﬁedonlyusingsophisticated,nearlyhuman-levelunderstandingofthedata.Whenitisnearlyasdiﬃculttoobtainarepresentationastosolvetheoriginalproblem,representationlearningdoesnot,atﬁrstglance,seemtohelpus.Deeplearningsolvesthiscentralprobleminrepresentationlearningbyintro-ducingrepresentationsthatareexpressedintermsofother,simplerrepresentations.Deeplearningallowsthecomputertobuildcomplexconceptsoutofsimplercon-cepts.Figureshowshowadeeplearningsystemcanrepresenttheconceptof1.2animageofapersonbycombiningsimplerconcepts,suchascornersandcontours,whichareinturndeﬁnedintermsofedges.Thequintessentialexampleofadeeplearningmodelisthefeedforwarddeepnetworkormultilayerperceptron(MLP).Amultilayerperceptronisjustamathematicalfunctionmappingsomesetofinputvaluestooutputvalues.Thefunctionisformedbycomposingmanysimplerfunctions.Wecanthinkofeachapplicationofadiﬀerentmathematicalfunctionasprovidinganewrepresentationoftheinput.Theideaoflearningtherightrepresentationforthedataprovidesoneperspec-tiveondeeplearning.Anotherperspectiveondeeplearningisthatdepthallowsthecomputertolearnamulti-stepcomputerprogram.Eachlayeroftherepresentationcanbethoughtofasthestateofthecomputer’smemoryafterexecutinganothersetofinstructionsinparallel.Networkswithgreaterdepthcanexecutemoreinstructionsinsequence.Sequentialinstructionsoﬀergreatpowerbecauselater5 CHAPTER1.INTRODUCTION Visible layer(input pixels)1st hidden layer(edges)2nd hidden layer(corners andcontours)3rd hidden layer(object parts)CARPERSONANIMALOutput(object identity) Figure1.2:Illustrationofadeeplearningmodel.Itisdiﬃcultforacomputertounderstandthemeaningofrawsensoryinputdata,suchasthisimagerepresentedasacollectionofpixelvalues.Thefunctionmappingfromasetofpixelstoanobjectidentityisverycomplicated.Learningorevaluatingthismappingseemsinsurmountableiftackleddirectly.Deeplearningresolvesthisdiﬃcultybybreakingthedesiredcomplicatedmappingintoaseriesofnestedsimplemappings,eachdescribedbyadiﬀerentlayerofthemodel.Theinputispresentedatthevisiblelayer,sonamedbecauseitcontainsthevariablesthatweareabletoobserve.Thenaseriesofhiddenlayersextractsincreasinglyabstractfeaturesfromtheimage.Theselayersarecalled“hidden”becausetheirvaluesarenotgiveninthedata;insteadthemodelmustdeterminewhichconceptsareusefulforexplainingtherelationshipsintheobserveddata.Theimagesherearevisualizationsofthekindoffeaturerepresentedbyeachhiddenunit.Giventhepixels,theﬁrstlayercaneasilyidentifyedges,bycomparingthebrightnessofneighboringpixels.Giventheﬁrsthiddenlayer’sdescriptionoftheedges,thesecondhiddenlayercaneasilysearchforcornersandextendedcontours,whicharerecognizableascollectionsofedges.Giventhesecondhiddenlayer’sdescriptionoftheimageintermsofcornersandcontours,thethirdhiddenlayercandetectentirepartsofspeciﬁcobjects,byﬁndingspeciﬁccollectionsofcontoursandcorners.Finally,thisdescriptionoftheimageintermsoftheobjectpartsitcontainscanbeusedtorecognizetheobjectspresentintheimage.ImagesreproducedwithpermissionfromZeilerandFergus2014().6 CHAPTER1.INTRODUCTION x1x1σ w1w1×x2x2w2w2×+ElementSet+×σx xw wElementSetLogisticRegressionLogisticRegressionFigure1.3:Illustrationofcomputationalgraphsmappinganinputtoanoutputwhereeachnodeperformsanoperation.Depthisthelengthofthelongestpathfrominputtooutputbutdependsonthedeﬁnitionofwhatconstitutesapossiblecomputationalstep.Thecomputationdepictedinthesegraphsistheoutputofalogisticregressionmodel,σ(wTx),whereσisthelogisticsigmoidfunction.Ifweuseaddition,multiplicationandlogisticsigmoidsastheelementsofourcomputerlanguage,thenthismodelhasdepththree.Ifweviewlogisticregressionasanelementitself,thenthismodelhasdepthone.instructionscanreferbacktotheresultsofearlierinstructions.Accordingtothisviewofdeeplearning,notalloftheinformationinalayer’sactivationsnecessarilyencodesfactorsofvariationthatexplaintheinput.Therepresentationalsostoresstateinformationthathelpstoexecuteaprogramthatcanmakesenseoftheinput.Thisstateinformationcouldbeanalogoustoacounterorpointerinatraditionalcomputerprogram.Ithasnothingtodowiththecontentoftheinputspeciﬁcally,butithelpsthemodeltoorganizeitsprocessing.Therearetwomainwaysofmeasuringthedepthofamodel.Theﬁrstviewisbasedonthenumberofsequentialinstructionsthatmustbeexecutedtoevaluatethearchitecture.Wecanthinkofthisasthelengthofthelongestpaththroughaﬂowchartthatdescribeshowtocomputeeachofthemodel’soutputsgivenitsinputs.Justastwoequivalentcomputerprogramswillhavediﬀerentlengthsdependingonwhichlanguagetheprogramiswrittenin,thesamefunctionmaybedrawnasaﬂowchartwithdiﬀerentdepthsdependingonwhichfunctionsweallowtobeusedasindividualstepsintheﬂowchart.Figureillustrateshowthis1.3choiceoflanguagecangivetwodiﬀerentmeasurementsforthesamearchitecture.Anotherapproach,usedbydeepprobabilisticmodels,regardsthedepthofamodelasbeingnotthedepthofthecomputationalgraphbutthedepthofthegraphdescribinghowconceptsarerelatedtoeachother.Inthiscase,thedepth7 CHAPTER1.INTRODUCTIONoftheﬂowchartofthecomputationsneededtocomputetherepresentationofeachconceptmaybemuchdeeperthanthegraphoftheconceptsthemselves.Thisisbecausethesystem’sunderstandingofthesimplerconceptscanbereﬁnedgiveninformationaboutthemorecomplexconcepts.Forexample,anAIsystemobservinganimageofafacewithoneeyeinshadowmayinitiallyonlyseeoneeye.Afterdetectingthatafaceispresent,itcantheninferthatasecondeyeisprobablypresentaswell. Inthiscase,thegraphofconceptsonlyincludestwolayers—alayerforeyesandalayerforfaces—butthegraphofcomputationsincludes2nlayersifwereﬁneourestimateofeachconceptgiventheothertimes.nBecauseitisnotalwaysclearwhichofthesetwoviews—thedepthofthecomputationalgraph,orthedepthoftheprobabilisticmodelinggraph—ismostrelevant,andbecausediﬀerentpeoplechoosediﬀerentsetsofsmallestelementsfromwhichtoconstructtheirgraphs,thereisnosinglecorrectvalueforthedepthofanarchitecture,justasthereisnosinglecorrectvalueforthelengthofacomputerprogram. Noristhereaconsensusabouthowmuchdepthamodelrequirestoqualifyas“deep.”However,deeplearningcansafelyberegardedasthestudyofmodelsthateitherinvolveagreateramountofcompositionoflearnedfunctionsorlearnedconceptsthantraditionalmachinelearningdoes.Tosummarize,deeplearning,thesubjectofthisbook,isanapproachtoAI.Speciﬁcally,itisatypeofmachinelearning,atechniquethatallowscomputersystemstoimprovewithexperienceanddata. Accordingtotheauthorsofthisbook,machinelearningistheonlyviableapproachtobuildingAIsystemsthatcanoperateincomplicated,real-worldenvironments.Deeplearningisaparticularkindofmachinelearningthatachievesgreatpowerandﬂexibilitybylearningtorepresenttheworldasanestedhierarchyofconcepts,witheachconceptdeﬁnedinrelationtosimplerconcepts,andmoreabstractrepresentationscomputedintermsoflessabstractones.Figureillustratestherelationshipbetweenthesediﬀerent1.4AIdisciplines.Figuregivesahigh-levelschematicofhoweachworks.1.51.1WhoShouldReadThisBook?Thisbookcanbeusefulforavarietyofreaders,butwewroteitwithtwomaintargetaudiencesinmind.Oneofthesetargetaudiencesisuniversitystudents(undergraduateorgraduate)learningaboutmachinelearning,includingthosewhoarebeginningacareerindeeplearningandartiﬁcialintelligenceresearch.Theothertargetaudienceissoftwareengineerswhodonothaveamachinelearningorstatisticsbackground,butwanttorapidlyacquireoneandbeginusingdeeplearningintheirproductorplatform.Deeplearninghasalreadyprovenusefulin8 CHAPTER1.INTRODUCTION AIMachine learningRepresentation learningDeep learningExample:KnowledgebasesExample:LogisticregressionExample:ShallowautoencodersExample:MLPs Figure1.4:AVenndiagramshowinghowdeeplearningisakindofrepresentationlearning,whichisinturnakindofmachinelearning,whichisusedformanybutnotallapproachestoAI.EachsectionoftheVenndiagramincludesanexampleofanAItechnology. 9 CHAPTER1.INTRODUCTION InputHand-designed programOutput InputHand-designed featuresMapping from featuresOutput InputFeaturesMapping from featuresOutput InputSimple featuresMapping from featuresOutput Additional layers of more abstract features Rule-basedsystemsClassicmachinelearningRepresentationlearningDeeplearningFigure1.5: FlowchartsshowinghowthediﬀerentpartsofanAIsystemrelatetoeachotherwithindiﬀerentAIdisciplines.Shadedboxesindicatecomponentsthatareabletolearnfromdata.10 CHAPTER1.INTRODUCTIONmanysoftwaredisciplinesincludingcomputervision,speechandaudioprocessing,naturallanguageprocessing,robotics,bioinformaticsandchemistry,videogames,searchengines,onlineadvertisingandﬁnance.Thisbookhasbeenorganizedintothreepartsinordertobestaccommodateavarietyofreaders.PartintroducesbasicmathematicaltoolsandmachinelearningIconcepts.PartdescribesthemostestablisheddeeplearningalgorithmsthatareIIessentiallysolvedtechnologies.PartdescribesmorespeculativeideasthatareIIIwidelybelievedtobeimportantforfutureresearchindeeplearning.Readersshouldfeelfreetoskippartsthatarenotrelevantgiventheirinterestsorbackground.Readersfamiliarwithlinearalgebra,probability,andfundamentalmachinelearningconceptscanskippart,forexample,whilereaderswhojustwantItoimplementaworkingsystemneednotreadbeyondpart.TohelpchoosewhichIIchapterstoread,ﬁgureprovidesaﬂowchartshowingthehigh-levelorganization1.6ofthebook.Wedoassumethatallreaderscomefromacomputersciencebackground.Weassumefamiliaritywithprogramming,abasicunderstandingofcomputationalperformanceissues,complexitytheory,introductorylevelcalculusandsomeoftheterminologyofgraphtheory.1.2HistoricalTrendsinDeepLearningItiseasiesttounderstanddeeplearningwithsomehistoricalcontext.Ratherthanprovidingadetailedhistoryofdeeplearning,weidentifyafewkeytrends:•Deeplearninghashadalongandrichhistory,buthasgonebymanynamesreﬂectingdiﬀerentphilosophicalviewpoints,andhaswaxedandwanedinpopularity.•Deeplearninghasbecomemoreusefulastheamountofavailabletrainingdatahasincreased.•Deeplearningmodelshavegrowninsizeovertimeascomputerinfrastructure(bothhardwareandsoftware)fordeeplearninghasimproved.•Deeplearninghassolvedincreasinglycomplicatedapplicationswithincreasingaccuracyovertime.11 CHAPTER1.INTRODUCTION1. IntroductionPart I: Applied Math and Machine Learning Basics2. Linear Algebra3. Probability and Information Theory4. Numerical Computation5. Machine Learning BasicsPart II: Deep Networks: Modern Practices6. Deep Feedforward Networks7. Regularization8. Optimization9. CNNs10. RNNs11. Practical Methodology12. ApplicationsPart III: Deep Learning Research13. Linear Factor Models14. Autoencoders15. Representation Learning16. Structured Probabilistic Models17. Monte Carlo Methods18. Partition Function19. Inference20. Deep Generative ModelsFigure1.6:Thehigh-levelorganizationofthebook.Anarrowfromonechaptertoanotherindicatesthattheformerchapterisprerequisitematerialforunderstandingthelatter.12 CHAPTER1.INTRODUCTION1.2.1TheManyNamesandChangingFortunesofNeuralNet-worksWeexpectthatmanyreadersofthisbookhaveheardofdeeplearningasanexcitingnewtechnology,andaresurprisedtoseeamentionof“history”inabookaboutanemergingﬁeld.Infact,deeplearningdatesbacktothe1940s.Deeplearningonlyappearstobenew,becauseitwasrelativelyunpopularforseveralyearsprecedingitscurrentpopularity,andbecauseithasgonethroughmanydiﬀerentnames,andhasonlyrecentlybecomecalled“deeplearning.”Theﬁeldhasbeenrebrandedmanytimes,reﬂectingtheinﬂuenceofdiﬀerentresearchersanddiﬀerentperspectives.Acomprehensivehistoryofdeeplearningisbeyondthescopeofthistextbook.However,somebasiccontextisusefulforunderstandingdeeplearning.Broadlyspeaking,therehavebeenthreewavesofdevelopmentofdeeplearning:deeplearning known ascyberneticsin the 1940s–1960s, deep learning knownasconnectionisminthe1980s–1990s,andthecurrentresurgenceunderthenamedeeplearningbeginningin2006.Thisisquantitativelyillustratedinﬁgure.1.7Someoftheearliestlearningalgorithmswerecognizetodaywereintendedtobecomputationalmodelsofbiologicallearning,i.e.modelsofhowlearninghappensorcouldhappeninthebrain. Asaresult,oneofthenamesthatdeeplearninghasgonebyisartiﬁcialneuralnetworks(ANNs).Thecorrespondingperspectiveondeeplearningmodelsisthattheyareengineeredsystemsinspiredbythebiologicalbrain(whetherthehumanbrainorthebrainofanotheranimal).Whilethekindsofneuralnetworksusedformachinelearninghavesometimesbeenusedtounderstandbrainfunction(,),theyareHintonandShallice1991generallynotdesignedtoberealisticmodelsofbiologicalfunction.Theneuralperspectiveondeeplearningismotivatedbytwomainideas.Oneideaisthatthebrainprovidesaproofbyexamplethatintelligentbehaviorispossible,andaconceptuallystraightforwardpathtobuildingintelligenceistoreverseengineerthecomputationalprinciplesbehindthebrainandduplicateitsfunctionality.Anotherperspectiveisthatitwouldbedeeplyinterestingtounderstandthebrainandtheprinciplesthatunderliehumanintelligence,somachinelearningmodelsthatshedlightonthesebasicscientiﬁcquestionsareusefulapartfromtheirabilitytosolveengineeringapplications.Themodernterm“deeplearning”goesbeyondtheneuroscientiﬁcperspectiveonthecurrentbreedofmachinelearningmodels.Itappealstoamoregeneralprincipleoflearningmultiplelevelsofcomposition,whichcanbeappliedinmachinelearningframeworksthatarenotnecessarilyneurallyinspired.13 CHAPTER1.INTRODUCTION 1940195019601970198019902000Year0.0000000.0000500.0001000.0001500.0002000.000250FrequencyofWordorPhrasecybernetics(connectionism+neuralnetworks) Figure1.7:Theﬁgureshowstwoofthethreehistoricalwavesofartiﬁcialneuralnetsresearch,asmeasuredbythefrequencyofthephrases“cybernetics”and“connectionism”or“neuralnetworks”accordingtoGoogleBooks(thethirdwaveistoorecenttoappear).Theﬁrstwavestartedwithcyberneticsinthe1940s–1960s,withthedevelopmentoftheoriesofbiologicallearning(,;,)andimplementationsofMcCullochandPitts1943Hebb1949theﬁrstmodelssuchastheperceptron(Rosenblatt1958,)allowingthetrainingofasingleneuron.Thesecondwavestartedwiththeconnectionistapproachofthe1980–1995period,withback-propagation(,)totrainaneuralnetworkwithoneortwoRumelhartetal.1986ahiddenlayers.Thecurrentandthirdwave,deeplearning,startedaround2006(Hintonetal.etal.etal.,;2006Bengio,;2007Ranzato,),andisjustnowappearinginbook2007aformasof2016.Theothertwowavessimilarlyappearedinbookformmuchlaterthanthecorrespondingscientiﬁcactivityoccurred. 14 CHAPTER1.INTRODUCTIONTheearliestpredecessorsofmoderndeeplearningweresimplelinearmodelsmotivatedfromaneuroscientiﬁcperspective.Thesemodelsweredesignedtotakeasetofninputvaluesx1,...,xnandassociatethemwithanoutputy.Thesemodelswouldlearnasetofweightsw1,...,wnandcomputetheiroutputf(xw,)=x1w1+···+xnwn.Thisﬁrstwaveofneuralnetworksresearchwasknownascybernetics,asillustratedinﬁgure.1.7TheMcCulloch-PittsNeuron(,)wasanearlymodelMcCullochandPitts1943ofbrainfunction.Thislinearmodelcouldrecognizetwodiﬀerentcategoriesofinputsbytestingwhetherf(xw,)ispositiveornegative.Ofcourse,forthemodeltocorrespondtothedesireddeﬁnitionofthecategories,theweightsneededtobesetcorrectly.Theseweightscouldbesetbythehumanoperator. Inthe1950s,theperceptron(Rosenblatt19581962,,)becametheﬁrstmodelthatcouldlearntheweightsdeﬁningthecategoriesgivenexamplesofinputsfromeachcategory.Theadaptivelinearelement(ADALINE),whichdatesfromaboutthesametime,simplyreturnedthevalueoff(x)itselftopredictarealnumber(WidrowandHoﬀ1960,),andcouldalsolearntopredictthesenumbersfromdata.Thesesimplelearningalgorithmsgreatlyaﬀectedthemodernlandscapeofma-chinelearning.ThetrainingalgorithmusedtoadapttheweightsoftheADALINEwasaspecialcaseofanalgorithmcalledstochasticgradientdescent.Slightlymodiﬁedversionsofthestochasticgradientdescentalgorithmremainthedominanttrainingalgorithmsfordeeplearningmodelstoday.Modelsbasedonthef(xw,)usedbytheperceptronandADALINEarecalledlinearmodels.Thesemodelsremainsomeofthemostwidelyusedmachinelearningmodels,thoughinmanycasestheyaretrainedindiﬀerentwaysthantheoriginalmodelsweretrained.Linearmodelshavemanylimitations.Mostfamously,theycannotlearntheXORfunction,wheref([0,1],w)=1andf([1,0],w)=1butf([1,1],w)=0andf([0,0],w)=0.Criticswhoobservedtheseﬂawsinlinearmodelscausedabacklashagainstbiologicallyinspiredlearningingeneral(MinskyandPapert,1969).Thiswastheﬁrstmajordipinthepopularityofneuralnetworks.Today,neuroscienceisregardedasanimportantsourceofinspirationfordeeplearningresearchers,butitisnolongerthepredominantguidefortheﬁeld.Themainreasonforthediminishedrole ofneuroscienceindeeplearningresearchtodayisthatwesimplydonothaveenoughinformationaboutthebraintouseitasaguide.Toobtainadeepunderstandingoftheactualalgorithmsusedbythebrain,wewouldneedtobeabletomonitortheactivityof(attheveryleast)thousandsofinterconnectedneuronssimultaneously.Becausewearenotabletodothis,wearefarfromunderstandingevensomeofthemostsimpleand15 CHAPTER1.INTRODUCTIONwell-studiedpartsofthebrain(,).OlshausenandField2005Neurosciencehasgivenusareasontohopethatasingledeeplearningalgorithmcansolvemanydiﬀerenttasks.Neuroscientistshavefoundthatferretscanlearnto“see”withtheauditoryprocessingregionoftheirbrainiftheirbrainsarerewiredtosendvisualsignalstothatarea(VonMelchner2000etal.,).Thissuggeststhatmuchofthemammalianbrainmightuseasinglealgorithmtosolvemostofthediﬀerenttasksthatthebrainsolves.Beforethishypothesis,machinelearningresearchwasmorefragmented,withdiﬀerentcommunitiesofresearchersstudyingnaturallanguageprocessing,vision,motionplanningandspeechrecognition.Today,theseapplicationcommunitiesarestillseparate,butitiscommonfordeeplearningresearchgroupstostudymanyorevenalloftheseapplicationareassimultaneously.Weareabletodrawsomeroughguidelinesfromneuroscience.Thebasicideaofhavingmanycomputationalunitsthatbecomeintelligentonlyviatheirinteractionswitheachotherisinspiredbythebrain.TheNeocognitron(Fukushima1980,)introducedapowerfulmodelarchitectureforprocessingimagesthatwasinspiredbythestructureofthemammalianvisualsystemandlaterbecamethebasisforthemodernconvolutionalnetwork(,),aswewillseeinLeCunetal.1998bsection.Mostneuralnetworkstodayarebasedonamodelneuroncalled9.10therectiﬁedlinearunit.TheoriginalCognitron(Fukushima1975,)introducedamorecomplicatedversionthatwashighlyinspiredbyourknowledgeofbrainfunction.Thesimpliﬁedmodernversionwasdevelopedincorporatingideasfrommanyviewpoints,with()and()citingNairandHinton2010Glorotetal.2011aneuroscienceasaninﬂuence,and()citingmoreengineering-Jarrettetal.2009orientedinﬂuences.Whileneuroscienceisanimportantsourceofinspiration,itneednotbetakenasarigidguide.Weknowthatactualneuronscomputeverydiﬀerentfunctionsthanmodernrectiﬁedlinearunits,butgreaterneuralrealismhasnotyetledtoanimprovementinmachinelearningperformance.Also,whileneurosciencehassuccessfullyinspiredseveralneuralnetworkarchitectures,wedonotyetknowenoughaboutbiologicallearningforneurosciencetooﬀermuchguidanceforthelearningalgorithmsweusetotrainthesearchitectures.Mediaaccountsoftenemphasizethesimilarityofdeeplearningtothebrain.WhileitistruethatdeeplearningresearchersaremorelikelytocitethebrainasaninﬂuencethanresearchersworkinginothermachinelearningﬁeldssuchaskernelmachinesorBayesianstatistics,oneshouldnotviewdeeplearningasanattempttosimulatethebrain.Moderndeeplearningdrawsinspirationfrommanyﬁelds,especiallyappliedmathfundamentalslikelinearalgebra,probability,informationtheory,andnumericaloptimization.Whilesomedeeplearningresearchersciteneuroscienceasanimportantsourceofinspiration,othersarenotconcernedwith16 CHAPTER1.INTRODUCTIONneuroscienceatall.Itis worth notingthat theeﬀorttounderstandhowthe brainworksonan algorithmic level is alive andwell.This endeavor is primarily knownas“computationalneuroscience”andisaseparateﬁeldofstudyfromdeeplearning.Itiscommonforresearcherstomovebackandforthbetweenbothﬁelds.Theﬁeldofdeeplearningisprimarilyconcernedwithhowtobuildcomputersystemsthatareabletosuccessfullysolvetasksrequiringintelligence,whiletheﬁeldofcomputationalneuroscienceisprimarilyconcernedwithbuildingmoreaccuratemodelsofhowthebrainactuallyworks.Inthe1980s,thesecondwaveofneuralnetworkresearchemergedingreatpartviaamovementcalledconnectionismorparalleldistributedprocess-ing(,;,). ConnectionismaroseinRumelhartetal.1986cMcClellandetal.1995thecontextofcognitivescience.Cognitivescienceisaninterdisciplinaryapproachtounderstandingthemind,combiningmultiplediﬀerentlevelsofanalysis.Duringtheearly1980s,mostcognitivescientistsstudiedmodelsofsymbolicreasoning.Despitetheirpopularity,symbolicmodelswerediﬃculttoexplainintermsofhowthebraincouldactuallyimplementthemusingneurons.Theconnectionistsbegantostudymodelsofcognitionthatcouldactuallybegroundedinneuralimplementations(TouretzkyandMinton1985,),revivingmanyideasdatingbacktotheworkofpsychologistDonaldHebbinthe1940s(,).Hebb1949Thecentralideainconnectionismisthatalargenumberofsimplecomputationalunitscanachieveintelligentbehaviorwhennetworkedtogether.Thisinsightappliesequallytoneuronsinbiologicalnervoussystemsandtohiddenunitsincomputationalmodels.Severalkeyconceptsaroseduringtheconnectionismmovementofthe1980sthatremaincentraltotoday’sdeeplearning.Oneoftheseconceptsisthatofdistributedrepresentation(Hintonetal.,1986).Thisistheideathateachinputtoasystemshouldberepresentedbymanyfeatures,andeachfeatureshouldbeinvolvedintherepresentationofmanypossibleinputs.Forexample,supposewehaveavisionsystemthatcanrecognizecars,trucks,andbirdsandtheseobjectscaneachbered,green,orblue.Onewayofrepresentingtheseinputswouldbetohaveaseparateneuronorhiddenunitthatactivatesforeachoftheninepossiblecombinations:redtruck,redcar,redbird,greentruck,andsoon.Thisrequiresninediﬀerentneurons,andeachneuronmustindependentlylearntheconceptofcolorandobjectidentity.Onewaytoimproveonthissituationistouseadistributedrepresentation,withthreeneuronsdescribingthecolorandthreeneuronsdescribingtheobjectidentity.Thisrequiresonlysixneuronstotalinsteadofnine,andtheneurondescribingrednessisableto17 CHAPTER1.INTRODUCTIONlearnaboutrednessfromimagesofcars,trucksandbirds,notonlyfromimagesofonespeciﬁccategoryofobjects. Theconceptofdistributedrepresentationiscentraltothisbook,andwillbedescribedingreaterdetailinchapter.15Anothermajoraccomplishmentoftheconnectionistmovementwasthesuc-cessfuluseofback-propagationtotraindeepneuralnetworkswithinternalrepre-sentationsandthepopularizationoftheback-propagationalgorithm(Rumelhartetal.,;,).Thisalgorithmhaswaxedandwanedinpopularity1986aLeCun1987butasofthiswritingiscurrentlythedominantapproachtotrainingdeepmodels.Duringthe1990s,researchersmadeimportantadvancesinmodelingsequenceswithneuralnetworks.()and()identiﬁedsomeofHochreiter1991Bengioetal.1994thefundamentalmathematicaldiﬃcultiesinmodelinglongsequences,describedinsection.10.7HochreiterandSchmidhuber1997()introducedthelongshort-termmemoryorLSTMnetworktoresolvesomeofthesediﬃculties.Today,theLSTMiswidelyusedformanysequencemodelingtasks,includingmanynaturallanguageprocessingtasksatGoogle.Thesecondwaveofneuralnetworksresearchlasteduntilthemid-1990s.Ven-turesbasedonneuralnetworksandotherAItechnologiesbegantomakeunrealisti-callyambitiousclaimswhileseekinginvestments.WhenAIresearchdidnotfulﬁlltheseunreasonableexpectations,investorsweredisappointed.Simultaneously,otherﬁeldsofmachinelearningmadeadvances.Kernelmachines(,Boseretal.1992CortesandVapnik1995Schölkopf1999Jor-;,;etal.,)andgraphicalmodels(dan1998,)bothachievedgoodresultsonmanyimportanttasks.Thesetwofactorsledtoadeclineinthepopularityofneuralnetworksthatlasteduntil2007.Duringthistime,neuralnetworkscontinuedtoobtainimpressiveperformanceonsometasks(,;,).TheCanadianInstituteLeCunetal.1998bBengioetal.2001forAdvancedResearch(CIFAR)helpedtokeepneuralnetworksresearchaliveviaitsNeuralComputationandAdaptivePerception(NCAP)researchinitiative.ThisprogramunitedmachinelearningresearchgroupsledbyGeoﬀreyHintonatUniversityofToronto,YoshuaBengioatUniversityofMontreal,andYannLeCunatNewYorkUniversity.TheCIFARNCAPresearchinitiativehadamulti-disciplinarynaturethatalsoincludedneuroscientistsandexpertsinhumanandcomputervision.Atthispointintime,deepnetworksweregenerallybelievedtobeverydiﬃculttotrain. Wenowknowthatalgorithmsthathaveexistedsincethe1980sworkquitewell,butthiswasnotapparentcirca2006.Theissueisperhapssimplythatthesealgorithmsweretoocomputationallycostlytoallowmuchexperimentationwiththehardwareavailableatthetime.Thethirdwaveofneuralnetworksresearchbeganwithabreakthroughin18 CHAPTER1.INTRODUCTION2006.GeoﬀreyHintonshowedthatakindofneuralnetworkcalledadeepbeliefnetworkcouldbeeﬃcientlytrainedusingastrategycalledgreedylayer-wisepre-training(,),whichwillbedescribedinmoredetailinsection.Hintonetal.200615.1TheotherCIFAR-aﬃliatedresearchgroupsquicklyshowedthatthesamestrategycouldbeusedtotrainmanyotherkindsofdeepnetworks(,;Bengioetal.2007Ranzato2007aetal.,)andsystematicallyhelpedtoimprovegeneralizationontestexamples.Thiswaveofneuralnetworksresearchpopularizedtheuseoftheterm“deeplearning”toemphasizethatresearcherswerenowabletotraindeeperneuralnetworksthanhadbeenpossiblebefore,andtofocusattentiononthetheoreticalimportanceofdepth(,;,BengioandLeCun2007DelalleauandBengio2011Pascanu2014aMontufar2014;etal.,;etal.,).Atthistime,deepneuralnetworksoutperformedcompetingAIsystemsbasedonothermachinelearningtechnologiesaswellashand-designedfunctionality.Thisthirdwaveofpopularityofneuralnetworkscontinuestothetimeofthiswriting,thoughthefocusofdeeplearningresearchhaschangeddramaticallywithinthetimeofthiswave.Thethirdwavebeganwithafocusonnewunsupervisedlearningtechniquesandtheabilityofdeepmodelstogeneralizewellfromsmalldatasets,buttodaythereismoreinterestinmucholdersupervisedlearningalgorithmsandtheabilityofdeepmodelstoleveragelargelabeleddatasets.1.2.2IncreasingDatasetSizesOnemaywonderwhydeeplearninghasonlyrecentlybecomerecognizedasacrucialtechnologythoughtheﬁrstexperimentswithartiﬁcialneuralnetworkswereconductedinthe1950s.Deeplearninghasbeensuccessfullyusedincommercialapplicationssincethe1990s,butwasoftenregardedasbeingmoreofanartthanatechnologyandsomethingthatonlyanexpertcoulduse,untilrecently.Itistruethatsomeskillisrequiredtogetgoodperformancefromadeeplearningalgorithm.Fortunately,theamountofskillrequiredreducesastheamountoftrainingdataincreases.Thelearningalgorithmsreachinghumanperformanceoncomplextaskstodayarenearlyidenticaltothelearningalgorithmsthatstruggledtosolvetoyproblemsinthe1980s,thoughthemodelswetrainwiththesealgorithmshaveundergonechangesthatsimplifythetrainingofverydeeparchitectures.Themostimportantnewdevelopmentisthattodaywecanprovidethesealgorithmswiththeresourcestheyneedtosucceed.Figureshowshowthesizeofbenchmark1.8datasetshasincreasedremarkablyovertime.Thistrendisdrivenbytheincreasingdigitizationofsociety.Asmoreandmoreofouractivitiestakeplaceoncomputers,moreandmoreofwhatwedoisrecorded.Asourcomputersareincreasinglynetworkedtogether,itbecomeseasiertocentralizetheserecordsandcuratethem19 CHAPTER1.INTRODUCTIONintoadatasetappropriateformachinelearningapplications.Theageof“BigData”hasmademachinelearningmucheasierbecausethekeyburdenofstatisticalestimation—generalizingwelltonewdataafterobservingonlyasmallamountofdata—hasbeenconsiderablylightened.Asof2016,aroughruleofthumbisthatasuperviseddeeplearningalgorithmwillgenerallyachieveacceptableperformancewitharound5,000labeledexamplespercategory,andwillmatchorexceedhumanperformancewhentrainedwithadatasetcontainingatleast10millionlabeledexamples.Workingsuccessfullywithdatasetssmallerthanthisisanimportantresearcharea,focusinginparticularonhowwecantakeadvantageoflargequantitiesofunlabeledexamples,withunsupervisedorsemi-supervisedlearning.1.2.3IncreasingModelSizesAnotherkeyreasonthatneuralnetworksarewildlysuccessfultodayafterenjoyingcomparativelylittlesuccesssincethe1980sisthatwehavethecomputationalresourcestorunmuchlargermodelstoday.Oneofthemaininsightsofconnection-ismisthatanimalsbecomeintelligentwhenmanyoftheirneuronsworktogether.Anindividualneuronorsmallcollectionofneuronsisnotparticularlyuseful.Biologicalneuronsarenotespeciallydenselyconnected.Asseeninﬁgure,1.10ourmachinelearningmodelshavehadanumberofconnectionsperneuronthatwaswithinanorderofmagnitudeofevenmammalianbrainsfordecades.Intermsofthetotalnumberofneurons,neuralnetworkshavebeenastonishinglysmalluntilquiterecently,asshowninﬁgure.Sincetheintroductionofhidden1.11units,artiﬁcialneuralnetworkshavedoubledinsizeroughlyevery2.4years.Thisgrowthisdrivenbyfastercomputerswithlargermemoryandbytheavailabilityoflargerdatasets.Largernetworksareabletoachievehigheraccuracyonmorecomplextasks.Thistrendlookssettocontinuefordecades.Unlessnewtechnologiesallowfasterscaling,artiﬁcialneuralnetworkswillnothavethesamenumberofneuronsasthehumanbrainuntilatleastthe2050s.Biologicalneuronsmayrepresentmorecomplicatedfunctionsthancurrentartiﬁcialneurons,sobiologicalneuralnetworksmaybeevenlargerthanthisplotportrays.Inretrospect,itisnotparticularlysurprisingthatneuralnetworkswithfewerneuronsthanaleechwereunabletosolvesophisticatedartiﬁcialintelligenceprob-lems.Eventoday’snetworks,whichweconsiderquitelargefromacomputationalsystemspointofview,aresmallerthanthenervoussystemofevenrelativelyprimitivevertebrateanimalslikefrogs.Theincreaseinmodelsizeovertime,duetotheavailabilityoffasterCPUs,20 CHAPTER1.INTRODUCTION 19001950198520002015Y100101102103104105106107108109Datasetsize(numberexamples)IrisMNISTPublicSVHNImageNetCIFAR-10ImageNet10kILSVRC 2014Sports-1M RotatedTvs.CTvs.Gvs.FCriminalsCanadianHansardWMT Figure1.8:Datasetsizeshaveincreasedgreatlyovertime.Intheearly1900s,statisticiansstudieddatasetsusinghundredsorthousandsofmanuallycompiledmeasurements(,Garson1900Gosset1908Anderson1935Fisher1936;,;,;,).Inthe1950sthrough1980s,thepioneersofbiologicallyinspiredmachinelearningoftenworkedwithsmall,syntheticdatasets,suchaslow-resolutionbitmapsofletters,thatweredesignedtoincurlowcomputationalcostanddemonstratethatneuralnetworkswereabletolearnspeciﬁckindsoffunctions(WidrowandHoﬀ1960Rumelhart1986b,;etal.,).Inthe1980sand1990s,machinelearningbecamemorestatisticalinnatureandbegantoleveragelargerdatasetscontainingtensofthousandsofexamplessuchastheMNISTdataset(showninﬁgure)ofscans1.9ofhandwrittennumbers(,).Intheﬁrstdecadeofthe2000s,moreLeCunetal.1998bsophisticateddatasetsofthissamesize,suchastheCIFAR-10dataset(KrizhevskyandHinton2009,)continuedtobeproduced.Towardtheendofthatdecadeandthroughouttheﬁrsthalfofthe2010s,signiﬁcantlylargerdatasets,containinghundredsofthousandstotensofmillionsofexamples,completelychangedwhatwaspossiblewithdeeplearning.ThesedatasetsincludedthepublicStreetViewHouseNumbersdataset(,Netzeretal.2011),variousversionsoftheImageNetdataset(,,;Dengetal.20092010aRussakovskyetal.etal.,),andtheSports-1Mdataset(2014aKarpathy,).Atthetopofthe2014graph,weseethatdatasetsoftranslatedsentences,suchasIBM’sdatasetconstructedfromtheCanadianHansard(,)andtheWMT2014EnglishtoFrenchBrownetal.1990dataset(Schwenk2014,)aretypicallyfaraheadofotherdatasetsizes.21 CHAPTER1.INTRODUCTION Figure1.9:ExampleinputsfromtheMNISTdataset.The“NIST”standsforNationalInstituteofStandardsandTechnology,theagencythatoriginallycollectedthisdata.The“M”standsfor“modiﬁed,”sincethedatahasbeenpreprocessedforeasierusewithmachinelearningalgorithms.TheMNISTdatasetconsistsofscansofhandwrittendigitsandassociatedlabelsdescribingwhichdigit0–9iscontainedineachimage.Thissimpleclassiﬁcationproblemisoneofthesimplestandmostwidelyusedtestsindeeplearningresearch.Itremainspopulardespitebeingquiteeasyformoderntechniquestosolve.GeoﬀreyHintonhasdescribeditas“thedrosophilaofmachinelearning,”meaningthatitallowsmachinelearningresearcherstostudytheiralgorithmsincontrolledlaboratoryconditions,muchasbiologistsoftenstudyfruitﬂies.22 CHAPTER1.INTRODUCTIONtheadventofgeneralpurposeGPUs(describedinsection),fasternetwork12.1.2connectivityandbettersoftwareinfrastructurefordistributedcomputing,isoneofthemostimportanttrendsinthehistoryofdeeplearning.Thistrendisgenerallyexpectedtocontinuewellintothefuture.1.2.4IncreasingAccuracy,ComplexityandReal-WorldImpactSincethe1980s,deeplearninghasconsistentlyimprovedinitsabilitytoprovideaccuraterecognitionorprediction.Moreover,deeplearninghasconsistentlybeenappliedwithsuccesstobroaderandbroadersetsofapplications.Theearliestdeepmodelswereusedtorecognizeindividualobjectsintightlycropped,extremelysmallimages(,).SincethentherehasRumelhartetal.1986abeenagradualincreaseinthesizeofimagesneuralnetworkscouldprocess.Modernobjectrecognitionnetworksprocessrichhigh-resolutionphotographsanddonothavearequirementthatthephotobecroppedneartheobjecttoberecognized(,).Similarly,theearliestnetworkscouldonlyrecognizeKrizhevskyetal.2012twokindsofobjects(orinsomecases,theabsenceorpresenceofasinglekindofobject),whilethesemodernnetworkstypicallyrecognizeatleast1,000diﬀerentcategoriesofobjects. ThelargestcontestinobjectrecognitionistheImageNetLargeScaleVisualRecognitionChallenge(ILSVRC)heldeachyear.Adramaticmomentinthemeteoricriseofdeeplearningcamewhenaconvolutionalnetworkwonthischallengefortheﬁrsttimeandbyawidemargin,bringingdownthestate-of-the-arttop-5errorratefrom26.1%to15.3%(,),Krizhevskyetal.2012meaningthattheconvolutionalnetworkproducesarankedlistofpossiblecategoriesforeachimageandthecorrectcategoryappearedintheﬁrstﬁveentriesofthislistforallbut15.3%ofthetestexamples.Sincethen,thesecompetitionsareconsistentlywonbydeepconvolutionalnets,andasofthiswriting,advancesindeeplearninghavebroughtthelatesttop-5errorrateinthiscontestdownto3.6%,asshowninﬁgure.1.12Deeplearninghasalsohadadramaticimpactonspeechrecognition.Afterimprovingthroughoutthe1990s,theerrorratesforspeechrecognitionstagnatedstartinginabout2000.Theintroductionofdeeplearning(,;Dahletal.2010Dengetal.etal.etal.,;2010bSeide,;2011Hinton,)tospeechrecognitionresulted2012ainasuddendropoferrorrates,withsomeerrorratescutinhalf.Wewillexplorethishistoryinmoredetailinsection.12.3Deepnetworkshavealsohadspectacularsuccessesforpedestriandetectionandimagesegmentation(,;Sermanetetal.2013Farabet2013Couprieetal.,;etal.,2013)andyieldedsuperhumanperformanceintraﬃcsignclassiﬁcation(Ciresan23 CHAPTER1.INTRODUCTION 1950198520002015101102103104Connectionsperneuron12345678910FruitﬂyMouseCatHuman Figure1.10:Initially,thenumberofconnectionsbetweenneuronsinartiﬁcialneuralnetworkswaslimitedbyhardwarecapabilities.Today,thenumberofconnectionsbetweenneuronsismostlyadesignconsideration.Someartiﬁcialneuralnetworkshavenearlyasmanyconnectionsperneuronasacat,anditisquitecommonforotherneuralnetworkstohaveasmanyconnectionsperneuronassmallermammalslikemice.Eventhehumanbraindoesnothaveanexorbitantamountofconnectionsperneuron.Biologicalneuralnetworksizesfrom().Wikipedia20151.Adaptivelinearelement(,)WidrowandHoﬀ19602.Neocognitron(Fukushima1980,)3.GPU-acceleratedconvolutionalnetwork(,)Chellapillaetal.20064.DeepBoltzmannmachine(SalakhutdinovandHinton2009a,)5.Unsupervisedconvolutionalnetwork(,)Jarrettetal.20096.GPU-acceleratedmultilayerperceptron(,)Ciresanetal.20107.Distributedautoencoder(,)Leetal.20128.Multi-GPUconvolutionalnetwork(,)Krizhevskyetal.20129.COTSHPCunsupervisedconvolutionalnetwork(,)Coatesetal.201310.GoogLeNet(,)Szegedyetal.2014a 24 CHAPTER1.INTRODUCTIONetal.,).2012Atthesametimethatthescaleandaccuracyofdeepnetworkshasincreased,sohasthecomplexityofthetasksthattheycansolve.()Goodfellowetal.2014dshowedthatneuralnetworkscouldlearntooutputanentiresequenceofcharacterstranscribedfromanimage,ratherthanjustidentifyingasingleobject.Previously,itwaswidelybelievedthatthiskindoflearningrequiredlabelingoftheindividualelementsofthesequence(,).Recurrentneuralnetworks,GülçehreandBengio2013suchastheLSTMsequencemodelmentionedabove,arenowusedtomodelrelationshipsbetweensequencessequencesandotherratherthanjustﬁxedinputs.Thissequence-to-sequencelearningseemstobeonthecuspofrevolutionizinganotherapplication:machinetranslation(Sutskever2014Bahdanauetal.,;etal.,2015).ThistrendofincreasingcomplexityhasbeenpushedtoitslogicalconclusionwiththeintroductionofneuralTuringmachines(Graves2014aetal.,)thatlearntoreadfrommemorycellsandwritearbitrarycontenttomemorycells.Suchneuralnetworkscanlearnsimpleprogramsfromexamplesofdesiredbehavior.Forexample,theycanlearntosortlistsofnumbersgivenexamplesofscrambledandsortedsequences.Thisself-programmingtechnologyisinitsinfancy,butinthefuturecouldinprinciplebeappliedtonearlyanytask.Anothercrowningachievementofdeeplearningisitsextensiontothedomainofreinforcementlearning.Inthecontextofreinforcementlearning,anautonomousagentmustlearntoperformataskbytrialanderror,withoutanyguidancefromthehumanoperator.DeepMinddemonstratedthatareinforcementlearningsystembasedondeeplearningiscapableoflearningtoplayAtarivideogames,reachinghuman-levelperformanceonmanytasks(,).DeeplearninghasMnihetal.2015alsosigniﬁcantlyimprovedtheperformanceofreinforcementlearningforrobotics(,).Finnetal.2015Manyoftheseapplicationsofdeeplearningarehighlyproﬁtable.Deeplearningisnowused bymanytoptechnologycompanies includingGoogle, Microsoft,Facebook,IBM,Baidu,Apple,Adobe,Netﬂix,NVIDIAandNEC.Advancesindeeplearninghavealsodependedheavilyonadvancesinsoftwareinfrastructure.SoftwarelibrariessuchasTheano(,;Bergstraetal.2010Bastienetal.etal.,),PyLearn2(2012Goodfellow,),Torch(,),2013cCollobertetal.2011bDistBelief(,),Caﬀe(,),MXNet(,),andDeanetal.2012Jia2013Chenetal.2015TensorFlow(,)haveallsupportedimportantresearchprojectsorAbadietal.2015commercialproducts.Deeplearninghasalsomadecontributionsbacktoothersciences.Modernconvolutionalnetworksforobjectrecognitionprovideamodelofvisualprocessing25 CHAPTER1.INTRODUCTIONthatneuroscientistscanstudy(,).DeeplearningalsoprovidesusefulDiCarlo2013toolsforprocessingmassiveamountsofdataandmakingusefulpredictionsinscientiﬁcﬁelds.Ithasbeensuccessfullyusedtopredicthowmoleculeswillinteractinordertohelppharmaceuticalcompaniesdesignnewdrugs(,),Dahletal.2014tosearchforsubatomicparticles(,),andtoautomaticallyparseBaldietal.2014microscopeimagesusedtoconstructa3-Dmapofthehumanbrain(Knowles-Barley2014etal.,).Weexpectdeeplearningtoappearinmoreandmorescientiﬁcﬁeldsinthefuture.Insummary,deeplearningisanapproachtomachinelearningthathasdrawnheavilyonourknowledgeofthehumanbrain,statisticsandappliedmathasitdevelopedoverthepastseveraldecades.Inrecentyears,ithasseentremendousgrowthinitspopularityandusefulness,dueinlargeparttomorepowerfulcom-puters,largerdatasetsandtechniquestotraindeepernetworks.Theyearsaheadarefullofchallengesandopportunitiestoimprovedeeplearningevenfurtherandbringittonewfrontiers. 26 CHAPTER1.INTRODUCTION 1950198520002015205610−210−110010110210310410510610710810910101011Numberofneurons(logarithmicscale) 1234567891011121314151617181920 SpongeRoundwormLeechAntBeeFrogOctopusHuman Figure1.11:Sincetheintroductionofhiddenunits,artiﬁcialneuralnetworkshavedoubledinsizeroughlyevery2.4years.Biologicalneuralnetworksizesfrom().Wikipedia20151.Perceptron(,,)Rosenblatt195819622.Adaptivelinearelement(,)WidrowandHoﬀ19603.Neocognitron(Fukushima1980,)4.Earlyback-propagationnetwork(,)Rumelhartetal.1986b5.Recurrentneuralnetworkforspeechrecognition(RobinsonandFallside1991,)6.Multilayerperceptronforspeechrecognition(,)Bengioetal.19917.Meanﬁeldsigmoidbeliefnetwork(,)Sauletal.19968.LeNet-5(,)LeCunetal.1998b9.Echostatenetwork(,)JaegerandHaas200410.Deepbeliefnetwork(,)Hintonetal.200611.GPU-acceleratedconvolutionalnetwork(,)Chellapillaetal.200612.DeepBoltzmannmachine(SalakhutdinovandHinton2009a,)13.GPU-accelerateddeepbeliefnetwork(,)Rainaetal.200914.Unsupervisedconvolutionalnetwork(,)Jarrettetal.200915.GPU-acceleratedmultilayerperceptron(,)Ciresanetal.201016.OMP-1network(,)CoatesandNg201117.Distributedautoencoder(,)Leetal.201218.Multi-GPUconvolutionalnetwork(,)Krizhevskyetal.201219.COTSHPCunsupervisedconvolutionalnetwork(,)Coatesetal.201320.GoogLeNet(,)Szegedyetal.2014a 27 CHAPTER1.INTRODUCTION 201020112012201320142015000.005.010.015.020.025.030.ILSVRC classiﬁcationerrorrate Figure1.12:SincedeepnetworksreachedthescalenecessarytocompeteintheImageNetLargeScaleVisualRecognitionChallenge,theyhaveconsistentlywonthecompetitioneveryyear,andyieldedlowerandlowererrorrateseachtime. DatafromRussakovskyetal.etal.()and2014bHe().2015 28 PartIAppliedMathandMachineLearningBasics 29 Thispartofthebookintroducesthebasicmathematicalconceptsneededtounderstanddeeplearning.Webeginwithgeneralideasfromappliedmaththatallowustodeﬁnefunctionsofmanyvariables,ﬁndthehighestandlowestpointsonthesefunctionsandquantifydegreesofbelief.Next,wedescribethefundamentalgoalsofmachinelearning.Wedescribehowtoaccomplishthesegoalsbyspecifyingamodelthatrepresentscertainbeliefs,designingacostfunctionthatmeasureshowwellthosebeliefscorrespondwithrealityandusingatrainingalgorithmtominimizethatcostfunction.Thiselementaryframeworkisthebasisforabroadvarietyofmachinelearningalgorithms,includingapproachestomachinelearningthatarenotdeep. Inthesubsequentpartsofthebook,wedevelopdeeplearningalgorithmswithinthisframework. 30 Chapter2LinearAlgebraLinearalgebraisabranchofmathematicsthatiswidelyusedthroughoutscienceandengineering.However,becauselinearalgebraisaformofcontinuousratherthandiscretemathematics,manycomputerscientistshavelittleexperiencewithit.Agoodunderstandingoflinearalgebraisessentialforunderstandingandworkingwithmanymachinelearningalgorithms,especiallydeeplearningalgorithms.Wethereforeprecedeourintroductiontodeeplearningwithafocusedpresentationofthekeylinearalgebraprerequisites.Ifyouarealreadyfamiliarwithlinearalgebra,feelfreetoskipthischapter.Ifyouhavepreviousexperiencewiththeseconceptsbutneedadetailedreferencesheettoreviewkeyformulas,werecommendTheMatrixCookbook(PetersenandPedersen2006,).Ifyouhavenoexposureatalltolinearalgebra,thischapterwillteachyouenoughtoreadthisbook,butwehighlyrecommendthatyoualsoconsultanotherresourcefocusedexclusivelyonteachinglinearalgebra,suchasShilov1977().Thischapterwillcompletelyomitmanyimportantlinearalgebratopicsthatarenotessentialforunderstandingdeeplearning.2.1Scalars,Vectors,MatricesandTensorsThestudyoflinearalgebrainvolvesseveraltypesofmathematicalobjects:•Scalars:Ascalarisjustasinglenumber,incontrasttomostoftheotherobjectsstudiedinlinearalgebra,whichareusuallyarraysofmultiplenumbers.Wewritescalarsinitalics.Weusuallygivescalarslower-casevariablenames.Whenweintroducethem,wespecifywhatkindofnumbertheyare.For31 CHAPTER2.LINEARALGEBRAexample,wemightsay“Lets∈Rbetheslopeoftheline,”whiledeﬁningareal-valuedscalar,or“Letn∈Nbethenumberofunits,”whiledeﬁninganaturalnumberscalar.•Vectors: Avectorisanarrayofnumbers.Thenumbersarearrangedinorder.Wecanidentifyeachindividualnumberbyitsindexinthatordering.Typicallywegivevectorslowercasenameswritteninboldtypeface,suchasx.Theelementsofthevectorareidentiﬁedbywritingitsnameinitalictypeface,withasubscript.Theﬁrstelementofxisx1,thesecondelementisx2andsoon.Wealsoneedtosaywhatkindofnumbersarestoredinthevector.IfeachelementisinR,andthevectorhasnelements,thenthevectorliesinthesetformedbytakingtheCartesianproductofRntimes,denotedasRn.Whenweneedtoexplicitlyidentifytheelementsofavector,wewritethemasacolumnenclosedinsquarebrackets:x=x1x2...xn.(2.1)Wecanthinkofvectorsasidentifyingpointsinspace,witheachelementgivingthecoordinatealongadiﬀerentaxis.Sometimesweneedtoindexasetofelementsofavector.Inthiscase,wedeﬁneasetcontainingtheindicesandwritethesetasasubscript.Forexample,toaccessx1,x3andx6,wedeﬁnethesetS={1,3,6}andwritexS.Weusethe−signtoindexthecomplementofaset.Forexamplex−1isthevectorcontainingallelementsofxexceptforx1,andx−Sisthevectorcontainingalloftheelementsofexceptforxx1,x3andx6.•Matrices:Amatrixisa2-Darrayofnumbers,soeachelementisidentiﬁedbytwoindicesinsteadofjustone.Weusuallygivematricesupper-casevariablenameswithboldtypeface,suchasA.Ifareal-valuedmatrixAhasaheightofmandawidthofn,thenwesaythatA∈Rmn×. Weusuallyidentifytheelementsofamatrixusingitsnameinitalicbutnotboldfont,andtheindicesarelistedwithseparatingcommas.Forexample,A11,istheupperleftentryofAandAm,nisthebottomrightentry.Wecanidentifyallofthenumberswithverticalcoordinateibywritinga“”forthehorizontal:coordinate.Forexample,Ai,:denotesthehorizontalcrosssectionofAwithverticalcoordinatei.Thisisknownasthei-throwofA.Likewise,A:,iis32 CHAPTER2.LINEARALGEBRA A=A11,A12,A21,A22,A31,A32,⇒A=A11,A21,A31,A12,A22,A32,Figure2.1:Thetransposeofthematrixcanbethoughtofasamirrorimageacrossthemaindiagonal.the-thof.WhenweneedtoexplicitlyidentifytheelementsoficolumnAamatrix,wewritethemasanarrayenclosedinsquarebrackets:A11,A12,A21,A22,.(2.2)Sometimeswemayneedtoindexmatrix-valuedexpressionsthatarenotjustasingleletter.Inthiscase,weusesubscriptsaftertheexpression,butdonotconvertanythingtolowercase.Forexample,f(A)i,jgiveselement(i,j)ofthematrixcomputedbyapplyingthefunctionto.fA•Tensors:Insomecaseswewillneedanarraywithmorethantwoaxes.Inthegeneralcase,anarrayofnumbersarrangedonaregulargridwithavariablenumberofaxesisknownasatensor.Wedenoteatensornamed“A”withthistypeface:A.WeidentifytheelementofAatcoordinates(i,j,k)bywritingAi,j,k.Oneimportantoperationonmatricesisthetranspose. Thetransposeofamatrixisthemirrorimageofthematrixacrossadiagonalline,calledthemaindiagonal,runningdownandtotheright,startingfromitsupperleftcorner.Seeﬁgureforagraphicaldepictionofthisoperation.Wedenotethetransposeofa2.1matrixasAA,anditisdeﬁnedsuchthat(A)i,j= Aj,i.(2.3)Vectorscanbethoughtofasmatricesthatcontainonlyonecolumn.Thetransposeofavectoristhereforeamatrixwithonlyonerow.Sometimeswe33 CHAPTER2.LINEARALGEBRAdeﬁneavectorbywritingoutitselementsinthetextinlineasarowmatrix,thenusingthetransposeoperatortoturnitintoastandardcolumnvector,e.g.,x= [x1,x2,x3].Ascalarcanbethoughtofasamatrixwithonlyasingleentry.Fromthis,wecanseethatascalarisitsowntranspose:aa= .Wecanaddmatricestoeachother,aslongastheyhavethesameshape,justbyaddingtheircorrespondingelements:whereCAB= +Ci,j= Ai,j+Bi,j.Wecanalsoaddascalartoamatrixormultiplyamatrixbyascalar,justbyperformingthatoperationoneachelementofamatrix:D=a·B+cwhereDi,j= aB·i,j+c.Inthecontextofdeeplearning,wealsousesomelessconventionalnotation.Weallowtheadditionofmatrixandavector,yieldinganothermatrix:C=A+b,whereCi,j=Ai,j+bj.Inotherwords,thevectorbisaddedtoeachrowofthematrix.Thisshorthandeliminatestheneedtodeﬁneamatrixwithbcopiedintoeachrowbeforedoingtheaddition.Thisimplicitcopyingofbtomanylocationsiscalled.broadcasting2.2MultiplyingMatricesandVectorsOneofthemostimportantoperationsinvolvingmatricesismultiplicationoftwomatrices.ThematrixproductofmatricesAandBisathirdmatrixC.Inorderforthisproducttobedeﬁned,AmusthavethesamenumberofcolumnsasBhasrows.IfAisofshapemn×andBisofshapenp×,thenCisofshapemp×.Wecanwritethematrixproductjustbyplacingtwoormorematricestogether,e.g.CAB= .(2.4)TheproductoperationisdeﬁnedbyCi,j=kAi,kBk,j.(2.5)Notethatthestandardproductoftwomatricesisjustamatrixcontainingnottheproductoftheindividualelements.Suchanoperationexistsandiscalledtheelement-wiseproductHadamardproductor,andisdenotedas.ABThedotproductbetweentwovectorsxandyofthesamedimensionalityisthematrixproductxy.WecanthinkofthematrixproductC=ABascomputingCi,jasthedotproductbetweenrowofandcolumnof.iAjB34 CHAPTER2.LINEARALGEBRAMatrixproductoperationshavemanyusefulpropertiesthatmakemathematicalanalysis ofmatrices moreconvenient.For example, matrix multiplication isdistributive:ABCABAC(+) = +.(2.6)Itisalsoassociative:ABCABC() = ().(2.7)Matrixmultiplicationiscommutative(theconditionnotAB=BAdoesnotalwayshold),unlikescalarmultiplication.However,thedotproductbetweentwovectorsiscommutative:xyy= x.(2.8)Thetransposeofamatrixproducthasasimpleform:()AB= BA.(2.9)Thisallowsustodemonstrateequation,byexploitingthefactthatthevalue2.8ofsuchaproductisascalarandthereforeequaltoitsowntranspose:xy=xy= yx.(2.10)Sincethefocusofthistextbookisnotlinearalgebra,wedonotattempttodevelopacomprehensivelistofusefulpropertiesofthematrixproducthere,butthereadershouldbeawarethatmanymoreexist.Wenowknowenoughlinearalgebranotationtowritedownasystemoflinearequations:Axb= (2.11)whereA∈Rmn×isaknownmatrix,b∈Rmisaknownvector,andx∈Rnisavectorofunknownvariableswewouldliketosolvefor.Eachelementxiofxisoneoftheseunknownvariables.EachrowofAandeachelementofbprovideanotherconstraint.Wecanrewriteequationas:2.11A1:,x= b1(2.12)A2:,x= b2(2.13)...(2.14)Am,:x= bm(2.15)or,evenmoreexplicitly,as:A11,x1+A12,x2++···A1,nxn= b1(2.16)35 CHAPTER2.LINEARALGEBRA100010001Figure2.2:Exampleidentitymatrix:ThisisI3.A21,x1+A22,x2++···A2,nxn= b2(2.17)...(2.18)Am,1x1+Am,2x2++···Am,nxn= bm.(2.19)Matrix-vectorproductnotationprovidesamorecompactrepresentationforequationsofthisform.2.3IdentityandInverseMatricesLinearalgebraoﬀersapowerfultoolcalledmatrixinversionthatallowsustoanalyticallysolveequationformanyvaluesof.2.11ATodescribematrixinversion,weﬁrstneedtodeﬁnetheconceptofanidentitymatrix.Anidentitymatrixisamatrixthatdoesnotchangeanyvectorwhenwemultiplythatvectorbythatmatrix.Wedenotetheidentitymatrixthatpreservesn-dimensionalvectorsasIn.Formally,In∈Rnn×,and∀∈xRn,Inxx= .(2.20)Thestructureoftheidentitymatrixissimple:alloftheentriesalongthemaindiagonalare1,whilealloftheotherentriesarezero.Seeﬁgureforanexample.2.2ThematrixinverseofAisdenotedasA−1,anditisdeﬁnedasthematrixsuchthatA−1AI= n.(2.21)Wecannowsolveequationbythefollowingsteps:2.11Axb= (2.22)A−1AxA= −1b(2.23)InxA= −1b(2.24)36 CHAPTER2.LINEARALGEBRAxA= −1b.(2.25)Ofcourse,thisprocessdependsonitbeingpossibletoﬁndA−1.WediscusstheconditionsfortheexistenceofA−1inthefollowingsection.WhenA−1exists,severaldiﬀerentalgorithmsexistforﬁndingitinclosedform.Intheory,thesameinversematrixcanthenbeusedtosolvetheequationmanytimesfordiﬀerentvaluesofb.However,A−1isprimarilyusefulasatheoreticaltool,andshouldnotactuallybeusedinpracticeformostsoftwareapplications.BecauseA−1canberepresentedwithonlylimitedprecisiononadigitalcomputer,algorithmsthatmakeuseofthevalueofbcanusuallyobtainmoreaccurateestimatesof.x2.4LinearDependenceandSpanInorderforA−1toexist,equationmusthaveexactlyonesolutionforevery2.11valueofb.However,itisalsopossibleforthesystemofequationstohavenosolutionsorinﬁnitelymanysolutionsforsomevaluesofb. Itisnotpossibletohavemorethanonebutlessthaninﬁnitelymanysolutionsforaparticularb;ifbothandaresolutionsthenxyzxy= α+(1)−α(2.26)isalsoasolutionforanyreal.αToanalyzehowmanysolutionstheequationhas,wecanthinkofthecolumnsofAasspecifyingdiﬀerentdirectionswecantravelfromtheorigin(thepointspeciﬁedbythevectorofallzeros),anddeterminehowmanywaysthereareofreachingb.Inthisview,eachelementofxspeciﬁeshowfarweshouldtravelineachofthesedirections,withxispecifyinghowfartomoveinthedirectionofcolumn:iAx=ixiA:,i.(2.27)Ingeneral,thiskindofoperationiscalledalinearcombination.Formally,alinearcombinationofsomesetofvectors{v(1),...,v()n}isgivenbymultiplyingeachvectorv()ibyacorrespondingscalarcoeﬃcientandaddingtheresults:iciv()i.(2.28)Thespanofasetofvectorsisthesetofallpointsobtainablebylinearcombinationoftheoriginalvectors.37 CHAPTER2.LINEARALGEBRADeterminingwhetherAx=bhasasolutionthusamountstotestingwhetherbisinthespanofthecolumnsofA.Thisparticularspanisknownasthecolumnspacerangeortheof.AInorderforthesystemAx=btohaveasolutionforallvaluesofb∈Rm,wethereforerequirethatthecolumnspaceofAbeallofRm.IfanypointinRmisexcludedfromthecolumnspace,thatpointisapotentialvalueofbthathasnosolution.TherequirementthatthecolumnspaceofAbeallofRmimpliesimmediatelythatAmusthaveatleastmcolumns,i.e.,nm≥. Otherwise,thedimensionalityofthecolumnspacewouldbelessthanm.Forexample,considera3×2matrix.Thetargetbis3-D,butxisonly2-D,somodifyingthevalueofxatbestallowsustotraceouta2-DplanewithinR3.Theequationhasasolutionifandonlyifliesonthatplane.bHavingnm≥isonlyanecessaryconditionforeverypointtohaveasolution.Itisnotasuﬃcientcondition,becauseitispossibleforsomeofthecolumnstoberedundant.Considera2×2matrixwherebothofthecolumnsareidentical.Thishasthesamecolumnspaceasa2×1matrixcontainingonlyonecopyofthereplicatedcolumn.Inotherwords,thecolumnspaceisstilljustaline,andfailstoencompassallofR2,eventhoughtherearetwocolumns.Formally,thiskindofredundancyisknownaslineardependence.Asetofvectorsislinearlyindependentifnovectorinthesetisalinearcombinationoftheothervectors. Ifweaddavectortoasetthatisalinearcombinationoftheothervectorsintheset,thenewvectordoesnotaddanypointstotheset’sspan.ThismeansthatforthecolumnspaceofthematrixtoencompassallofRm,thematrixmustcontainatleastonesetofmlinearlyindependentcolumns.Thisconditionisbothnecessaryandsuﬃcientforequationtohaveasolutionfor2.11everyvalueofb.Notethattherequirementisforasettohaveexactlymlinearindependentcolumns,notatleastm.Nosetofm-dimensionalvectorscanhavemorethanmmutuallylinearlyindependentcolumns,butamatrixwithmorethanmcolumnsmayhavemorethanonesuchset.Inorderforthematrixtohaveaninverse,weadditionallyneedtoensurethatequationhasonesolutionforeachvalueof2.11atmostb.Todoso,weneedtoensurethatthematrixhasatmostmcolumns.Otherwisethereismorethanonewayofparametrizingeachsolution.Together,thismeansthatthematrixmustbesquare,thatis,werequirethatm=nandthatallofthecolumnsmustbelinearlyindependent.Asquarematrixwithlinearlydependentcolumnsisknownas.singularIfAisnotsquareorissquarebutsingular,itcanstillbepossibletosolvetheequation.However,wecannotusethemethodofmatrixinversiontoﬁndthe38 CHAPTER2.LINEARALGEBRAsolution.Sofarwehavediscussedmatrixinversesasbeingmultipliedontheleft.Itisalsopossibletodeﬁneaninversethatismultipliedontheright:AA−1= I.(2.29)Forsquarematrices,theleftinverseandrightinverseareequal.2.5NormsSometimesweneedtomeasurethesizeofavector.Inmachinelearning,weusuallymeasurethesizeofvectorsusingafunctioncalledanorm.Formally,theLpnormisgivenby||||xp=i|xi|p1p(2.30)forp,p.∈R≥1Norms,includingtheLpnorm,arefunctionsmappingvectorstonon-negativevalues.Onanintuitivelevel,thenormofavectorxmeasuresthedistancefromtheorigintothepointx.Morerigorously,anormisanyfunctionfthatsatisﬁesthefollowingproperties:•⇒f() = 0 xx= 0•≤f(+) xyff()+x()y(thetriangleinequality)•∀∈||αR,fα(x) = αf()xTheL2norm,withp= 2,isknownastheEuclideannorm.ItissimplytheEuclideandistancefromtheorigintothepointidentiﬁedbyx.TheL2normisusedsofrequentlyinmachinelearningthatitisoftendenotedsimplyas||||x,withthesubscriptomitted.Itisalsocommontomeasurethesizeofavectorusing2thesquaredL2norm,whichcanbecalculatedsimplyasxx.ThesquaredL2normismoreconvenienttoworkwithmathematicallyandcomputationallythantheL2normitself.Forexample,thederivativesofthesquaredL2normwithrespecttoeachelementofxeachdependonlyonthecorrespondingelementofx,whileallofthederivativesoftheL2normdependontheentirevector.Inmanycontexts,thesquaredL2normmaybeundesirablebecauseitincreasesveryslowlyneartheorigin.Inseveralmachinelearning39 CHAPTER2.LINEARALGEBRAapplications,itisimportanttodiscriminatebetweenelementsthatareexactlyzeroandelementsthataresmallbutnonzero.Inthesecases,weturntoafunctionthatgrowsatthesamerateinalllocations,butretainsmathematicalsimplicity:theL1norm.TheL1normmaybesimpliﬁedto||||x1=i|xi|.(2.31)TheL1normiscommonlyusedinmachinelearningwhenthediﬀerencebetweenzeroandnonzeroelementsisveryimportant.Everytimeanelementofxmovesawayfrom0by,theL1normincreasesby.Wesometimesmeasurethesizeofthevectorbycountingitsnumberofnonzeroelements.Someauthorsrefertothisfunctionasthe“L0norm,”butthisisincorrectterminology.Thenumberofnon-zeroentriesinavectorisnotanorm,becausescalingthevectorbyαdoesnotchangethenumberofnonzeroentries. TheL1normisoftenusedasasubstituteforthenumberofnonzeroentries.OneothernormthatcommonlyarisesinmachinelearningistheL∞norm,alsoknownasthemaxnorm.Thisnormsimpliﬁestotheabsolutevalueoftheelementwiththelargestmagnitudeinthevector,||||x∞= maxi|xi|.(2.32)Sometimeswemayalsowishtomeasurethesizeofamatrix.Inthecontextofdeeplearning,themostcommonwaytodothisiswiththeotherwiseobscureFrobeniusnorm:||||AF=i,jA2i,j,(2.33)whichisanalogoustotheL2normofavector.Thedotproductoftwovectorscanberewrittenintermsofnorms.Speciﬁcally,xyx= ||||2||||y2cosθ(2.34)whereistheanglebetweenand.θxy2.6SpecialKindsofMatricesandVectorsSomespecialkindsofmatricesandvectorsareparticularlyuseful.Diagonalmatricesconsistmostlyofzerosandhavenon-zeroentriesonlyalongthemaindiagonal. Formally,amatrixDisdiagonalifandonlyifDi,j=0for40 CHAPTER2.LINEARALGEBRAalli=j. Wehavealreadyseenoneexampleofadiagonalmatrix: theidentitymatrix,whereallofthediagonalentriesare1.Wewritediag(v) todenoteasquarediagonalmatrixwhosediagonalentriesaregivenbytheentriesofthevectorv.Diagonalmatricesareofinterestinpartbecausemultiplyingbyadiagonalmatrixisverycomputationallyeﬃcient.Tocomputediag(v)x,weonlyneedtoscaleeachelementxibyvi.Inotherwords,diag(v)x=vx.Invertingasquarediagonalmatrixisalsoeﬃcient.Theinverseexistsonlyifeverydiagonalentryisnonzero,andinthatcase,diag(v)−1=diag([1/v1,...,1/vn]).Inmanycases,wemayderivesomeverygeneralmachinelearningalgorithmintermsofarbitrarymatrices,butobtainalessexpensive(andlessdescriptive)algorithmbyrestrictingsomematricestobediagonal.Notalldiagonalmatricesneedbesquare.Itispossibletoconstructarectangulardiagonalmatrix.Non-squarediagonalmatricesdonothaveinversesbutitisstillpossibletomultiplybythemcheaply.Foranon-squarediagonalmatrixD,theproductDxwillinvolvescalingeachelementofx,andeitherconcatenatingsomezerostotheresultifDistallerthanitiswide,ordiscardingsomeofthelastelementsofthevectorifiswiderthanitistall.DAmatrixisanymatrixthatisequaltoitsowntranspose:symmetricAA= .(2.35)Symmetricmatricesoftenarisewhentheentriesaregeneratedbysomefunctionoftwoargumentsthatdoesnotdependontheorderofthearguments.Forexample,ifAisamatrixofdistancemeasurements,withAi,jgivingthedistancefrompointitopoint,thenjAi,j= Aj,ibecausedistancefunctionsaresymmetric.Aisavectorwith:unitvectorunitnorm||||x2= 1.(2.36)Avectorxandavectoryareorthogonaltoeachotherifxy= 0.Ifbothvectorshavenonzeronorm,thismeansthattheyareata90degreeangletoeachother.InRn,atmostnvectorsmaybemutuallyorthogonalwithnonzeronorm.Ifthevectorsarenotonlyorthogonalbutalsohaveunitnorm,wecallthemorthonormal.Anorthogonalmatrixisasquarematrixwhoserowsaremutuallyorthonor-malandwhosecolumnsaremutuallyorthonormal:AAAA= = I.(2.37)41 CHAPTER2.LINEARALGEBRAThisimpliesthatA−1= A,(2.38)soorthogonalmatricesareofinterestbecausetheirinverseisverycheaptocompute.Paycarefulattentiontothedeﬁnitionoforthogonalmatrices.Counterintuitively,theirrowsarenotmerelyorthogonalbutfullyorthonormal.Thereisnospecialtermforamatrixwhoserowsorcolumnsareorthogonalbutnotorthonormal.2.7EigendecompositionManymathematicalobjectscanbeunderstoodbetterbybreakingthemintoconstituentparts,orﬁndingsomepropertiesofthemthatareuniversal,notcausedbythewaywechoosetorepresentthem.Forexample,integerscanbedecomposedintoprimefactors.Thewaywerepresentthenumberwillchangedependingonwhetherwewriteitinbaseten12orinbinary,butitwillalwaysbetruethat12 = 2×2×3.Fromthisrepresentationwecanconcludeusefulproperties,suchasthatisnotdivisibleby,orthatany125integermultipleofwillbedivisibleby.123Muchaswecandiscoversomethingaboutthetruenatureofanintegerbydecomposingitintoprimefactors,wecanalsodecomposematricesinwaysthatshowusinformationabouttheirfunctionalpropertiesthatisnotobviousfromtherepresentationofthematrixasanarrayofelements.Oneofthemostwidelyusedkindsofmatrixdecompositioniscalledeigen-decomposition,inwhichwedecomposeamatrixintoasetofeigenvectorsandeigenvalues.AneigenvectorofasquarematrixAisanon-zerovectorvsuchthatmulti-plicationbyaltersonlythescaleof:AvAvv= λ.(2.39)Thescalarλisknownastheeigenvaluecorrespondingtothiseigenvector.(OnecanalsoﬁndalefteigenvectorsuchthatvA=λv, butweareusuallyconcernedwithrighteigenvectors).IfvisaneigenvectorofA,thensoisanyrescaledvectorsvfors,s∈R= 0.Moreover,svstillhasthesameeigenvalue.Forthisreason,weusuallyonlylookforuniteigenvectors.SupposethatamatrixAhasnlinearlyindependenteigenvectors,{v(1),...,v()n},withcorrespondingeigenvalues{λ1,...,λn}.Wemayconcatenateallofthe42 CHAPTER2.LINEARALGEBRA   Figure2.3:Anexampleoftheeﬀectofeigenvectorsandeigenvalues.Here,wehaveamatrixAwithtwoorthonormaleigenvectors,v(1)witheigenvalueλ1andv(2)witheigenvalueλ2.(Left)Weplotthesetofallunitvectorsu∈R2asaunitcircle.(Right)WeplotthesetofallpointsAu.ByobservingthewaythatAdistortstheunitcircle,wecanseethatitscalesspaceindirectionv()ibyλi.eigenvectorstoformamatrixVwithoneeigenvectorpercolumn:V= [v(1),...,v()n].Likewise,wecanconcatenatetheeigenvaluestoformavectorλ= [λ1,...,λn].TheofisthengivenbyeigendecompositionAAVλV= diag()−1.(2.40)Wehaveseenthatconstructingmatriceswithspeciﬁceigenvaluesandeigenvec-torsallowsustostretchspaceindesireddirections. However,weoftenwanttodecomposematricesintotheireigenvaluesandeigenvectors.Doingsocanhelpustoanalyzecertainpropertiesofthematrix,muchasdecomposinganintegerintoitsprimefactorscanhelpusunderstandthebehaviorofthatinteger.Noteverymatrixcanbedecomposedintoeigenvaluesandeigenvectors.Insome43 CHAPTER2.LINEARALGEBRAcases,thedecompositionexists,butmayinvolvecomplexratherthanrealnumbers.Fortunately,inthisbook,weusuallyneedtodecomposeonlyaspeciﬁcclassofmatricesthathaveasimpledecomposition.Speciﬁcally,everyrealsymmetricmatrixcanbedecomposedintoanexpressionusingonlyreal-valuedeigenvectorsandeigenvalues:AQQ= Λ,(2.41)whereQisanorthogonalmatrixcomposedofeigenvectorsofA,andΛisadiagonalmatrix.TheeigenvalueΛi,iisassociatedwiththeeigenvectorincolumniofQ,denotedasQ:,i.BecauseQisanorthogonalmatrix,wecanthinkofAasscalingspacebyλiindirectionv()i.Seeﬁgureforanexample.2.3WhileanyrealsymmetricmatrixAisguaranteedtohaveaneigendecomposi-tion,theeigendecompositionmaynotbeunique.Ifanytwoormoreeigenvectorssharethesameeigenvalue,thenanysetoforthogonalvectorslyingintheirspanarealsoeigenvectorswiththateigenvalue,andwecouldequivalentlychooseaQusingthoseeigenvectorsinstead.Byconvention,weusuallysorttheentriesofΛindescendingorder.Underthisconvention,theeigendecompositionisuniqueonlyifalloftheeigenvaluesareunique.Theeigendecompositionof amatrix tellsus many usefulfactsabout thematrix.Thematrixissingularifandonlyifanyoftheeigenvaluesarezero.Theeigendecompositionofarealsymmetricmatrixcanalsobeusedtooptimizequadraticexpressionsoftheformf(x) =xAxsubjectto||||x2= 1.WheneverxisequaltoaneigenvectorofA,ftakesonthevalueofthecorrespondingeigenvalue.Themaximumvalueoffwithintheconstraintregionisthemaximumeigenvalueanditsminimumvaluewithintheconstraintregionistheminimumeigenvalue.Amatrixwhoseeigenvaluesareallpositiveiscalledpositivedeﬁnite.Amatrixwhoseeigenvaluesareallpositiveorzero-valuediscalledpositivesemideﬁ-nite.Likewise,ifalleigenvaluesarenegative,thematrixisnegativedeﬁnite,andifalleigenvaluesarenegativeorzero-valued,itisnegativesemideﬁnite.Positivesemideﬁnitematricesareinterestingbecausetheyguaranteethat∀xx,Ax≥0.PositivedeﬁnitematricesadditionallyguaranteethatxAxx= 0 ⇒= 0.2.8SingularValueDecompositionInsection,wesawhowtodecomposeamatrixintoeigenvectorsandeigenvalues.2.7Thesingularvaluedecomposition(SVD)providesanotherwaytofactorizeamatrix,intosingularvectorsandsingularvalues.TheSVDallowsustodiscoversomeofthesamekindofinformationastheeigendecomposition.However,44 CHAPTER2.LINEARALGEBRAtheSVDismoregenerallyapplicable.Everyrealmatrixhasasingularvaluedecomposition,butthesameisnottrueoftheeigenvaluedecomposition.Forexample,ifamatrixisnotsquare,theeigendecompositionisnotdeﬁned,andwemustuseasingularvaluedecompositioninstead.RecallthattheeigendecompositioninvolvesanalyzingamatrixAtodiscoveramatrixVofeigenvectorsandavectorofeigenvaluesλsuchthatwecanrewriteAasAVλV= diag()−1.(2.42)Thesingularvaluedecompositionissimilar,exceptthistimewewillwriteAasaproductofthreematrices:AUDV= .(2.43)SupposethatAisanmn×matrix.ThenUisdeﬁnedtobeanmm×matrix,DVtobeanmatrix,andmn×tobeanmatrix.nn×Eachofthesematricesisdeﬁnedtohaveaspecialstructure.ThematricesUandVarebothdeﬁnedtobeorthogonalmatrices.ThematrixDisdeﬁnedtobeadiagonalmatrix.Notethatisnotnecessarilysquare.DTheelementsalongthediagonalofDareknownasthesingularvaluesofthematrixA.ThecolumnsofUareknownastheleft-singularvectors.ThecolumnsofareknownasastheVright-singularvectors.WecanactuallyinterpretthesingularvaluedecompositionofAintermsoftheeigendecompositionoffunctionsofA.Theleft-singularvectorsofAaretheeigenvectorsofAA.Theright-singularvectorsofAaretheeigenvectorsofAA.Thenon-zerosingularvaluesofAarethesquarerootsoftheeigenvaluesofAA.ThesameistrueforAA.PerhapsthemostusefulfeatureoftheSVDisthatwecanuseittopartiallygeneralizematrixinversiontonon-squarematrices,aswewillseeinthenextsection.2.9TheMoore-PenrosePseudoinverseMatrixinversionisnotdeﬁnedformatricesthatarenotsquare.Supposewewanttomakealeft-inverseofamatrix,sothatwecansolvealinearequationBAAxy= (2.44)45 CHAPTER2.LINEARALGEBRAbyleft-multiplyingeachsidetoobtainxBy= .(2.45)Dependingonthestructureoftheproblem,itmaynotbepossibletodesignauniquemappingfromto.ABIfAistallerthanitiswide, thenitispossibleforthisequationtohavenosolution.IfAiswiderthanitistall,thentherecouldbemultiplepossiblesolutions.TheMoore-Penrosepseudoinverseallowsustomakesomeheadwayinthesecases.ThepseudoinverseofisdeﬁnedasamatrixAA+=limα0(AAI+α)−1A.(2.46)Practicalalgorithmsforcomputingthepseudoinversearenotbasedonthisdeﬁni-tion,butrathertheformulaA+= VD+U,(2.47)whereU,DandVarethesingularvaluedecompositionofA,andthepseudoinverseD+ofadiagonalmatrixDisobtainedbytakingthereciprocalofitsnon-zeroelementsthentakingthetransposeoftheresultingmatrix.WhenAhasmorecolumnsthanrows,thensolvingalinearequationusingthepseudoinverseprovidesoneofthemanypossiblesolutions.Speciﬁcally,itprovidesthesolutionx=A+ywithminimalEuclideannorm||||x2amongallpossiblesolutions.WhenAhasmorerowsthancolumns,itispossiblefortheretobenosolution.Inthiscase,usingthepseudoinversegivesusthexforwhichAxisascloseaspossibletointermsofEuclideannormy||−||Axy2.2.10TheTraceOperatorThetraceoperatorgivesthesumofallofthediagonalentriesofamatrix:Tr() =AiAi,i.(2.48)Thetraceoperatorisusefulforavarietyofreasons.Someoperationsthatarediﬃculttospecifywithoutresortingtosummationnotationcanbespeciﬁedusing46 CHAPTER2.LINEARALGEBRAmatrixproductsandthetraceoperator.Forexample,thetraceoperatorprovidesanalternativewayofwritingtheFrobeniusnormofamatrix:||||AF=Tr(AA).(2.49)Writinganexpressionintermsofthetraceoperatoropensupopportunitiestomanipulatetheexpressionusingmanyusefulidentities. Forexample,thetraceoperatorisinvarianttothetransposeoperator:Tr() = Tr(AA).(2.50)Thetraceofasquarematrixcomposedofmanyfactorsisalsoinvarianttomovingthelastfactorintotheﬁrstposition,iftheshapesofthecorrespondingmatricesallowtheresultingproducttobedeﬁned:Tr() = Tr() = Tr()ABCCABBCA(2.51)ormoregenerally,Tr(ni=1F()i) = Tr(F()nn−1i=1F()i).(2.52)Thisinvariancetocyclicpermutationholdseveniftheresultingproducthasadiﬀerentshape.Forexample,forA∈Rmn×andB∈Rnm×,wehaveTr() = Tr()ABBA(2.53)eventhoughAB∈Rmm×andBA∈Rnn×.Anotherusefulfacttokeepinmindisthatascalarisitsowntrace:a=Tr(a).2.11TheDeterminantThedeterminantofa squarematrix, denoteddet(A), isa functionmappingmatricesto realscalars.Thedeterminantisequal totheproductof alltheeigenvaluesofthematrix.Theabsolutevalueofthedeterminantcanbethoughtofasameasureofhowmuchmultiplicationbythematrixexpandsorcontractsspace.Ifthedeterminantis0,thenspaceiscontractedcompletelyalongatleastonedimension,causingittoloseallofitsvolume.Ifthedeterminantis1,thenthetransformationpreservesvolume.47 CHAPTER2.LINEARALGEBRA2.12Example:PrincipalComponentsAnalysisOnesimplemachinelearningalgorithm,principalcomponentsanalysisorPCAcanbederivedusingonlyknowledgeofbasiclinearalgebra.Supposewehaveacollectionofmpoints{x(1),...,x()m}inRn.Supposewewouldliketoapplylossycompressiontothesepoints.Lossycompressionmeansstoringthepointsinawaythatrequireslessmemorybutmaylosesomeprecision.Wewouldliketoloseaslittleprecisionaspossible.Onewaywecanencodethesepointsistorepresentalower-dimensionalversionofthem.Foreachpointx()i∈Rnwewillﬁndacorrespondingcodevectorc()i∈Rl.Iflissmallerthann,itwilltakelessmemorytostorethecodepointsthantheoriginaldata.Wewillwanttoﬁndsomeencodingfunctionthatproducesthecodeforaninput,f(x) =c,andadecodingfunctionthatproducesthereconstructedinputgivenitscode,.xx≈gf(())PCAisdeﬁnedbyourchoiceofthedecodingfunction.Speciﬁcally,tomakethedecoderverysimple,wechoosetousematrixmultiplicationtomapthecodebackintoRn.Let,whereg() = cDcD∈Rnl×isthematrixdeﬁningthedecoding.Computingtheoptimalcodeforthisdecodercouldbeadiﬃcultproblem.Tokeeptheencodingproblemeasy,PCAconstrainsthecolumnsofDtobeorthogonaltoeachother.(NotethatDisstillnottechnically“anorthogonalmatrix”unlessln= )Withtheproblemasdescribedsofar,manysolutionsarepossible,becausewecanincreasethescaleofD:,iifwedecreaseciproportionallyforallpoints.Togivetheproblemauniquesolution,weconstrainallofthecolumnsoftohaveunitDnorm.Inordertoturnthisbasicideaintoanalgorithmwecanimplement,theﬁrstthingweneedtodoisﬁgureouthowtogeneratetheoptimalcodepointc∗foreachinputpointx.Onewaytodothisistominimizethedistancebetweentheinputpointxanditsreconstruction,g(c∗).Wecanmeasurethisdistanceusinganorm.Intheprincipalcomponentsalgorithm,weusetheL2norm:c∗= argminc||−||xg()c2.(2.54)WecanswitchtothesquaredL2norminsteadoftheL2normitself,becausebothareminimizedbythesamevalueofc.BothareminimizedbythesamevalueofcbecausetheL2normisnon-negativeandthesquaringoperationis48 CHAPTER2.LINEARALGEBRAmonotonicallyincreasingfornon-negativearguments.c∗= argminc||−||xg()c22.(2.55)Thefunctionbeingminimizedsimpliﬁesto(())x−gc(())x−gc(2.56)(bythedeﬁnitionoftheL2norm,equation)2.30= xxx−gg()c−()cxc+(g)g()c(2.57)(bythedistributiveproperty)= xxx−2gg()+c()cg()c(2.58)(becausethescalarg()cxisequaltothetransposeofitself).Wecannowchangethefunctionbeingminimizedagain,toomittheﬁrstterm,sincethistermdoesnotdependon:cc∗= argminc−2xgg()+c()cg.()c(2.59)Tomakefurtherprogress,wemustsubstituteinthedeﬁnitionof:g()cc∗= argminc−2xDcc+DDc(2.60)= argminc−2xDcc+Ilc(2.61)(bytheorthogonalityandunitnormconstraintson)D= argminc−2xDcc+c(2.62)Wecansolvethisoptimizationproblemusingvectorcalculus(seesectionif4.3youdonotknowhowtodothis):∇c(2−xDcc+c) = 0(2.63)−2Dxc+2= 0(2.64)cD= x.(2.65)49 CHAPTER2.LINEARALGEBRAThismakesthealgorithmeﬃcient: wecanoptimallyencodexjustusingamatrix-vectoroperation.Toencodeavector,weapplytheencoderfunctionf() = xDx.(2.66)Usingafurthermatrixmultiplication,wecanalsodeﬁnethePCAreconstructionoperation:rgf() = x(()) = xDDx.(2.67)Next,weneedtochoosetheencodingmatrixD.Todoso,werevisittheideaofminimizingtheL2distancebetweeninputsandreconstructions.SincewewillusethesamematrixDtodecodeallofthepoints,wecannolongerconsiderthepointsinisolation.Instead,wemustminimizetheFrobeniusnormofthematrixoferrorscomputedoveralldimensionsandallpoints:D∗= argminDi,jx()ij−r(x()i)j2subjecttoDDI= l(2.68)ToderivethealgorithmforﬁndingD∗,wewillstartbyconsideringthecasewherel= 1.Inthiscase,Disjustasinglevector,d.Substitutingequation2.67intoequationandsimplifyinginto,theproblemreducesto2.68Ddd∗= argmindi||x()i−ddx()i||22subjectto||||d2= 1.(2.69)Theaboveformulationisthemostdirectwayofperformingthesubstitution,butisnotthemoststylisticallypleasingwaytowritetheequation.Itplacesthescalarvaluedx()iontherightofthevectord.Itismoreconventionaltowritescalarcoeﬃcientsontheleftofvectortheyoperateon.Wethereforeusuallywritesuchaformulaasd∗= argmindi||x()i−dx()id||22subjectto||||d2= 1,(2.70)or,exploitingthefactthatascalarisitsowntranspose,asd∗= argmindi||x()i−x()idd||22subjectto||||d2= 1.(2.71)Thereadershouldaimtobecomefamiliarwithsuchcosmeticrearrangements.50 CHAPTER2.LINEARALGEBRAAtthispoint,itcanbehelpfultorewritetheproblemintermsofasingledesignmatrixofexamples,ratherthanasasumoverseparateexamplevectors.Thiswillallowustousemorecompactnotation.LetX∈Rmn×bethematrixdeﬁnedbystackingallofthevectorsdescribingthepoints,suchthatXi,:=x()i.Wecannowrewritetheproblemasd∗= argmind||−XXdd||2Fsubjecttodd= 1.(2.72)Disregardingtheconstraintforthemoment,wecansimplifytheFrobeniusnormportionasfollows:argmind||−XXdd||2F(2.73)= argmindTrXXdd−XXdd−(2.74)(byequation)2.49= argmindTr(XXX−Xdd−ddXXdd+XXdd)(2.75)= argmindTr(XX)Tr(−XXdd)Tr(−ddXX)+Tr(ddXXdd)(2.76)= argmind−Tr(XXdd)Tr(−ddXX)+Tr(ddXXdd)(2.77)(becausetermsnotinvolvingdonotaﬀectthe)dargmin= argmind−2Tr(XXdd)+Tr(ddXXdd)(2.78)(becausewecancycletheorderofthematricesinsideatrace,equation)2.52= argmind−2Tr(XXdd)+Tr(XXdddd)(2.79)(usingthesamepropertyagain)Atthispoint,were-introducetheconstraint:argmind−2Tr(XXdd)+Tr(XXdddd)subjecttodd= 1(2.80)= argmind−2Tr(XXdd)+Tr(XXdd)subjecttodd= 1(2.81)(duetotheconstraint)= argmind−Tr(XXdd)subjecttodd= 1(2.82)51 CHAPTER2.LINEARALGEBRA= argmaxdTr(XXdd)subjecttodd= 1(2.83)= argmaxdTr(dXXdd)subjecttod= 1(2.84)Thisoptimizationproblemmaybesolvedusingeigendecomposition.Speciﬁcally,theoptimaldisgivenbytheeigenvectorofXXcorrespondingtothelargesteigenvalue.Thisderivationisspeciﬁctothecaseofl=1andrecoversonlytheﬁrstprincipalcomponent.Moregenerally,whenwewishtorecoverabasisofprincipalcomponents,thematrixDisgivenbytheleigenvectorscorrespondingtothelargesteigenvalues.Thismaybeshownusingproofbyinduction.Werecommendwritingthisproofasanexercise.Linearalgebraisoneofthefundamentalmathematicaldisciplinesthatisnecessarytounderstanddeeplearning.Anotherkeyareaofmathematicsthatisubiquitousinmachinelearningisprobabilitytheory,presentednext. 52 Chapter3ProbabilityandInformationTheoryInthischapter,wedescribeprobabilitytheoryandinformationtheory.Probabilitytheoryisamathematicalframeworkforrepresentinguncertainstatements.Itprovidesameansofquantifyinguncertaintyandaxiomsforderivingnewuncertainstatements.Inartiﬁcialintelligenceapplications,weuseprobabilitytheoryintwomajorways.First,thelawsofprobabilitytellushowAIsystemsshouldreason,sowedesignouralgorithmstocomputeorapproximatevariousexpressionsderivedusingprobabilitytheory.Second,wecanuseprobabilityandstatisticstotheoreticallyanalyzethebehaviorofproposedAIsystems.Probabilitytheoryisafundamentaltoolofmanydisciplinesofscienceandengineering.Weprovidethischaptertoensurethatreaderswhosebackgroundisprimarilyinsoftwareengineeringwithlimitedexposuretoprobabilitytheorycanunderstandthematerialinthisbook.Whileprobabilitytheoryallowsustomakeuncertainstatementsandreasoninthepresenceofuncertainty,informationtheoryallowsustoquantifytheamountofuncertaintyinaprobabilitydistribution.Ifyouarealreadyfamiliarwithprobabilitytheoryandinformationtheory,youmaywishtoskipallofthischapterexceptforsection,whichdescribesthe3.14graphsweusetodescribestructuredprobabilisticmodelsformachinelearning.Ifyouhaveabsolutelynopriorexperiencewiththesesubjects,thischaptershouldbesuﬃcienttosuccessfullycarryoutdeeplearningresearchprojects,butwedosuggestthatyouconsultanadditionalresource,suchasJaynes2003().53 CHAPTER3.PROBABILITYANDINFORMATIONTHEORY3.1WhyProbability?Manybranchesofcomputersciencedealmostlywithentitiesthatareentirelydeterministicandcertain.AprogrammercanusuallysafelyassumethataCPUwillexecuteeachmachineinstructionﬂawlessly.Errorsinhardwaredooccur,butarerareenoughthatmostsoftwareapplicationsdonotneedtobedesignedtoaccountforthem.Giventhatmanycomputerscientistsandsoftwareengineersworkinarelativelycleanandcertainenvironment,itcanbesurprisingthatmachinelearningmakesheavyuseofprobabilitytheory.Thisisbecausemachinelearningmustalwaysdealwithuncertainquantities,andsometimesmayalsoneedtodealwithstochastic(non-deterministic)quantities.Uncertaintyandstochasticitycanarisefrommanysources.Researchershavemadecompellingargumentsforquantifyinguncertaintyusingprobabilitysinceatleastthe1980s.ManyoftheargumentspresentedherearesummarizedfromorinspiredbyPearl1988().Nearlyallactivitiesrequiresomeabilitytoreasoninthepresenceofuncertainty.Infact,beyondmathematicalstatementsthataretruebydeﬁnition,itisdiﬃculttothinkofanypropositionthatisabsolutelytrueoranyeventthatisabsolutelyguaranteedtooccur.Therearethreepossiblesourcesofuncertainty:1.Inherentstochasticityinthesystembeingmodeled.Forexample,mostinterpretationsofquantummechanicsdescribethedynamicsofsubatomicparticlesasbeingprobabilistic.Wecanalsocreatetheoreticalscenariosthatwepostulatetohaverandomdynamics,suchasahypotheticalcardgamewhereweassumethatthecardsaretrulyshuﬄedintoarandomorder.2.Incompleteobservability.Evendeterministicsystemscanappearstochasticwhenwecannotobserveallofthevariablesthatdrivethebehaviorofthesystem.Forexample,intheMontyHallproblem,agameshowcontestantisaskedtochoosebetweenthreedoorsandwinsaprizeheldbehindthechosendoor.Twodoorsleadtoagoatwhileathirdleadstoacar. Theoutcomegiventhecontestant’schoiceisdeterministic,butfromthecontestant’spointofview,theoutcomeisuncertain.3.Incompletemodeling.Whenweuseamodelthatmustdiscardsomeofthe informationwehave observed, the discarded informationresults inuncertaintyinthemodel’spredictions.Forexample,supposewebuildarobotthatcanexactlyobservethelocationofeveryobjectaroundit.Ifthe54 CHAPTER3.PROBABILITYANDINFORMATIONTHEORYrobotdiscretizesspacewhenpredictingthefuturelocationoftheseobjects,thenthediscretizationmakestherobotimmediatelybecomeuncertainabouttheprecisepositionofobjects: eachobjectcouldbeanywherewithinthediscretecellthatitwasobservedtooccupy.Inmanycases,itismorepracticaltouseasimplebutuncertainruleratherthanacomplexbutcertainone,evenifthetrueruleisdeterministicandourmodelingsystemhastheﬁdelitytoaccommodateacomplexrule.Forexample,thesimplerule“Mostbirdsﬂy”ischeaptodevelopandisbroadlyuseful,whilearuleoftheform,“Birdsﬂy,exceptforveryyoungbirdsthathavenotyetlearnedtoﬂy,sickorinjuredbirdsthathavelosttheabilitytoﬂy,ﬂightlessspeciesofbirdsincludingthecassowary,ostrichandkiwi...” isexpensivetodevelop,maintainandcommunicate,andafterallofthiseﬀortisstillverybrittleandpronetofailure.Whileitshouldbeclearthatweneedameansofrepresentingandreasoningaboutuncertainty,itisnotimmediatelyobviousthatprobabilitytheorycanprovideallofthetoolswewantforartiﬁcialintelligenceapplications.Probabilitytheorywasoriginallydevelopedtoanalyzethefrequenciesofevents.Itiseasytoseehowprobabilitytheorycanbeusedtostudyeventslikedrawingacertainhandofcardsinagameofpoker.Thesekindsofeventsareoftenrepeatable. Whenwesaythatanoutcomehasaprobabilitypofoccurring,itmeansthatifwerepeatedtheexperiment(e.g.,drawahandofcards)inﬁnitelymanytimes,thenproportionpoftherepetitionswouldresultinthatoutcome.Thiskindofreasoningdoesnotseemimmediatelyapplicabletopropositionsthatarenotrepeatable.Ifadoctoranalyzesapatientandsaysthatthepatienthasa40%chanceofhavingtheﬂu,thismeanssomethingverydiﬀerent—wecannotmakeinﬁnitelymanyreplicasofthepatient,noristhereanyreasontobelievethatdiﬀerentreplicasofthepatientwouldpresentwiththesamesymptomsyethavevaryingunderlyingconditions.Inthecaseofthedoctordiagnosingthepatient,weuseprobabilitytorepresentadegreeofbelief,with1indicatingabsolutecertaintythatthepatienthastheﬂuand0indicatingabsolutecertaintythatthepatientdoesnothavetheﬂu. Theformerkindofprobability,relateddirectlytotheratesatwhicheventsoccur,isknownasfrequentistprobability,whilethelatter,relatedtoqualitativelevelsofcertainty,isknownasBayesianprobability.Ifwelistseveralpropertiesthatweexpectcommonsensereasoningaboutuncertaintytohave,thentheonlywaytosatisfythosepropertiesistotreatBayesianprobabilitiesasbehavingexactlythesameasfrequentistprobabilities.Forexample,ifwewanttocomputetheprobabilitythataplayerwillwinapokergamegiventhatshehasacertainsetofcards,weuseexactlythesameformulasaswhenwecomputetheprobabilitythatapatienthasadiseasegiventhatshe55 CHAPTER3.PROBABILITYANDINFORMATIONTHEORYhascertainsymptoms.Formoredetailsaboutwhyasmallsetofcommonsenseassumptionsimpliesthatthesameaxiomsmustcontrolbothkindsofprobability,see().Ramsey1926Probabilitycanbeseenastheextensionoflogictodealwithuncertainty.Logicprovidesasetofformalrulesfordeterminingwhatpropositionsareimpliedtobetrueorfalsegiventheassumptionthatsomeothersetofpropositionsistrueorfalse.Probabilitytheoryprovidesasetofformalrulesfordeterminingthelikelihoodofapropositionbeingtruegiventhelikelihoodofotherpropositions.3.2RandomVariablesArandomvariableisavariablethatcantakeondiﬀerentvaluesrandomly.Wetypicallydenotetherandomvariableitselfwithalowercaseletterinplaintypeface,andthevaluesitcantakeonwithlowercasescriptletters.Forexample,x1andx2arebothpossiblevaluesthattherandomvariablexcantakeon.Forvector-valuedvariables,wewouldwritetherandomvariableasxandoneofitsvaluesasx.Onitsown,arandomvariableisjustadescriptionofthestatesthatarepossible;itmustbecoupledwithaprobabilitydistributionthatspeciﬁeshowlikelyeachofthesestatesare.Randomvariablesmaybediscreteorcontinuous.Adiscreterandomvariableisonethathasaﬁniteorcountablyinﬁnitenumberofstates.Notethatthesestatesarenotnecessarilytheintegers;theycanalsojustbenamedstatesthatarenotconsideredtohaveanynumericalvalue.Acontinuousrandomvariableisassociatedwitharealvalue.3.3ProbabilityDistributionsAprobabilitydistributionisadescriptionofhowlikelyarandomvariableorsetofrandomvariablesistotakeoneachofitspossiblestates.Thewaywedescribeprobabilitydistributionsdependsonwhetherthevariablesarediscreteorcontinuous.3.3.1DiscreteVariablesandProbabilityMassFunctionsAprobabilitydistributionoverdiscretevariablesmaybedescribedusingaproba-bilitymassfunction(PMF).WetypicallydenoteprobabilitymassfunctionswithacapitalP.Oftenweassociateeachrandomvariablewithadiﬀerentprobability56 CHAPTER3.PROBABILITYANDINFORMATIONTHEORYmassfunctionandthereadermustinferwhichprobabilitymassfunctiontousebasedontheidentityoftherandomvariable,ratherthanthenameofthefunction;PP()xisusuallynotthesameas()y.Theprobabilitymassfunctionmapsfromastateofarandomvariabletotheprobabilityofthatrandomvariabletakingonthatstate.Theprobabilitythatx=xisdenotedasP(x),withaprobabilityof1indicatingthatx=xiscertainandaprobabilityof0indicatingthatx=xisimpossible.SometimestodisambiguatewhichPMFtouse,wewritethenameoftherandomvariableexplicitly:P(x=x).Sometimeswedeﬁneavariableﬁrst,thenuse∼notationtospecifywhichdistributionitfollowslater:xx.∼P()Probabilitymassfunctionscanactonmanyvariablesatthesametime.Suchaprobabilitydistributionovermanyvariablesisknownasajointprobabilitydistribution.P(x=x,y=y)denotestheprobabilitythatx=xandy=ysimultaneously.Wemayalsowriteforbrevity.Px,y()Tobeaprobabilitymassfunctiononarandomvariablex,afunctionPmustsatisfythefollowingproperties:•Thedomainofmustbethesetofallpossiblestatesofx.P•∀∈xx,0≤P(x)≤1.Animpossibleeventhasprobabilityandnostatecan0 belessprobablethanthat.Likewise,aneventthatisguaranteedtohappenhasprobability,andnostatecanhaveagreaterchanceofoccurring.1•x∈xP(x) = 1.Werefertothispropertyasbeingnormalized.Withoutthisproperty,wecouldobtainprobabilitiesgreaterthanonebycomputingtheprobabilityofoneofmanyeventsoccurring.Forexample,considerasinglediscreterandomvariablexwithkdiﬀerentstates.Wecanplaceauniformdistributiononx—thatis,makeeachofitsstatesequallylikely—bysettingitsprobabilitymassfunctiontoPx(= xi) =1k(3.1)foralli.Wecanseethatthisﬁtstherequirementsforaprobabilitymassfunction.Thevalue1kispositivebecauseisapositiveinteger.WealsoseethatkiPx(= xi) =i1k=kk= 1,(3.2)sothedistributionisproperlynormalized.57 CHAPTER3.PROBABILITYANDINFORMATIONTHEORY3.3.2ContinuousVariablesandProbabilityDensityFunctionsWhenworkingwithcontinuousrandomvariables,wedescribeprobabilitydistri-butionsusingaprobabilitydensityfunction(PDF)ratherthanaprobabilitymassfunction.Tobeaprobabilitydensityfunction,afunctionpmustsatisfythefollowingproperties:•Thedomainofmustbethesetofallpossiblestatesofx.p•∀∈≥≤xx,px() 0() .pNotethatwedonotrequirex1.•pxdx()= 1.Aprobabilitydensityfunctionp(x)doesnotgivetheprobabilityofaspeciﬁcstatedirectly,insteadtheprobabilityoflandinginsideaninﬁnitesimalregionwithvolumeisgivenby.δxpxδx()Wecanintegratethedensityfunctiontoﬁndtheactualprobabilitymassofasetofpoints.Speciﬁcally,theprobabilitythatxliesinsomesetSisgivenbytheintegralofp(x)overthatset.Intheunivariateexample,theprobabilitythatxliesintheintervalisgivenby[]a,b[]a,bpxdx().Foranexampleofaprobabilitydensityfunctioncorrespondingtoaspeciﬁcprobabilitydensityoveracontinuousrandomvariable,considerauniformdistribu-tiononanintervaloftherealnumbers.Wecandothiswithafunctionu(x;a,b),whereaandbaretheendpointsoftheinterval,withb>a.The“;”notationmeans“parametrizedby”;weconsiderxtobetheargumentofthefunction,whileaandbareparametersthatdeﬁnethefunction.Toensurethatthereisnoprobabilitymassoutsidetheinterval,wesayu(x;a,b)=0forallx∈[a,b][.Withina,b],uxa,b(;) =1ba−.Wecanseethatthisisnonnegativeeverywhere.Additionally,itintegratesto1.Weoftendenotethatxfollowstheuniformdistributionon[a,b]bywritingx.∼Ua,b()3.4MarginalProbabilitySometimesweknowtheprobabilitydistributionoverasetofvariablesandwewanttoknowtheprobabilitydistributionoverjustasubsetofthem.Theprobabilitydistributionoverthesubsetisknownasthedistribution.marginalprobabilityForexample,supposewehavediscreterandomvariablesxandy,andweknowP,(xy.Wecanﬁndxwiththe:)P()sumrule∀∈xxx,P(= ) =xyPx,y.(= xy= )(3.3)58 CHAPTER3.PROBABILITYANDINFORMATIONTHEORYThename“marginalprobability”comesfromtheprocessofcomputingmarginalprobabilitiesonpaper.WhenthevaluesofP(xy,)arewritteninagridwithdiﬀerentvaluesofxinrowsanddiﬀerentvaluesofyincolumns,itisnaturaltosumacrossarowofthegrid,thenwriteP(x)inthemarginofthepaperjusttotherightoftherow.Forcontinuousvariables,weneedtouseintegrationinsteadofsummation:px() =px,ydy.()(3.4)3.5ConditionalProbabilityInmanycases,weareinterestedintheprobabilityofsomeevent,giventhatsomeothereventhashappened.Thisiscalledaconditionalprobability.Wedenotetheconditionalprobabilitythaty=ygivenx=xasP(y=y|x=x).ThisconditionalprobabilitycanbecomputedwiththeformulaPyx(= y|x= ) =Py,x(= yx= )Px(= x).(3.5)TheconditionalprobabilityisonlydeﬁnedwhenP(x=x)>0.Wecannotcomputetheconditionalprobabilityconditionedonaneventthatneverhappens.Itisimportantnottoconfuseconditionalprobabilitywithcomputingwhatwouldhappenifsomeactionwereundertaken.TheconditionalprobabilitythatapersonisfromGermanygiventhattheyspeakGermanisquitehigh,butifarandomlyselectedpersonistaughttospeakGerman,theircountryoforigindoesnotchange.Computingtheconsequencesofanactioniscalledmakinganinterventionquery.Interventionqueriesarethedomainofcausalmodeling,whichwedonotexploreinthisbook.3.6TheChainRuleofConditionalProbabilitiesAnyjointprobabilitydistributionovermanyrandomvariablesmaybedecomposedintoconditionaldistributionsoveronlyonevariable:P(x(1),...,x()n) = (Px(1))Πni=2P(x()i|x(1),...,x(1)i−).(3.6)Thisobservationisknownasthechainruleorproductruleofprobability.Itfollowsimmediatelyfromthedeﬁnitionofconditionalprobabilityinequation.3.559 CHAPTER3.PROBABILITYANDINFORMATIONTHEORYForexample,applyingthedeﬁnitiontwice,wegetP,,P,P,(abc)=(ab|c)(bc)P,PP(bc)=()bc|()cP,,P,PP.(abc)=(ab|c)()bc|()c3.7IndependenceandConditionalIndependenceTworandomvariablesxandyareindependentiftheirprobabilitydistributioncanbeexpressedasaproductoftwofactors,oneinvolvingonlyxandoneinvolvingonlyy:∀∈∈xx,yyxyxy(3.7),p(= x,= ) = (yp= )(xp= )y.Tworandomvariablesxandyareconditionallyindependentgivenarandomvariableziftheconditionalprobabilitydistributionoverxandyfactorizesinthiswayforeveryvalueofz:∀∈∈∈|||xx,yy,zzxy,p(= x,= yzx= ) = (zp= xzy= )(zp= yz= )z.(3.8)We candenoteindependence andconditionalindependence withcompactnotation:xy⊥meansthatxandyareindependent,whilexyz⊥|meansthatxandyareconditionallyindependentgivenz.3.8Expectation,VarianceandCovarianceTheexpectationorexpectedvalueofsomefunctionf(x)withrespecttoaprobabilitydistributionP(x)istheaverageormeanvaluethatftakesonwhenxisdrawnfrom.Fordiscretevariablesthiscanbecomputedwithasummation:PEx∼P[()] =fxxPxfx,()()(3.9)whileforcontinuousvariables,itiscomputedwithanintegral:Ex∼p[()] =fxpxfxdx.()()(3.10)60 CHAPTER3.PROBABILITYANDINFORMATIONTHEORYWhentheidentityofthedistributionisclearfromthecontext,wemaysimplywritethenameoftherandomvariablethattheexpectationisover,asinEx[f(x)].Ifitisclearwhichrandomvariabletheexpectationisover,wemayomitthesubscriptentirely,asinE[f(x)].Bydefault,wecanassumethatE[·]averagesoverthevaluesofalltherandomvariablesinsidethebrackets.Likewise,whenthereisnoambiguity,wemayomitthesquarebrackets.Expectationsarelinear,forexample,Ex[()+()] = αfxβgxαEx[()]+fxβEx[()]gx,(3.11)whenandarenotdependenton.αβxThevariancegivesameasureofhowmuchthevaluesofafunctionofarandomvariablexvaryaswesamplediﬀerentvaluesofxfromitsprobabilitydistribution:Var(()) = fxE(()[()])fx−Efx2.(3.12)Whenthevarianceislow,thevaluesoff(x)clusterneartheirexpectedvalue.Thesquarerootofthevarianceisknownasthe.standarddeviationThecovariancegivessomesenseofhowmuchtwovaluesarelinearlyrelatedtoeachother,aswellasthescaleofthesevariables:Cov(()()) = [(()[()])(()[()])]fx,gyEfx−Efxgy−Egy.(3.13)Highabsolutevaluesofthecovariancemeanthatthevalueschangeverymuchandarebothfarfromtheirrespectivemeansatthesametime.Ifthesignofthecovarianceispositive,thenbothvariablestendtotakeonrelativelyhighvaluessimultaneously.Ifthesignofthecovarianceisnegative,thenonevariabletendstotakeonarelativelyhighvalueatthetimesthattheothertakesonarelativelylowvalueandviceversa.Othermeasuressuchascorrelationnormalizethecontributionofeachvariableinordertomeasureonlyhowmuchthevariablesarerelated,ratherthanalsobeingaﬀectedbythescaleoftheseparatevariables.Thenotionsofcovarianceanddependencearerelated,butareinfactdistinctconcepts.Theyarerelatedbecausetwovariablesthatareindependenthavezerocovariance,andtwovariablesthathavenon-zerocovariancearedependent.How-ever,independenceisadistinctpropertyfromcovariance.Fortwovariablestohavezerocovariance,theremustbenolineardependencebetweenthem.Independenceisastrongerrequirementthanzerocovariance,becauseindependencealsoexcludesnonlinearrelationships.Itispossiblefortwovariablestobedependentbuthavezerocovariance.Forexample,supposeweﬁrstsamplearealnumberxfromauniformdistributionovertheinterval[−1,1].Wenextsamplearandomvariable61 CHAPTER3.PROBABILITYANDINFORMATIONTHEORYs.Withprobability12,wechoosethevalueofstobe.Otherwise,wechoose1thevalueofstobe−1.Wecanthengeneratearandomvariableybyassigningy=sx.Clearly,xandyarenotindependent,becausexcompletelydeterminesthemagnitudeof.However,yCov() = 0x,y.Thecovariancematrixofarandomvectorx∈Rnisannn×matrix,suchthatCov()xi,j= Cov(xi,xj).(3.14)Thediagonalelementsofthecovariancegivethevariance:Cov(xi,xi) = Var(xi).(3.15)3.9CommonProbabilityDistributionsSeveralsimpleprobabilitydistributionsareusefulinmanycontextsinmachinelearning.3.9.1BernoulliDistributionTheBernoullidistributionisadistributionoverasinglebinaryrandomvariable.Itiscontrolledbyasingleparameterφ∈[0,1],whichgivestheprobabilityoftherandomvariablebeingequalto1.Ithasthefollowingproperties:Pφ(= 1) = x(3.16)Pφ(= 0) = 1x−(3.17)Pxφ(= x) = x(1)−φ1−x(3.18)Ex[] = xφ(3.19)Varx() = (1)xφ−φ(3.20)3.9.2MultinoulliDistributionThemultinoulliorcategoricaldistributionisadistributionoverasinglediscretevariablewithkdiﬀerentstates,wherekisﬁnite.1Themultinoullidistributionis1“Multinoulli”isatermthatwasrecentlycoinedbyGustavoLacerdoandpopularizedbyMurphy2012().Themultinoullidistributionisaspecialcaseofthemultinomialdistribution.Amultinomialdistributionisthedistributionovervectorsin{0,...,n}krepresentinghowmanytimeseachofthekcategoriesisvisitedwhennsamplesaredrawnfromamultinoullidistribution.Manytextsusetheterm“multinomial”torefertomultinoullidistributionswithoutclarifyingthattheyreferonlytothecase.n= 162 CHAPTER3.PROBABILITYANDINFORMATIONTHEORYparametrizedbyavectorp∈[0,1]k−1,wherepigivestheprobabilityofthei-thstate.Theﬁnal,k-thstate’sprobabilityisgivenby1−1p.Notethatwemustconstrain1p≤1.Multinoullidistributionsareoftenusedtorefertodistributionsovercategoriesofobjects,sowedonotusuallyassumethatstate1hasnumericalvalue1,etc.Forthisreason,wedonotusuallyneedtocomputetheexpectationorvarianceofmultinoulli-distributedrandomvariables.TheBernoulliandmultinoullidistributionsaresuﬃcienttodescribeanydistri-butionovertheirdomain. Theyareabletodescribeanydistributionovertheirdomainnotsomuchbecausetheyareparticularlypowerfulbutratherbecausetheirdomainissimple;theymodeldiscretevariablesforwhichitisfeasibletoenumerateallofthestates.Whendealingwithcontinuousvariables,thereareuncountablymanystates,soanydistributiondescribedbyasmallnumberofparametersmustimposestrictlimitsonthedistribution.3.9.3GaussianDistributionThemostcommonlyuseddistributionoverrealnumbersisthenormaldistribu-tion,alsoknownasthe:GaussiandistributionN(;xµ,σ2) =12πσ2exp−12σ2()xµ−2.(3.21)Seeﬁgureforaplotofthedensityfunction.3.1Thetwoparametersµ∈Randσ∈(0,∞)controlthenormaldistribution.Theparameterµgivesthecoordinateofthecentralpeak.Thisisalsothemeanofthedistribution:E[x] =µ.Thestandarddeviationofthedistributionisgivenbyσ,andthevariancebyσ2.WhenweevaluatethePDF,weneedtosquareandinvertσ.WhenweneedtofrequentlyevaluatethePDFwithdiﬀerentparametervalues,amoreeﬃcientwayofparametrizingthedistributionistouseaparameterβ∈(0,∞)tocontroltheprecisionorinversevarianceofthedistribution:N(;xµ,β−1) =β2πexp−12βxµ(−)2.(3.22)Normaldistributionsareasensiblechoiceformanyapplications.Intheabsenceofpriorknowledgeaboutwhatformadistributionovertherealnumbersshouldtake,thenormaldistributionisagooddefaultchoicefortwomajorreasons.63 CHAPTER3.PROBABILITYANDINFORMATIONTHEORY −−−−20.15.10.050005101520......000.005.010.015.020.025.030.035.040.p(x)Maximumat= xµInﬂectionpointsatxµσ= ± Figure3.1:Thenormaldistribution:ThenormaldistributionN(x;µ,σ2)exhibitsaclassic“bellcurve”shape,withthexcoordinateofitscentralpeakgivenbyµ,andthewidthofitspeakcontrolledbyσ.Inthisexample,wedepictthestandardnormaldistribution,withand.µ= 0σ= 1First,manydistributionswewishtomodelaretrulyclosetobeingnormaldistributions.Thecentrallimittheoremshowsthatthesumofmanyindepen-dentrandomvariablesisapproximatelynormallydistributed.Thismeansthatinpractice,manycomplicatedsystemscanbemodeledsuccessfullyasnormallydistributednoise,evenifthesystemcanbedecomposedintopartswithmorestructuredbehavior.Second,outofallpossibleprobabilitydistributionswiththesamevariance,thenormaldistributionencodesthemaximumamountofuncertaintyovertherealnumbers.Wecanthusthinkofthenormaldistributionasbeingtheonethatinsertstheleastamountofpriorknowledgeintoamodel.Fullydevelopingandjustifyingthisidearequiresmoremathematicaltools,andispostponedtosection.19.4.2ThenormaldistributiongeneralizestoRn,inwhichcaseitisknownasthemultivariatenormaldistribution.Itmaybeparametrizedwithapositivedeﬁnitesymmetricmatrix:ΣN(;) =xµ,Σ1(2)πndet()Σexp−12()xµ−Σ−1()xµ−.(3.23)64 CHAPTER3.PROBABILITYANDINFORMATIONTHEORYTheparameterµstillgivesthemeanofthedistribution,thoughnowitisvector-valued.TheparameterΣgivesthecovariancematrixofthedistribution.Asintheunivariatecase,whenwewishtoevaluatethePDFseveraltimesformanydiﬀerentvaluesoftheparameters,thecovarianceisnotacomputationallyeﬃcientwaytoparametrizethedistribution,sinceweneedtoinvertΣtoevaluatethePDF.Wecaninsteadusea:precisionmatrixβN(;xµβ,−1) =det()β(2)πnexp−12()xµ−βxµ(−).(3.24)Weoftenﬁxthecovariancematrixtobeadiagonalmatrix.AnevensimplerversionistheisotropicGaussiandistribution,whosecovariancematrixisascalartimestheidentitymatrix.3.9.4ExponentialandLaplaceDistributionsInthecontextofdeeplearning,weoftenwanttohaveaprobabilitydistributionwithasharppointatx=0.Toaccomplishthis,wecanusetheexponentialdistribution:pxλλ(;) = 1x≥0exp()−λx.(3.25)Theexponentialdistributionusestheindicatorfunction1x≥0toassignprobabilityzerotoallnegativevaluesof.xAcloselyrelatedprobabilitydistributionthatallowsustoplaceasharppeakofprobabilitymassatanarbitrarypointistheµLaplacedistributionLaplace(;) =xµ,γ12γexp−|−|xµγ.(3.26)3.9.5TheDiracDistributionandEmpiricalDistributionInsomecases,wewishtospecifythatallofthemassinaprobabilitydistributionclustersaroundasinglepoint.ThiscanbeaccomplishedbydeﬁningaPDFusingtheDiracdeltafunction,:δx()pxδxµ.() = (−)(3.27)TheDiracdeltafunctionisdeﬁnedsuchthatitiszero-valuedeverywhereexcept0,yetintegratesto1.TheDiracdeltafunctionisnotanordinaryfunctionthatassociateseachvaluexwithareal-valuedoutput,insteaditisadiﬀerentkindof65 CHAPTER3.PROBABILITYANDINFORMATIONTHEORYmathematicalobjectcalledageneralizedfunctionthatisdeﬁnedintermsofitspropertieswhenintegrated.WecanthinkoftheDiracdeltafunctionasbeingthelimitpointofaseriesoffunctionsthatputlessandlessmassonallpointsotherthanzero.Bydeﬁningp(x)tobeδshiftedby−µweobtainaninﬁnitelynarrowandinﬁnitelyhighpeakofprobabilitymasswhere.xµ= AcommonuseoftheDiracdeltadistributionisasacomponentofanempiricaldistribution,ˆp() =x1mmi=1δ(xx−()i)(3.28)whichputsprobabilitymass1moneachofthempointsx(1),...,x()mformingagivendatasetorcollectionofsamples.TheDiracdeltadistributionisonlynecessarytodeﬁnetheempiricaldistributionovercontinuousvariables.Fordiscretevariables,thesituationissimpler:anempiricaldistributioncanbeconceptualizedasamultinoullidistribution,withaprobabilityassociatedtoeachpossibleinputvaluethatissimplyequaltotheempiricalfrequencyofthatvalueinthetrainingset.Wecanviewtheempiricaldistributionformedfromadatasetoftrainingexamplesasspecifyingthedistributionthatwesamplefromwhenwetrainamodelonthisdataset. Anotherimportantperspectiveontheempiricaldistributionisthatitistheprobabilitydensitythatmaximizesthelikelihoodofthetrainingdata(seesection).5.53.9.6MixturesofDistributionsItisalsocommontodeﬁneprobabilitydistributionsbycombiningothersimplerprobabilitydistributions.Onecommon wayof combining distributionsis toconstructamixturedistribution.Amixturedistributionismadeupofseveralcomponentdistributions.Oneachtrial,thechoiceofwhichcomponentdistributiongeneratesthesampleisdeterminedbysamplingacomponentidentityfromamultinoullidistribution:P() =xiPiPi(= c)(= xc|)(3.29)wherecisthemultinoullidistributionovercomponentidentities.P()Wehavealreadyseenoneexampleofamixturedistribution:theempiricaldistributionoverreal-valuedvariablesisamixturedistributionwithoneDiraccomponentforeachtrainingexample.66 CHAPTER3.PROBABILITYANDINFORMATIONTHEORYThemixturemodelisonesimplestrategyforcombiningprobabilitydistributionstocreatearicherdistribution.Inchapter,weexploretheartofbuildingcomplex16probabilitydistributionsfromsimpleonesinmoredetail.Themixturemodelallowsustobrieﬂyglimpseaconceptthatwillbeofparamountimportancelater—thelatentvariable.Alatentvariableisarandomvariablethatwecannotobservedirectly.Thecomponentidentityvariablecofthemixturemodelprovidesanexample.Latentvariablesmayberelatedtoxthroughthejointdistribution,inthiscase,P(xc,) =P(xc|)P(c).ThedistributionP(c)overthelatentvariableandthedistributionP(xc|)relatingthelatentvariablestothevisiblevariablesdeterminestheshapeofthedistributionP(x)eventhoughitispossibletodescribeP(x)withoutreferencetothelatentvariable.Latentvariablesarediscussedfurtherinsection.16.5AverypowerfulandcommontypeofmixturemodelistheGaussianmixturemodel,inwhichthecomponentsp(x|c=i)areGaussians.Eachcomponenthasaseparatelyparametrizedmeanµ()iandcovarianceΣ()i.Somemixturescanhavemoreconstraints.Forexample,thecovariancescouldbesharedacrosscomponentsviatheconstraintΣ()i=Σ,i∀.AswithasingleGaussiandistribution,themixtureofGaussiansmightconstrainthecovariancematrixforeachcomponenttobediagonalorisotropic.Inadditiontothemeansandcovariances,theparametersofaGaussianmixturespecifythepriorprobabilityαi=P(c=i) giventoeachcomponenti.Theword“prior”indicatesthatitexpressesthemodel’sbeliefsaboutcbeforeithasobservedx.Bycomparison,P(c|x)isaposteriorprobability,becauseitiscomputedafterobservationofx.AGaussianmixturemodelisauniversalapproximatorofdensities,inthesensethatanysmoothdensitycanbeapproximatedwithanyspeciﬁc,non-zeroamountoferrorbyaGaussianmixturemodelwithenoughcomponents.FigureshowssamplesfromaGaussianmixturemodel.3.23.10UsefulPropertiesofCommonFunctionsCertainfunctionsariseoftenwhileworkingwithprobabilitydistributions,especiallytheprobabilitydistributionsusedindeeplearningmodels.Oneofthesefunctionsisthe:logisticsigmoidσx() =11+exp()−x.(3.30)ThelogisticsigmoidiscommonlyusedtoproducetheφparameterofaBernoulli67 CHAPTER3.PROBABILITYANDINFORMATIONTHEORY x1x2 Figure3.2: SamplesfromaGaussianmixturemodel.Inthisexample,therearethreecomponents.Fromlefttoright,theﬁrstcomponenthasanisotropiccovariancematrix,meaningithasthesameamountofvarianceineachdirection.Thesecondhasadiagonalcovariancematrix,meaningitcancontrolthevarianceseparatelyalongeachaxis-aligneddirection.Thisexamplehasmorevariancealongthex2axisthanalongthex1axis.Thethirdcomponenthasafull-rankcovariancematrix,allowingittocontrolthevarianceseparatelyalonganarbitrarybasisofdirections.distributionbecauseitsrangeis(0,1),whichlieswithinthevalidrangeofvaluesfortheφparameter.Seeﬁgureforagraphofthesigmoidfunction.The3.3sigmoidfunctionsaturateswhenitsargumentisverypositiveorverynegative,meaningthatthefunctionbecomesveryﬂatandinsensitivetosmallchangesinitsinput.Anothercommonlyencounteredfunctionisthesoftplusfunction(,Dugasetal.2001):ζxx.() = log(1+exp())(3.31)Thesoftplusfunctioncanbeusefulforproducingtheβorσparameterofanormaldistributionbecauseitsrangeis(0,∞).Italsoarisescommonlywhenmanipulatingexpressionsinvolvingsigmoids.Thenameofthesoftplusfunctioncomesfromthefactthatitisasmoothedor“softened”versionofx+= max(0),x.(3.32)Seeﬁgureforagraphofthesoftplusfunction.3.4Thefollowingpropertiesareallusefulenoughthatyoumaywishtomemorizethem:68 CHAPTER3.PROBABILITYANDINFORMATIONTHEORY −−105051000.02.04.06.08.10.σx() Figure3.3:Thelogisticsigmoidfunction. −−10505100246810ζx() Figure3.4:Thesoftplusfunction.69 CHAPTER3.PROBABILITYANDINFORMATIONTHEORYσx() =exp()xexp()+exp(0)x(3.33)ddxσxσxσx() = ()(1−())(3.34)1() = ()−σxσ−x(3.35)log() = ()σx−ζ−x(3.36)ddxζxσx() = ()(3.37)∀∈x(01),,σ−1() = logxx1−x(3.38)∀x>,ζ0−1() = log(exp()1)xx−(3.39)ζx() =x−∞σydy()(3.40)ζxζxx()−(−) = (3.41)Thefunctionσ−1(x)iscalledthelogitinstatistics,butthistermismorerarelyusedinmachinelearning.Equationprovidesextrajustiﬁcationforthename“softplus.”Thesoftplus3.41functionisintendedasasmoothedversionofthepositivepartfunction,x+=max{0,x}.Thepositivepartfunctionisthecounterpartofthenegativepartfunction,x−=max{0,x−}.Toobtainasmoothfunctionthatisanalogoustothenegativepart,onecanuseζ(−x).Justasxcanberecoveredfromitspositivepartandnegativepartviatheidentityx+−x−=x,itisalsopossibletorecoverxusingthesamerelationshipbetweenand,asshowninequation.ζx()ζx(−)3.413.11Bayes’RuleWeoftenﬁndourselvesinasituationwhereweknowP(yx|)andneedtoknowP(xy|).Fortunately,ifwealsoknowP(x),wecancomputethedesiredquantityusingBayes’rule:P() =xy|PP()x()yx|P()y.(3.42)NotethatwhileP(y)appearsintheformula,itisusuallyfeasibletocomputeP() =yxPxPxP(y|)(),sowedonotneedtobeginwithknowledgeof()y.70 CHAPTER3.PROBABILITYANDINFORMATIONTHEORYBayes’ruleis straightforwardto derivefrom thedeﬁnitionofconditionalprobability,butitisusefultoknowthenameofthisformulasincemanytextsrefertoitbyname.ItisnamedaftertheReverendThomasBayes,whoﬁrstdiscoveredaspecialcaseoftheformula.ThegeneralversionpresentedherewasindependentlydiscoveredbyPierre-SimonLaplace.3.12TechnicalDetailsofContinuousVariablesAproperformalunderstandingofcontinuousrandomvariablesandprobabilitydensityfunctionsrequiresdevelopingprobabilitytheoryintermsofabranchofmathematicsknownasmeasuretheory.Measuretheoryisbeyondthescopeofthistextbook,butwecanbrieﬂysketchsomeoftheissuesthatmeasuretheoryisemployedtoresolve.Insection,wesawthattheprobabilityofacontinuousvector-valued3.3.2xlyinginsomesetSisgivenbytheintegralofp(x)overthesetS.SomechoicesofsetScanproduceparadoxes.Forexample,itispossibletoconstructtwosetsS1andS2suchthatp(x∈S1) +p(x∈S2)>1butS1∩S2=∅.Thesesetsaregenerallyconstructedmakingveryheavyuseoftheinﬁniteprecisionofrealnumbers,forexamplebymakingfractal-shapedsetsorsetsthataredeﬁnedbytransformingthesetofrationalnumbers.2Oneofthekeycontributionsofmeasuretheoryistoprovideacharacterizationofthesetofsetsthatwecancomputetheprobabilityofwithoutencounteringparadoxes. Inthisbook,weonlyintegrateoversetswithrelativelysimpledescriptions,sothisaspectofmeasuretheoryneverbecomesarelevantconcern.Forourpurposes,measuretheoryismoreusefulfordescribingtheoremsthatapplytomostpointsinRnbutdonotapplytosomecornercases.Measuretheoryprovidesarigorouswayofdescribingthatasetofpointsisnegligiblysmall.Suchasetissaidtohavemeasurezero.Wedonotformallydeﬁnethisconceptinthistextbook.Forourpurposes,itissuﬃcienttounderstandtheintuitionthatasetofmeasurezerooccupiesnovolumeinthespacewearemeasuring.Forexample,withinR2,alinehasmeasurezero,whileaﬁlledpolygonhaspositivemeasure.Likewise,anindividualpointhasmeasurezero.Anyunionofcountablymanysetsthateachhavemeasurezeroalsohasmeasurezero(sothesetofalltherationalnumbershasmeasurezero,forinstance).Anotherusefultermfrommeasuretheoryisalmosteverywhere.Apropertythatholdsalmosteverywhereholdsthroughoutallofspaceexceptforonasetof2TheBanach-Tarskitheoremprovidesafunexampleofsuchsets.71 CHAPTER3.PROBABILITYANDINFORMATIONTHEORYmeasurezero.Becausetheexceptionsoccupyanegligibleamountofspace,theycanbesafelyignoredformanyapplications.Someimportantresultsinprobabilitytheoryholdforalldiscretevaluesbutonlyhold“almosteverywhere”forcontinuousvalues.Anothertechnicaldetailofcontinuousvariablesrelatestohandlingcontinuousrandomvariablesthataredeterministicfunctionsofoneanother.Supposewehavetworandomvariables,xandy,suchthaty=g(x),wheregisaninvertible,con-tinuous,diﬀerentiabletransformation.Onemightexpectthatpy(y) =px(g−1(y)).Thisisactuallynotthecase.Asasimpleexample,supposewehavescalarrandomvariablesxandy.Supposey=x2andx∼U(0,1).Ifweusetherulepy(y)=px(2y)thenpywillbe0everywhereexcepttheinterval[0,12]1,anditwillbeonthisinterval.Thismeanspy()=ydy12,(3.43)whichviolatesthedeﬁnitionofaprobabilitydistribution.Thisisacommonmistake.Theproblemwiththisapproachisthatitfailstoaccountforthedistortionofspaceintroducedbythefunctiong.Recallthattheprobabilityofxlyinginaninﬁnitesimallysmallregionwithvolumeδxisgivenbyp(x)δx.Sincegcanexpandorcontractspace,theinﬁnitesimalvolumesurroundingxinxspacemayhavediﬀerentvolumeinspace.yToseehowtocorrecttheproblem,wereturntothescalarcase.Weneedtopreservetheproperty|py(())= gxdy||px()xdx.|(3.44)Solvingfromthis,weobtainpy() = ypx(g−1())y∂x∂y(3.45)orequivalentlypx() = xpy(())gx∂gx()∂x.(3.46)Inhigherdimensions,thederivativegeneralizestothedeterminantoftheJacobianmatrix—thematrixwithJi,j=∂xi∂yj.Thus,forreal-valuedvectorsand,xypx() = xpy(())gxdet∂g()x∂x.(3.47)72 CHAPTER3.PROBABILITYANDINFORMATIONTHEORY3.13InformationTheoryInformationtheory isa branchof appliedmathematics thatrevolvesaroundquantifyinghowmuchinformationispresentinasignal.Itwasoriginallyinventedtostudysendingmessagesfromdiscretealphabetsoveranoisychannel,suchascommunicationviaradiotransmission.Inthiscontext,informationtheorytellshowtodesignoptimalcodesandcalculatetheexpectedlengthofmessagessampledfromspeciﬁcprobabilitydistributionsusingvariousencodingschemes.Inthecontextofmachinelearning,wecanalsoapplyinformationtheorytocontinuousvariableswheresomeofthesemessagelengthinterpretationsdonotapply.Thisﬁeldisfundamentaltomanyareasofelectricalengineeringandcomputerscience.Inthistextbook,wemostlyuseafewkeyideasfrominformationtheorytocharacterizeprobabilitydistributionsorquantifysimilaritybetweenprobabilitydistributions.Formoredetailoninformationtheory,seeCoverandThomas2006MacKay()or().2003Thebasicintuitionbehindinformationtheoryisthatlearningthatanunlikelyeventhas occurredismoreinformativethanlearningthata likely eventhasoccurred.Amessagesaying“thesunrosethismorning”issouninformativeastobeunnecessarytosend,butamessagesaying“therewasasolareclipsethismorning”isveryinformative.Wewouldliketoquantifyinformationinawaythatformalizesthisintuition.Speciﬁcally,•Likelyeventsshouldhavelowinformationcontent,andintheextremecase,eventsthatareguaranteedtohappenshouldhavenoinformationcontentwhatsoever.•Lesslikelyeventsshouldhavehigherinformationcontent.•Independenteventsshouldhaveadditiveinformation.Forexample,ﬁndingoutthatatossedcoinhascomeupasheadstwiceshouldconveytwiceasmuchinformationasﬁndingoutthatatossedcoinhascomeupasheadsonce.Inordertosatisfyallthreeoftheseproperties,wedeﬁnetheself-informationofaneventxtobe= xIxPx.() = log−()(3.48)Inthisbook,wealwaysuselogtomeanthenaturallogarithm,withbasee.OurdeﬁnitionofI(x)isthereforewritteninunitsofnats.Onenatistheamountof73 CHAPTER3.PROBABILITYANDINFORMATIONTHEORYinformationgainedbyobservinganeventofprobability1e.Othertextsusebase-2logarithmsandunitscalledbitsorshannons;informationmeasuredinbitsisjustarescalingofinformationmeasuredinnats.Whenxiscontinuous,weusethesamedeﬁnitionofinformationbyanalogy,butsomeofthepropertiesfromthediscretecasearelost.Forexample,aneventwithunitdensitystillhaszeroinformation,despitenotbeinganeventthatisguaranteedtooccur.Self-informationdealsonlywithasingleoutcome.WecanquantifytheamountofuncertaintyinanentireprobabilitydistributionusingtheShannonentropy:H() = xEx∼P[()] = Ix−Ex∼P[log()]Px.(3.49)alsodenotedH(P).Inotherwords,theShannonentropyofadistributionistheexpectedamountofinformationinaneventdrawnfromthatdistribution.Itgivesalowerboundonthenumberofbits(ifthelogarithmisbase2,otherwisetheunitsarediﬀerent)neededonaveragetoencodesymbolsdrawnfromadistributionP.Distributionsthatarenearlydeterministic(wheretheoutcomeisnearlycertain)havelowentropy;distributionsthatareclosertouniformhavehighentropy.Seeﬁgureforademonstration.When3.5xiscontinuous,theShannonentropyisknownasthediﬀerentialentropy.IfwehavetwoseparateprobabilitydistributionsP(x)andQ(x)overthesamerandomvariablex,wecanmeasurehowdiﬀerentthesetwodistributionsareusingtheKullback-Leibler(KL)divergence:DKL() = PQEx∼PlogPx()Qx()= Ex∼P[log()log()]Px−Qx.(3.50)Inthecaseofdiscretevariables,itistheextraamountofinformation(measuredinbitsifweusethebaselogarithm,butinmachinelearningweusuallyusenats2andthenaturallogarithm)neededtosendamessagecontainingsymbolsdrawnfromprobabilitydistributionP,whenweuseacodethatwasdesignedtominimizethelengthofmessagesdrawnfromprobabilitydistribution.QTheKLdivergencehasmanyusefulproperties,mostnotablythatitisnon-negative.TheKLdivergenceis0ifandonlyifPandQarethesamedistributioninthecaseofdiscretevariables,orequal“almosteverywhere”inthecaseofcontinuousvariables.BecausetheKLdivergenceisnon-negativeandmeasuresthediﬀerencebetweentwodistributions,itisoftenconceptualizedasmeasuringsomesortofdistancebetweenthesedistributions.However,itisnotatruedistancemeasurebecauseitisnotsymmetric:DKL(PQ)=DKL(QP)forsomePandQ. This74 CHAPTER3.PROBABILITYANDINFORMATIONTHEORY 000204060810......00.01.02.03.04.05.06.07.Shannonentropyinnats Figure3.5:ThisplotshowshowdistributionsthatareclosertodeterministichavelowShannonentropywhiledistributionsthatareclosetouniformhavehighShannonentropy.Onthehorizontalaxis,weplotp,theprobabilityofabinaryrandomvariablebeingequalto.Theentropyisgivenby1(p−1)log(1−p)−pplog.Whenpisnear0,thedistributionisnearlydeterministic,becausetherandomvariableisnearlyalways0.Whenpisnear1,thedistributionisnearlydeterministic,becausetherandomvariableisnearlyalways1.Whenp= 0.5,theentropyismaximal,becausethedistributionisuniformoverthetwooutcomes.asymmetrymeansthatthereareimportantconsequencestothechoiceofwhethertouseDKL()PQorDKL()QP.Seeﬁgureformoredetail.3.6AquantitythatiscloselyrelatedtotheKLdivergenceisthecross-entropyH(P,Q) =H(P)+DKL(PQ),whichissimilartotheKLdivergencebutlackingthetermontheleft:HP,Q() = −Ex∼Plog()Qx.(3.51)Minimizingthecross-entropywithrespecttoQisequivalenttominimizingtheKLdivergence,becausedoesnotparticipateintheomittedterm.QWhencomputingmanyofthesequantities,itiscommontoencounterexpres-sionsoftheform0log0.Byconvention,inthecontextofinformationtheory,wetreattheseexpressionsaslimx→0xxlog= 0.3.14StructuredProbabilisticModelsMachinelearningalgorithmsofteninvolveprobabilitydistributionsoveraverylargenumberofrandomvariables.Often,theseprobabilitydistributionsinvolvedirectinteractionsbetweenrelativelyfewvariables.Usingasinglefunctionto75 CHAPTER3.PROBABILITYANDINFORMATIONTHEORY xProbability Densityq∗= argminqDKL()pqpx()q∗()x xProbability Densityq∗= argminqDKL()qpp()xq∗()x Figure3.6:TheKLdivergenceisasymmetric.Supposewehaveadistributionp(x)andwishtoapproximateitwithanotherdistributionq(x).WehavethechoiceofminimizingeitherDKL(pq)orDKL(qp).WeillustratetheeﬀectofthischoiceusingamixtureoftwoGaussiansforp,andasingleGaussianforq. ThechoiceofwhichdirectionoftheKLdivergencetouseisproblem-dependent.Someapplicationsrequireanapproximationthatusuallyplaceshighprobabilityanywherethatthetruedistributionplaceshighprobability,whileotherapplicationsrequireanapproximationthatrarelyplaceshighprobabilityanywherethatthetruedistributionplaceslowprobability.ThechoiceofthedirectionoftheKLdivergencereﬂectswhichoftheseconsiderationstakespriorityforeachapplication.(Left)TheeﬀectofminimizingDKL(pq).Inthiscase,weselectaqthathashighprobabilitywherephashighprobability.Whenphasmultiplemodes,qchoosestoblurthemodestogether,inordertoputhighprobabilitymassonallofthem.(Right)TheeﬀectofminimizingDKL(qp).Inthiscase,weselectaqthathaslowprobabilitywherephaslowprobability.Whenphasmultiplemodesthataresuﬃcientlywidelyseparated,asinthisﬁgure,theKLdivergenceisminimizedbychoosingasinglemode,inordertoavoidputtingprobabilitymassinthelow-probabilityareasbetweenmodesofp.Here,weillustratetheoutcomewhenqischosentoemphasizetheleftmode.WecouldalsohaveachievedanequalvalueoftheKLdivergencebychoosingtherightmode.Ifthemodesarenotseparatedbyasuﬃcientlystronglowprobabilityregion,thenthisdirectionoftheKLdivergencecanstillchoosetoblurthemodes.76 CHAPTER3.PROBABILITYANDINFORMATIONTHEORYdescribetheentirejointprobabilitydistributioncanbeveryineﬃcient(bothcomputationallyandstatistically).Insteadofusingasinglefunctiontorepresentaprobabilitydistribution,wecansplitaprobabilitydistributionintomanyfactorsthatwemultiplytogether.Forexample,supposewehavethreerandomvariables:a,bandc.Supposethatainﬂuencesthevalueofbandbinﬂuencesthevalueofc,butthataandcareindependentgivenb.Wecanrepresenttheprobabilitydistributionoverallthreevariablesasaproductofprobabilitydistributionsovertwovariables:p,,ppp.(abc) = ()a()ba|()cb|(3.52)Thesefactorizationscangreatlyreducethenumberofparametersneededtodescribethedistribution.Eachfactorusesanumberofparametersthatisexponentialinthenumberofvariablesinthefactor.Thismeansthatwecangreatlyreducethecostofrepresentingadistributionifweareabletoﬁndafactorizationintodistributionsoverfewervariables.Wecandescribethesekindsoffactorizationsusinggraphs.Hereweusetheword“graph”inthesenseofgraphtheory:asetofverticesthatmaybeconnectedtoeachotherwithedges.Whenwerepresentthefactorizationofaprobabilitydistributionwithagraph,wecallitastructuredprobabilisticmodelorgraphicalmodel.Therearetwomainkindsofstructuredprobabilisticmodels:directedandundirected.BothkindsofgraphicalmodelsuseagraphGinwhicheachnodeinthegraphcorrespondstoarandomvariable, andanedgeconnectingtworandomvariablesmeansthattheprobabilitydistributionisabletorepresentdirectinteractionsbetweenthosetworandomvariables.Directedmodelsuse graphswithdirectededges, andtheyrepresentfac-torizationsintoconditionalprobabilitydistributions,asintheexampleabove.Speciﬁcally,adirectedmodelcontainsonefactorforeveryrandomvariablexiinthedistribution,andthatfactorconsistsoftheconditionaldistributionoverxigiventheparentsofxi,denotedPaG(xi):p() =xip(xi|PaG(xi)).(3.53)Seeﬁgureforanexampleofadirectedgraphandthefactorizationofprobability3.7distributionsitrepresents.Undirectedmodelsusegraphswithundirectededges,andtheyrepresentfactorizationsintoasetoffunctions;unlikeinthedirectedcase,thesefunctions77 CHAPTER3.PROBABILITYANDINFORMATIONTHEORYa ac cb b e ed dFigure3.7:Adirectedgraphicalmodeloverrandomvariablesa,b,c,dande.Thisgraphcorrespondstoprobabilitydistributionsthatcanbefactoredasp,,,,ppp,pp.(abcde) = ()a()ba|(ca|b)()db|()ec|(3.54)Thisgraphallowsustoquicklyseesomepropertiesofthedistribution.Forexample,aandcinteractdirectly,butaandeinteractonlyindirectlyviac.areusuallynotprobabilitydistributionsofanykind.AnysetofnodesthatareallconnectedtoeachotherinGiscalledaclique.EachcliqueC()iinanundirectedmodelisassociatedwithafactorφ()i(C()i).Thesefactorsarejustfunctions,notprobabilitydistributions.Theoutputofeachfactormustbenon-negative,butthereisnoconstraintthatthefactormustsumorintegrateto1likeaprobabilitydistribution.Theprobabilityofaconﬁgurationofrandomvariablesisproportionaltotheproductofallofthesefactors—assignmentsthatresultinlargerfactorvaluesaremorelikely.Ofcourse,thereisnoguaranteethatthisproductwillsumto1.WethereforedividebyanormalizingconstantZ,deﬁnedtobethesumorintegraloverallstatesoftheproductoftheφfunctions,inordertoobtainanormalizedprobabilitydistribution:p() =x1Ziφ()iC()i.(3.55)Seeﬁgureforanexampleofanundirectedgraphandthefactorizationof3.8probabilitydistributionsitrepresents.Keep inmind thatthese graphicalrepresentationsof factorizationsare alanguagefordescribingprobabilitydistributions.Theyarenotmutuallyexclusivefamiliesofprobabilitydistributions.Beingdirectedorundirectedisnotapropertyofaprobabilitydistribution;itisapropertyofaparticulardescriptionofa78 CHAPTER3.PROBABILITYANDINFORMATIONTHEORYa ac cb b e ed dFigure3.8:Anundirectedgraphicalmodeloverrandomvariablesa,b,c,dande.Thisgraphcorrespondstoprobabilitydistributionsthatcanbefactoredasp,,,,(abcde) =1Zφ(1)()abc,,φ(2)()bd,φ(3)()ce,.(3.56)Thisgraphallowsustoquicklyseesomepropertiesofthedistribution.Forexample,aandcinteractdirectly,butaandeinteractonlyindirectlyviac.probabilitydistribution,butanyprobabilitydistributionmaybedescribedinbothways.Throughoutpartsandofthisbook,wewillusestructuredprobabilisticIIImodelsmerelyasalanguagetodescribewhichdirectprobabilisticrelationshipsdiﬀerentmachinelearningalgorithmschoosetorepresent.Nofurtherunderstandingofstructuredprobabilisticmodelsisneededuntilthediscussionofresearchtopics,inpart,wherewewillexplorestructuredprobabilisticmodelsinmuchgreaterIIIdetail.Thischapterhasreviewedthebasicconceptsofprobabilitytheorythataremostrelevanttodeeplearning.Onemoresetoffundamentalmathematicaltoolsremains:numericalmethods. 79 Chapter4NumericalComputationMachinelearningalgorithmsusuallyrequireahighamountofnumericalcompu-tation.Thistypicallyreferstoalgorithmsthatsolvemathematicalproblemsbymethodsthatupdateestimatesofthesolutionviaaniterativeprocess,ratherthananalyticallyderivingaformulaprovidingasymbolicexpressionforthecorrectso-lution.Commonoperationsincludeoptimization(ﬁndingthevalueofanargumentthatminimizesormaximizesafunction)andsolvingsystemsoflinearequations.Evenjustevaluatingamathematicalfunctiononadigitalcomputercanbediﬃcultwhenthefunctioninvolvesrealnumbers,whichcannotberepresentedpreciselyusingaﬁniteamountofmemory.4.1OverﬂowandUnderﬂowThefundamentaldiﬃcultyinperformingcontinuousmathonadigitalcomputeristhatweneedtorepresentinﬁnitelymanyrealnumberswithaﬁnitenumberofbitpatterns.Thismeansthatforalmostallrealnumbers, weincursomeapproximationerrorwhenwerepresentthenumberinthecomputer.Inmanycases,thisisjustroundingerror.Roundingerrorisproblematic,especiallywhenitcompoundsacrossmanyoperations,andcancausealgorithmsthatworkintheorytofailinpracticeiftheyarenotdesignedtominimizetheaccumulationofroundingerror.Oneformofroundingerrorthatisparticularlydevastatingisunderﬂow.Underﬂowoccurswhennumbersnearzeroareroundedtozero.Manyfunctionsbehavequalitativelydiﬀerentlywhentheirargumentiszeroratherthanasmallpositivenumber.Forexample,weusuallywanttoavoiddivisionbyzero(some80 CHAPTER4.NUMERICALCOMPUTATIONsoftwareenvironmentswillraiseexceptionswhenthisoccurs,otherswillreturnaresultwithaplaceholdernot-a-numbervalue)ortakingthelogarithmofzero(thisisusuallytreatedas−∞,whichthenbecomesnot-a-numberifitisusedformanyfurtherarithmeticoperations).Anotherhighlydamagingformofnumericalerrorisoverﬂow.Overﬂowoccurswhennumberswithlargemagnitudeareapproximatedas∞or−∞.Furtherarithmeticwillusuallychangetheseinﬁnitevaluesintonot-a-numbervalues.Oneexampleofafunctionthatmustbestabilizedagainstunderﬂowandoverﬂowisthesoftmaxfunction.Thesoftmaxfunctionisoftenusedtopredicttheprobabilitiesassociatedwithamultinoullidistribution.Thesoftmaxfunctionisdeﬁnedtobesoftmax()xi=exp(xi)nj=1exp(xj).(4.1)Considerwhathappenswhenallofthexiareequaltosomeconstantc.Analytically,wecanseethatalloftheoutputsshouldbeequalto1n.Numerically,thismaynotoccurwhenchaslargemagnitude.Ifcisverynegative,thenexp(c)willunderﬂow.Thismeansthedenominatorofthesoftmaxwillbecome0,sotheﬁnalresultisundeﬁned.Whencisverylargeandpositive,exp(c)willoverﬂow,againresultingintheexpressionasawholebeingundeﬁned.Bothofthesediﬃcultiescanberesolvedbyinsteadevaluatingsoftmax(z)wherez=x−maxixi.Simplealgebrashowsthatthevalueofthesoftmaxfunctionisnotchangedanalyticallybyaddingorsubtractingascalarfromtheinputvector.Subtractingmaxixiresultsinthelargestargumenttoexpbeing0,whichrulesoutthepossibilityofoverﬂow.Likewise,atleastoneterminthedenominatorhasavalueof1,whichrulesoutthepossibilityofunderﬂowinthedenominatorleadingtoadivisionbyzero.Thereisstillonesmallproblem.Underﬂowinthenumeratorcanstillcausetheexpressionasawholetoevaluatetozero.Thismeansthatifweimplementlogsoftmax(x)byﬁrstrunningthesoftmaxsubroutinethenpassingtheresulttothelogfunction,wecoulderroneouslyobtain−∞.Instead,wemustimplementaseparatefunctionthatcalculateslogsoftmaxinanumericallystableway.Thelogsoftmaxfunctioncanbestabilizedusingthesametrickasweusedtostabilizethefunction.softmaxForthemostpart,wedonotexplicitlydetailallofthenumericalconsiderationsinvolvedinimplementingthevariousalgorithmsdescribedinthisbook.Developersoflow-levellibrariesshouldkeepnumericalissuesinmindwhenimplementingdeeplearningalgorithms.Mostreadersofthisbookcansimplyrelyonlow-levellibrariesthatprovidestableimplementations.Insomecases,itispossibletoimplementanewalgorithmandhavethenewimplementationautomatically81 CHAPTER4.NUMERICALCOMPUTATIONstabilized.Theano(,;,)isanexampleBergstraetal.2010Bastienetal.2012ofasoftwarepackagethatautomaticallydetectsandstabilizesmanycommonnumericallyunstableexpressionsthatariseinthecontextofdeeplearning.4.2PoorConditioningConditioningreferstohowrapidlyafunctionchangeswithrespecttosmallchangesinitsinputs.Functionsthatchangerapidlywhentheirinputsareperturbedslightlycanbeproblematicforscientiﬁccomputationbecauseroundingerrorsintheinputscanresultinlargechangesintheoutput.Considerthefunctionf(x)=A−1x.WhenA∈Rnn×hasaneigenvaluedecomposition,itsconditionnumberismaxi,jλiλj.(4.2)Thisistheratioofthemagnitudeofthelargestandsmallesteigenvalue.Whenthisnumberislarge,matrixinversionisparticularlysensitivetoerrorintheinput.Thissensitivityisanintrinsicpropertyofthematrixitself,nottheresultofroundingerrorduringmatrixinversion.Poorlyconditionedmatricesamplifypre-existingerrorswhenwemultiplybythetruematrixinverse.Inpractice,theerrorwillbecompoundedfurtherbynumericalerrorsintheinversionprocessitself.4.3Gradient-BasedOptimizationMostdeeplearningalgorithmsinvolveoptimizationofsomesort. Optimizationreferstothetaskofeitherminimizingormaximizingsomefunctionf(x) byalteringx. Weusuallyphrasemostoptimizationproblemsintermsofminimizingf(x).Maximizationmaybeaccomplishedviaaminimizationalgorithmbyminimizing−f()x.Thefunctionwewanttominimizeormaximizeiscalledtheobjectivefunc-tionorcriterion.Whenweareminimizingit, wemayalsocallitthecostfunction,lossfunction,orerrorfunction. Inthisbook,weusethesetermsinterchangeably,thoughsomemachinelearningpublicationsassignspecialmeaningtosomeoftheseterms.Weoftendenotethevaluethatminimizesormaximizesafunctionwithasuperscript.Forexample,wemightsay∗x∗= argmin()fx.82 CHAPTER4.NUMERICALCOMPUTATION −−−−20.15.10.050005101520......x−20.−15.−10.−05.00.05.10.15.20.Globalminimumat= 0.xSincef() = 0,gradientxdescent haltshere.For0,wehavex<f() 0,x<sowecandecreasebyfmoving rightward.For0,wehavex>f() 0,x>sowecandecreasebyfmoving leftward.fx() =12x2f() = xxFigure4.1:Anillustrationofhowthegradientdescentalgorithmusesthederivativesofafunctioncanbeusedtofollowthefunctiondownhilltoaminimum.Weassumethereaderisalreadyfamiliarwithcalculus,butprovideabriefreviewofhowcalculusconceptsrelatetooptimizationhere.Supposewehaveafunctiony=f(x),wherebothxandyarerealnumbers.Thederivativeofthisfunctionisdenotedasf(x)orasdydx.Thederivativef(x)givestheslopeoff(x)atthepointx.Inotherwords,itspeciﬁeshowtoscaleasmallchangeintheinputinordertoobtainthecorrespondingchangeintheoutput:fxfxf(+) ≈()+()x.Thederivativeisthereforeusefulforminimizingafunctionbecauseittellsushowtochangexinordertomakeasmallimprovementiny.Forexample,weknowthatf(x−sign(f(x)))islessthanf(x)forsmallenough.Wecanthusreducef(x)bymovingxinsmallstepswithoppositesignofthederivative.Thistechniqueiscalledgradientdescent(Cauchy1847,).Seeﬁgureforan4.1exampleofthistechnique.Whenf(x) = 0,thederivativeprovidesnoinformationaboutwhichdirectiontomove.Pointswheref(x)=0areknownascriticalpointsorstationarypoints.Alocalminimumisapointwheref(x)islowerthanatallneighboringpoints,soitisnolongerpossibletodecreasef(x)bymakinginﬁnitesimalsteps.Alocalmaximumisapointwheref(x)ishigherthanatallneighboringpoints,83 CHAPTER4.NUMERICALCOMPUTATIONMinimumMaximumSaddlepoint Figure4.2:Examplesofeachofthethreetypesofcriticalpointsin1-D.Acriticalpointisapointwithzeroslope.Suchapointcaneitherbealocalminimum,whichislowerthantheneighboringpoints,alocalmaximum,whichishigherthantheneighboringpoints,orasaddlepoint,whichhasneighborsthatarebothhigherandlowerthanthepointitself.soitisnotpossibletoincreasef(x)bymakinginﬁnitesimalsteps.Somecriticalpointsareneithermaximanorminima.Theseareknownassaddlepoints.Seeﬁgureforexamplesofeachtypeofcriticalpoint.4.2Apointthatobtainstheabsolutelowestvalueoff(x)isaglobalminimum.Itispossiblefortheretobeonlyoneglobalminimumormultipleglobalminimaofthefunction.Itisalsopossiblefortheretobelocalminimathatarenotgloballyoptimal.Inthecontextofdeeplearning,weoptimizefunctionsthatmayhavemanylocalminimathatarenotoptimal,andmanysaddlepointssurroundedbyveryﬂatregions.Allofthismakesoptimizationverydiﬃcult,especiallywhentheinputtothefunctionismultidimensional.Wethereforeusuallysettleforﬁndingavalueoffthatisverylow,butnotnecessarilyminimalinanyformalsense.Seeﬁgureforanexample.4.3Weoftenminimizefunctionsthathavemultipleinputs:f:Rn→R.Fortheconceptof“minimization” tomakesense,theremuststillbeonlyone(scalar)output.Forfunctionswithmultipleinputs,wemustmakeuseoftheconceptofpartialderivatives.Thepartialderivative∂∂xif(x)measureshowfchangesasonlythevariablexiincreasesatpointx.Thegradientgeneralizesthenotionofderivativetothecasewherethederivativeiswithrespecttoavector:thegradientoffisthevectorcontainingallofthepartialderivatives,denoted∇xf(x).Elementiofthegradientisthepartialderivativeoffwithrespecttoxi.Inmultipledimensions,84 CHAPTER4.NUMERICALCOMPUTATION xfx()Ideally,wewouldliketoarriveattheglobalminimum, butthismight notbepossible.Thislocalminimumperformsnearlyaswellastheglobalone,soitisanacceptablehaltingpoint.Thislocalminimumperformspoorlyandshouldbeavoided.Figure4.3:Optimizationalgorithmsmayfailtoﬁndaglobalminimumwhentherearemultiplelocalminimaorplateauspresent.Inthecontextofdeeplearning,wegenerallyacceptsuchsolutionseventhoughtheyarenottrulyminimal,solongastheycorrespondtosigniﬁcantlylowvaluesofthecostfunction.criticalpointsarepointswhereeveryelementofthegradientisequaltozero.Thedirectionalderivativeindirection(aunitvector)istheslopeoftheufunctionfindirectionu.Inotherwords,thedirectionalderivativeisthederivativeofthefunctionf(x+αu)withrespecttoα,evaluatedatα= 0.Usingthechainrule,wecanseethat∂∂αfα(+xu)evaluatestou∇xfα()xwhen= 0.Tominimizef,wewouldliketoﬁndthedirectioninwhichfdecreasesthefastest.Wecandothisusingthedirectionalderivative:minuu,u=1u∇xf()x(4.3)=minuu,u=1||||u2||∇xf()x||2cosθ(4.4)whereθistheanglebetweenuandthegradient.Substitutingin||||u2= 1andignoringfactorsthatdonotdependonu,thissimpliﬁestominucosθ.Thisisminimizedwhenupointsintheoppositedirectionasthegradient.Inotherwords,thegradientpointsdirectlyuphill,andthenegativegradientpointsdirectlydownhill.Wecandecreasefbymovinginthedirectionofthenegativegradient.Thisisknownastheor.methodofsteepestdescentgradientdescentSteepestdescentproposesanewpointx= x−∇xf()x(4.5)85 CHAPTER4.NUMERICALCOMPUTATIONwhereisthelearningrate,apositivescalardeterminingthesizeofthestep.Wecanchooseinseveraldiﬀerentways.Apopularapproachistosettoasmallconstant.Sometimes,wecansolveforthestepsizethatmakesthedirectionalderivativevanish.Anotherapproachistoevaluatef(x−∇xf())xforseveralvaluesofandchoosetheonethatresultsinthesmallestobjectivefunctionvalue.Thislaststrategyiscalledalinesearch.Steepestdescentconvergeswheneveryelementofthegradientiszero(or,inpractice,veryclosetozero).Insomecases,wemaybeabletoavoidrunningthisiterativealgorithm,andjustjumpdirectlytothecriticalpointbysolvingtheequation∇xf() = 0xfor.xAlthoughgradientdescentislimitedtooptimizationincontinuousspaces,thegeneralconceptofrepeatedlymakingasmallmove(thatisapproximatelythebestsmallmove)towardsbetterconﬁgurationscanbegeneralizedtodiscretespaces.Ascendinganobjectivefunctionofdiscreteparametersiscalledhillclimbing(,).RusselandNorvig20034.3.1BeyondtheGradient:JacobianandHessianMatricesSometimesweneedtoﬁndallofthepartialderivativesofafunctionwhoseinputandoutputarebothvectors.ThematrixcontainingallsuchpartialderivativesisknownasaJacobianmatrix.Speciﬁcally,ifwehaveafunctionf:Rm→Rn,thentheJacobianmatrixJ∈Rnm×ofisdeﬁnedsuchthatfJi,j=∂∂xjf()xi.Wearealsosometimesinterestedinaderivativeofaderivative.Thisisknownasasecondderivative.Forexample,forafunctionf:Rn→R,thederivativewithrespecttoxiofthederivativeoffwithrespecttoxjisdenotedas∂2∂xi∂xjf.Inasingledimension,wecandenoted2dx2fbyf(x).Thesecondderivativetellsushowtheﬁrstderivativewillchangeaswevarytheinput.Thisisimportantbecauseittellsuswhetheragradientstepwillcauseasmuchofanimprovementaswewouldexpectbasedonthegradientalone.Wecanthinkofthesecondderivativeasmeasuringcurvature.Supposewehaveaquadraticfunction(manyfunctionsthatariseinpracticearenotquadraticbutcanbeapproximatedwellasquadratic,atleastlocally).Ifsuchafunctionhasasecondderivativeofzero,thenthereisnocurvature.Itisaperfectlyﬂatline,anditsvaluecanbepredictedusingonlythegradient.Ifthegradientis,thenwecanmakeastepofsize1alongthenegativegradient,andthecostfunctionwilldecreaseby.Ifthesecondderivativeisnegative,thefunctioncurvesdownward,sothecostfunctionwillactuallydecreasebymorethan.Finally,ifthesecondderivativeispositive,thefunctioncurvesupward,sothecostfunctioncandecreasebylessthan.See86 CHAPTER4.NUMERICALCOMPUTATION xfx()Negativecurvature xfx()Nocurvature xfx()Positivecurvature Figure4.4:Thesecondderivativedeterminesthecurvatureofafunction.Hereweshowquadraticfunctionswithvariouscurvature.Thedashedlineindicatesthevalueofthecostfunctionwewouldexpectbasedonthegradientinformationaloneaswemakeagradientstepdownhill.Inthecaseofnegativecurvature,thecostfunctionactuallydecreasesfasterthanthegradientpredicts.Inthecaseofnocurvature,thegradientpredictsthedecreasecorrectly.Inthecaseofpositivecurvature,thefunctiondecreasesslowerthanexpectedandeventuallybeginstoincrease,sostepsthataretoolargecanactuallyincreasethefunctioninadvertently.ﬁguretoseehowdiﬀerentformsofcurvatureaﬀecttherelationshipbetween4.4thevalueofthecostfunctionpredictedbythegradientandthetruevalue.Whenourfunctionhasmultipleinputdimensions,therearemanysecondderivatives.ThesederivativescanbecollectedtogetherintoamatrixcalledtheHessianmatrix.TheHessianmatrixisdeﬁnedsuchthatHx()(f)Hx()(f)i,j=∂2∂xi∂xjf.()x(4.6)Equivalently,theHessianistheJacobianofthegradient.Anywherethatthesecondpartialderivativesarecontinuous,thediﬀerentialoperatorsarecommutative,i.e.theirordercanbeswapped:∂2∂xi∂xjf() =x∂2∂xj∂xif.()x(4.7)ThisimpliesthatHi,j=Hj,i,sotheHessianmatrixissymmetricatsuchpoints.MostofthefunctionsweencounterinthecontextofdeeplearninghaveasymmetricHessianalmosteverywhere. BecausetheHessianmatrixisrealandsymmetric,wecandecomposeitintoasetofrealeigenvaluesandanorthogonalbasisof87 CHAPTER4.NUMERICALCOMPUTATIONeigenvectors.ThesecondderivativeinaspeciﬁcdirectionrepresentedbyaunitvectordisgivenbydHd.WhendisaneigenvectorofH,thesecondderivativeinthatdirectionisgivenbythecorrespondingeigenvalue.Forotherdirectionsofd,thedirectionalsecondderivativeisaweightedaverageofalloftheeigenvalues,withweightsbetween0and1,andeigenvectorsthathavesmalleranglewithdreceivingmoreweight.Themaximumeigenvaluedeterminesthemaximumsecondderivativeandtheminimumeigenvaluedeterminestheminimumsecondderivative.The(directional)secondderivativetellsushowwellwecanexpectagradientdescentsteptoperform.Wecanmakeasecond-orderTaylorseriesapproximationtothefunctionaroundthecurrentpointf()xx(0):ff() x≈(x(0))+(xx−(0))g+12(xx−(0))Hxx(−(0)).(4.8)wheregisthegradientandHistheHessianatx(0). Ifweusealearningrateof,thenthenewpointxwillbegivenbyx(0)−g.Substitutingthisintoourapproximation,weobtainf(x(0)−≈g) f(x(0))−gg+122gHg.(4.9)Therearethree termshere:theoriginalvalue ofthefunction, theexpectedimprovementduetotheslopeofthefunction,andthecorrectionwemustapplytoaccountforthecurvatureofthefunction.Whenthislasttermistoolarge,thegradientdescentstepcanactuallymoveuphill.WhengHgiszeroornegative,theTaylorseriesapproximationpredictsthatincreasingforeverwilldecreasefforever.Inpractice,theTaylorseriesisunlikelytoremainaccurateforlarge,soonemustresorttomoreheuristicchoicesofinthiscase.WhengHgispositive,solvingfortheoptimalstepsizethatdecreasestheTaylorseriesapproximationofthefunctionthemostyields∗=gggHg.(4.10)Intheworstcase,whengalignswiththeeigenvectorofHcorrespondingtothemaximaleigenvalueλmax,thenthisoptimalstepsizeisgivenby1λmax.Totheextentthatthefunctionweminimizecanbeapproximatedwellbyaquadraticfunction,theeigenvaluesoftheHessianthusdeterminethescaleofthelearningrate.Thesecondderivativecanbeusedtodeterminewhetheracriticalpointisalocalmaximum,alocalminimum,orsaddlepoint.Recallthatonacriticalpoint,f(x) = 0.Whenthesecondderivativef(x)>0,theﬁrstderivativef(x)increasesaswemovetotherightanddecreasesaswemovetotheleft.Thismeans88 CHAPTER4.NUMERICALCOMPUTATIONf(x−)<0andf(x+)>0forsmallenough.Inotherwords,aswemoveright,theslopebeginstopointuphilltotheright,andaswemoveleft,theslopebeginstopointuphilltotheleft. Thus,whenf(x)=0andf(x)>0,wecanconcludethatxisalocalminimum.Similarly,whenf(x) = 0andf(x)<0,wecanconcludethatxisalocalmaximum.Thisisknownasthesecondderivativetest.Unfortunately,whenf(x) = 0,thetestisinconclusive.Inthiscasexmaybeasaddlepoint,orapartofaﬂatregion.Inmultipledimensions,weneedtoexamineallofthesecondderivativesofthefunction.UsingtheeigendecompositionoftheHessianmatrix,wecangeneralizethesecondderivativetesttomultipledimensions.Atacriticalpoint,where∇xf(x) = 0,wecanexaminetheeigenvaluesoftheHessiantodeterminewhetherthecriticalpointisalocalmaximum,localminimum,orsaddlepoint.WhentheHessianispositivedeﬁnite(allitseigenvaluesarepositive),thepointisalocalminimum.Thiscanbeseenbyobservingthatthedirectionalsecondderivativeinanydirectionmustbepositive,andmakingreferencetotheunivariatesecondderivativetest.Likewise,whentheHessianisnegativedeﬁnite(allitseigenvaluesarenegative),thepointisalocalmaximum.Inmultipledimensions,itisactuallypossibletoﬁndpositiveevidenceofsaddlepointsinsomecases. Whenatleastoneeigenvalueispositiveandatleastoneeigenvalueisnegative,weknowthatxisalocalmaximumononecrosssectionoffbutalocalminimumonanothercrosssection.Seeﬁgureforanexample.Finally,themultidimensionalsecond4.5derivativetestcanbeinconclusive,justliketheunivariateversion.Thetestisinconclusivewheneverallofthenon-zeroeigenvalueshavethesamesign,butatleastoneeigenvalueiszero.Thisisbecausetheunivariatesecondderivativetestisinconclusiveinthecrosssectioncorrespondingtothezeroeigenvalue.Inmultipledimensions,thereisadiﬀerentsecondderivativeforeachdirectionatasinglepoint.TheconditionnumberoftheHessianatthispointmeasureshowmuchthesecondderivativesdiﬀerfromeachother.WhentheHessianhasapoorconditionnumber,gradientdescentperformspoorly.Thisisbecauseinonedirection,thederivativeincreasesrapidly,whileinanotherdirection,itincreasesslowly.Gradientdescentisunawareofthischangeinthederivativesoitdoesnotknowthatitneedstoexplorepreferentiallyinthedirectionwherethederivativeremainsnegativeforlonger.Italsomakesitdiﬃculttochooseagoodstepsize.Thestepsizemustbesmallenoughtoavoidovershootingtheminimumandgoinguphillindirectionswithstrongpositivecurvature.Thisusuallymeansthatthestepsizeistoosmalltomakesigniﬁcantprogressinotherdirectionswithlesscurvature.Seeﬁgureforanexample.4.6ThisissuecanberesolvedbyusinginformationfromtheHessianmatrixtoguide89 CHAPTER4.NUMERICALCOMPUTATION  Figure4.5:Asaddlepointcontainingbothpositiveandnegativecurvature.Thefunctioninthisexampleisf(x)=x21−x22.Alongtheaxiscorrespondingtox1,thefunctioncurvesupward.ThisaxisisaneigenvectoroftheHessianandhasapositiveeigenvalue.Alongtheaxiscorrespondingtox2,thefunctioncurvesdownward.ThisdirectionisaneigenvectoroftheHessianwithnegativeeigenvalue.Thename“saddlepoint”derivesfromthesaddle-likeshapeofthisfunction.Thisisthequintessentialexampleofafunctionwithasaddlepoint.Inmorethanonedimension,itisnotnecessarytohaveaneigenvalueof0inordertogetasaddlepoint:itisonlynecessarytohavebothpositiveandnegativeeigenvalues.Wecanthinkofasaddlepointwithbothsignsofeigenvaluesasbeingalocalmaximumwithinonecrosssectionandalocalminimumwithinanothercrosssection. 90 CHAPTER4.NUMERICALCOMPUTATION −−−30201001020x1−30−20−1001020x2 Figure4.6:GradientdescentfailstoexploitthecurvatureinformationcontainedintheHessianmatrix.Hereweusegradientdescenttominimizeaquadraticfunctionf(x) whoseHessianmatrixhasconditionnumber5.Thismeansthatthedirectionofmostcurvaturehasﬁvetimesmorecurvaturethanthedirectionofleastcurvature.Inthiscase,themostcurvatureisinthedirection[1,1]andtheleastcurvatureisinthedirection[1,−1].Theredlinesindicatethepathfollowedbygradientdescent.Thisveryelongatedquadraticfunctionresemblesalongcanyon.Gradientdescentwastestimerepeatedlydescendingcanyonwalls,becausetheyarethesteepestfeature.Becausethestepsizeissomewhattoolarge,ithasatendencytoovershootthebottomofthefunctionandthusneedstodescendtheoppositecanyonwallonthenextiteration.ThelargepositiveeigenvalueoftheHessiancorrespondingtotheeigenvectorpointedinthisdirectionindicatesthatthisdirectionalderivativeisrapidlyincreasing,soanoptimizationalgorithmbasedontheHessiancouldpredictthatthesteepestdirectionisnotactuallyapromisingsearchdirectioninthiscontext. 91 CHAPTER4.NUMERICALCOMPUTATIONthesearch.ThesimplestmethodfordoingsoisknownasNewton’smethod.Newton’smethodisbasedonusingasecond-orderTaylorseriesexpansiontoapproximatenearsomepointf()xx(0):ff() x≈(x(0))+(xx−(0))∇xf(x(0))+12(xx−(0))Hx()(f(0))(xx−(0)).(4.11)Ifwethensolveforthecriticalpointofthisfunction,weobtain:x∗= x(0)−Hx()(f(0))−1∇xf(x(0)).(4.12)Whenfisapositivedeﬁnitequadraticfunction,Newton’smethodconsistsofapplyingequationoncetojumptotheminimumofthefunctiondirectly.4.12Whenfisnottrulyquadraticbutcanbelocallyapproximatedasapositivedeﬁnitequadratic,Newton’smethodconsistsofapplyingequationmultiple4.12times. Iterativelyupdatingtheapproximationandjumpingtotheminimumoftheapproximationcanreachthecriticalpointmuchfasterthangradientdescentwould.Thisisausefulpropertynearalocalminimum,butitcanbeaharmfulpropertynearasaddlepoint.Asdiscussedinsection,Newton’smethodis8.2.3onlyappropriatewhenthenearbycriticalpointisaminimum(alltheeigenvaluesoftheHessianarepositive),whereasgradientdescentisnotattractedtosaddlepointsunlessthegradientpointstowardthem.Optimizationalgorithmsthatuseonlythegradient,suchasgradientdescent,arecalledﬁrst-orderoptimizationalgorithms.OptimizationalgorithmsthatalsousetheHessianmatrix,suchasNewton’smethod,arecalledsecond-orderoptimizationalgorithms(NocedalandWright2006,).The optimizationalgorithms employedin mostcontextsin this book areapplicabletoawidevarietyoffunctions,butcomewithalmostnoguarantees.Deeplearningalgorithmstendtolackguaranteesbecausethefamilyoffunctionsusedindeeplearningisquitecomplicated.Inmanyotherﬁelds,thedominantapproachtooptimizationistodesignoptimizationalgorithmsforalimitedfamilyoffunctions.Inthecontextofdeeplearning,wesometimesgainsomeguaranteesbyrestrict-ingourselvestofunctionsthatareeitherLipschitzcontinuousorhaveLipschitzcontinuousderivatives.ALipschitzcontinuousfunctionisafunctionfwhoserateofchangeisboundedbyaLipschitzconstantL:∀∀|−|≤L||−||x,y,f()xf()yxy2.(4.13)Thispropertyisusefulbecauseitallowsustoquantifyourassumptionthatasmallchangeintheinputmadebyanalgorithmsuchasgradientdescentwillhave92 CHAPTER4.NUMERICALCOMPUTATIONasmallchangeintheoutput.Lipschitzcontinuityisalsoafairlyweakconstraint,andmanyoptimizationproblemsindeeplearningcanbemadeLipschitzcontinuouswithrelativelyminormodiﬁcations.Perhapsthemostsuccessfulﬁeldofspecializedoptimizationisconvexop-timization.Convexoptimizationalgorithmsareabletoprovidemanymoreguaranteesbymakingstrongerrestrictions.Convexoptimizationalgorithmsareapplicableonlytoconvexfunctions—functionsforwhichtheHessianispositivesemideﬁniteeverywhere.Suchfunctionsarewell-behavedbecausetheylacksaddlepointsandalloftheirlocalminimaarenecessarilyglobalminima.However,mostproblemsindeeplearningarediﬃculttoexpressintermsofconvexoptimization.Convexoptimizationisusedonlyasasubroutineofsomedeeplearningalgorithms.Ideasfromtheanalysisofconvexoptimizationalgorithmscanbeusefulforprovingtheconvergenceofdeeplearningalgorithms.However,ingeneral,theimportanceofconvexoptimizationisgreatlydiminishedinthecontextofdeeplearning.Formoreinformationaboutconvexoptimization,seeBoydandVandenberghe2004()orRockafellar1997().4.4ConstrainedOptimizationSometimeswewishnotonlytomaximizeorminimizeafunctionf(x)overallpossible values ofx.Insteadwemay wishto ﬁnd themaximal or minimalvalue off(x)for valuesofxinsome setS.Thisis known asconstrainedoptimization.PointsxthatliewithinthesetSarecalledfeasiblepointsinconstrainedoptimizationterminology.Weoftenwishtoﬁndasolutionthatissmallinsomesense.Acommonapproachinsuchsituationsistoimposeanormconstraint,suchas.||||≤x1Onesimpleapproachtoconstrainedoptimizationissimplytomodifygradientdescenttakingtheconstraintintoaccount.Ifweuseasmallconstantstepsize,wecanmakegradientdescentsteps,thenprojecttheresultbackintoS.Ifweusealinesearch,wecansearchonlyoverstepsizesthatyieldnewxpointsthatarefeasible,orwecanprojecteachpointonthelinebackintotheconstraintregion.Whenpossible,thismethodcanbemademoreeﬃcientbyprojectingthegradientintothetangentspaceofthefeasibleregionbeforetakingthesteporbeginningthelinesearch(,).Rosen1960Amoresophisticatedapproachistodesignadiﬀerent,unconstrainedopti-mizationproblemwhosesolutioncanbeconvertedintoasolutiontotheoriginal,constrainedoptimizationproblem.Forexample,ifwewanttominimizef(x)for93 CHAPTER4.NUMERICALCOMPUTATIONx∈R2withxconstrainedtohaveexactlyunitL2norm,wecaninsteadminimizeg(θ) =f([cossinθ,θ])withrespecttoθ,thenreturn[cossinθ,θ]asthesolutiontotheoriginalproblem.Thisapproachrequirescreativity;thetransformationbetweenoptimizationproblemsmustbedesignedspeciﬁcallyforeachcaseweencounter.TheKarush–Kuhn–Tucker(KKT)approach1providesaverygeneralso-lutiontoconstrainedoptimization.WiththeKKTapproach,weintroduceanewfunctioncalledthegeneralizedLagrangianorgeneralizedLagrangefunction.TodeﬁnetheLagrangian,weﬁrstneedtodescribeSintermsofequationsandinequalities. WewantadescriptionofSintermsofmfunctionsg()iandnfunctionsh()jsothatS={|∀xi,g()i(x) = 0and∀j,h()j(x)≤0}.Theequationsinvolvingg()iarecalledtheequalityconstraintsandtheinequalitiesinvolvingh()jarecalled.inequalityconstraintsWeintroducenewvariablesλiandαjforeachconstraint,thesearecalledtheKKTmultipliers.ThegeneralizedLagrangianisthendeﬁnedasL,,f(xλα) = ()+xiλig()i()+xjαjh()j()x.(4.14)WecannowsolveaconstrainedminimizationproblemusingunconstrainedoptimizationofthegeneralizedLagrangian.Observethat,solongasatleastonefeasiblepointexistsandisnotpermittedtohavevalue,thenf()x∞minxmaxλmaxαα,≥0L,,.(xλα)(4.15)hasthesameoptimalobjectivefunctionvalueandsetofoptimalpointsasxminx∈Sf.()x(4.16)Thisfollowsbecauseanytimetheconstraintsaresatisﬁed,maxλmaxαα,≥0L,,f,(xλα) = ()x(4.17)whileanytimeaconstraintisviolated,maxλmaxαα,≥0L,,.(xλα) = ∞(4.18)1TheKKTapproachgeneralizesthemethodofLagrangemultiplierswhichallowsequalityconstraintsbutnotinequalityconstraints.94 CHAPTER4.NUMERICALCOMPUTATIONThesepropertiesguaranteethatnoinfeasiblepointcanbeoptimal,andthattheoptimumwithinthefeasiblepointsisunchanged.Toperformconstrainedmaximization,wecanconstructthegeneralizedLa-grangefunctionof,whichleadstothisoptimizationproblem:−f()xminxmaxλmaxαα,≥0−f()+xiλig()i()+xjαjh()j()x.(4.19)Wemayalsoconvertthistoaproblemwithmaximizationintheouterloop:maxxminλminαα,≥0f()+xiλig()i()x−jαjh()j()x.(4.20)Thesignofthetermfortheequalityconstraintsdoesnotmatter;wemaydeﬁneitwithadditionorsubtractionaswewish,becausetheoptimizationisfreetochooseanysignforeachλi.Theinequalityconstraintsareparticularlyinteresting.Wesaythataconstrainth()i(x)isactiveifh()i(x∗) = 0.Ifaconstraintisnotactive,thenthesolutiontotheproblemfoundusingthatconstraintwouldremainatleastalocalsolutionifthatconstraintwereremoved.Itispossiblethataninactiveconstraintexcludesothersolutions.Forexample,aconvexproblemwithanentireregionofgloballyoptimalpoints(awide,ﬂat,regionofequalcost)couldhaveasubsetofthisregioneliminatedbyconstraints,oranon-convexproblemcouldhavebetterlocalstationarypointsexcludedbyaconstraintthatisinactiveatconvergence.However,thepointfoundatconvergenceremainsastationarypointwhetherornottheinactiveconstraintsareincluded.Becauseaninactiveh()ihasnegativevalue,thenthesolutiontominxmaxλmaxαα,≥0L(xλα,,)willhaveαi=0.Wecanthusobservethatatthesolution,αh(x)=0.Inotherwords,foralli,weknowthatatleastoneoftheconstraintsαi≥0andh()i(x)≤0mustbeactiveatthesolution.Togainsomeintuitionforthisidea,wecansaythateitherthesolutionisontheboundaryimposedbytheinequalityandwemustuseitsKKTmultipliertoinﬂuencethesolutiontox,ortheinequalityhasnoinﬂuenceonthesolutionandwerepresentthisbyzeroingoutitsKKTmultiplier.Asimplesetofpropertiesdescribetheoptimalpointsofconstrainedopti-mizationproblems.ThesepropertiesarecalledtheKarush-Kuhn-Tucker(KKT)conditions(,;Karush1939KuhnandTucker1951,).Theyarenecessaryconditions,butnotalwayssuﬃcientconditions,forapointtobeoptimal.Theconditionsare:•ThegradientofthegeneralizedLagrangianiszero.•AllconstraintsonbothandtheKKTmultipliersaresatisﬁed.x95 CHAPTER4.NUMERICALCOMPUTATION•Theinequalityconstraintsexhibit“complementaryslackness”:αh(x) =0.FormoreinformationabouttheKKTapproach,seeNocedalandWright2006().4.5Example:LinearLeastSquaresSupposewewanttoﬁndthevalueofthatminimizesxf() =x12||−||Axb22.(4.21)Therearespecializedlinearalgebraalgorithmsthatcansolvethisproblemeﬃciently.However,wecanalsoexplorehowtosolveitusinggradient-basedoptimizationasasimpleexampleofhowthesetechniqueswork.First,weneedtoobtainthegradient:∇xf() = xA() = Axb−AAxA−b.(4.22)Wecanthenfollowthisgradientdownhill,takingsmallsteps.Seealgorithm4.1fordetails.Algorithm4.1Analgorithmtominimizef(x) =12||−||Axb22withrespecttoxusinggradientdescent,startingfromanarbitraryvalueof.xSetthestepsize()andtolerance()tosmall,positivenumbers.δwhile||AAxA−b||2>δdoxx←−AAxA−bendwhileOnecanalsosolvethisproblemusingNewton’smethod.Inthiscase,becausethetruefunctionisquadratic,thequadraticapproximationemployedbyNewton’smethodisexact,andthealgorithmconvergestotheglobalminimuminasinglestep.Nowsuppose we wishto minimizethesame function,butsubjectto theconstraintxx≤1.Todoso,weintroducetheLagrangianL,λfλ(x) = ()+xxx−1.(4.23)Wecannowsolvetheproblemminxmaxλ,λ≥0L,λ.(x)(4.24)96 CHAPTER4.NUMERICALCOMPUTATIONThesmallest-normsolutiontotheunconstrainedleastsquaresproblemmaybefoundusingtheMoore-Penrosepseudoinverse:x=A+b.Ifthispointisfeasible,thenitisthesolutiontotheconstrainedproblem.Otherwise,wemustﬁndasolutionwheretheconstraintisactive.BydiﬀerentiatingtheLagrangianwithrespectto,weobtaintheequationxAAxA−bx+2λ= 0.(4.25)ThistellsusthatthesolutionwilltaketheformxA= (AI+2λ)−1Ab.(4.26)Themagnitudeofλmustbechosensuchthattheresultobeystheconstraint.Wecanﬁndthisvaluebyperforminggradientascenton.Todoso,observeλ∂∂λL,λ(x) = xx−1.(4.27)Whenthenormofxexceeds1,thisderivativeispositive,sotofollowthederivativeuphillandincreasetheLagrangianwithrespecttoλ,weincreaseλ.Becausethecoeﬃcientonthexxpenaltyhasincreased,solvingthelinearequationforxwillnowyieldasolutionwithsmallernorm.Theprocessofsolvingthelinearequationandadjustingλcontinuesuntilxhasthecorrectnormandthederivativeonλis0.Thisconcludesthemathematicalpreliminariesthatweusetodevelopmachinelearningalgorithms.Wearenowreadytobuildandanalyzesomefull-ﬂedgedlearningsystems. 97 Chapter5MachineLearningBasicsDeeplearningisaspeciﬁckindofmachinelearning.Inordertounderstanddeeplearningwell,onemusthaveasolidunderstandingofthebasicprinciplesofmachinelearning.Thischapterprovidesabriefcourseinthemostimportantgeneralprinciplesthatwillbeappliedthroughouttherestofthebook.Novicereadersorthosewhowantawiderperspectiveareencouragedtoconsidermachinelearningtextbookswithamorecomprehensivecoverageofthefundamentals,suchasMurphy()or().Ifyouarealreadyfamiliarwithmachinelearningbasics,2012Bishop2006feelfreetoskipaheadtosection.Thatsectioncoverssomeperspectives5.11on traditionalmachinelearning techniques thathavestrongly inﬂuencedthedevelopmentofdeeplearningalgorithms.Webeginwithadeﬁnitionofwhatalearningalgorithmis,andpresentanexample:thelinearregressionalgorithm. Wethenproceedtodescribehowthechallengeofﬁttingthetrainingdatadiﬀersfromthechallengeofﬁndingpatternsthatgeneralizetonewdata.Mostmachinelearningalgorithmshavesettingscalledhyperparametersthatmustbedeterminedexternaltothelearningalgorithmitself;wediscusshowtosettheseusingadditionaldata.Machinelearningisessentiallyaformofappliedstatisticswithincreasedemphasisontheuseofcomputerstostatisticallyestimatecomplicatedfunctionsandadecreasedemphasisonprovingconﬁdenceintervalsaroundthesefunctions;wethereforepresentthetwocentralapproachestostatistics:frequentistestimatorsandBayesianinference.Mostmachinelearningalgorithmscanbedividedintothecategoriesofsupervisedlearningandunsupervisedlearning;wedescribethesecategoriesandgivesomeexamplesofsimplelearningalgorithmsfromeachcategory. Mostdeeplearningalgorithmsare basedonan optimizationalgorithmcalled stochasticgradientdescent.Wedescribehowtocombinevariousalgorithmcomponentssuchas98 CHAPTER5.MACHINELEARNINGBASICSanoptimizationalgorithm,acostfunction,amodel,andadatasettobuildamachinelearningalgorithm.Finally,insection,wedescribesomeofthe5.11factorsthathavelimitedtheabilityoftraditionalmachinelearningtogeneralize.Thesechallengeshavemotivatedthedevelopmentofdeeplearningalgorithmsthatovercometheseobstacles.5.1LearningAlgorithmsAmachinelearningalgorithmisanalgorithmthatisabletolearnfromdata.Butwhatdowemeanbylearning?Mitchell1997()providesthedeﬁnition“AcomputerprogramissaidtolearnfromexperienceEwithrespecttosomeclassoftasksTandperformancemeasureP,ifitsperformanceattasksinT,asmeasuredbyP,improveswithexperienceE.”OnecanimagineaverywidevarietyofexperiencesE,tasksT,andperformancemeasuresP,andwedonotmakeanyattemptinthisbooktoprovideaformaldeﬁnitionofwhatmaybeusedforeachoftheseentities.Instead,thefollowingsectionsprovideintuitivedescriptionsandexamplesofthediﬀerentkindsoftasks,performancemeasuresandexperiencesthatcanbeusedtoconstructmachinelearningalgorithms.5.1.1TheTask,TMachinelearningallowsustotackletasksthataretoodiﬃculttosolvewithﬁxedprogramswrittenanddesignedbyhumanbeings.Fromascientiﬁcandphilosophicalpointofview,machinelearningisinterestingbecausedevelopingourunderstandingofmachinelearningentailsdevelopingourunderstandingoftheprinciplesthatunderlieintelligence.Inthisrelativelyformaldeﬁnitionoftheword“task,”theprocessoflearningitselfisnotthetask.Learningisourmeansofattainingtheabilitytoperformthetask.Forexample,ifwewantarobottobeabletowalk,thenwalkingisthetask.Wecouldprogramtherobottolearntowalk,orwecouldattempttodirectlywriteaprogramthatspeciﬁeshowtowalkmanually.Machinelearningtasksareusuallydescribedintermsofhowthemachinelearningsystemshouldprocessanexample.Anexampleisacollectionoffeaturesthathavebeenquantitativelymeasuredfromsomeobjectoreventthatwewantthemachinelearningsystemtoprocess.Wetypicallyrepresentanexampleasavectorx∈Rnwhereeachentryxiofthevectorisanotherfeature.Forexample,thefeaturesofanimageareusuallythevaluesofthepixelsintheimage.99 CHAPTER5.MACHINELEARNINGBASICSManykindsoftaskscanbesolvedwithmachinelearning.Someofthemostcommonmachinelearningtasksincludethefollowing:•Classiﬁcation:Inthistypeoftask,thecomputerprogramisaskedtospecifywhichofkcategoriessomeinputbelongsto.Tosolvethistask,thelearningalgorithmisusuallyaskedtoproduceafunctionf:Rn→{1,...,k}.Wheny=f(x),themodelassignsaninputdescribedbyvectorxtoacategoryidentiﬁedbynumericcodey.Thereareothervariantsoftheclassiﬁcationtask,forexample,wherefoutputsaprobabilitydistributionoverclasses.Anexampleofaclassiﬁcationtaskisobjectrecognition,wheretheinputisanimage(usuallydescribedasasetofpixelbrightnessvalues),andtheoutputisanumericcodeidentifyingtheobjectintheimage.Forexample,theWillowGaragePR2robotisabletoactasawaiterthatcanrecognizediﬀerentkindsofdrinksanddeliverthemtopeopleoncommand(Good-fellow2010etal.,).Modernobjectrecognitionisbestaccomplishedwithdeeplearning(,;,).ObjectKrizhevskyetal.2012IoﬀeandSzegedy2015recognitionisthesamebasictechnologythatallowscomputerstorecognizefaces(Taigman2014etal.,),whichcanbeusedtoautomaticallytagpeopleinphotocollectionsandallowcomputerstointeractmorenaturallywiththeirusers.•Classiﬁcationwithmissinginputs:Classiﬁcationbecomesmorechal-lengingifthecomputerprogramisnotguaranteedthateverymeasurementinitsinputvectorwillalwaysbeprovided.Inordertosolvetheclassiﬁcationtask,thelearningalgorithmonlyhastodeﬁneafunctionmappingsinglefromavectorinputtoacategoricaloutput.Whensomeoftheinputsmaybemissing,ratherthanprovidingasingleclassiﬁcationfunction,thelearningalgorithmmustlearnaoffunctions.Eachfunctioncorrespondstoclassi-setfyingxwithadiﬀerentsubsetofitsinputsmissing.Thiskindofsituationarisesfrequentlyinmedicaldiagnosis,becausemanykindsofmedicaltestsareexpensiveorinvasive.Onewaytoeﬃcientlydeﬁnesuchalargesetoffunctionsistolearnaprobabilitydistributionoveralloftherelevantvariables,thensolvetheclassiﬁcationtaskbymarginalizingoutthemissingvariables.Withninputvariables,wecannowobtainall2ndiﬀerentclassiﬁ-cationfunctionsneededforeachpossiblesetofmissinginputs,butweonlyneedtolearnasinglefunctiondescribingthejointprobabilitydistribution.SeeGoodfellow2013betal.()foranexampleofadeepprobabilisticmodelappliedtosuchataskinthisway.Manyoftheothertasksdescribedinthissectioncanalsobegeneralizedtoworkwithmissinginputs;classiﬁcationwithmissinginputsisjustoneexampleofwhatmachinelearningcando.100 CHAPTER5.MACHINELEARNINGBASICS•Regression:Inthistypeoftask,thecomputerprogramisaskedtopredictanumericalvaluegivensomeinput.Tosolvethistask,thelearningalgorithmisaskedtooutputafunctionf:Rn→R.Thistypeoftaskissimilartoclassiﬁcation,exceptthattheformatofoutputisdiﬀerent.Anexampleofaregressiontaskisthepredictionoftheexpectedclaimamountthataninsuredpersonwillmake(usedtosetinsurancepremiums),orthepredictionoffuturepricesofsecurities.Thesekindsofpredictionsarealsousedforalgorithmictrading.•Transcription:Inthistypeoftask,themachinelearningsystemisaskedtoobservearelativelyunstructuredrepresentationofsomekindofdataandtranscribeitintodiscrete,textualform.Forexample,inopticalcharacterrecognition,thecomputerprogramisshownaphotographcontaininganimageoftextandisaskedtoreturnthistextintheformofasequenceofcharacters(e.g.,inASCIIorUnicodeformat).GoogleStreetViewusesdeeplearningtoprocessaddressnumbersinthisway(,Goodfellowetal.2014d).Anotherexampleisspeechrecognition,wherethecomputerprogramisprovidedanaudiowaveformandemitsasequenceofcharactersorwordIDcodesdescribingthewordsthatwerespokenintheaudiorecording.DeeplearningisacrucialcomponentofmodernspeechrecognitionsystemsusedatmajorcompaniesincludingMicrosoft,IBMandGoogle(,Hintonetal.2012b).•Machinetranslation:Inamachinetranslationtask,theinputalreadyconsistsofasequenceofsymbolsinsomelanguage,andthecomputerprogrammustconvertthisintoasequenceofsymbolsinanotherlanguage.Thisiscommonlyappliedtonaturallanguages,suchastranslatingfromEnglishtoFrench.Deeplearninghasrecentlybeguntohaveanimportantimpactonthiskindoftask(Sutskever2014Bahdanau2015etal.,;etal.,).•Structuredoutput:Structuredoutputtasksinvolveanytaskwheretheoutputisavector(orotherdatastructurecontainingmultiplevalues)withimportantrelationshipsbetweenthediﬀerentelements.Thisisabroadcategory,andsubsumesthetranscriptionandtranslationtasksdescribedabove,butalsomanyothertasks.Oneexampleisparsing—mappinganaturallanguagesentenceintoatreethatdescribesitsgrammaticalstructureandtaggingnodesofthetreesasbeingverbs,nouns,oradverbs,andsoon.See()foranexampleofdeeplearningappliedtoaparsingCollobert2011task.Anotherexampleispixel-wisesegmentationofimages, wherethecomputerprogramassignseverypixelinanimagetoaspeciﬁccategory.For101 CHAPTER5.MACHINELEARNINGBASICSexample,deeplearningcanbeusedtoannotatethelocationsofroadsinaerialphotographs(MnihandHinton2010,).Theoutputneednothaveitsformmirrorthestructureoftheinputascloselyasintheseannotation-styletasks.Forexample,inimagecaptioning,thecomputerprogramobservesanimageandoutputsanaturallanguagesentencedescribingtheimage(Kirosetal.etal.,,;2014abMao,;2015Vinyals2015bDonahue2014etal.,;etal.,;KarpathyandLi2015Fang2015Xu2015,;etal.,;etal.,).Thesetasksarecalledstructuredoutputtasksbecausetheprogrammustoutputseveralvaluesthatarealltightlyinter-related.Forexample,thewordsproducedbyanimagecaptioningprogrammustformavalidsentence.•Anomalydetection:Inthistypeoftask,thecomputerprogramsiftsthroughasetofeventsorobjects,andﬂagssomeofthemasbeingunusualoratypical.Anexampleofananomalydetectiontaskiscreditcardfrauddetection.Bymodelingyourpurchasinghabits,acreditcardcompanycandetectmisuseofyourcards.Ifathiefstealsyourcreditcardorcreditcardinformation,thethief’spurchaseswilloftencomefromadiﬀerentprobabilitydistributionoverpurchasetypesthanyourown.Thecreditcardcompanycanpreventfraudbyplacingaholdonanaccountassoonasthatcardhasbeenusedforanuncharacteristicpurchase.See()foraChandolaetal.2009surveyofanomalydetectionmethods.•Synthesisandsampling:Inthistypeoftask,themachinelearningal-gorithmisaskedtogeneratenewexamplesthataresimilartothoseinthetrainingdata. Synthesisandsamplingviamachinelearningcanbeusefulformediaapplicationswhereitcanbeexpensiveorboringforanartisttogeneratelargevolumesofcontentbyhand.Forexample,videogamescanautomaticallygeneratetexturesforlargeobjectsorlandscapes,ratherthanrequiringanartisttomanuallylabeleachpixel(,).InsomeLuoetal.2013cases,wewantthesamplingorsynthesisproceduretogeneratesomespeciﬁckindofoutputgiventheinput.Forexample,inaspeechsynthesistask,weprovideawrittensentenceandasktheprogramtoemitanaudiowaveformcontainingaspokenversionofthatsentence. Thisisakindofstructuredoutputtask,butwiththeaddedqualiﬁcationthatthereisnosinglecorrectoutputforeachinput,andweexplicitlydesirealargeamountofvariationintheoutput,inorderfortheoutputtoseemmorenaturalandrealistic.•Imputationofmissingvalues:Inthistypeoftask,themachinelearningalgorithmisgivenanewexamplex∈Rn,butwithsomeentriesxiofxmissing.Thealgorithmmustprovideapredictionofthevaluesofthemissingentries.102 CHAPTER5.MACHINELEARNINGBASICS•Denoising:Inthistypeoftask,themachinelearningalgorithmisgivenininputacorruptedexample˜x∈Rnobtainedbyanunknowncorruptionprocessfromacleanexamplex∈Rn.Thelearnermustpredictthecleanexamplexfromitscorruptedversion˜x,ormoregenerallypredicttheconditionalprobabilitydistributionp(x|˜x).•Densityestimationorprobabilitymassfunctionestimation:Inthedensityestimationproblem,themachinelearningalgorithmisaskedtolearnafunctionpmodel:Rn→R,wherepmodel(x)canbeinterpretedasaprobabilitydensityfunction(ifxiscontinuous)oraprobabilitymassfunction(ifxisdiscrete)onthespacethattheexamplesweredrawnfrom.Todosuchataskwell(wewillspecifyexactlywhatthatmeanswhenwediscussperformancemeasuresP),thealgorithmneedstolearnthestructureofthedataithasseen.Itmustknowwhereexamplesclustertightlyandwheretheyareunlikelytooccur.Mostofthetasksdescribedaboverequirethelearningalgorithmtoatleastimplicitlycapturethestructureoftheprobabilitydistribution.Densityestimationallowsustoexplicitlycapturethatdistribution.Inprinciple,wecanthenperformcomputationsonthatdistributioninordertosolvetheothertasksaswell.Forexample,ifwehaveperformeddensityestimationtoobtainaprobabilitydistributionp(x),wecanusethatdistributiontosolvethemissingvalueimputationtask.Ifavaluexiismissingandalloftheothervalues,denotedx−i,aregiven,thenweknowthedistributionoveritisgivenbyp(xi|x−i).Inpractice,densityestimationdoesnotalwaysallowustosolvealloftheserelatedtasks,becauseinmanycasestherequiredoperationsonp(x)arecomputationallyintractable.Ofcourse,manyothertasksandtypesoftasksarepossible.Thetypesoftaskswelisthereareintendedonlytoprovideexamplesofwhatmachinelearningcando,nottodeﬁnearigidtaxonomyoftasks.5.1.2ThePerformanceMeasure,PInordertoevaluatetheabilitiesofamachinelearningalgorithm,wemustdesignaquantitativemeasureofitsperformance.UsuallythisperformancemeasurePisspeciﬁctothetaskbeingcarriedoutbythesystem.TFortaskssuchasclassiﬁcation,classiﬁcationwithmissinginputs,andtran-scription,weoftenmeasuretheaccuracyofthemodel.Accuracyisjusttheproportionofexamplesforwhichthemodelproducesthecorrectoutput.Wecan103 CHAPTER5.MACHINELEARNINGBASICSalsoobtainequivalentinformationbymeasuringtheerrorrate,theproportionofexamplesforwhichthemodelproducesanincorrectoutput.Weoftenrefertotheerrorrateastheexpected0-1loss.The0-1lossonaparticularexampleis0ifitiscorrectlyclassiﬁedand1ifitisnot.Fortaskssuchasdensityestimation,itdoesnotmakesensetomeasureaccuracy,errorrate,oranyotherkindof0-1loss.Instead,wemustuseadiﬀerentperformancemetricthatgivesthemodelacontinuous-valuedscoreforeachexample.Themostcommonapproachistoreporttheaveragelog-probabilitythemodelassignstosomeexamples.Usuallyweareinterestedinhowwellthemachinelearningalgorithmperformsondatathatithasnotseenbefore,sincethisdetermineshowwellitwillworkwhendeployedintherealworld.Wethereforeevaluatetheseperformancemeasuresusingatestsetofdatathatisseparatefromthedatausedfortrainingthemachinelearningsystem.Thechoiceofperformancemeasuremayseemstraightforwardandobjective,butitisoftendiﬃculttochooseaperformancemeasurethatcorrespondswelltothedesiredbehaviorofthesystem.Insomecases,thisisbecauseitisdiﬃculttodecidewhatshouldbemeasured.Forexample,whenperformingatranscriptiontask,shouldwemeasuretheaccuracyofthesystemattranscribingentiresequences,orshouldweuseamoreﬁne-grainedperformancemeasurethatgivespartialcreditforgettingsomeelementsofthesequencecorrect?Whenperformingaregressiontask,shouldwepenalizethesystemmoreifitfrequentlymakesmedium-sizedmistakesorifitrarelymakesverylargemistakes?Thesekindsofdesignchoicesdependontheapplication.Inothercases,weknowwhatquantitywewouldideallyliketomeasure,butmeasuringitisimpractical.Forexample,thisarisesfrequentlyinthecontextofdensityestimation.Manyofthebestprobabilisticmodelsrepresentprobabilitydistributionsonlyimplicitly.Computingtheactualprobabilityvalueassignedtoaspeciﬁcpointinspaceinmanysuchmodelsisintractable.Inthesecases,onemustdesignanalternativecriterionthatstillcorrespondstothedesignobjectives,ordesignagoodapproximationtothedesiredcriterion.5.1.3TheExperience,EMachinelearningalgorithmscanbebroadlycategorizedasunsupervisedorsupervisedbywhatkindofexperiencetheyareallowedtohaveduringthelearningprocess.Mostofthelearningalgorithmsinthisbookcanbeunderstoodasbeingallowedtoexperienceanentiredataset.Adatasetisacollectionofmanyexamples,as104 CHAPTER5.MACHINELEARNINGBASICSdeﬁnedinsection.Sometimeswewillalsocallexamples.5.1.1datapointsOneoftheoldestdatasetsstudiedbystatisticiansandmachinelearningre-searchersistheIrisdataset(,).ItisacollectionofmeasurementsofFisher1936diﬀerentpartsof150irisplants.Eachindividualplantcorrespondstooneexample.Thefeatureswithineachexamplearethemeasurementsofeachofthepartsoftheplant:thesepallength,sepalwidth,petallengthandpetalwidth.Thedatasetalsorecordswhichspecieseachplantbelongedto.Threediﬀerentspeciesarerepresentedinthedataset.Unsupervisedlearningalgorithmsexperienceadatasetcontainingmanyfeatures,thenlearnusefulpropertiesofthestructureofthisdataset.Inthecontextofdeeplearning,weusuallywanttolearntheentireprobabilitydistributionthatgeneratedadataset,whetherexplicitlyasindensityestimationorimplicitlyfortaskslikesynthesisordenoising.Someotherunsupervisedlearningalgorithmsperformotherroles,likeclustering,whichconsistsofdividingthedatasetintoclustersofsimilarexamples.Supervisedlearningalgorithmsexperienceadatasetcontainingfeatures,buteachexampleisalsoassociatedwithalabelortarget.Forexample,theIrisdatasetisannotatedwiththespeciesofeachirisplant.AsupervisedlearningalgorithmcanstudytheIrisdatasetandlearntoclassifyirisplantsintothreediﬀerentspeciesbasedontheirmeasurements.Roughlyspeaking,unsupervisedlearninginvolvesobservingseveralexamplesofarandomvectorx,andattemptingtoimplicitlyorexplicitlylearntheproba-bilitydistributionp(x),orsomeinterestingpropertiesofthatdistribution,whilesupervisedlearninginvolvesobservingseveralexamplesofarandomvectorxandanassociatedvalueorvectory,andlearningtopredictyfromx,usuallybyestimatingp(yx|).Thetermsupervisedlearningoriginatesfromtheviewofthetargetybeingprovidedbyaninstructororteacherwhoshowsthemachinelearningsystemwhattodo.Inunsupervisedlearning,thereisnoinstructororteacher,andthealgorithmmustlearntomakesenseofthedatawithoutthisguide.Unsupervisedlearningandsupervisedlearningarenotformallydeﬁnedterms.Thelinesbetweenthemareoftenblurred.Manymachinelearningtechnologiescanbeusedtoperformbothtasks.Forexample,thechainruleofprobabilitystatesthatforavectorx∈Rn,thejointdistributioncanbedecomposedasp() =xni=1p(xi|x1,...,xi−1).(5.1)Thisdecompositionmeansthatwecansolvetheostensiblyunsupervisedproblemofmodelingp(x) bysplittingitintonsupervisedlearningproblems.Alternatively,we105 CHAPTER5.MACHINELEARNINGBASICScansolvethesupervisedlearningproblemoflearningp(y|x)byusingtraditionalunsupervised learningtechnologiesto learn thejointdistributionp(x,y)andinferringpy(|x) =p,y(x)yp,y(x).(5.2)Thoughunsupervisedlearningandsupervisedlearningarenotcompletelyformalordistinctconcepts,theydohelptoroughlycategorizesomeofthethingswedowithmachinelearningalgorithms.Traditionally,peoplerefertoregression,classiﬁcationandstructuredoutputproblemsassupervisedlearning.Densityestimationinsupportofothertasksisusuallyconsideredunsupervisedlearning.Othervariantsofthelearningparadigmarepossible.Forexample,insemi-supervisedlearning,someexamplesincludeasupervisiontargetbutothersdonot.Inmulti-instancelearning,anentirecollectionofexamplesislabeledascontainingornotcontaininganexampleofaclass,buttheindividualmembersofthecollectionarenotlabeled.Forarecentexampleofmulti-instancelearningwithdeepmodels,seeKotzias2015etal.().Somemachinelearningalgorithmsdonotjustexperienceaﬁxeddataset.Forexample,reinforcementlearningalgorithmsinteractwithanenvironment,sothereisafeedbackloopbetweenthelearningsystemanditsexperiences. Suchalgorithmsarebeyondthescopeofthisbook.Pleasesee()SuttonandBarto1998orBertsekasandTsitsiklis1996()forinformationaboutreinforcementlearning,and()forthedeeplearningapproachtoreinforcementlearning.Mnihetal.2013Mostmachinelearningalgorithmssimplyexperienceadataset.Adatasetcanbedescribedinmanyways.Inallcases,adatasetisacollectionofexamples,whichareinturncollectionsoffeatures.Onecommonwayofdescribingadatasetiswitha.Adesigndesignmatrixmatrixisamatrixcontainingadiﬀerentexampleineachrow.Eachcolumnofthematrixcorrespondstoadiﬀerentfeature.Forinstance,theIrisdatasetcontains150exampleswithfourfeaturesforeachexample.ThismeanswecanrepresentthedatasetwithadesignmatrixX∈R1504×,whereXi,1isthesepallengthofplanti,Xi,2isthesepalwidthofplanti,etc.Wewilldescribemostofthelearningalgorithmsinthisbookintermsofhowtheyoperateondesignmatrixdatasets.Ofcourse,todescribeadatasetasadesignmatrix,itmustbepossibletodescribeeachexampleasavector,andeachofthesevectorsmustbethesamesize.Thisisnotalwayspossible.Forexample,ifyouhaveacollectionofphotographswithdiﬀerentwidthsandheights,thendiﬀerentphotographswillcontaindiﬀerentnumbersofpixels,sonotallofthephotographsmaybedescribedwiththesamelengthofvector.Sectionandchapterdescribehowtohandlediﬀerent9.710106 CHAPTER5.MACHINELEARNINGBASICStypesofsuchheterogeneousdata.Incaseslikethese,ratherthandescribingthedatasetasamatrixwithmrows,wewilldescribeitasasetcontainingmelements:{x(1),x(2),...,x()m}.Thisnotationdoesnotimplythatanytwoexamplevectorsx()iandx()jhavethesamesize.Inthecaseofsupervisedlearning,theexamplecontainsalabelortargetaswellasacollectionoffeatures.Forexample,ifwewanttousealearningalgorithmtoperformobjectrecognitionfromphotographs,weneedtospecifywhichobjectappearsineachofthephotos.Wemightdothiswithanumericcode,with0signifyingaperson,1signifyingacar,2signifyingacat,etc.OftenwhenworkingwithadatasetcontainingadesignmatrixoffeatureobservationsX,wealsoprovideavectoroflabels,withyyiprovidingthelabelforexample.iOfcourse,sometimesthelabelmaybemorethanjustasinglenumber.Forexample,ifwewanttotrainaspeechrecognitionsystemtotranscribeentiresentences,thenthelabelforeachexamplesentenceisasequenceofwords.Justasthereisnoformaldeﬁnitionofsupervisedandunsupervisedlearning,thereisnorigidtaxonomyofdatasetsorexperiences.Thestructuresdescribedherecovermostcases,butitisalwayspossibletodesignnewonesfornewapplications.5.1.4Example:LinearRegressionOurdeﬁnitionofamachinelearningalgorithmasanalgorithmthatiscapableofimprovingacomputerprogram’sperformanceatsometaskviaexperienceissomewhatabstract.Tomakethismoreconcrete,wepresentanexampleofasimplemachinelearningalgorithm:linearregression.Wewillreturntothisexamplerepeatedlyasweintroducemoremachinelearningconceptsthathelptounderstanditsbehavior.Asthenameimplies,linearregressionsolvesaregressionproblem. Inotherwords,thegoalistobuildasystemthatcantakeavectorx∈Rnasinputandpredictthevalueofascalary∈Rasitsoutput.Inthecaseoflinearregression,theoutputisalinearfunctionoftheinput.Letˆybethevaluethatourmodelpredictsshouldtakeon.Wedeﬁnetheoutputtobeyˆy= wx(5.3)wherew∈Rnisavectorof.parametersParametersarevaluesthatcontrolthebehaviorofthesystem.Inthiscase,wiisthecoeﬃcientthatwemultiplybyfeaturexibeforesummingupthecontributionsfromallthefeatures.Wecanthinkofwasasetofweightsthatdeterminehoweachfeatureaﬀectstheprediction. Ifafeaturexireceivesapositiveweightwi,107 CHAPTER5.MACHINELEARNINGBASICSthenincreasingthevalueofthatfeatureincreasesthevalueofourpredictionˆy.Ifafeaturereceivesanegativeweight,thenincreasingthevalueofthatfeaturedecreasesthevalueofourprediction.Ifafeature’sweightislargeinmagnitude,thenithasalargeeﬀectontheprediction.Ifafeature’sweightiszero,ithasnoeﬀectontheprediction.WethushaveadeﬁnitionofourtaskT: topredictyfromxbyoutputtingˆy= wx.Nextweneedadeﬁnitionofourperformancemeasure,.PSupposethatwehaveadesignmatrixofmexampleinputsthatwewillnotusefortraining,onlyforevaluatinghowwellthemodelperforms.Wealsohaveavectorofregressiontargetsprovidingthecorrectvalueofyforeachoftheseexamples.Becausethisdatasetwillonlybeusedforevaluation,wecallitthetestset.WerefertothedesignmatrixofinputsasX()testandthevectorofregressiontargetsasy()test.Onewayofmeasuringtheperformanceofthemodelistocomputethemeansquarederrorofthemodelonthetestset.Ifˆy()testgivesthepredictionsofthemodelonthetestset,thenthemeansquarederrorisgivenbyMSEtest=1mi(ˆy()test−y()test)2i.(5.4)Intuitively,onecanseethatthiserrormeasuredecreasesto0whenˆy()test=y()test.WecanalsoseethatMSEtest=1m||ˆy()test−y()test||22,(5.5)sotheerrorincreaseswhenevertheEuclideandistancebetweenthepredictionsandthetargetsincreases.Tomakeamachinelearningalgorithm,weneedtodesignanalgorithmthatwillimprovetheweightswinawaythatreducesMSEtestwhenthealgorithmisallowedtogainexperiencebyobservingatrainingset(X()train,y()train).Oneintuitivewayofdoingthis(whichwewilljustifylater,insection)isjustto5.5.1minimizethemeansquarederroronthetrainingset,MSEtrain.TominimizeMSEtrain,wecansimplysolveforwhereitsgradientis:0∇wMSEtrain= 0(5.6)⇒∇w1m||ˆy()train−y()train||22= 0(5.7)⇒1m∇w||X()trainwy−()train||22= 0(5.8)108 CHAPTER5.MACHINELEARNINGBASICS −−10.05000510....x1−3−2−10123yLinearregressionexample 051015...w1020.025.030.035.040.045.050.055.MSE(train)Optimizationofw Figure5.1:Alinearregressionproblem,withatrainingsetconsistingoftendatapoints,eachcontainingonefeature.Becausethereisonlyonefeature,theweightvectorwcontainsonlyasingleparametertolearn,w1.(Left)Observethatlinearregressionlearnstosetw1suchthattheliney=w1xcomesascloseaspossibletopassingthroughallthetrainingpoints.Theplottedpointindicatesthevalueof(Right)w1foundbythenormalequations,whichwecanseeminimizesthemeansquarederroronthetrainingset.⇒∇wX()trainwy−()trainX()trainwy−()train= 0(5.9)⇒∇wwX()trainX()trainww−2X()trainy()train+y()trainy()train= 0(5.10)⇒2X()trainX()trainwX−2()trainy()train= 0(5.11)⇒w=X()trainX()train−1X()trainy()train(5.12)Thesystemofequationswhosesolutionisgivenbyequationisknownas5.12thenormalequations.Evaluatingequationconstitutesasimplelearning5.12algorithm.Foranexampleofthelinearregressionlearningalgorithminaction,seeﬁgure.5.1Itisworthnotingthatthetermlinearregressionisoftenusedtorefertoaslightlymoresophisticatedmodelwithoneadditionalparameter—aninterceptterm.Inthismodelbˆy= wx+b(5.13)sothemappingfromparameterstopredictionsisstillalinearfunctionbutthemappingfromfeaturestopredictionsisnowanaﬃnefunction.Thisextensiontoaﬃnefunctionsmeansthattheplotofthemodel’spredictionsstilllookslikealine,butitneednotpassthroughtheorigin.Insteadofaddingthebiasparameter109 CHAPTER5.MACHINELEARNINGBASICSb,onecancontinuetousethemodelwithonlyweightsbutaugmentxwithanextraentrythatisalwayssetto.Theweightcorrespondingtotheextraentry11playstheroleofthebiasparameter.Wewillfrequentlyusetheterm“linear”whenreferringtoaﬃnefunctionsthroughoutthisbook.Theintercepttermbisoftencalledthebiasparameteroftheaﬃnetransfor-mation.Thisterminologyderivesfromthepointofviewthattheoutputofthetransformationisbiasedtowardbeingbintheabsenceofanyinput.Thistermisdiﬀerentfromtheideaofastatisticalbias,inwhichastatisticalestimationalgorithm’sexpectedestimateofaquantityisnotequaltothetruequantity.Linearregressionisofcourseanextremelysimpleandlimitedlearningalgorithm,butitprovidesanexampleofhowalearningalgorithmcanwork.Inthesubsequentsectionswewilldescribesomeofthebasicprinciplesunderlyinglearningalgorithmdesignanddemonstratehowtheseprinciplescanbeusedtobuildmorecomplicatedlearningalgorithms.5.2Capacity,OverﬁttingandUnderﬁttingThecentralchallengeinmachinelearningisthatwemustperformwellonnew,previouslyunseeninputs—notjustthoseonwhichourmodelwastrained. Theabilitytoperformwellonpreviouslyunobservedinputsiscalledgeneralization.Typically,whentrainingamachinelearningmodel,wehaveaccesstoatrainingset,wecancomputesomeerrormeasureonthetrainingsetcalledthetrainingerror,andwereducethistrainingerror.Sofar,whatwehavedescribedissimplyanoptimizationproblem.Whatseparatesmachinelearningfromoptimizationisthatwewantthegeneralizationerror,alsocalledthetesterror,tobelowaswell. Thegeneralizationerrorisdeﬁnedastheexpectedvalueoftheerroronanewinput.Heretheexpectationistakenacrossdiﬀerentpossibleinputs,drawnfromthedistributionofinputsweexpectthesystemtoencounterinpractice.Wetypicallyestimatethegeneralizationerrorofamachinelearningmodelbymeasuringitsperformanceonatestsetofexamplesthatwerecollectedseparatelyfromthetrainingset.Inourlinearregressionexample,wetrainedthemodelbyminimizingthetrainingerror,1m()train||X()trainwy−()train||22,(5.14)butweactuallycareaboutthetesterror,1m()test||X()testwy−()test||22.Howcanweaﬀectperformanceonthetestsetwhenwegettoobserveonlythe110 CHAPTER5.MACHINELEARNINGBASICStrainingset?Theﬁeldofstatisticallearningtheoryprovidessomeanswers.Ifthetrainingandthetestsetarecollectedarbitrarily,thereisindeedlittlewecando.Ifweareallowedtomakesomeassumptionsabouthowthetrainingandtestsetarecollected,thenwecanmakesomeprogress.Thetrainandtestdataaregeneratedbyaprobabilitydistributionoverdatasetscalledthedatageneratingprocess.Wetypicallymakeasetofassumptionsknowncollectivelyasthei.i.d. assumptions. Theseassumptionsarethattheexamplesineachdatasetareindependentfromeachother,andthatthetrainsetandtestsetareidenticallydistributed,drawnfromthesameprobabilitydistributionaseachother. Thisassumptionallowsustodescribethedatagen-eratingprocesswithaprobabilitydistributionoverasingleexample.Thesamedistributionisthenusedtogenerateeverytrainexampleandeverytestexample.Wecallthatsharedunderlyingdistributionthedatageneratingdistribution,denotedpdata.Thisprobabilisticframeworkandthei.i.d.assumptionsallowustomathematicallystudytherelationshipbetweentrainingerrorandtesterror.Oneimmediateconnectionwecanobservebetweenthetrainingandtesterroristhattheexpectedtrainingerrorofarandomlyselectedmodelisequaltotheexpectedtesterrorofthatmodel.Supposewehaveaprobabilitydistributionp(x,y)andwesamplefromitrepeatedlytogeneratethetrainsetandthetestset.Forsomeﬁxedvaluew,theexpectedtrainingseterrorisexactlythesameastheexpectedtestseterror,becausebothexpectationsareformedusingthesamedatasetsamplingprocess.Theonlydiﬀerencebetweenthetwoconditionsisthenameweassigntothedatasetwesample.Ofcourse, when weuseamachinelearning algorithm, wedonotﬁxtheparametersaheadoftime,thensamplebothdatasets.Wesamplethetrainingset,thenuseittochoosetheparameterstoreducetrainingseterror,thensamplethetestset.Underthisprocess,theexpectedtesterrorisgreaterthanorequaltotheexpectedvalueoftrainingerror.Thefactorsdetermininghowwellamachinelearningalgorithmwillperformareitsabilityto:1. Makethetrainingerrorsmall.2. Makethegapbetweentrainingandtesterrorsmall.Thesetwofactorscorrespondtothetwocentralchallengesinmachinelearning:underﬁttingandoverﬁtting.Underﬁttingoccurswhenthemodelisnotabletoobtainasuﬃcientlylowerrorvalueonthetrainingset.Overﬁttingoccurswhenthegapbetweenthetrainingerrorandtesterroristoolarge.Wecancontrolwhetheramodelismorelikelytooverﬁtorunderﬁtbyalteringitscapacity.Informally,amodel’scapacityisitsabilitytoﬁtawidevarietyof111 CHAPTER5.MACHINELEARNINGBASICSfunctions.Modelswithlowcapacitymaystruggletoﬁtthetrainingset.Modelswithhighcapacitycanoverﬁtbymemorizingpropertiesofthetrainingsetthatdonotservethemwellonthetestset.Onewaytocontrolthecapacityofalearningalgorithmisbychoosingitshypothesisspace,thesetoffunctionsthatthelearningalgorithmisallowedtoselectasbeingthesolution.Forexample,thelinearregressionalgorithmhasthesetofalllinearfunctionsofitsinputasitshypothesisspace.Wecangeneralizelinearregressiontoincludepolynomials,ratherthanjustlinearfunctions,initshypothesisspace.Doingsoincreasesthemodel’scapacity.Apolynomialofdegreeonegivesusthelinearregressionmodelwithwhichwearealreadyfamiliar,withpredictionˆybwx.= +(5.15)Byintroducingx2asanotherfeatureprovidedtothelinearregressionmodel,wecanlearnamodelthatisquadraticasafunctionof:xˆybw= +1xw+2x2.(5.16)Thoughthismodelimplementsaquadraticfunctionofits,theoutputisinputstillalinearfunctionoftheparameters,sowecanstillusethenormalequationstotrainthemodelinclosedform.Wecancontinuetoaddmorepowersofxasadditionalfeatures,forexampletoobtainapolynomialofdegree9:ˆyb= +9i=1wixi.(5.17)Machinelearningalgorithmswillgenerallyperformbestwhentheircapacityisappropriateforthetruecomplexityofthetasktheyneedtoperformandtheamountoftrainingdatatheyareprovidedwith.Modelswithinsuﬃcientcapacityareunabletosolvecomplextasks.Modelswithhighcapacitycansolvecomplextasks,butwhentheircapacityishigherthanneededtosolvethepresenttasktheymayoverﬁt.Figureshowsthisprincipleinaction.Wecomparealinear,quadratic5.2anddegree-9predictorattemptingtoﬁtaproblemwherethetrueunderlyingfunctionisquadratic. Thelinearfunctionisunabletocapturethecurvatureinthetrueunderlyingproblem,soitunderﬁts.Thedegree-9predictoriscapableofrepresentingthecorrectfunction,butitisalsocapableofrepresentinginﬁnitelymanyotherfunctionsthatpassexactlythroughthetrainingpoints,becausewe112 CHAPTER5.MACHINELEARNINGBASICShavemoreparametersthantrainingexamples.Wehavelittlechanceofchoosingasolutionthatgeneralizeswellwhensomanywildlydiﬀerentsolutionsexist.Inthisexample,thequadraticmodelisperfectlymatchedtothetruestructureofthetasksoitgeneralizeswelltonewdata.    Figure5.2:Weﬁtthreemodelstothisexampletrainingset.Thetrainingdatawasgeneratedsynthetically,byrandomlysamplingxvaluesandchoosingydeterministicallybyevaluatingaquadraticfunction. (Left)Alinearfunctionﬁttothedatasuﬀersfromunderﬁtting—itcannotcapturethecurvaturethatispresentinthedata.A(Center)quadraticfunctionﬁttothedatageneralizeswelltounseenpoints.Itdoesnotsuﬀerfromasigniﬁcantamountofoverﬁttingorunderﬁtting.Apolynomialofdegree9ﬁtto(Right)thedatasuﬀersfromoverﬁtting.HereweusedtheMoore-Penrosepseudoinversetosolvetheunderdeterminednormalequations.Thesolutionpassesthroughallofthetrainingpointsexactly,butwehavenotbeenluckyenoughforittoextractthecorrectstructure.Itnowhasadeepvalleyinbetweentwotrainingpointsthatdoesnotappearinthetrueunderlyingfunction.Italsoincreasessharplyontheleftsideofthedata,whilethetruefunctiondecreasesinthisarea.Sofarwehavedescribedonlyonewayofchangingamodel’scapacity:bychangingthenumberofinputfeaturesithas,andsimultaneouslyaddingnewparametersassociatedwiththosefeatures.Thereareinfactmanywaysofchangingamodel’scapacity.Capacityisnotdeterminedonlybythechoiceofmodel.Themodelspeciﬁeswhichfamilyoffunctionsthelearningalgorithmcanchoosefromwhenvaryingtheparametersinordertoreduceatrainingobjective.Thisiscalledtherepresentationalcapacityofthemodel.Inmanycases,ﬁndingthebestfunctionwithinthisfamilyisaverydiﬃcultoptimizationproblem.Inpractice,thelearningalgorithmdoesnotactuallyﬁndthebestfunction,butmerelyonethatsigniﬁcantlyreducesthetrainingerror.Theseadditionallimitations,suchas113 CHAPTER5.MACHINELEARNINGBASICStheimperfectionoftheoptimizationalgorithm,meanthatthelearningalgorithm’seﬀectivecapacitymaybelessthantherepresentationalcapacityofthemodelfamily.OurmodernideasaboutimprovingthegeneralizationofmachinelearningmodelsarereﬁnementsofthoughtdatingbacktophilosophersatleastasearlyasPtolemy.ManyearlyscholarsinvokeaprincipleofparsimonythatisnowmostwidelyknownasOccam’srazor(c.1287-1347).Thisprinciplestatesthatamongcompetinghypothesesthatexplainknownobservationsequallywell,oneshouldchoosethe“simplest”one.Thisideawasformalizedandmademorepreciseinthe20thcenturybythefoundersofstatisticallearningtheory(VapnikandChervonenkis1971Vapnik1982Blumer1989Vapnik1995,;,;etal.,;,).Statisticallearningtheoryprovidesvariousmeansofquantifyingmodelcapacity.Amongthese,themostwell-knownistheVapnik-Chervonenkisdimension,orVCdimension.TheVCdimensionmeasuresthecapacityofabinaryclassiﬁer.TheVCdimensionisdeﬁnedasbeingthelargestpossiblevalueofmforwhichthereexistsatrainingsetofmdiﬀerentxpointsthattheclassiﬁercanlabelarbitrarily.Quantifyingthecapacityofthemodelallowsstatisticallearningtheorytomakequantitativepredictions.Themostimportantresultsinstatisticallearningtheoryshowthatthediscrepancybetweentrainingerrorandgeneralizationerrorisboundedfromabovebyaquantitythatgrowsasthemodelcapacitygrowsbutshrinksasthenumberoftrainingexamplesincreases(VapnikandChervonenkis,1971Vapnik1982Blumer1989Vapnik1995;,;etal.,;,).Theseboundsprovideintellectualjustiﬁcationthatmachinelearningalgorithmscanwork,buttheyarerarelyusedinpracticewhenworkingwithdeeplearningalgorithms.Thisisinpartbecausetheboundsareoftenquitelooseandinpartbecauseitcanbequitediﬃculttodeterminethecapacityofdeeplearningalgorithms. Theproblemofdeterminingthecapacityofadeeplearningmodelisespeciallydiﬃcultbecausetheeﬀectivecapacityislimitedbythecapabilitiesoftheoptimizationalgorithm,andwehavelittletheoreticalunderstandingoftheverygeneralnon-convexoptimizationproblemsinvolvedindeeplearning.Wemustrememberthatwhilesimplerfunctionsaremorelikelytogeneralize(tohaveasmallgapbetweentrainingandtesterror)wemuststillchooseasuﬃcientlycomplexhypothesistoachievelowtrainingerror.Typically,trainingerrordecreasesuntilitasymptotestotheminimumpossibleerrorvalueasmodelcapacityincreases(assumingtheerrormeasurehasaminimumvalue).Typically,generalizationerrorhasaU-shapedcurveasafunctionofmodelcapacity.Thisisillustratedinﬁgure.5.3Toreachthemostextremecaseofarbitrarilyhighcapacity,weintroduce114 CHAPTER5.MACHINELEARNINGBASICS 0OptimalCapacityCapacityErrorUnderﬁttingzoneOverﬁttingzoneGeneralizationgapTrainingerrorGeneralizationerror Figure5.3:Typicalrelationshipbetweencapacityanderror.Trainingandtesterrorbehavediﬀerently.Attheleftendofthegraph,trainingerrorandgeneralizationerrorarebothhigh.Thisistheunderﬁttingregime.Asweincreasecapacity,trainingerrordecreases,butthegapbetweentrainingandgeneralizationerrorincreases.Eventually,thesizeofthisgapoutweighsthedecreaseintrainingerror,andweentertheoverﬁttingregime,wherecapacityistoolarge,abovetheoptimalcapacity.theconceptofnon-parametricmodels.Sofar,wehaveseenonlyparametricmodels,suchaslinearregression.Parametricmodelslearnafunctiondescribedbyaparametervectorwhosesizeisﬁniteandﬁxedbeforeanydataisobserved.Non-parametricmodelshavenosuchlimitation.Sometimes,non-parametricmodelsarejusttheoreticalabstractions(suchasanalgorithmthatsearchesoverallpossibleprobabilitydistributions)thatcannotbeimplementedinpractice.However,wecanalsodesignpracticalnon-parametricmodelsbymakingtheircomplexityafunctionofthetrainingsetsize.Oneexampleofsuchanalgorithmisnearestneighborregression.Unlikelinearregression,whichhasaﬁxed-lengthvectorofweights,thenearestneighborregressionmodelsimplystorestheXandyfromthetrainingset. Whenaskedtoclassifyatestpointx,themodellooksupthenearestentryinthetrainingsetandreturnstheassociatedregressiontarget.Inotherwords,ˆy=yiwherei=argmin||Xi,:−||x22.ThealgorithmcanalsobegeneralizedtodistancemetricsotherthantheL2norm,suchaslearneddistancemetrics(,).IfthealgorithmisGoldbergeretal.2005allowedtobreaktiesbyaveragingtheyivaluesforallXi,:thataretiedfornearest,thenthisalgorithmisabletoachievetheminimumpossibletrainingerror(whichmightbegreaterthanzero,iftwoidenticalinputsareassociatedwithdiﬀerentoutputs)onanyregressiondataset.Finally,wecanalsocreateanon-parametriclearningalgorithmbywrappinga115 CHAPTER5.MACHINELEARNINGBASICSparametriclearningalgorithminsideanotheralgorithmthatincreasesthenumberofparametersasneeded.Forexample,wecouldimagineanouterloopoflearningthatchangesthedegreeofthepolynomiallearnedbylinearregressionontopofapolynomialexpansionoftheinput.Theidealmodelisanoraclethatsimplyknowsthetrueprobabilitydistributionthatgeneratesthedata. Evensuchamodelwillstillincursomeerroronmanyproblems,becausetheremaystillbesomenoiseinthedistribution.Inthecaseofsupervisedlearning,themappingfromxtoymaybeinherentlystochastic,orymaybeadeterministicfunctionthatinvolvesothervariablesbesidesthoseincludedinx.Theerrorincurredbyanoraclemakingpredictionsfromthetruedistributioniscalledthep,y(x)Bayeserror.Trainingandgeneralizationerrorvaryasthesizeofthetrainingsetvaries.Expectedgeneralizationerrorcanneverincreaseasthenumberoftrainingexamplesincreases.Fornon-parametricmodels,moredatayieldsbettergeneralizationuntilthebestpossibleerrorisachieved.AnyﬁxedparametricmodelwithlessthanoptimalcapacitywillasymptotetoanerrorvaluethatexceedstheBayeserror.Seeﬁgureforanillustration.Notethatitispossibleforthemodeltohaveoptimal5.4capacityandyetstillhavealargegapbetweentrainingandgeneralizationerror.Inthissituation,wemaybeabletoreducethisgapbygatheringmoretrainingexamples.5.2.1TheNoFreeLunchTheoremLearningtheoryclaimsthatamachinelearningalgorithmcangeneralizewellfromaﬁnitetrainingsetofexamples.Thisseemstocontradictsomebasicprinciplesoflogic.Inductivereasoning,orinferringgeneralrulesfromalimitedsetofexamples,isnotlogicallyvalid. Tologicallyinferaruledescribingeverymemberofaset,onemusthaveinformationabouteverymemberofthatset.Inpart,machinelearningavoidsthisproblembyoﬀeringonlyprobabilisticrules,ratherthantheentirelycertainrulesusedinpurelylogicalreasoning. Machinelearningpromisestoﬁndrulesthatareprobablymostcorrectaboutmembersofthesettheyconcern.Unfortunately,eventhisdoesnotresolvetheentireproblem.Thenofreelunchtheoremformachinelearning(Wolpert1996,)statesthat,averagedoverallpossibledatageneratingdistributions,everyclassiﬁcationalgorithmhasthesameerrorratewhenclassifyingpreviouslyunobservedpoints.Inotherwords,insomesense,nomachinelearningalgorithmisuniversallyanybetterthananyother.Themostsophisticatedalgorithmwecanconceiveofhasthesameaverage116 CHAPTER5.MACHINELEARNINGBASICS  Figure5.4:Theeﬀectofthetrainingdatasetsizeonthetrainandtesterror,aswellasontheoptimalmodelcapacity.Weconstructedasyntheticregressionproblembasedonaddingamoderateamountofnoisetoadegree-5polynomial,generatedasingletestset,andthengeneratedseveraldiﬀerentsizesoftrainingset.Foreachsize,wegenerated40diﬀerenttrainingsetsinordertoploterrorbarsshowing95percentconﬁdenceintervals.(Top)TheMSEonthetrainingandtestsetfortwodiﬀerentmodels:aquadraticmodel,andamodelwithdegreechosentominimizethetesterror.Bothareﬁtinclosedform.Forthequadraticmodel,thetrainingerrorincreasesasthesizeofthetrainingsetincreases.Thisisbecauselargerdatasetsarehardertoﬁt.Simultaneously,thetesterrordecreases,becausefewerincorrecthypothesesareconsistentwiththetrainingdata.Thequadraticmodeldoesnothaveenoughcapacitytosolvethetask,soitstesterrorasymptotestoahighvalue.ThetesterroratoptimalcapacityasymptotestotheBayeserror.ThetrainingerrorcanfallbelowtheBayeserror,duetotheabilityofthetrainingalgorithmtomemorizespeciﬁcinstancesofthetrainingset.Asthetrainingsizeincreasestoinﬁnity,thetrainingerrorofanyﬁxed-capacitymodel(here,thequadraticmodel)mustrisetoatleasttheBayeserror. Asthetrainingsetsizeincreases,theoptimalcapacity(Bottom)(shownhereasthedegreeoftheoptimalpolynomialregressor)increases. Theoptimalcapacityplateausafterreachingsuﬃcientcomplexitytosolvethetask.117 CHAPTER5.MACHINELEARNINGBASICSperformance(overallpossibletasks)asmerelypredictingthateverypointbelongstothesameclass.Fortunately,theseresultsholdonlywhenweaverageoverpossibledataallgeneratingdistributions.Ifwemakeassumptionsaboutthekindsofprobabilitydistributionsweencounterinreal-worldapplications,thenwecandesignlearningalgorithmsthatperformwellonthesedistributions.Thismeansthatthegoalofmachinelearningresearchisnottoseekauniversallearningalgorithmortheabsolutebestlearningalgorithm.Instead,ourgoalistounderstandwhatkindsofdistributionsarerelevanttothe“realworld”thatanAIagentexperiences,andwhatkindsofmachinelearningalgorithmsperformwellondatadrawnfromthekindsofdatageneratingdistributionswecareabout.5.2.2RegularizationThenofreelunchtheoremimpliesthatwemustdesignourmachinelearningalgorithmstoperformwellonaspeciﬁctask.Wedosobybuildingasetofpreferencesintothelearningalgorithm.Whenthesepreferencesarealignedwiththelearningproblemsweaskthealgorithmtosolve,itperformsbetter.Sofar,theonlymethodofmodifyingalearningalgorithmthatwehavediscussedconcretelyistoincreaseordecreasethemodel’srepresentationalcapacitybyaddingorremovingfunctionsfromthehypothesisspaceofsolutionsthelearningalgorithmisabletochoose.Wegavethespeciﬁcexampleofincreasingordecreasingthedegreeofapolynomialforaregressionproblem.Theviewwehavedescribedsofarisoversimpliﬁed.Thebehaviorofouralgorithmisstronglyaﬀectednotjustbyhowlargewemakethesetoffunctionsallowedinitshypothesisspace,butbythespeciﬁcidentityofthosefunctions.Thelearningalgorithmwehavestudiedsofar,linearregression,hasahypothesisspaceconsistingofthesetoflinearfunctionsofitsinput.Theselinearfunctionscanbeveryusefulforproblemswheretherelationshipbetweeninputsandoutputstrulyisclosetolinear.Theyarelessusefulforproblemsthatbehaveinaverynonlinearfashion.Forexample,linearregressionwouldnotperformverywellifwetriedtouseittopredictsin(x)fromx.Wecanthuscontroltheperformanceofouralgorithmsbychoosingwhatkindoffunctionsweallowthemtodrawsolutionsfrom,aswellasbycontrollingtheamountofthesefunctions.Wecanalsogivealearningalgorithmapreferenceforonesolutioninitshypothesisspacetoanother.Thismeansthatbothfunctionsareeligible,butoneispreferred.Theunpreferredsolutionwillbechosenonlyifitﬁtsthetraining118 CHAPTER5.MACHINELEARNINGBASICSdatasigniﬁcantlybetterthanthepreferredsolution.Forexample,wecanmodifythetrainingcriterionforlinearregressiontoincludeweightdecay.Toperformlinearregressionwithweightdecay,weminimizeasumcomprisingboththemeansquarederroronthetrainingandacriterionJ(w)thatexpressesapreferencefortheweightstohavesmallersquaredL2norm.Speciﬁcally,J() = wMSEtrain+λww,(5.18)whereλisavaluechosenaheadoftimethatcontrolsthestrengthofourpreferenceforsmallerweights.Whenλ= 0,weimposenopreference,andlargerλforcestheweightstobecomesmaller. MinimizingJ(w)resultsinachoiceofweightsthatmakeatradeoﬀbetweenﬁttingthetrainingdataandbeingsmall.Thisgivesussolutionsthathaveasmallerslope,orputweightonfewerofthefeatures.Asanexampleofhowwecancontrolamodel’stendencytooverﬁtorunderﬁtviaweightdecay,wecantrainahigh-degreepolynomialregressionmodelwithdiﬀerentvaluesof.Seeﬁgurefortheresults.λ5.5    Figure5.5:Weﬁtahigh-degreepolynomialregressionmodeltoourexampletrainingsetfromﬁgure.Thetruefunctionisquadratic,buthereweuseonlymodelswithdegree9.5.2Wevarytheamountofweightdecaytopreventthesehigh-degreemodelsfromoverﬁtting.(Left)Withverylargeλ,wecanforcethemodeltolearnafunctionwithnoslopeatall.Thisunderﬁtsbecauseitcanonlyrepresentaconstantfunction.Witha(Center)mediumvalueof,thelearningalgorithmrecoversacurvewiththerightgeneralshape.λEventhoughthemodeliscapableofrepresentingfunctionswithmuchmorecomplicatedshape,weightdecayhasencouragedittouseasimplerfunctiondescribedbysmallercoeﬃcients.Withweightdecayapproachingzero(i.e.,usingtheMoore-Penrose(Right)pseudoinversetosolvetheunderdeterminedproblemwithminimalregularization),thedegree-9polynomialoverﬁtssigniﬁcantly,aswesawinﬁgure.5.2119 CHAPTER5.MACHINELEARNINGBASICSMoregenerally,wecanregularizeamodelthatlearnsafunctionf(x;θ)byaddingapenaltycalledaregularizertothecostfunction.Inthecaseofweightdecay,theregularizerisΩ(w) =ww.Inchapter,wewillseethatmanyother7regularizersarepossible.Expressingpreferencesforonefunctionoveranotherisamoregeneralwayofcontrollingamodel’scapacitythanincludingorexcludingmembersfromthehypothesisspace.Wecanthinkofexcludingafunctionfromahypothesisspaceasexpressinganinﬁnitelystrongpreferenceagainstthatfunction.Inourweightdecayexample,weexpressedourpreferenceforlinearfunctionsdeﬁnedwithsmallerweightsexplicitly, viaanextraterminthecriterionweminimize.Thereare many otherwaysof expressingpreferencesfor diﬀerentsolutions,bothimplicitlyandexplicitly.Together,thesediﬀerentapproachesareknownasregularization. Regularizationisanymodiﬁcationwemaketoalearningalgorithmthatisintendedtoreduceitsgeneralizationerrorbutnotitstrainingerror.Regularizationisoneofthecentralconcernsoftheﬁeldofmachinelearning,rivaledinitsimportanceonlybyoptimization.Thenofreelunchtheoremhasmadeitclearthatthereisnobestmachinelearningalgorithm,and,inparticular,nobestformofregularization.Insteadwemustchooseaformofregularizationthatiswell-suitedtotheparticulartaskwewanttosolve.Thephilosophyofdeeplearningingeneralandthisbookinparticularisthataverywiderangeoftasks(suchasalloftheintellectualtasksthatpeoplecando)mayallbesolvedeﬀectivelyusingverygeneral-purposeformsofregularization.5.3HyperparametersandValidationSetsMostmachinelearningalgorithmshaveseveralsettingsthatwecanusetocontrolthebehaviorofthelearningalgorithm.Thesesettingsarecalledhyperparame-ters.Thevaluesofhyperparametersarenotadaptedbythelearningalgorithmitself(thoughwecan designa nestedlearning procedure whereone learningalgorithmlearnsthebesthyperparametersforanotherlearningalgorithm).Inthepolynomialregressionexamplewesawinﬁgure,thereisasingle5.2hyperparameter:thedegreeofthepolynomial,whichactsasacapacityhyper-parameter.Theλvalueusedtocontrolthestrengthofweightdecayisanotherexampleofahyperparameter.Sometimesasettingischosentobeahyperparameterthatthelearningal-gorithmdoesnotlearnbecauseitisdiﬃculttooptimize.Morefrequently,the120 CHAPTER5.MACHINELEARNINGBASICSsettingmustbeahyperparameterbecauseitisnotappropriatetolearnthathyperparameteronthetrainingset.Thisappliestoallhyperparametersthatcontrolmodelcapacity.Iflearnedonthetrainingset,suchhyperparameterswouldalwayschoosethemaximumpossiblemodelcapacity,resultinginoverﬁtting(refertoﬁgure).Forexample,wecanalwaysﬁtthetrainingsetbetterwithahigher5.3degreepolynomialandaweightdecaysettingofλ= 0thanwecouldwithalowerdegreepolynomialandapositiveweightdecaysetting.Tosolvethisproblem,weneedavalidationsetofexamplesthatthetrainingalgorithmdoesnotobserve.Earlierwediscussedhowaheld-outtestset,composedofexamplescomingfromthesamedistributionasthetrainingset,canbeusedtoestimatethegeneralizationerrorofalearner,afterthelearningprocesshascompleted.Itisimportantthatthetestexamplesarenotusedinanywaytomakechoicesaboutthemodel,includingitshyperparameters. Forthisreason,noexamplefromthetestsetcanbeusedinthevalidationset.Therefore,wealwaysconstructthevalidationsetfromthetrainingdata.Speciﬁcally,wesplitthetrainingdataintotwodisjointsubsets.Oneofthesesubsetsisusedtolearntheparameters.Theothersubsetisourvalidationset,usedtoestimatethegeneralizationerrorduringoraftertraining,allowingforthehyperparameterstobeupdatedaccordingly.Thesubsetofdatausedtolearntheparametersisstilltypicallycalledthetrainingset,eventhoughthismaybeconfusedwiththelargerpoolofdatausedfortheentiretrainingprocess.Thesubsetofdatausedtoguidetheselectionofhyperparametersiscalledthevalidationset.Typically,oneusesabout80%ofthetrainingdatafortrainingand20%forvalidation.Sincethevalidationsetisusedto“train”thehyperparameters,thevalidationseterrorwillunderestimatethegeneralizationerror,thoughtypicallybyasmalleramountthanthetrainingerror.Afterallhyperparameteroptimizationiscomplete,thegeneralizationerrormaybeestimatedusingthetestset.Inpractice, whenthesametestsethasbeenusedrepeatedlytoevaluateperformanceofdiﬀerentalgorithmsovermanyyears,andespeciallyifweconsideralltheattemptsfromthescientiﬁccommunityatbeatingthereportedstate-of-the-artperformanceonthattestset,weenduphavingoptimisticevaluationswiththetestsetaswell.Benchmarkscanthusbecomestaleandthendonotreﬂectthetrueﬁeldperformanceofatrainedsystem.Thankfully,thecommunitytendstomoveontonew(andusuallymoreambitiousandlarger)benchmarkdatasets.121 CHAPTER5.MACHINELEARNINGBASICS5.3.1Cross-ValidationDividingthedatasetintoaﬁxedtrainingsetandaﬁxedtestsetcanbeproblematicifitresultsinthetestsetbeingsmall.Asmalltestsetimpliesstatisticaluncertaintyaroundtheestimatedaveragetesterror,makingitdiﬃculttoclaimthatalgorithmAworksbetterthanalgorithmonthegiventask.BWhenthedatasethashundredsofthousandsofexamplesormore,thisisnotaseriousissue.Whenthedatasetistoosmall,arealternativeproceduresenableonetousealloftheexamplesintheestimationofthemeantesterror,atthepriceofincreasedcomputationalcost.Theseproceduresarebasedontheideaofrepeatingthetrainingandtestingcomputationondiﬀerentrandomlychosensubsetsorsplitsoftheoriginaldataset.Themostcommonoftheseisthek-foldcross-validationprocedure,showninalgorithm,inwhichapartitionofthedatasetisformedby5.1splittingitintoknon-overlappingsubsets.Thetesterrormaythenbeestimatedbytakingtheaveragetesterroracrossktrials.Ontriali,thei-thsubsetofthedataisusedasthetestsetandtherestofthedataisusedasthetrainingset.Oneproblemisthatthereexistnounbiasedestimatorsofthevarianceofsuchaverageerrorestimators(BengioandGrandvalet2004,),butapproximationsaretypicallyused.5.4Estimators,BiasandVarianceTheﬁeldofstatisticsgivesusmanytoolsthatcanbeusedtoachievethemachinelearninggoalofsolvingatasknotonlyonthetrainingsetbutalsotogeneralize.Foundationalconceptssuchasparameterestimation,biasandvarianceareusefultoformallycharacterizenotionsofgeneralization,underﬁttingandoverﬁtting.5.4.1PointEstimationPointestimationistheattempttoprovidethesingle“best”predictionofsomequantityofinterest.Ingeneralthequantityofinterestcanbeasingleparameteroravectorofparametersinsomeparametricmodel,suchastheweightsinourlinearregressionexampleinsection,butitcanalsobeawholefunction.5.1.4Inordertodistinguishestimatesofparametersfromtheirtruevalue,ourconventionwillbetodenoteapointestimateofaparameterbyθˆθ.Let{x(1),...,x()m}beasetofmindependentandidenticallydistributed122 CHAPTER5.MACHINELEARNINGBASICSAlgorithm5.1Thek-foldcross-validationalgorithm.ItcanbeusedtoestimategeneralizationerrorofalearningalgorithmAwhenthegivendatasetDistoosmallforasimpletrain/testortrain/validsplittoyieldaccurateestimationofgeneralizationerror,becausethemeanofalossLonasmalltestsetmayhavetoohighvariance.ThedatasetDcontainsaselementstheabstractexamplesz()i(forthei-thexample),whichcouldstandforan(input,target)pairz()i= (x()i,y()i)inthecaseofsupervisedlearning,orforjustaninputz()i=x()iinthecaseofunsupervisedlearning. ThealgorithmreturnsthevectoroferrorseforeachexampleinD,whosemeanistheestimatedgeneralizationerror. Theerrorsonindividualexamplescanbeusedtocomputeaconﬁdenceintervalaroundthemean(equation). Whiletheseconﬁdenceintervalsarenotwell-justiﬁedafterthe5.47useofcross-validation,itisstillcommonpracticetousethemtodeclarethatalgorithmAisbetterthanalgorithmBonlyiftheconﬁdenceintervaloftheerrorofalgorithmAliesbelowanddoesnotintersecttheconﬁdenceintervalofalgorithmB.DeﬁneKFoldXV():D,A,L,kRequire:D,thegivendataset,withelementsz()iRequire:A,thelearningalgorithm,seenasafunctionthattakesadatasetasinputandoutputsalearnedfunctionRequire:L,thelossfunction,seenasafunctionfromalearnedfunctionfandanexamplez()i∈∈DtoascalarRRequire:k,thenumberoffoldsSplitintomutuallyexclusivesubsetsDkDi,whoseunionis.Dfordoikfromto1fi= (ADD\\i)forz()jinDidoej= (Lfi,z()j)endforendforReturne123 CHAPTER5.MACHINELEARNINGBASICS(i.i.d.)datapoints.Aorisanyfunctionofthedata:pointestimatorstatisticˆθm= (gx(1),...,x()m).(5.19)Thedeﬁnitiondoesnotrequirethatgreturnavaluethatisclosetothetrueθoreventhattherangeofgisthesameasthesetofallowablevaluesofθ.Thisdeﬁnitionofapointestimatorisverygeneralandallowsthedesignerofanestimatorgreatﬂexibility.Whilealmostanyfunctionthusqualiﬁesasanestimator,agoodestimatorisafunctionwhoseoutputisclosetothetrueunderlyingθthatgeneratedthetrainingdata.Fornow,wetakethefrequentistperspectiveonstatistics.Thatis,weassumethatthetrueparametervalueθisﬁxedbutunknown,whilethepointestimateˆθisafunctionofthedata.Sincethedataisdrawnfromarandomprocess,anyfunctionofthedataisrandom.Thereforeˆθisarandomvariable.Pointestimationcanalsorefertotheestimationoftherelationshipbetweeninputandtargetvariables.Werefertothesetypesofpointestimatesasfunctionestimators.FunctionEstimationAswementionedabove,sometimesweareinterestedinperformingfunctionestimation(orfunctionapproximation).Herewearetryingtopredictavariableygivenaninputvectorx.Weassumethatthereisafunctionf(x)thatdescribestheapproximaterelationshipbetweenyandx.Forexample,wemayassumethaty=f(x)+,wherestandsforthepartofythatisnotpredictablefromx. Infunctionestimation,weareinterestedinapproximatingfwithamodelorestimateˆf.Functionestimationisreallyjustthesameasestimatingaparameterθ;thefunctionestimatorˆfissimplyapointestimatorinfunctionspace.Thelinearregressionexample(discussedaboveinsection)and5.1.4thepolynomialregressionexample(discussedinsection)arebothexamplesof5.2scenariosthatmaybeinterpretedeitherasestimatingaparameterworestimatingafunctionˆfymappingfromtox.Wenowreviewthemostcommonlystudiedpropertiesofpointestimatorsanddiscusswhattheytellusabouttheseestimators.5.4.2BiasThebiasofanestimatorisdeﬁnedas:bias(ˆθm) = (Eˆθm)−θ(5.20)124 CHAPTER5.MACHINELEARNINGBASICSwheretheexpectationisoverthedata(seenassamplesfromarandomvariable)andθisthetrueunderlyingvalueofθusedtodeﬁnethedatageneratingdistri-bution.Anestimatorˆθmissaidtobeunbiasedifbias(ˆθm) =0,whichimpliesthatE(ˆθm)=θ.Anestimatorˆθmissaidtobeasymptoticallyunbiasediflimm→∞bias(ˆθm) = 0,whichimpliesthatlimm→∞E(ˆθm) = θ.Example:BernoulliDistributionConsiderasetofsamples{x(1),...,x()m}thatareindependentlyandidenticallydistributedaccordingtoaBernoullidistri-butionwithmean:θPx(()i;) = θθx()i(1)−θ(1−x()i).(5.21)Acommonestimatorfortheθparameterofthisdistributionisthemeanofthetrainingsamples:ˆθm=1mmi=1x()i.(5.22)Todeterminewhetherthisestimatorisbiased,wecansubstituteequation5.22intoequation:5.20bias(ˆθm) = [Eˆθm]−θ(5.23)= E1mmi=1x()i−θ(5.24)=1mmi=1Ex()i−θ(5.25)=1mmi=11x()i=0x()iθx()i(1)−θ(1−x()i)−θ(5.26)=1mmi=1()θ−θ(5.27)= = 0θθ−(5.28)Sincebias(ˆθ) = 0,wesaythatourestimatorˆθisunbiased.Example:GaussianDistributionEstimatoroftheMeanNow,considerasetofsamples{x(1),...,x()m}thatareindependentlyandidenticallydistributedaccordingtoaGaussiandistributionp(x()i) =N(x()i;µ,σ2),wherei∈{1,...,m}.125 CHAPTER5.MACHINELEARNINGBASICSRecallthattheGaussianprobabilitydensityfunctionisgivenbypx(()i;µ,σ2) =1√2πσ2exp−12(x()i−µ)2σ2.(5.29)AcommonestimatoroftheGaussianmeanparameterisknownasthesamplemean:ˆµm=1mmi=1x()i(5.30)Todeterminethebiasofthesamplemean,weareagaininterestedincalculatingitsexpectation:bias(ˆµm) = [ˆEµm]−µ(5.31)= E1mmi=1x()i−µ(5.32)=1mmi=1Ex()i−µ(5.33)=1mmi=1µ−µ(5.34)= = 0µµ−(5.35)ThusweﬁndthatthesamplemeanisanunbiasedestimatorofGaussianmeanparameter.Example:EstimatorsoftheVarianceofaGaussianDistributionAsanexample,wecomparetwodiﬀerentestimatorsofthevarianceparameterσ2ofaGaussiandistribution.Weareinterestedinknowingifeitherestimatorisbiased.Theﬁrstestimatorofσ2weconsiderisknownasthesamplevariance:ˆσ2m=1mmi=1x()i−ˆµm2,(5.36)whereˆµmisthesamplemean,deﬁnedabove.Moreformally,weareinterestedincomputingbias(ˆσ2m) = [ˆEσ2m]−σ2(5.37)126 CHAPTER5.MACHINELEARNINGBASICSWebeginbyevaluatingthetermE[ˆσ2m]:E[ˆσ2m] =E1mmi=1x()i−ˆµm2(5.38)=m−1mσ2(5.39)Returningtoequation,weconcludethatthebiasof5.37ˆσ2mis−σ2/m.Therefore,thesamplevarianceisabiasedestimator.Theunbiasedsamplevarianceestimator˜σ2m=1m−1mi=1x()i−ˆµm2(5.40)providesanalternativeapproach.Asthenamesuggeststhisestimatorisunbiased.Thatis,weﬁndthatE[˜σ2m] = σ2:E[˜σ2m] = E1m−1mi=1x()i−ˆµm2(5.41)=mm−1E[ˆσ2m](5.42)=mm−1m−1mσ2(5.43)= σ2.(5.44)Wehavetwoestimators:oneisbiasedandtheotherisnot.Whileunbiasedestimatorsareclearlydesirable,theyarenotalwaysthe“best”estimators.Aswewillseeweoftenusebiasedestimatorsthatpossessotherimportantproperties.5.4.3VarianceandStandardErrorAnotherpropertyoftheestimatorthatwemightwanttoconsiderishowmuchweexpectittovaryasafunctionofthedatasample.Justaswecomputedtheexpectationoftheestimatortodetermineitsbias,wecancomputeitsvariance.ThevarianceofanestimatorissimplythevarianceVar(ˆθ)(5.45)wheretherandomvariableisthetrainingset.Alternately,thesquarerootofthevarianceiscalledthe,denotedstandarderrorSE(ˆθ).127 CHAPTER5.MACHINELEARNINGBASICSThevarianceorthestandarderrorofanestimatorprovidesameasureofhowwewouldexpecttheestimatewecomputefromdatatovaryasweindependentlyresamplethedatasetfromtheunderlyingdatageneratingprocess.Justaswemightlikeanestimatortoexhibitlowbiaswewouldalsolikeittohaverelativelylowvariance.Whenwecomputeanystatisticusingaﬁnitenumberofsamples,ourestimateofthetrueunderlyingparameterisuncertain,inthesensethatwecouldhaveobtainedothersamplesfromthesamedistributionandtheirstatisticswouldhavebeendiﬀerent.Theexpecteddegreeofvariationinanyestimatorisasourceoferrorthatwewanttoquantify.ThestandarderrorofthemeanisgivenbySE(ˆµm) =Var1mmi=1x()i=σ√m,(5.46)whereσ2isthetruevarianceofthesamplesxi.Thestandarderrorisoftenestimatedbyusinganestimateofσ.Unfortunately,neitherthesquarerootofthesamplevariancenorthesquarerootoftheunbiasedestimatorofthevarianceprovideanunbiasedestimateofthestandarddeviation.Bothapproachestendtounderestimatethetruestandarddeviation,butarestillusedinpractice.Thesquarerootoftheunbiasedestimatorofthevarianceislessofanunderestimate.Forlarge,theapproximationisquitereasonable.mThestandarderrorofthemeanisveryusefulinmachinelearningexperiments.Weoftenestimatethegeneralizationerrorbycomputingthesamplemeanoftheerroronthetestset.Thenumberofexamplesinthetestsetdeterminestheaccuracyofthisestimate.Takingadvantageofthecentrallimittheorem,whichtellsusthatthemeanwillbeapproximatelydistributedwithanormaldistribution,wecanusethestandarderrortocomputetheprobabilitythatthetrueexpectationfallsinanychoseninterval.Forexample,the95%conﬁdenceintervalcenteredonthemeanˆµmis(ˆµm−196SE(ˆ.µm)ˆ,µm+196SE(ˆ.µm)),(5.47)underthenormaldistributionwithmeanˆµmandvarianceSE(ˆµm)2.Inmachinelearningexperiments,itiscommontosaythatalgorithmAisbetterthanalgorithmBiftheupperboundofthe95%conﬁdenceintervalfortheerrorofalgorithmAislessthanthelowerboundofthe95%conﬁdenceintervalfortheerrorofalgorithmB.128 CHAPTER5.MACHINELEARNINGBASICSExample: BernoulliDistributionWeonceagainconsiderasetofsamples{x(1),...,x()m}drawnindependentlyandidenticallyfromaBernoullidistribution(recallP(x()i;θ) =θx()i(1−θ)(1−x()i)).Thistimeweareinterestedincomputingthevarianceoftheestimatorˆθm=1mmi=1x()i.Varˆθm= Var1mmi=1x()i(5.48)=1m2mi=1Varx()i(5.49)=1m2mi=1θθ(1−)(5.50)=1m2mθθ(1−)(5.51)=1mθθ(1−)(5.52)Thevarianceoftheestimatordecreasesasafunctionofm,thenumberofexamplesinthedataset.Thisisacommonpropertyofpopularestimatorsthatwewillreturntowhenwediscussconsistency(seesection).5.4.55.4.4TradingoﬀBiasandVariancetoMinimizeMeanSquaredErrorBiasandvariancemeasuretwodiﬀerentsourcesoferrorinanestimator.Biasmeasurestheexpecteddeviationfromthetruevalueofthefunctionorparameter.Varianceontheotherhand,providesameasureofthedeviationfromtheexpectedestimatorvaluethatanyparticularsamplingofthedataislikelytocause.Whathappenswhenwearegivenachoicebetweentwoestimators,onewithmorebiasandonewithmorevariance?Howdowechoosebetweenthem?Forexample,imaginethatweareinterestedinapproximatingthefunctionshowninﬁgureandweareonlyoﬀeredthechoicebetweenamodelwithlargebiasand5.2onethatsuﬀersfromlargevariance.Howdowechoosebetweenthem?Themostcommonwaytonegotiatethistrade-oﬀistousecross-validation.Empirically,cross-validationishighlysuccessfulonmanyreal-worldtasks.Alter-natively,wecanalsocomparethemeansquarederror(MSE)oftheestimates:MSE = [(Eˆθm−θ)2](5.53)= Bias(ˆθm)2+Var(ˆθm)(5.54)129 CHAPTER5.MACHINELEARNINGBASICSTheMSEmeasurestheoverallexpecteddeviation—inasquarederrorsense—betweentheestimatorandthetruevalueoftheparameterθ.Asisclearfromequation,evaluatingtheMSEincorporatesboththebiasandthevariance.5.54DesirableestimatorsarethosewithsmallMSEandtheseareestimatorsthatmanagetokeepboththeirbiasandvariancesomewhatincheck. CapacityBiasGeneralizationerrorVarianceOptimalcapacityOverﬁtting zoneUnderﬁtting zone Figure5.6:Ascapacityincreases(x-axis),bias(dotted)tendstodecreaseandvariance(dashed)tendstoincrease,yieldinganotherU-shapedcurveforgeneralizationerror(boldcurve).Ifwevarycapacityalongoneaxis,thereisanoptimalcapacity,withunderﬁttingwhenthecapacityisbelowthisoptimumandoverﬁttingwhenitisabove.Thisrelationshipissimilartotherelationshipbetweencapacity,underﬁtting,andoverﬁtting,discussedinsectionandﬁgure.5.25.3Therelationshipbetweenbiasandvarianceistightlylinkedtothemachinelearningconceptsofcapacity,underﬁttingandoverﬁtting.Inthecasewheregen-eralizationerrorismeasuredbytheMSE(wherebiasandvariancearemeaningfulcomponentsofgeneralizationerror),increasingcapacitytendstoincreasevarianceanddecreasebias.Thisisillustratedinﬁgure,whereweseeagaintheU-shaped5.6curveofgeneralizationerrorasafunctionofcapacity.5.4.5ConsistencySofarwehavediscussedthepropertiesofvariousestimatorsforatrainingsetofﬁxedsize.Usually,wearealsoconcernedwiththebehaviorofanestimatorastheamountoftrainingdatagrows.Inparticular,weusuallywishthat,asthenumberofdatapointsminourdatasetincreases,ourpointestimatesconvergetothetrue130 CHAPTER5.MACHINELEARNINGBASICSvalueofthecorrespondingparameters.Moreformally,wewouldlikethatplimm→∞ˆθm= θ.(5.55)Thesymbolplimindicatesconvergenceinprobability,meaningthatforany>0,P(|ˆθm−|θ>)→0asm→∞.Theconditiondescribedbyequationis5.55knownasconsistency.Itissometimesreferredtoasweakconsistency,withstrongconsistencyreferringtothealmostsureconvergenceofˆθtoθ.Almostsureconvergenceofasequenceofrandomvariablesx(1),x(2),...toavaluexoccurswhenp(limm→∞x()m= ) = 1x.Consistencyensuresthatthebiasinducedbytheestimatordiminishesasthenumberofdataexamplesgrows.However,thereverseisnottrue—asymptoticunbiasednessdoesnotimplyconsistency. Forexample,considerestimatingthemeanparameterµofanormaldistributionN(x;µ,σ2),withadatasetconsistingofmsamples:{x(1),...,x()m}.Wecouldusetheﬁrstsamplex(1)ofthedatasetasanunbiasedestimator:ˆθ=x(1).Inthatcase,E(ˆθm)=θsotheestimatorisunbiasednomatterhowmanydatapointsareseen.This,ofcourse,impliesthattheestimateisasymptoticallyunbiased.However,thisisnotaconsistentestimatorasitisthecasethatnotˆθm→→∞θmas.5.5MaximumLikelihoodEstimationPreviously,wehaveseensomedeﬁnitionsofcommonestimatorsandanalyzedtheirproperties.Butwheredidtheseestimatorscomefrom?Ratherthanguessingthatsomefunctionmightmakeagoodestimatorandthenanalyzingitsbiasandvariance,wewouldliketohavesomeprinciplefromwhichwecanderivespeciﬁcfunctionsthataregoodestimatorsfordiﬀerentmodels.Themostcommonsuchprincipleisthemaximumlikelihoodprinciple.ConsiderasetofmexamplesX={x(1),...,x()m}drawnindependentlyfromthetruebutunknowndatageneratingdistributionpdata()x.Letpmodel(x;θ)beaparametricfamilyofprobabilitydistributionsoverthesamespaceindexedbyθ.Inotherwords,pmodel(x;θ)mapsanyconﬁgurationxtoarealnumberestimatingthetrueprobabilitypdata()x.ThemaximumlikelihoodestimatorforisthendeﬁnedasθθML= argmaxθpmodel(;)Xθ(5.56)= argmaxθmi=1pmodel(x()i;)θ(5.57)131 CHAPTER5.MACHINELEARNINGBASICSThisproductovermanyprobabilitiescanbeinconvenientforavarietyofreasons.Forexample,itispronetonumericalunderﬂow.Toobtainamoreconvenientbutequivalentoptimizationproblem,weobservethattakingthelogarithmofthelikelihooddoesnotchangeitsargmaxbutdoesconvenientlytransformaproductintoasum:θML= argmaxθmi=1logpmodel(x()i;)θ.(5.58)Becausetheargmaxdoesnotchangewhenwerescalethecostfunction,wecandividebymtoobtainaversionofthecriterionthatisexpressedasanexpectationwithrespecttotheempiricaldistributionˆpdatadeﬁnedbythetrainingdata:θML= argmaxθEx∼ˆpdatalogpmodel(;)xθ.(5.59)Onewaytointerpretmaximumlikelihoodestimationistoviewitasminimizingthedissimilaritybetweentheempiricaldistributionˆpdatadeﬁnedbythetrainingsetandthemodeldistribution,withthedegreeofdissimilaritybetweenthetwomeasuredbytheKLdivergence.TheKLdivergenceisgivenbyDKL(ˆpdatapmodel) = Ex∼ˆpdata[log ˆpdata()logx−pmodel()]x.(5.60)Thetermontheleftisafunctiononlyofthedatageneratingprocess,notthemodel.ThismeanswhenwetrainthemodeltominimizetheKLdivergence,weneedonlyminimize−Ex∼ˆpdata[logpmodel()]x(5.61)whichisofcoursethesameasthemaximizationinequation.5.59MinimizingthisKLdivergencecorrespondsexactlytominimizingthecross-entropybetweenthedistributions.Manyauthorsusetheterm“cross-entropy”toidentifyspeciﬁcallythenegativelog-likelihoodofaBernoulliorsoftmaxdistribution,butthatisamisnomer.Anylossconsistingofanegativelog-likelihoodisacross-entropybetweentheempiricaldistributiondeﬁnedbythetrainingsetandtheprobabilitydistributiondeﬁnedbymodel.Forexample,meansquarederroristhecross-entropybetweentheempiricaldistributionandaGaussianmodel.Wecanthusseemaximumlikelihoodasanattempttomakethemodeldis-tributionmatchtheempiricaldistributionˆpdata.Ideally,wewouldliketomatchthetruedatageneratingdistributionpdata,butwehavenodirectaccesstothisdistribution.WhiletheoptimalθisthesameregardlessofwhetherwearemaximizingthelikelihoodorminimizingtheKLdivergence,thevaluesoftheobjectivefunctions132 CHAPTER5.MACHINELEARNINGBASICSarediﬀerent.Insoftware,weoftenphrasebothasminimizingacostfunction.Maximumlikelihoodthusbecomesminimizationofthenegativelog-likelihood(NLL),orequivalently,minimizationofthecrossentropy.TheperspectiveofmaximumlikelihoodasminimumKLdivergencebecomeshelpfulinthiscasebecausetheKLdivergencehasaknownminimumvalueofzero.Thenegativelog-likelihoodcanactuallybecomenegativewhenisreal-valued.x5.5.1ConditionalLog-LikelihoodandMeanSquaredErrorThemaximumlikelihoodestimatorcanreadilybegeneralizedtothecasewhereourgoalistoestimateaconditionalprobabilityP(yx|;θ)inordertopredictygivenx.Thisisactuallythemostcommonsituationbecauseitformsthebasisformostsupervisedlearning.IfXrepresentsallourinputsandYallourobservedtargets,thentheconditionalmaximumlikelihoodestimatorisθML= argmaxθP.(;)YX|θ(5.62)Iftheexamplesareassumedtobei.i.d.,thenthiscanbedecomposedintoθML= argmaxθmi=1log(Py()i|x()i;)θ.(5.63)Example:LinearRegressionasMaximumLikelihoodLinearregression,introducedearlierinsection,maybejustiﬁedasamaximumlikelihood5.1.4procedure.Previously,wemotivatedlinearregressionasanalgorithmthatlearnstotakeaninputxandproduceanoutputvalueˆy.Themappingfromxtoˆyischosentominimizemeansquarederror,acriterionthatweintroducedmoreorlessarbitrarily.Wenowrevisitlinearregressionfromthepointofviewofmaximumlikelihoodestimation.Insteadofproducingasinglepredictionˆy,wenowthinkofthemodelasproducingaconditionaldistributionp(y|x).Wecanimaginethatwithaninﬁnitelylargetrainingset,wemightseeseveraltrainingexampleswiththesameinputvaluexbutdiﬀerentvaluesofy. Thegoalofthelearningalgorithmisnowtoﬁtthedistributionp(y|x)toallofthosediﬀerentyvaluesthatareallcompatiblewithx.Toderivethesamelinearregressionalgorithmweobtainedbefore,wedeﬁnep(y|x) =N(y;ˆy(x;w),σ2).Thefunctionˆy(x;w)givesthepredictionofthemeanoftheGaussian.Inthisexample,weassumethatthevarianceisﬁxedtosomeconstantσ2chosenbytheuser.Wewillseethatthischoiceofthefunctionalformofp(y|x)causesthemaximumlikelihoodestimationproceduretoyieldthesamelearningalgorithmaswedevelopedbefore.Sincethe133 CHAPTER5.MACHINELEARNINGBASICSexamplesareassumedtobei.i.d.,theconditionallog-likelihood(equation)is5.63givenbymi=1log(py()i|x()i;)θ(5.64)=log−mσ−m2log(2)π−mi=1ˆy()i−y()i22σ2,(5.65)whereˆy()iistheoutputofthelinearregressiononthei-thinputx()iandmisthenumberofthetrainingexamples.Comparingthelog-likelihoodwiththemeansquarederror,MSEtrain=1mmi=1||ˆy()i−y()i||2,(5.66)weimmediatelyseethatmaximizingthelog-likelihoodwithrespecttowyieldsthesameestimateoftheparameterswasdoesminimizingthemeansquarederror.Thetwocriteriahavediﬀerentvaluesbutthesamelocationoftheoptimum.ThisjustiﬁestheuseoftheMSEasamaximumlikelihoodestimationprocedure.Aswewillsee,themaximumlikelihoodestimatorhasseveraldesirableproperties.5.5.2PropertiesofMaximumLikelihoodThemainappealofthemaximumlikelihoodestimatoristhatitcanbeshowntobethebestestimatorasymptotically,asthenumberofexamplesm→∞,intermsofitsrateofconvergenceasincreases.mUnderappropriate conditions, themaximumlikelihood estimatorhas thepropertyofconsistency(seesectionabove),meaningthatasthenumber5.4.5oftrainingexamplesapproachesinﬁnity,themaximumlikelihoodestimateofaparameterconvergestothetruevalueoftheparameter.Theseconditionsare:•Thetruedistributionpdatamustliewithinthemodelfamilypmodel(·;θ).Otherwise,noestimatorcanrecoverpdata.•Thetruedistributionpdatamustcorrespondtoexactlyonevalueofθ.Other-wise,maximumlikelihoodcanrecoverthecorrectpdata,butwillnotbeabletodeterminewhichvalueofwasusedbythedatageneratingprocessing.θThereareotherinductiveprinciplesbesidesthemaximumlikelihoodestima-tor,manyofwhichsharethepropertyofbeingconsistentestimators. However,134 CHAPTER5.MACHINELEARNINGBASICSconsistentestimatorscandiﬀerintheirstatisticeﬃciency,meaningthatoneconsistentestimatormayobtainlowergeneralizationerrorforaﬁxednumberofsamplesm,orequivalently,mayrequirefewerexamplestoobtainaﬁxedlevelofgeneralizationerror.Statisticaleﬃciencyistypicallystudiedintheparametriccase(likeinlinearregression)whereourgoalistoestimatethevalueofaparameter(andassumingitispossibletoidentifythetrueparameter),notthevalueofafunction.Awaytomeasurehowclosewearetothetrueparameterisbytheexpectedmeansquarederror,computingthesquareddiﬀerencebetweentheestimatedandtrueparametervalues,wheretheexpectationisovermtrainingsamplesfromthedatageneratingdistribution.Thatparametricmeansquarederrordecreasesasmincreases,andformlarge,theCramér-Raolowerbound(,;,)showsthatnoRao1945Cramér1946consistentestimatorhasalowermeansquarederrorthanthemaximumlikelihoodestimator.Forthesereasons(consistencyandeﬃciency),maximumlikelihoodisoftenconsideredthepreferredestimatortouseformachinelearning.Whenthenumberofexamplesissmallenoughtoyieldoverﬁttingbehavior,regularizationstrategiessuchasweightdecaymaybeusedtoobtainabiasedversionofmaximumlikelihoodthathaslessvariancewhentrainingdataislimited.5.6BayesianStatisticsSofarwehavediscussedfrequentiststatisticsandapproachesbasedonestimat-ingasinglevalueofθ,thenmakingallpredictionsthereafterbasedonthatoneestimate.Anotherapproachistoconsiderallpossiblevaluesofθwhenmakingaprediction.ThelatteristhedomainofBayesianstatistics.Asdiscussed insection , the frequentist perspective isthat thetrue5.4.1parametervalueθisﬁxedbutunknown,whilethepointestimateˆθisarandomvariableonaccountofitbeingafunctionofthedataset(whichisseenasrandom).TheBayesianperspectiveonstatisticsisquitediﬀerent. TheBayesianusesprobabilitytoreﬂectdegreesofcertaintyofstatesofknowledge.Thedatasetisdirectlyobservedandsoisnotrandom.Ontheotherhand,thetrueparameterθisunknownoruncertainandthusisrepresentedasarandomvariable.Beforeobservingthedata,werepresentourknowledgeofθusingthepriorprobabilitydistribution,p(θ)(sometimesreferredtoassimply“theprior”).Generally,themachinelearningpractitionerselectsapriordistributionthatisquitebroad(i.e.withhighentropy)toreﬂectahighdegreeofuncertaintyinthe135 CHAPTER5.MACHINELEARNINGBASICSvalueofθbeforeobservinganydata.Forexample,onemightassumethataprioriθliesinsomeﬁniterangeorvolume,withauniformdistribution. Manypriorsinsteadreﬂectapreferencefor“simpler” solutions(suchassmallermagnitudecoeﬃcients,orafunctionthatisclosertobeingconstant).Nowconsiderthatwehaveasetofdatasamples{x(1),...,x()m}.Wecanrecovertheeﬀectofdataonourbeliefaboutθbycombiningthedatalikelihoodpx((1),...,x()m|θ)withthepriorviaBayes’rule:px(θ|(1),...,x()m) =px((1),...,x()m|θθ)(p)px((1),...,x()m)(5.67)InthescenarioswhereBayesianestimationistypicallyused,thepriorbeginsasarelativelyuniformorGaussiandistributionwithhighentropy,andtheobservationofthedatausuallycausestheposteriortoloseentropyandconcentratearoundafewhighlylikelyvaluesoftheparameters.Relativetomaximumlikelihoodestimation,Bayesianestimationoﬀerstwoimportantdiﬀerences.First,unlikethemaximumlikelihoodapproachthatmakespredictionsusingapointestimateofθ,theBayesianapproachistomakepredictionsusingafulldistributionoverθ.Forexample,afterobservingmexamples,thepredicteddistributionoverthenextdatasample,x(+1)m,isgivenbypx((+1)m|x(1),...,x()m) =px((+1)m||θθ)(px(1),...,x()m)d.θ(5.68)Hereeachvalueofθwithpositiveprobabilitydensitycontributestothepredictionofthenextexample,withthecontributionweightedbytheposteriordensityitself.Afterhavingobserved{x(1),...,x()m},ifwearestillquiteuncertainaboutthevalueofθ,thenthisuncertaintyisincorporateddirectlyintoanypredictionswemightmake.Insection,wediscussedhowthefrequentistapproachaddressestheuncer-5.4taintyinagivenpointestimateofθbyevaluatingitsvariance.Thevarianceoftheestimatorisanassessmentofhowtheestimatemightchangewithalternativesamplingsoftheobserveddata.TheBayesiananswertothequestionofhowtodealwiththeuncertaintyintheestimatoristosimplyintegrateoverit,whichtendstoprotectwellagainstoverﬁtting. Thisintegralisofcoursejustanapplicationofthelawsofprobability,makingtheBayesianapproachsimpletojustify,whilethefrequentistmachineryforconstructinganestimatorisbasedontheratheradhocdecisiontosummarizeallknowledgecontainedinthedatasetwithasinglepointestimate.ThesecondimportantdiﬀerencebetweentheBayesianapproachtoestimationandthemaximumlikelihoodapproachisduetothecontributionoftheBayesian136 CHAPTER5.MACHINELEARNINGBASICSpriordistribution.Thepriorhasaninﬂuencebyshiftingprobabilitymassdensitytowardsregionsoftheparameterspacethatarepreferred.Inpractice,aprioritheprioroftenexpressesapreferenceformodelsthataresimplerormoresmooth.CriticsoftheBayesianapproachidentifythepriorasasourceofsubjectivehumanjudgmentimpactingthepredictions.Bayesianmethodstypicallygeneralizemuchbetterwhenlimitedtrainingdataisavailable,buttypicallysuﬀerfromhighcomputationalcostwhenthenumberoftrainingexamplesislarge.Example:BayesianLinearRegressionHereweconsidertheBayesianesti-mationapproachtolearningthelinearregressionparameters.Inlinearregression,welearnalinearmappingfromaninputvectorx∈Rntopredictthevalueofascalar.Thepredictionisparametrizedbythevectory∈Rw∈Rn:ˆy= wx.(5.69)Givenasetofmtrainingsamples(X()train,y()train),wecanexpressthepredictionofovertheentiretrainingsetas:yˆy()train= X()trainw.(5.70)ExpressedasaGaussianconditionaldistributionony()train,wehavep(y()train|X()train,wy) = (N()train;X()trainwI,)(5.71)∝exp−12(y()train−X()trainw)(y()train−X()trainw),(5.72)wherewefollowthestandardMSEformulationinassumingthattheGaussianvarianceonyisone.Inwhatfollows,toreducethenotationalburden,wereferto(X()train,y()train)()assimplyXy,.Todeterminetheposteriordistributionoverthemodelparametervectorw,weﬁrstneedtospecifyapriordistribution.Thepriorshouldreﬂectournaivebeliefaboutthevalueoftheseparameters.Whileitissometimesdiﬃcultorunnaturaltoexpressourpriorbeliefsintermsoftheparametersofthemodel,inpracticewetypicallyassumeafairlybroaddistributionexpressingahighdegreeofuncertaintyaboutθ. Forreal-valuedparametersitiscommontouseaGaussianasapriordistribution:p() = (;wNwµ0,Λ0) exp∝−12(wµ−0)Λ−10(wµ−0),(5.73)137 CHAPTER5.MACHINELEARNINGBASICSwhereµ0andΛ0arethepriordistributionmeanvectorandcovariancematrixrespectively.1Withthepriorthusspeciﬁed,wecannowproceedindeterminingtheposteriordistributionoverthemodelparameters.p,p,p(wX|y) ∝(yX|w)()w(5.74)∝exp−12()yXw−()yXw−exp−12(wµ−0)Λ−10(wµ−0)(5.75)∝exp−12−2yXww+XXww+Λ−10wµ−20Λ−10w.(5.76)WenowdeﬁneΛm=XX+Λ−10−1andµm= ΛmXy+Λ−10µ0.Usingthesenewvariables,weﬁndthattheposteriormayberewrittenasaGaussiandistribution:p,(wX|y) exp∝−12(wµ−m)Λ−1m(wµ−m)+12µmΛ−1mµm(5.77)∝exp−12(wµ−m)Λ−1m(wµ−m).(5.78)Alltermsthatdonotincludetheparametervectorwhavebeenomitted;theyareimpliedbythefactthatthedistributionmustbenormalizedtointegrateto.1EquationshowshowtonormalizeamultivariateGaussiandistribution.3.23ExaminingthisposteriordistributionallowsustogainsomeintuitionfortheeﬀectofBayesianinference.Inmostsituations,wesetµ0to0.IfwesetΛ0=1αI,thenµmgivesthesameestimateofwasdoesfrequentistlinearregressionwithaweightdecaypenaltyofαww.OnediﬀerenceisthattheBayesianestimateisundeﬁnedifαissettozero—-wearenotallowedtobegintheBayesianlearningprocesswithaninﬁnitelywideprioronw.ThemoreimportantdiﬀerenceisthattheBayesianestimateprovidesacovariancematrix,showinghowlikelyallthediﬀerentvaluesofare,ratherthanprovidingonlytheestimatewµm.5.6.1Maximum(MAP)EstimationAPosterioriWhilethemostprincipledapproachistomakepredictionsusingthefullBayesianposteriordistributionovertheparameterθ,itisstilloftendesirabletohavea1Unlessthereisareasontoassumeaparticularcovariancestructure,wetypicallyassumeadiagonalcovariancematrixΛ0= diag(λ0).138 CHAPTER5.MACHINELEARNINGBASICSsinglepointestimate. OnecommonreasonfordesiringapointestimateisthatmostoperationsinvolvingtheBayesianposteriorformostinterestingmodelsareintractable,andapointestimateoﬀersatractableapproximation.Ratherthansimplyreturningtothemaximumlikelihoodestimate,wecanstillgainsomeofthebeneﬁtoftheBayesianapproachbyallowingthepriortoinﬂuencethechoiceofthepointestimate.Onerationalwaytodothisistochoosethemaximumaposteriori(MAP)pointestimate.TheMAPestimatechoosesthepointofmaximalposteriorprobability(ormaximalprobabilitydensityinthemorecommoncaseofcontinuous):θθMAP= argmaxθp() = argmaxθx|θlog()+log()pxθ|pθ.(5.79)Werecognize,aboveontherighthandside,logp(xθ|),i.e.thestandardlog-likelihoodterm,and,correspondingtothepriordistribution.log()pθAsanexample,consideralinearregressionmodelwithaGaussianpriorontheweightsw.IfthispriorisgivenbyN(w;0,1λI2),thenthelog-priorterminequationisproportionaltothefamiliar5.79λwwweightdecaypenalty,plusatermthatdoesnotdependonwanddoesnotaﬀectthelearningprocess.MAPBayesianinferencewithaGaussianpriorontheweightsthuscorrespondstoweightdecay.AswithfullBayesianinference,MAPBayesianinferencehastheadvantageofleveraginginformationthatisbroughtbythepriorandcannotbefoundinthetrainingdata.ThisadditionalinformationhelpstoreducethevarianceintheMAPpointestimate(incomparisontotheMLestimate).However,itdoessoatthepriceofincreasedbias.Manyregularizedestimationstrategies,suchasmaximumlikelihoodlearningregularizedwithweightdecay,canbeinterpretedasmakingtheMAPapproxima-tiontoBayesianinference.Thisviewapplieswhentheregularizationconsistsofaddinganextratermtotheobjectivefunctionthatcorrespondstologp(θ).NotallregularizationpenaltiescorrespondtoMAPBayesianinference.Forexample,someregularizertermsmaynotbethelogarithmofaprobabilitydistribution.Otherregularizationtermsdependonthedata,whichofcourseapriorprobabilitydistributionisnotallowedtodo.MAPBayesianinferenceprovidesastraightforwardwaytodesigncomplicatedyetinterpretableregularizationterms.Forexample,amorecomplicatedpenaltytermcanbederivedbyusingamixtureofGaussians,ratherthanasingleGaussiandistribution,astheprior(NowlanandHinton1992,).139 CHAPTER5.MACHINELEARNINGBASICS5.7SupervisedLearningAlgorithmsRecallfromsectionthatsupervisedlearningalgorithmsare,roughlyspeaking,5.1.3learningalgorithmsthatlearntoassociatesomeinputwithsomeoutput,givenatrainingsetofexamplesofinputsxandoutputsy. Inmanycasestheoutputsymaybediﬃculttocollectautomaticallyandmustbeprovidedbyahuman“supervisor,”butthetermstillappliesevenwhenthetrainingsettargetswerecollectedautomatically.5.7.1ProbabilisticSupervisedLearningMost supervised learning algorithms inthis book are based onestimating aprobabilitydistributionp(y|x).Wecandothissimplybyusingmaximumlikelihoodestimationtoﬁndthebestparametervectorθforaparametricfamilyofdistributions.py(|xθ;)Wehavealreadyseenthatlinearregressioncorrespondstothefamilypyy(|Nxθ;) = (;θxI,).(5.80)Wecangeneralizelinearregressiontotheclassiﬁcationscenariobydeﬁningadiﬀerentfamilyofprobabilitydistributions.Ifwehavetwoclasses,class0andclass1,thenweneedonlyspecifytheprobabilityofoneoftheseclasses.Theprobabilityofclass1determinestheprobabilityofclass0,becausethesetwovaluesmustaddupto1.Thenormaldistributionoverreal-valuednumbersthatweusedforlinearregressionisparametrizedintermsofamean.Anyvaluewesupplyforthismeanisvalid.Adistributionoverabinaryvariableisslightlymorecomplicated,becauseitsmeanmustalwaysbebetween0and1.Onewaytosolvethisproblemistousethelogisticsigmoidfunctiontosquashtheoutputofthelinearfunctionintotheinterval(0,1)andinterpretthatvalueasaprobability:pyσ(= 1 ;) = |xθ(θx).(5.81)Thisapproachisknownaslogisticregression(asomewhatstrangenamesinceweusethemodelforclassiﬁcationratherthanregression).Inthecaseoflinearregression,wewereabletoﬁndtheoptimalweightsbysolvingthenormalequations.Logisticregressionissomewhatmorediﬃcult.Thereisnoclosed-formsolutionforitsoptimalweights.Instead,wemustsearchforthembymaximizingthelog-likelihood.Wecandothisbyminimizingthenegativelog-likelihood(NLL)usinggradientdescent.140 CHAPTER5.MACHINELEARNINGBASICSThissamestrategycanbeappliedtoessentiallyanysupervisedlearningproblem,bywritingdownaparametricfamilyofconditionalprobabilitydistributionsovertherightkindofinputandoutputvariables.5.7.2SupportVectorMachinesOneofthemostinﬂuentialapproachestosupervisedlearningisthesupportvectormachine(,;Boseretal.1992CortesandVapnik1995,).Thismodelissimilartologisticregressioninthatitisdrivenbyalinearfunctionwx+b.Unlikelogisticregression,thesupportvectormachinedoesnotprovideprobabilities,butonlyoutputsaclassidentity.TheSVMpredictsthatthepositiveclassispresentwhenwx+bispositive.Likewise,itpredictsthatthenegativeclassispresentwhenwx+bisnegative.Onekeyinnovationassociatedwithsupportvectormachinesisthekerneltrick.Thekerneltrickconsistsofobservingthatmanymachinelearningalgorithmscanbewrittenexclusivelyintermsofdotproductsbetweenexamples.Forexample,itcanbeshownthatthelinearfunctionusedbythesupportvectormachinecanbere-writtenaswx+= +bbmi=1αixx()i(5.82)wherex()iisatrainingexampleandαisavectorofcoeﬃcients.Rewritingthelearningalgorithmthiswayallowsustoreplacexbytheoutputofagivenfeaturefunctionφ(x) andthedotproductwithafunctionk(xx,()i) =φ(x)·φ(x()i) calledakernel.The·operatorrepresentsaninnerproductanalogoustoφ(x)φ(x()i).Forsomefeaturespaces,wemaynotuseliterallythevectorinnerproduct.Insomeinﬁnitedimensionalspaces,weneedtouseotherkindsofinnerproducts,forexample,innerproductsbasedonintegrationratherthansummation.Acompletedevelopmentofthesekindsofinnerproductsisbeyondthescopeofthisbook.Afterreplacingdotproductswithkernelevaluations,wecanmakepredictionsusingthefunctionfb() = x+iαik,(xx()i).(5.83)Thisfunctionisnonlinearwithrespecttox,buttherelationshipbetweenφ(x)andf(x)islinear.Also,therelationshipbetweenαandf(x)islinear.Thekernel-basedfunctionisexactlyequivalenttopreprocessingthedatabyapplyingφ()xtoallinputs,thenlearningalinearmodelinthenewtransformedspace.Thekerneltrickispowerfulfortworeasons.First,itallowsustolearnmodelsthatarenonlinearasafunctionofxusingconvexoptimizationtechniquesthatare141 CHAPTER5.MACHINELEARNINGBASICSguaranteedtoconvergeeﬃciently.Thisispossiblebecauseweconsiderφﬁxedandoptimizeonlyα,i.e.,theoptimizationalgorithmcanviewthedecisionfunctionasbeinglinearinadiﬀerentspace.Second,thekernelfunctionkoftenadmitsanimplementationthatissigniﬁcantlymorecomputationaleﬃcientthannaivelyconstructingtwovectorsandexplicitlytakingtheirdotproduct.φ()xInsomecases,φ(x)canevenbeinﬁnitedimensional,whichwouldresultinaninﬁnitecomputationalcostforthenaive,explicitapproach.Inmanycases,k(xx,)isanonlinear,tractablefunctionofxevenwhenφ(x)isintractable.Asanexampleofaninﬁnite-dimensionalfeaturespacewithatractablekernel,weconstructafeaturemappingφ(x)overthenon-negativeintegersx.Supposethatthismappingreturnsavectorcontainingxonesfollowedbyinﬁnitelymanyzeros.Wecanwriteakernelfunctionk(x,x()i) =min(x,x()i)thatisexactlyequivalenttothecorrespondinginﬁnite-dimensionaldotproduct.ThemostcommonlyusedkernelistheGaussiankernelk,,σ(uvuv) = (N−;02I)(5.84)whereN(x;µ,Σ)isthestandardnormaldensity.Thiskernelisalsoknownastheradialbasisfunction(RBF)kernel,becauseitsvaluedecreasesalonglinesinvspaceradiatingoutwardfromu.TheGaussiankernelcorrespondstoadotproductinaninﬁnite-dimensionalspace,butthederivationofthisspaceislessstraightforwardthaninourexampleofthekernelovertheintegers.minWecanthinkoftheGaussiankernelasperformingakindoftemplatematch-ing.Atrainingexamplexassociatedwithtraininglabelybecomesatemplateforclassy.WhenatestpointxisnearxaccordingtoEuclideandistance,theGaussiankernelhasalargeresponse,indicatingthatxisverysimilartothextemplate.Themodelthenputsalargeweightontheassociatedtraininglabely.Overall,thepredictionwillcombinemanysuchtraininglabelsweightedbythesimilarityofthecorrespondingtrainingexamples.Supportvectormachinesarenottheonlyalgorithmthatcanbeenhancedusingthekerneltrick.Manyotherlinearmodelscanbeenhancedinthisway.Thecategoryofalgorithmsthatemploythekerneltrickisknownaskernelmachinesorkernelmethods(,;WilliamsandRasmussen1996Schölkopf1999etal.,).Amajordrawbacktokernelmachinesisthatthecostofevaluatingthedecisionfunctionislinearinthenumberoftrainingexamples,becausethei-thexamplecontributesatermαik(xx,()i)tothedecisionfunction.Supportvectormachinesareabletomitigatethisbylearninganαvectorthatcontainsmostlyzeros.Classifyinganewexamplethenrequiresevaluatingthekernelfunctiononlyforthetrainingexamplesthathavenon-zeroαi.Thesetrainingexamplesareknown142 CHAPTER5.MACHINELEARNINGBASICSassupportvectors.Kernelmachinesalsosuﬀerfromahighcomputationalcostoftrainingwhenthedatasetislarge.Wewillrevisitthisideainsection.Kernelmachineswith5.9generickernelsstruggletogeneralizewell.Wewillexplainwhyinsection.The5.11modernincarnationofdeeplearningwasdesignedtoovercometheselimitationsofkernelmachines.ThecurrentdeeplearningrenaissancebeganwhenHintonetal.()demonstratedthataneuralnetworkcouldoutperformtheRBFkernelSVM2006ontheMNISTbenchmark.5.7.3OtherSimpleSupervisedLearningAlgorithmsWehavealreadybrieﬂyencounteredanothernon-probabilisticsupervisedlearningalgorithm,nearestneighborregression.Moregenerally,k-nearestneighborsisafamilyoftechniquesthatcanbeusedforclassiﬁcationorregression.Asanon-parametriclearningalgorithm,k-nearestneighborsisnotrestrictedtoaﬁxednumberofparameters.Weusuallythinkofthek-nearestneighborsalgorithmasnothavinganyparameters,butratherimplementingasimplefunctionofthetrainingdata.Infact,thereisnotevenreallyatrainingstageorlearningprocess.Instead,attesttime,whenwewanttoproduceanoutputyforanewtestinputx,weﬁndthek-nearestneighborstoxinthetrainingdataX.Wethenreturntheaverageofthecorrespondingyvaluesinthetrainingset.Thisworksforessentiallyanykindofsupervisedlearningwherewecandeﬁneanaverageoveryvalues.Inthecaseofclassiﬁcation,wecanaverageoverone-hotcodevectorscwithcy= 1andci= 0forallothervaluesofi.Wecantheninterprettheaverageovertheseone-hotcodesasgivingaprobabilitydistributionoverclasses.Asanon-parametriclearningalgorithm,k-nearestneighborcanachieveveryhighcapacity.Forexample,supposewehaveamulticlassclassiﬁcationtaskandmeasureperformancewith0-1loss.Inthissetting,-nearestneighborconvergestodoubletheBayeserrorasthe1numberoftrainingexamplesapproachesinﬁnity.TheerrorinexcessoftheBayeserrorresultsfromchoosingasingleneighborbybreakingtiesbetweenequallydistantneighborsrandomly.Whenthereisinﬁnitetrainingdata,alltestpointsxwillhaveinﬁnitelymanytrainingsetneighborsatdistancezero.Ifweallowthealgorithmtousealloftheseneighborstovote,ratherthanrandomlychoosingoneofthem,theprocedureconvergestotheBayeserrorrate. Thehighcapacityofk-nearestneighborsallowsittoobtainhighaccuracygivenalargetrainingset.However,itdoessoathighcomputationalcost,anditmaygeneralizeverybadlygivenasmall,ﬁnitetrainingset.Oneweaknessofk-nearestneighborsisthatitcannotlearnthatonefeatureismorediscriminativethananother.Forexample,imaginewehavearegressiontaskwithx∈R100drawnfromanisotropicGaussian143 CHAPTER5.MACHINELEARNINGBASICSdistribution,butonlyasinglevariablex1isrelevanttotheoutput.Supposefurtherthatthisfeaturesimplyencodestheoutputdirectly,i.e.thaty=x1inallcases.Nearestneighborregressionwillnotbeabletodetectthissimplepattern.Thenearestneighborofmostpointsxwillbedeterminedbythelargenumberoffeaturesx2throughx100,notbythelonefeaturex1. Thustheoutputonsmalltrainingsetswillessentiallyberandom. 144 CHAPTER5.MACHINELEARNINGBASICS 0101 11101 011 1111111011010010001110111111010010001001111111 11Figure5.7:Diagramsdescribinghowadecisiontreeworks.(Top)Eachnodeofthetreechoosestosendtheinputexampletothechildnodeontheleft(0)ororthechildnodeontheright(1).Internalnodesaredrawnascirclesandleafnodesassquares.Eachnodeisdisplayedwithabinarystringidentiﬁercorrespondingtoitspositioninthetree,obtainedbyappendingabittoitsparentidentiﬁer(0=chooseleftortop,1=chooserightorbottom).(Bottom)Thetreedividesspaceintoregions.The2DplaneshowshowadecisiontreemightdivideR2.Thenodesofthetreeareplottedinthisplane,witheachinternalnodedrawnalongthedividinglineitusestocategorizeexamples,andleafnodesdrawninthecenteroftheregionofexamplestheyreceive.Theresultisapiecewise-constantfunction,withonepieceperleaf.Eachleafrequiresatleastonetrainingexampletodeﬁne,soitisnotpossibleforthedecisiontreetolearnafunctionthathasmorelocalmaximathanthenumberoftrainingexamples.145 CHAPTER5.MACHINELEARNINGBASICSAnothertypeoflearningalgorithmthatalsobreakstheinputspaceintoregionsandhasseparateparametersforeachregionisthedecisiontree(,Breimanetal.1984)anditsmanyvariants.Asshowninﬁgure,eachnodeofthedecision5.7treeisassociatedwitharegionintheinputspace,andinternalnodesbreakthatregionintoonesub-regionforeachchildofthenode(typicallyusinganaxis-alignedcut). Spaceisthussub-dividedintonon-overlappingregions,withaone-to-onecorrespondencebetweenleafnodesandinputregions.Eachleafnodeusuallymapseverypointinitsinputregiontothesameoutput.Decisiontreesareusuallytrainedwithspecializedalgorithmsthatarebeyondthescopeofthisbook.Thelearningalgorithmcanbeconsiderednon-parametricifitisallowedtolearnatreeofarbitrarysize,thoughdecisiontreesareusuallyregularizedwithsizeconstraintsthatturnthemintoparametricmodelsinpractice.Decisiontreesastheyaretypicallyused,withaxis-alignedsplitsandconstantoutputswithineachnode,struggletosolvesomeproblemsthatareeasyevenforlogisticregression.Forexample,ifwehaveatwo-classproblemandthepositiveclassoccurswhereverx2>x1,thedecisionboundaryisnotaxis-aligned.Thedecisiontreewillthusneedtoapproximatethedecisionboundarywithmanynodes,implementingastepfunctionthatconstantlywalksbackandforthacrossthetruedecisionfunctionwithaxis-alignedsteps.Aswehaveseen,nearestneighborpredictorsanddecisiontreeshavemanylimitations.Nonetheless,theyareusefullearningalgorithmswhencomputationalresourcesareconstrained.Wecanalsobuildintuitionformoresophisticatedlearningalgorithmsbythinkingaboutthesimilaritiesanddiﬀerencesbetweensophisticatedalgorithmsand-NNordecisiontreebaselines.kSee(), (), ()orothermachineMurphy2012Bishop2006Hastieetal.2001learningtextbooksformorematerialontraditionalsupervisedlearningalgorithms.5.8UnsupervisedLearningAlgorithmsRecallfromsectionthatunsupervisedalgorithmsarethosethatexperience5.1.3only“features”butnotasupervisionsignal.Thedistinctionbetweensupervisedandunsupervisedalgorithmsisnotformallyandrigidlydeﬁnedbecausethereisnoobjectivetestfordistinguishingwhetheravalueisafeatureoratargetprovidedbyasupervisor.Informally,unsupervisedlearningreferstomostattemptstoextractinformationfromadistributionthatdonotrequirehumanlabortoannotateexamples.Thetermisusuallyassociatedwithdensityestimation,learningtodrawsamplesfromadistribution,learningtodenoisedatafromsomedistribution,ﬁndingamanifoldthatthedataliesnear,orclusteringthedataintogroupsof146 CHAPTER5.MACHINELEARNINGBASICSrelatedexamples.Aclassicunsupervisedlearningtaskistoﬁndthe“best”representationofthedata.By‘best’wecanmeandiﬀerentthings,butgenerallyspeakingwearelookingforarepresentationthatpreservesasmuchinformationaboutxaspossiblewhileobeyingsomepenaltyorconstraintaimedatkeepingtherepresentationorsimplermoreaccessiblethanitself.xTherearemultiplewaysofdeﬁningarepresentation.Threeofthesimpler mostcommonincludelowerdimensionalrepresentations,sparserepresentationsandindependentrepresentations.Low-dimensionalrepresentationsattempttocompressasmuchinformationaboutxaspossibleinasmallerrepresentation.Sparserepresentations(,;,;Barlow1989OlshausenandField1996HintonandGhahramani1997,)embedthedatasetintoarepresentationwhoseentriesaremostlyzeroesformostinputs.Theuseofsparserepresentationstypicallyrequiresincreasingthedimensionalityoftherepresentation,sothattherepresentationbecomingmostlyzeroesdoesnotdiscardtoomuchinformation.Thisresultsinanoverallstructureoftherepresentationthattendstodistributedataalongtheaxesoftherepresentationspace.Independentrepresentationsattempttodisentanglethesourcesofvariationunderlyingthedatadistributionsuchthatthedimensionsoftherepresentationarestatisticallyindependent.Of coursethese three criteriaare certainly notmutuallyexclusive.Low-dimensionalrepresentationsoftenyieldelementsthathavefewerorweakerde-pendenciesthantheoriginalhigh-dimensionaldata.Thisisbecauseonewaytoreducethesizeofarepresentationistoﬁndandremoveredundancies.Identifyingandremovingmoreredundancyallowsthedimensionalityreductionalgorithmtoachievemorecompressionwhilediscardinglessinformation.Thenotionofrepresentationisoneofthecentralthemesofdeeplearningandthereforeoneofthecentralthemesinthisbook.Inthissection,wedevelopsomesimpleexamplesofrepresentationlearningalgorithms.Together,theseexamplealgorithmsshowhowtooperationalizeallthreeofthecriteriaabove.Mostoftheremainingchaptersintroduceadditionalrepresentationlearningalgorithmsthatdevelopthesecriteriaindiﬀerentwaysorintroduceothercriteria.5.8.1PrincipalComponentsAnalysisInsection,wesawthattheprincipalcomponentsanalysisalgorithmprovides2.12ameansofcompressingdata.WecanalsoviewPCAasanunsupervisedlearningalgorithmthatlearnsarepresentationofdata.Thisrepresentationisbasedontwoofthecriteriaforasimplerepresentationdescribedabove.PCAlearnsa147 CHAPTER5.MACHINELEARNINGBASICS −−201001020x1−20−1001020x2−−201001020z1−20−1001020z2 Figure5.8:PCAlearnsalinearprojectionthatalignsthedirectionofgreatestvariancewiththeaxesofthenewspace.(Left)Theoriginaldataconsistsofsamplesofx.Inthisspace,thevariancemightoccuralongdirectionsthatarenotaxis-aligned. (Right)Thetransformeddataz=xWnowvariesmostalongtheaxisz1.Thedirectionofsecondmostvarianceisnowalongz2.representationthathaslowerdimensionalitythantheoriginalinput.Italsolearnsarepresentationwhoseelementshavenolinearcorrelationwitheachother.Thisisaﬁrststeptowardthecriterionoflearningrepresentationswhoseelementsarestatisticallyindependent.Toachievefullindependence,arepresentationlearningalgorithmmustalsoremovethenonlinearrelationshipsbetweenvariables.PCAlearnsanorthogonal,lineartransformationofthedatathatprojectsaninputxtoarepresentationzasshowninﬁgure.Insection,wesawthat5.82.12wecouldlearnaone-dimensionalrepresentationthatbestreconstructstheoriginaldata(inthesenseofmeansquarederror)andthatthisrepresentationactuallycorrespondstotheﬁrstprincipalcomponentofthedata.ThuswecanusePCAasasimpleandeﬀectivedimensionalityreductionmethodthatpreservesasmuchoftheinformationinthedataaspossible(again,asmeasuredbyleast-squaresreconstructionerror).Inthefollowing,wewillstudyhowthePCArepresentationdecorrelatestheoriginaldatarepresentation.XLetusconsiderthemn×-dimensionaldesignmatrixX.Wewillassumethatthedatahasameanofzero,E[x] =0.Ifthisisnotthecase,thedatacaneasilybecenteredbysubtractingthemeanfromallexamplesinapreprocessingstep.Theunbiasedsamplecovariancematrixassociatedwithisgivenby:XVar[] =x1m−1XX.(5.85)148 CHAPTER5.MACHINELEARNINGBASICSPCAﬁndsarepresentation(throughlineartransformation)z=xWwhereVar[]zisdiagonal.Insection,wesawthattheprincipalcomponentsofadesignmatrix2.12XaregivenbytheeigenvectorsofXX.Fromthisview,XXWW= Λ.(5.86)Inthissection,weexploitanalternativederivationoftheprincipalcomponents.Theprincipalcomponentsmayalsobeobtainedviathesingularvaluedecomposition.Speciﬁcally,theyaretherightsingularvectorsofX.Toseethis,letWbetherightsingularvectorsinthedecompositionX=UWΣ. Wethenrecovertheoriginaleigenvectorequationwithastheeigenvectorbasis:WXX=UWΣUWΣ= WΣ2W.(5.87)TheSVDishelpfultoshowthatPCAresultsinadiagonalVar[z].UsingtheSVDof,wecanexpressthevarianceofas:XXVar[] =x1m−1XX(5.88)=1m−1(UWΣ)UWΣ(5.89)=1m−1WΣUUWΣ(5.90)=1m−1WΣ2W,(5.91)whereweusethefactthatUU=IbecausetheUmatrixofthesingularvaluedecompositionisdeﬁnedtobeorthogonal.Thisshowsthatifwetakez=xW,wecanensurethatthecovarianceofisdiagonalasrequired:zVar[] =z1m−1ZZ(5.92)=1m−1WXXW(5.93)=1m−1WWΣ2WW(5.94)=1m−1Σ2,(5.95)wherethistimeweusethefactthatWW=I,againfromthedeﬁnitionoftheSVD.149 CHAPTER5.MACHINELEARNINGBASICSTheaboveanalysisshowsthatwhenweprojectthedataxtoz,viathelineartransformationW,theresultingrepresentationhasadiagonalcovariancematrix(asgivenbyΣ2)whichimmediatelyimpliesthattheindividualelementsofzaremutuallyuncorrelated.ThisabilityofPCAtotransformdataintoarepresentationwheretheelementsaremutuallyuncorrelatedisaveryimportantpropertyofPCA.Itisasimpleexampleofarepresentationthatattemptstodisentangletheunknownfactorsofvariationunderlyingthedata. InthecaseofPCA,thisdisentanglingtakestheformofﬁndingarotationoftheinputspace(describedbyW)thatalignstheprincipalaxesofvariancewiththebasisofthenewrepresentationspaceassociatedwith.zWhilecorrelationisanimportantcategoryofdependencybetweenelementsofthedata,wearealsointerestedinlearningrepresentationsthatdisentanglemorecomplicatedformsoffeaturedependencies.Forthis,wewillneedmorethanwhatcanbedonewithasimplelineartransformation.5.8.2-meansClusteringkAnotherexampleofasimplerepresentationlearningalgorithmisk-meansclustering.Thek-meansclusteringalgorithmdividesthetrainingsetintokdiﬀerentclustersofexamplesthatareneareachother.Wecanthusthinkofthealgorithmasprovidingak-dimensionalone-hotcodevectorhrepresentinganinputx.Ifxbelongstoclusteri,thenhi= 1andallotherentriesoftherepresentationharezero.Theone-hotcodeprovidedbyk-meansclusteringisanexampleofasparserepresentation,becausethemajorityofitsentriesarezeroforeveryinput.Later,wewilldevelopotheralgorithmsthatlearnmoreﬂexiblesparserepresentations,wheremorethanoneentrycanbenon-zeroforeachinputx.One-hotcodesareanextremeexampleofsparserepresentationsthatlosemanyofthebeneﬁtsofadistributedrepresentation.Theone-hotcodestillconferssomestatisticaladvantages(itnaturallyconveystheideathatallexamplesinthesameclusteraresimilartoeachother)anditconfersthecomputationaladvantagethattheentirerepresentationmaybecapturedbyasingleinteger.Thek-meansalgorithmworksbyinitializingkdiﬀerentcentroids{µ(1),...,µ()k}todiﬀerentvalues,thenalternatingbetweentwodiﬀerentstepsuntilconvergence.Inonestep,eachtrainingexampleisassignedtoclusteri,whereiistheindexofthenearestcentroidµ()i.Intheotherstep,eachcentroidµ()iisupdatedtothemeanofalltrainingexamplesx()jassignedtocluster.i150 CHAPTER5.MACHINELEARNINGBASICSOnediﬃcultypertainingtoclusteringisthattheclusteringproblemisinherentlyill-posed,inthesensethatthereisnosinglecriterionthatmeasureshowwellaclusteringofthedatacorrespondstotherealworld.WecanmeasurepropertiesoftheclusteringsuchastheaverageEuclideandistancefromaclustercentroidtothemembersofthecluster.Thisallowsustotellhowwellweareabletoreconstructthetrainingdatafromtheclusterassignments.Wedonotknowhowwelltheclusterassignmentscorrespondtopropertiesoftherealworld.Moreover,theremaybemanydiﬀerentclusteringsthatallcorrespondwelltosomepropertyoftherealworld.Wemayhopetoﬁndaclusteringthatrelatestoonefeaturebutobtainadiﬀerent,equallyvalidclusteringthatisnotrelevanttoourtask.Forexample,supposethatweruntwoclusteringalgorithmsonadatasetconsistingofimagesofredtrucks,imagesofredcars,imagesofgraytrucks,andimagesofgraycars.Ifweaskeachclusteringalgorithmtoﬁndtwoclusters,onealgorithmmayﬁndaclusterofcarsandaclusteroftrucks,whileanothermayﬁndaclusterofredvehiclesandaclusterofgrayvehicles.Supposewealsorunathirdclusteringalgorithm,whichisallowedtodeterminethenumberofclusters.Thismayassigntheexamplestofourclusters,redcars,redtrucks,graycars,andgraytrucks.Thisnewclusteringnowatleastcapturesinformationaboutbothattributes,butithaslostinformationaboutsimilarity.Redcarsareinadiﬀerentclusterfromgraycars,justastheyareinadiﬀerentclusterfromgraytrucks. Theoutputoftheclusteringalgorithmdoesnottellusthatredcarsaremoresimilartograycarsthantheyaretograytrucks.Theyarediﬀerentfromboththings,andthatisallweknow.Theseissuesillustratesomeofthereasonsthatwemaypreferadistributedrepresentationtoaone-hotrepresentation.Adistributedrepresentationcouldhavetwoattributesforeachvehicle—onerepresentingitscolorandonerepresentingwhetheritisacaroratruck.Itisstillnotentirelyclearwhattheoptimaldistributedrepresentationis(howcanthelearningalgorithmknowwhetherthetwoattributesweareinterestedinarecolorandcar-versus-truckratherthanmanufacturerandage?)buthavingmanyattributesreducestheburdenonthealgorithmtoguesswhichsingleattributewecareabout,andallowsustomeasuresimilaritybetweenobjectsinaﬁne-grainedwaybycomparingmanyattributesinsteadofjusttestingwhetheroneattributematches.5.9StochasticGradientDescentNearlyallofdeeplearningispoweredbyoneveryimportantalgorithm:stochasticgradientdescentorSGD.Stochasticgradientdescentisanextensionofthe151 CHAPTER5.MACHINELEARNINGBASICSgradientdescentalgorithmintroducedinsection.4.3Arecurringprobleminmachinelearningisthatlargetrainingsetsarenecessaryforgoodgeneralization,butlargetrainingsetsarealsomorecomputationallyexpensive.Thecostfunctionusedbyamachinelearningalgorithmoftendecomposesasasumovertrainingexamplesofsomeper-examplelossfunction.Forexample,thenegativeconditionallog-likelihoodofthetrainingdatacanbewrittenasJ() = θEx,y∼ˆpdataL,y,(xθ) =1mmi=1L(x()i,y()i,θ)(5.96)whereistheper-examplelossLL,y,py.(xθ) = log−(|xθ;)Fortheseadditivecostfunctions,gradientdescentrequirescomputing∇θJ() =θ1mmi=1∇θL(x()i,y()i,.θ)(5.97)ThecomputationalcostofthisoperationisO(m).Asthetrainingsetsizegrowstobillionsofexamples,thetimetotakeasinglegradientstepbecomesprohibitivelylong.Theinsightofstochasticgradientdescentisthatthegradientisanexpectation.Theexpectationmaybeapproximatelyestimatedusingasmallsetofsamples.Speciﬁcally,oneachstepofthealgorithm,wecansampleaminibatchofexamplesB={x(1),...,x(m)}drawnuniformlyfromthetrainingset.Theminibatchsizemistypicallychosentobearelativelysmallnumberofexamples,rangingfrom1toafewhundred.Crucially,misusuallyheldﬁxedasthetrainingsetsizemgrows.Wemayﬁtatrainingsetwithbillionsofexamplesusingupdatescomputedononlyahundredexamples.Theestimateofthegradientisformedasg=1m∇θmi=1L(x()i,y()i,.θ)(5.98)usingexamplesfromtheminibatch.ThestochasticgradientdescentalgorithmBthenfollowstheestimatedgradientdownhill:θθg←−,(5.99)whereisthelearningrate.152 CHAPTER5.MACHINELEARNINGBASICSGradientdescentingeneralhasoftenbeenregardedassloworunreliable.Inthepast,theapplicationofgradientdescenttonon-convexoptimizationproblemswasregardedasfoolhardyorunprincipled.Today,weknowthatthemachinelearningmodelsdescribedinpartworkverywellwhentrainedwithgradientIIdescent.Theoptimizationalgorithmmaynotbeguaranteedtoarriveatevenalocalminimuminareasonableamountoftime,butitoftenﬁndsaverylowvalueofthecostfunctionquicklyenoughtobeuseful.Stochasticgradientdescenthasmanyimportantusesoutsidethecontextofdeeplearning.Itisthemainwaytotrainlargelinearmodelsonverylargedatasets.Foraﬁxedmodelsize,thecostperSGDupdatedoesnotdependonthetrainingsetsizem.Inpractice,weoftenusealargermodelasthetrainingsetsizeincreases,butwearenotforcedtodoso.Thenumberofupdatesrequiredtoreachconvergenceusuallyincreaseswithtrainingsetsize. However,asmapproachesinﬁnity,themodelwilleventuallyconvergetoitsbestpossibletesterrorbeforeSGDhassampledeveryexampleinthetrainingset.Increasingmfurtherwillnotextendtheamountoftrainingtimeneededtoreachthemodel’sbestpossibletesterror.Fromthispointofview,onecanarguethattheasymptoticcostoftrainingamodelwithSGDisasafunctionof.O(1)mPriortotheadventofdeeplearning,themainwaytolearnnonlinearmodelswastousethekerneltrickincombinationwithalinearmodel.Manykernellearningalgorithmsrequireconstructinganmm×matrixGi,j=k(x()i,x()j).ConstructingthismatrixhascomputationalcostO(m2),whichisclearlyundesirablefordatasetswith billionsof examples.In academia,starting in2006,deep learningwasinitiallyinterestingbecauseitwasabletogeneralizetonewexamplesbetterthancompetingalgorithmswhentrainedonmedium-sizeddatasetswithtensofthousandsofexamples.Soonafter,deeplearninggarneredadditionalinterestinindustry,becauseitprovidedascalablewayoftrainingnonlinearmodelsonlargedatasets.Stochasticgradientdescentandmanyenhancementstoitaredescribedfurtherinchapter.85.10BuildingaMachineLearningAlgorithmNearlyalldeeplearningalgorithmscanbedescribedasparticularinstancesofafairlysimplerecipe:combineaspeciﬁcationofadataset,acostfunction,anoptimizationprocedureandamodel.Forexample,thelinearregressionalgorithmcombinesadatasetconsistingof153 CHAPTER5.MACHINELEARNINGBASICSXyand,thecostfunctionJ,b(w) = −Ex,y∼ˆpdatalogpmodel()y|x,(5.100)themodelspeciﬁcationpmodel(y|x) =N(y;xw+b,1),and,inmostcases,theoptimizationalgorithmdeﬁnedbysolvingforwherethegradientofthecostiszerousingthenormalequations.Byrealizingthatwecanreplaceanyofthesecomponentsmostlyindependentlyfromtheothers,wecanobtainaverywidevarietyofalgorithms.Thecostfunctiontypicallyincludesatleastonetermthatcausesthelearningprocesstoperformstatisticalestimation.Themostcommoncostfunctionisthenegativelog-likelihood,sothatminimizingthecostfunctioncausesmaximumlikelihoodestimation.Thecostfunctionmayalsoincludeadditionalterms,suchasregularizationterms.Forexample,wecanaddweightdecaytothelinearregressioncostfunctiontoobtainJ,bλ(w) = ||||w22−Ex,y∼ˆpdatalogpmodel()y|x.(5.101)Thisstillallowsclosed-formoptimization.Ifwechangethemodeltobenonlinear,thenmostcostfunctionscannolongerbeoptimizedinclosedform.Thisrequiresustochooseaniterativenumericaloptimizationprocedure,suchasgradientdescent.Therecipeforconstructingalearningalgorithmbycombiningmodels,costs,andoptimizationalgorithmssupportsbothsupervisedandunsupervisedlearning.Thelinearregressionexampleshowshowtosupportsupervisedlearning.UnsupervisedlearningcanbesupportedbydeﬁningadatasetthatcontainsonlyXandprovidinganappropriateunsupervisedcostandmodel.Forexample,wecanobtaintheﬁrstPCAvectorbyspecifyingthatourlossfunctionisJ() = wEx∼ˆpdata||−||xr(;)xw22(5.102)whileourmodelisdeﬁnedtohavewwithnormoneandreconstructionfunctionr() = xwxw.Insomecases,thecostfunctionmaybeafunctionthatwecannotactuallyevaluate,forcomputationalreasons.Inthesecases,wecanstillapproximatelyminimizeitusingiterativenumericaloptimizationsolongaswehavesomewayofapproximatingitsgradients.Mostmachinelearningalgorithmsmakeuseofthisrecipe,thoughitmaynotimmediatelybeobvious.Ifamachinelearningalgorithmseemsespeciallyuniqueor154 CHAPTER5.MACHINELEARNINGBASICShand-designed,itcanusuallybeunderstoodasusingaspecial-caseoptimizer.Somemodelssuchasdecisiontreesork-meansrequirespecial-caseoptimizersbecausetheircostfunctionshaveﬂatregionsthatmaketheminappropriateforminimizationbygradient-basedoptimizers.Recognizingthatmostmachinelearningalgorithmscanbedescribedusingthisrecipehelpstoseethediﬀerentalgorithmsaspartofataxonomyofmethodsfordoingrelatedtasksthatworkforsimilarreasons,ratherthanasalonglistofalgorithmsthateachhaveseparatejustiﬁcations.5.11ChallengesMotivatingDeepLearningThesimplemachinelearningalgorithmsdescribedinthischapterworkverywellonawidevarietyofimportantproblems.However,theyhavenotsucceededinsolvingthecentralproblemsinAI,suchasrecognizingspeechorrecognizingobjects.ThedevelopmentofdeeplearningwasmotivatedinpartbythefailureoftraditionalalgorithmstogeneralizewellonsuchAItasks.Thissectionisabouthowthechallengeofgeneralizingtonewexamplesbecomesexponentiallymorediﬃcultwhenworkingwithhigh-dimensionaldata,andhowthemechanismsusedtoachievegeneralizationintraditionalmachinelearningareinsuﬃcienttolearncomplicatedfunctionsinhigh-dimensionalspaces.Suchspacesalsooftenimposehighcomputationalcosts.Deeplearningwasdesignedtoovercometheseandotherobstacles.5.11.1TheCurseofDimensionalityManymachinelearningproblemsbecomeexceedinglydiﬃcultwhenthenumberofdimensionsinthedataishigh.Thisphenomenonisknownasthecurseofdimensionality.Ofparticularconcernisthatthenumberofpossibledistinctconﬁgurationsofasetofvariablesincreasesexponentiallyasthenumberofvariablesincreases. 155 CHAPTER5.MACHINELEARNINGBASICS Figure5.9:Asthenumberofrelevantdimensionsofthedataincreases(fromlefttoright),thenumberofconﬁgurationsofinterestmaygrowexponentially.(Left)Inthisone-dimensionalexample,wehaveonevariableforwhichweonlycaretodistinguish10regionsofinterest.Withenoughexamplesfallingwithineachoftheseregions(eachregioncorrespondstoacellintheillustration),learningalgorithmscaneasilygeneralizecorrectly.Astraightforwardwaytogeneralizeistoestimatethevalueofthetargetfunctionwithineachregion(andpossiblyinterpolatebetweenneighboringregions).With2(Center)dimensionsitismorediﬃculttodistinguish10diﬀerentvaluesofeachvariable. Weneedtokeeptrackofupto10×10=100regions,andweneedatleastthatmanyexamplestocoverallthoseregions.With3dimensionsthisgrowsto(Right)103= 1000regionsandatleastthatmanyexamples.Forddimensionsandvvaluestobedistinguishedalongeachaxis,weseemtoneedO(vd)regionsandexamples. Thisisaninstanceofthecurseofdimensionality.FiguregraciouslyprovidedbyNicolasChapados.Thecurseofdimensionalityarisesinmanyplacesincomputerscience,andespeciallysoinmachinelearning.Onechallengeposedbythecurseofdimensionalityisastatisticalchallenge.Asillustratedinﬁgure,astatisticalchallengearisesbecausethenumberof5.9possibleconﬁgurationsofxismuchlargerthanthenumberoftrainingexamples.Tounderstandtheissue,letusconsiderthattheinputspaceisorganizedintoagrid,likeintheﬁgure.Wecandescribelow-dimensionalspacewithalownumberofgridcellsthataremostlyoccupiedbythedata.Whengeneralizingtoanewdatapoint,wecanusuallytellwhattodosimplybyinspectingthetrainingexamplesthatlieinthesamecellasthenewinput.Forexample,ifestimatingtheprobabilitydensityatsomepointx,wecanjustreturnthenumberoftrainingexamplesinthesameunitvolumecellasx,dividedbythetotalnumberoftrainingexamples.Ifwewishtoclassifyanexample,wecanreturnthemostcommonclassoftrainingexamplesinthesamecell. Ifwearedoingregressionwecanaveragethetargetvaluesobservedovertheexamplesinthatcell.Butwhataboutthecellsforwhichwehaveseennoexample?Becauseinhigh-dimensionalspacesthenumberofconﬁgurationsishuge,muchlargerthanournumberofexamples,atypicalgridcellhasnotrainingexampleassociatedwithit.Howcouldwepossiblysaysomething156 CHAPTER5.MACHINELEARNINGBASICSmeaningfulaboutthesenewconﬁgurations?Manytraditionalmachinelearningalgorithmssimplyassumethattheoutputatanewpointshouldbeapproximatelythesameastheoutputatthenearesttrainingpoint.5.11.2LocalConstancyandSmoothnessRegularizationInordertogeneralizewell,machinelearningalgorithmsneedtobeguidedbypriorbeliefsaboutwhatkindoffunctiontheyshouldlearn.Previously,wehaveseenthesepriorsincorporatedasexplicitbeliefsintheformofprobabilitydistributionsoverparametersofthemodel.Moreinformally,wemayalsodiscusspriorbeliefsasdirectlyinﬂuencingtheitselfandonlyindirectlyactingontheparametersfunctionviatheireﬀectonthefunction.Additionally,weinformallydiscusspriorbeliefsasbeingexpressedimplicitly,bychoosingalgorithmsthatarebiasedtowardchoosingsomeclassoffunctionsoveranother,eventhoughthesebiasesmaynotbeexpressed(orevenpossibletoexpress)intermsofaprobabilitydistributionrepresentingourdegreeofbeliefinvariousfunctions.Amongthemostwidelyusedoftheseimplicit“priors” isthesmoothnesspriororlocalconstancyprior.Thispriorstatesthatthefunctionwelearnshouldnotchangeverymuchwithinasmallregion.Manysimpleralgorithmsrelyexclusivelyonthispriortogeneralizewell,andasaresulttheyfailtoscaletothestatisticalchallengesinvolvedinsolvingAI-leveltasks.Throughoutthisbook,wewilldescribehowdeeplearningintroducesadditional(explicit andimplicit)priorsinorder toreducethegeneralizationerroronsophisticatedtasks.Here,weexplainwhythesmoothnessprioraloneisinsuﬃcientforthesetasks.Therearemanydiﬀerentwaystoimplicitlyorexplicitlyexpressapriorbeliefthatthelearnedfunctionshouldbesmoothorlocallyconstant.Allofthesediﬀerentmethodsaredesignedtoencouragethelearningprocesstolearnafunctionf∗thatsatisﬁestheconditionf∗() x≈f∗(+)x(5.103)formostconﬁgurationsxandsmallchange.Inotherwords,ifweknowagoodanswerforaninputx(forexample,ifxisalabeledtrainingexample)thenthatanswerisprobablygoodintheneighborhoodofx.Ifwehaveseveralgoodanswersinsomeneighborhoodwewouldcombinethem(bysomeformofaveragingorinterpolation)toproduceananswerthatagreeswithasmanyofthemasmuchaspossible.Anextremeexampleofthelocalconstancyapproachisthek-nearestneighborsfamilyoflearningalgorithms.Thesepredictorsareliterallyconstantovereach157 CHAPTER5.MACHINELEARNINGBASICSregioncontainingallthepointsxthathavethesamesetofknearestneighborsinthetrainingset.Fork= 1,thenumberofdistinguishableregionscannotbemorethanthenumberoftrainingexamples.Whilethek-nearestneighborsalgorithmcopiestheoutputfromnearbytrainingexamples,mostkernelmachinesinterpolatebetweentrainingsetoutputsassociatedwithnearbytrainingexamples.Animportantclassofkernelsisthefamilyoflocalkernelswherek(uv,)islargewhenu=vanddecreasesasuandvgrowfartherapartfromeachother.Alocalkernelcanbethoughtofasasimilarityfunctionthatperformstemplatematching,bymeasuringhowcloselyatestexamplexresembleseachtrainingexamplex()i. Muchofthemodernmotivationfordeeplearningisderivedfromstudyingthelimitationsoflocaltemplatematchingandhowdeepmodelsareabletosucceedincaseswherelocaltemplatematchingfails(,).Bengioetal.2006bDecisiontreesalsosuﬀerfromthelimitationsofexclusivelysmoothness-basedlearningbecausetheybreaktheinputspaceintoasmanyregionsasthereareleavesanduseaseparateparameter(orsometimesmanyparametersforextensionsofdecisiontrees)ineachregion.Ifthetargetfunctionrequiresatreewithatleastnleavestoberepresentedaccurately,thenatleastntrainingexamplesarerequiredtoﬁtthetree.Amultipleofnisneededtoachievesomelevelofstatisticalconﬁdenceinthepredictedoutput.Ingeneral,todistinguishO(k)regionsininputspace,allofthesemethodsrequireO(k) examples.TypicallythereareO(k) parameters,withO(1) parametersassociatedwitheachoftheO(k)regions.Thecaseofanearestneighborscenario,whereeachtrainingexamplecanbeusedtodeﬁneatmostoneregion,isillustratedinﬁgure.5.10Isthereawaytorepresentacomplexfunctionthathasmanymoreregionstobedistinguishedthanthenumberoftrainingexamples?Clearly,assumingonlysmoothnessoftheunderlyingfunctionwillnotallowalearnertodothat.For example, imaginethat thetargetfunctionis akind ofcheckerboard.Acheckerboardcontainsmanyvariationsbutthereisasimplestructuretothem.Imaginewhathappenswhenthenumberoftrainingexamplesissubstantiallysmallerthanthenumberofblackandwhitesquaresonthecheckerboard.Basedononlylocalgeneralizationandthesmoothnessorlocalconstancyprior,wewouldbeguaranteedtocorrectlyguessthecolorofanewpointifitlieswithinthesamecheckerboardsquareasatrainingexample.Thereisnoguaranteethatthelearnercouldcorrectlyextendthecheckerboardpatterntopointslyinginsquaresthatdonotcontaintrainingexamples.Withthisprioralone,theonlyinformationthatanexampletellsusisthecolorofitssquare,andtheonlywaytogetthecolorsofthe158 CHAPTER5.MACHINELEARNINGBASICS Figure5.10: Illustrationofhowthenearestneighboralgorithmbreaksuptheinputspaceintoregions. Anexample(representedherebyacircle)withineachregiondeﬁnestheregionboundary(representedherebythelines).Theyvalueassociatedwitheachexampledeﬁneswhattheoutputshouldbeforallpointswithinthecorrespondingregion. TheregionsdeﬁnedbynearestneighbormatchingformageometricpatterncalledaVoronoidiagram.Thenumberofthesecontiguousregionscannotgrowfasterthanthenumberoftrainingexamples.Whilethisﬁgureillustratesthebehaviorofthenearestneighboralgorithmspeciﬁcally,othermachinelearningalgorithmsthatrelyexclusivelyonthelocalsmoothnesspriorforgeneralizationexhibitsimilarbehaviors:eachtrainingexampleonlyinformsthelearnerabouthowtogeneralizeinsomeneighborhoodimmediatelysurroundingthatexample. 159 CHAPTER5.MACHINELEARNINGBASICSentirecheckerboardrightistocovereachofitscellswithatleastoneexample.Thesmoothnessassumptionandtheassociatednon-parametriclearningalgo-rithmsworkextremelywellsolongasthereareenoughexamplesforthelearningalgorithmtoobservehighpointsonmostpeaksandlowpointsonmostvalleysofthetrueunderlyingfunctiontobelearned.Thisisgenerallytruewhenthefunctiontobelearnedissmoothenoughandvariesinfewenoughdimensions.Inhighdimensions,evenaverysmoothfunctioncanchangesmoothlybutinadiﬀerentwayalongeachdimension.Ifthefunctionadditionallybehavesdiﬀerentlyindiﬀerentregions,itcanbecomeextremelycomplicatedtodescribewithasetoftrainingexamples.Ifthefunctioniscomplicated(wewanttodistinguishahugenumberofregionscomparedtothenumberofexamples),isthereanyhopetogeneralizewell?Theanswertobothofthesequestions—whetheritispossibletorepresentacomplicatedfunctioneﬃciently,andwhetheritispossiblefortheestimatedfunctiontogeneralizewelltonewinputs—isyes.Thekeyinsightisthataverylargenumberofregions,e.g.,O(2k),canbedeﬁnedwithO(k)examples,solongasweintroducesomedependenciesbetweentheregionsviaadditionalassumptionsabouttheunderlyingdatageneratingdistribution.Inthisway,wecanactuallygeneralizenon-locally(,;,).ManyBengioandMonperrus2005Bengioetal.2006cdiﬀerentdeeplearningalgorithmsprovideimplicitorexplicitassumptionsthatarereasonableforabroadrangeofAItasksinordertocapturetheseadvantages.Otherapproachestomachinelearningoftenmakestronger,task-speciﬁcas-sumptions.Forexample,wecouldeasilysolvethecheckerboardtaskbyprovidingtheassumptionthatthetargetfunctionisperiodic.Usuallywedonotincludesuchstrong,task-speciﬁcassumptionsintoneuralnetworkssothattheycangeneralizetoamuchwidervarietyofstructures.AItaskshavestructurethatismuchtoocomplextobelimitedtosimple,manuallyspeciﬁedpropertiessuchasperiodicity,sowewantlearningalgorithmsthatembodymoregeneral-purposeassumptions.Thecoreideaindeeplearningisthatweassumethatthedatawasgeneratedbythecompositionoffactorsorfeatures,potentiallyatmultiplelevelsinahierarchy.Manyothersimilarlygenericassumptionscanfurtherimprovedeeplearningal-gorithms. Theseapparentlymildassumptionsallowanexponentialgainintherelationshipbetweenthenumberofexamplesandthenumberofregionsthatcanbedistinguished.Theseexponentialgainsaredescribedmorepreciselyinsections6.4.115.415.5,and.Theexponentialadvantagesconferredbytheuseofdeep,distributedrepresentationscountertheexponentialchallengesposedbythecurseofdimensionality.160 CHAPTER5.MACHINELEARNINGBASICS5.11.3ManifoldLearningAnimportantconceptunderlyingmanyideasinmachinelearningisthatofamanifold.Amanifoldisaconnected region.Mathematically, it isasetofpoints,associatedwithaneighborhoodaroundeachpoint.Fromanygivenpoint,themanifoldlocallyappearstobeaEuclideanspace.Ineverydaylife,weexperiencethesurfaceoftheworldasa2-Dplane,butitisinfactasphericalmanifoldin3-Dspace.Thedeﬁnitionofaneighborhoodsurroundingeachpointimpliestheexistenceoftransformationsthatcanbeappliedtomoveonthemanifoldfromonepositiontoaneighboringone.Intheexampleoftheworld’ssurfaceasamanifold,onecanwalknorth,south,east,orwest.Althoughthereisaformalmathematicalmeaningtotheterm“manifold,”inmachinelearningittendstobeusedmorelooselytodesignateaconnectedsetofpointsthatcanbeapproximatedwellbyconsideringonlyasmallnumberofdegreesoffreedom,ordimensions,embeddedinahigher-dimensionalspace.Eachdimensioncorrespondstoalocaldirectionofvariation.Seeﬁgureforan5.11exampleoftrainingdatalyingnearaone-dimensionalmanifoldembeddedintwo-dimensionalspace.Inthecontextofmachinelearning,weallowthedimensionalityofthemanifoldtovaryfromonepointtoanother. Thisoftenhappenswhenamanifoldintersectsitself.Forexample,aﬁgureeightisamanifoldthathasasingledimensioninmostplacesbuttwodimensionsattheintersectionatthecenter. 0510152025303540........−10.−05.00.05.10.15.20.25. Figure5.11:Datasampledfromadistributioninatwo-dimensionalspacethatisactuallyconcentratednearaone-dimensionalmanifold,likeatwistedstring.Thesolidlineindicatestheunderlyingmanifoldthatthelearnershouldinfer.161 CHAPTER5.MACHINELEARNINGBASICSManymachinelearningproblemsseemhopelessifweexpectthemachinelearningalgorithmtolearnfunctionswithinterestingvariationsacrossallofRn.ManifoldlearningalgorithmssurmountthisobstaclebyassumingthatmostofRnconsistsofinvalidinputs, andthatinterestinginputsoccuronlyalongacollectionofmanifoldscontainingasmallsubsetofpoints,withinterestingvariationsintheoutputofthelearnedfunctionoccurringonlyalongdirectionsthatlieonthemanifold,orwithinterestingvariationshappeningonlywhenwemovefromonemanifoldtoanother.Manifoldlearningwasintroducedinthecaseofcontinuous-valueddataandtheunsupervisedlearningsetting,althoughthisprobabilityconcentrationideacanbegeneralizedtobothdiscretedataandthesupervisedlearningsetting:thekeyassumptionremainsthatprobabilitymassishighlyconcentrated.Theassumptionthatthedataliesalongalow-dimensionalmanifoldmaynotalwaysbecorrectoruseful.WearguethatinthecontextofAItasks,suchasthosethatinvolveprocessingimages,sounds,ortext,themanifoldassumptionisatleastapproximatelycorrect.Theevidenceinfavorofthisassumptionconsistsoftwocategoriesofobservations.Theﬁrstobservationinfavorofthemanifoldhypothesisisthattheproba-bilitydistributionoverimages,textstrings,andsoundsthatoccurinreallifeishighlyconcentrated.Uniformnoiseessentiallyneverresemblesstructuredinputsfromthesedomains. Figureshowshow,instead,uniformlysampledpoints5.12looklikethepatternsofstaticthatappearonanalogtelevisionsetswhennosignalisavailable.Similarly,ifyougenerateadocumentbypickinglettersuniformlyatrandom,whatistheprobabilitythatyouwillgetameaningfulEnglish-languagetext?Almostzero,again,becausemostofthelongsequencesoflettersdonotcorrespondtoanaturallanguagesequence:thedistributionofnaturallanguagesequencesoccupiesaverysmallvolumeinthetotalspaceofsequencesofletters. 162 CHAPTER5.MACHINELEARNINGBASICS Figure5.12:Samplingimagesuniformlyatrandom(byrandomlypickingeachpixelaccordingtoauniformdistribution)givesrisetonoisyimages.Althoughthereisanon-zeroprobabilitytogenerateanimageofafaceoranyotherobjectfrequentlyencounteredinAIapplications,weneveractuallyobservethishappeninginpractice.ThissuggeststhattheimagesencounteredinAIapplicationsoccupyanegligibleproportionofthevolumeofimagespace.Ofcourse,concentratedprobabilitydistributionsarenotsuﬃcienttoshowthatthedataliesonareasonablysmallnumberofmanifolds.Wemustalsoestablishthattheexamplesweencounterareconnectedtoeachotherbyother163 CHAPTER5.MACHINELEARNINGBASICSexamples,witheachexamplesurroundedbyotherhighlysimilarexamplesthatmaybereachedbyapplyingtransformationstotraversethemanifold.Thesecondargumentinfavorofthemanifoldhypothesisisthatwecanalsoimaginesuchneighborhoodsandtransformations,atleastinformally.Inthecaseofimages,wecancertainlythinkofmanypossibletransformationsthatallowustotraceoutamanifoldinimagespace:wecangraduallydimorbrightenthelights,graduallymoveorrotateobjectsintheimage,graduallyalterthecolorsonthesurfacesofobjects,etc.Itremainslikelythattherearemultiplemanifoldsinvolvedinmostapplications.Forexample,themanifoldofimagesofhumanfacesmaynotbeconnectedtothemanifoldofimagesofcatfaces.Thesethoughtexperimentssupportingthemanifoldhypothesesconveysomein-tuitivereasonssupportingit.Morerigorousexperiments (Cayton2005Narayanan,;andMitter2010Schölkopf1998RoweisandSaul2000Tenenbaum,;etal.,;,;etal.,2000Brand2003BelkinandNiyogi2003DonohoandGrimes2003Weinberger;,;,;,;andSaul2004,)clearlysupportthehypothesisforalargeclassofdatasetsofinterestinAI.Whenthedataliesonalow-dimensionalmanifold,itcanbemostnaturalformachinelearningalgorithmstorepresentthedataintermsofcoordinatesonthemanifold,ratherthanintermsofcoordinatesinRn.Ineverydaylife,wecanthinkofroadsas1-Dmanifoldsembeddedin3-Dspace.Wegivedirectionstospeciﬁcaddressesintermsofaddressnumbersalongthese1-Droads,notintermsofcoordinatesin3-Dspace.Extractingthesemanifoldcoordinatesischallenging,butholdsthepromisetoimprovemanymachinelearningalgorithms.Thisgeneralprincipleisappliedinmanycontexts.Figureshowsthemanifoldstructureof5.13adatasetconsistingoffaces.Bytheendofthisbook,wewillhavedevelopedthemethodsnecessarytolearnsuchamanifoldstructure.Inﬁgure,wewillsee20.6howamachinelearningalgorithmcansuccessfullyaccomplishthisgoal.Thisconcludespart,whichhasprovidedthebasicconceptsinmathematicsIandmachinelearningwhichareemployedthroughouttheremainingpartsofthebook.Youarenowpreparedtoembarkuponyourstudyofdeeplearning. 164 CHAPTER5.MACHINELEARNINGBASICS Figure5.13:TrainingexamplesfromtheQMULMultiviewFaceDataset(,)Gongetal.2000forwhichthesubjectswereaskedtomoveinsuchawayastocoverthetwo-dimensionalmanifoldcorrespondingtotwoanglesofrotation.Wewouldlikelearningalgorithmstobeabletodiscoveranddisentanglesuchmanifoldcoordinates.Figureillustratessucha20.6feat. 165 PartIIDeepNetworks:ModernPractices 166 Thispartofthebooksummarizesthestateofmoderndeeplearningasitisusedtosolvepracticalapplications.Deeplearninghasalonghistoryandmanyaspirations.Severalapproacheshavebeenproposedthathaveyettoentirelybearfruit.Severalambitiousgoalshaveyettoberealized.Theseless-developedbranchesofdeeplearningappearintheﬁnalpartofthebook.Thispartfocusesonlyonthoseapproachesthatareessentiallyworkingtech-nologiesthatarealreadyusedheavilyinindustry.Modern deeplearning provides avery powerful framework forsupervisedlearning.Byaddingmorelayersandmoreunitswithinalayer,adeepnetworkcanrepresentfunctionsofincreasingcomplexity.Mosttasksthatconsistofmappinganinputvectortoanoutputvector,andthatareeasyforapersontodorapidly,canbeaccomplishedviadeeplearning,givensuﬃcientlylargemodelsandsuﬃcientlylargedatasetsoflabeledtrainingexamples.Othertasks,thatcannotbedescribedasassociatingonevectortoanother,orthatarediﬃcultenoughthatapersonwouldrequiretimetothinkandreﬂectinordertoaccomplishthetask,remainbeyondthescopeofdeeplearningfornow.Thispartofthebookdescribesthecoreparametricfunctionapproximationtechnologythatisbehindnearlyallmodernpracticalapplicationsofdeeplearning.We begin by describingthe feedforward deepnetworkmodelthatisusedtorepresentthesefunctions.Next,wepresentadvancedtechniquesforregularizationandoptimizationofsuchmodels.Scalingthesemodelstolargeinputssuchashighresolutionimagesorlongtemporalsequencesrequiresspecialization.Weintroducetheconvolutionalnetworkforscalingtolargeimagesandtherecurrentneuralnetworkforprocessingtemporalsequences.Finally,wepresentgeneralguidelinesforthepracticalmethodologyinvolvedindesigning,building,andconﬁguringanapplicationinvolvingdeeplearning,andreviewsomeoftheapplicationsofdeeplearning.Thesechaptersarethemostimportantforapractitioner—someonewhowantstobeginimplementingandusingdeeplearningalgorithmstosolvereal-worldproblemstoday. 167 Chapter6DeepFeedforwardNetworksDeepfeedforwardnetworks,alsooftencalledfeedforwardneuralnetworks,ormultilayerperceptrons(MLPs),arethequintessentialdeeplearningmodels.Thegoalofafeedforwardnetworkistoapproximatesomefunctionf∗.Forexample,foraclassiﬁer,y=f∗(x)mapsaninputxtoacategoryy.Afeedforwardnetworkdeﬁnesamappingy=f(x;θ)andlearnsthevalueoftheparametersθthatresultinthebestfunctionapproximation.Thesemodelsarecalledfeedforwardbecauseinformationﬂowsthroughthefunctionbeingevaluatedfromx,throughtheintermediatecomputationsusedtodeﬁnef,andﬁnallytotheoutputy.Therearenofeedbackconnectionsinwhichoutputsofthemodelarefedbackintoitself.Whenfeedforwardneuralnetworksareextendedtoincludefeedbackconnections,theyarecalledrecurrentneuralnetworks,presentedinchapter.10Feedforwardnetworksareofextremeimportancetomachinelearningpracti-tioners.Theyformthebasisofmanyimportantcommercialapplications.Forexample,theconvolutionalnetworksusedforobjectrecognitionfromphotosareaspecializedkindoffeedforwardnetwork.Feedforwardnetworksareaconceptualsteppingstoneonthepathtorecurrentnetworks,whichpowermanynaturallanguageapplications.Feedforwardneuralnetworksarecallednetworksbecausetheyaretypicallyrepresentedbycomposingtogethermanydiﬀerentfunctions.Themodelisasso-ciatedwithadirectedacyclicgraphdescribinghowthefunctionsarecomposedtogether.Forexample,wemighthavethreefunctionsf(1),f(2),andf(3)connectedinachain,toformf(x) =f(3)(f(2)(f(1)(x))).Thesechainstructuresarethemostcommonlyusedstructuresofneuralnetworks.Inthiscase,f(1)iscalledtheﬁrstlayerofthenetwork,f(2)iscalledthesecondlayer,andsoon.Theoverall168 CHAPTER6.DEEPFEEDFORWARDNETWORKSlengthofthechaingivesthedepthofthemodel.Itisfromthisterminologythatthename“deeplearning”arises.Theﬁnallayerofafeedforwardnetworkiscalledtheoutputlayer.Duringneuralnetworktraining,wedrivef(x)tomatchf∗(x).Thetrainingdataprovidesuswithnoisy,approximateexamplesoff∗(x) evaluatedatdiﬀerenttrainingpoints.Eachexamplexisaccompaniedbyalabelyf≈∗(x).Thetrainingexamplesspecifydirectlywhattheoutputlayermustdoateachpointx;itmustproduceavaluethatisclosetoy.Thebehavioroftheotherlayersisnotdirectlyspeciﬁedbythetrainingdata. Thelearningalgorithmmustdecidehowtousethoselayerstoproducethedesiredoutput,butthetrainingdatadoesnotsaywhateachindividuallayershoulddo.Instead,thelearningalgorithmmustdecidehowtousetheselayerstobestimplementanapproximationoff∗.Becausethetrainingdatadoesnotshowthedesiredoutputforeachoftheselayers,theselayersarecalledhiddenlayers.Finally,thesenetworksarecalledneuralbecausetheyarelooselyinspiredbyneuroscience.Eachhiddenlayerofthenetworkistypicallyvector-valued.Thedimensionalityofthesehiddenlayersdeterminesthewidthofthemodel.Eachelementofthevectormaybeinterpretedasplayingaroleanalogoustoaneuron.Ratherthanthinkingofthelayerasrepresentingasinglevector-to-vectorfunction,wecanalsothinkofthelayerasconsistingofmanyunitsthatactinparallel,eachrepresentingavector-to-scalarfunction.Eachunitresemblesaneuroninthesensethatitreceivesinputfrommanyotherunitsandcomputesitsownactivationvalue. Theideaofusingmanylayersofvector-valuedrepresentationisdrawnfromneuroscience.Thechoiceofthefunctionsf()i(x)usedtocomputetheserepresentationsisalsolooselyguidedbyneuroscientiﬁcobservationsaboutthefunctionsthatbiologicalneuronscompute.However,modernneuralnetworkresearchisguidedbymanymathematicalandengineeringdisciplines,andthegoalofneuralnetworksisnottoperfectlymodelthebrain.Itisbesttothinkoffeedforwardnetworksasfunctionapproximationmachinesthataredesignedtoachievestatisticalgeneralization,occasionallydrawingsomeinsightsfromwhatweknowaboutthebrain,ratherthanasmodelsofbrainfunction.Onewaytounderstandfeedforwardnetworksistobeginwithlinearmodelsandconsiderhowtoovercometheirlimitations. Linearmodels,suchaslogisticregressionandlinearregression,areappealingbecausetheymaybeﬁteﬃcientlyandreliably,eitherinclosedformorwithconvexoptimization.Linearmodelsalsohavetheobviousdefectthatthemodelcapacityislimitedtolinearfunctions,sothemodelcannotunderstandtheinteractionbetweenanytwoinputvariables.Toextendlinearmodelstorepresentnonlinearfunctionsofx,wecanapplythelinearmodelnottoxitselfbuttoatransformedinputφ(x),whereφisa169 CHAPTER6.DEEPFEEDFORWARDNETWORKSnonlineartransformation.Equivalently,wecanapplythekerneltrickdescribedinsection,toobtainanonlinearlearningalgorithmbasedonimplicitlyapplying5.7.2theφmapping.Wecanthinkofφasprovidingasetoffeaturesdescribingx,orasprovidinganewrepresentationfor.xThequestionisthenhowtochoosethemapping.φ1.Oneoptionistouseaverygenericφ,suchastheinﬁnite-dimensionalφthatisimplicitlyusedbykernelmachinesbasedontheRBFkernel. Ifφ(x)isofhighenoughdimension,wecanalwayshaveenoughcapacitytoﬁtthetrainingset,butgeneralizationtothetestsetoftenremainspoor.Verygenericfeaturemappingsareusuallybasedonlyontheprincipleoflocalsmoothnessanddonotencodeenoughpriorinformationtosolveadvancedproblems.2.Anotheroptionistomanuallyengineerφ.Untiltheadventofdeeplearning,thiswasthedominantapproach.Thisapproachrequiresdecadesofhumaneﬀortfor eachseparate task, withpractitioners specializing indiﬀerentdomainssuchasspeech recognitionor computervision, and withlittletransferbetweendomains.3.Thestrategyofdeeplearningistolearnφ.Inthisapproach,wehaveamodely=f(x;θw,) =φ(x;θ)w.Wenowhaveparametersθthatweusetolearnφfromabroadclassoffunctions,andparameterswthatmapfromφ(x)tothedesiredoutput.Thisisanexampleofadeepfeedforwardnetwork,withφdeﬁningahiddenlayer. Thisapproachistheonlyoneofthethreethatgivesupontheconvexityofthetrainingproblem,butthebeneﬁtsoutweightheharms.Inthisapproach,weparametrizetherepresentationasφ(x;θ)andusetheoptimizationalgorithmtoﬁndtheθthatcorrespondstoagoodrepresentation.Ifwewish,thisapproachcancapturethebeneﬁtoftheﬁrstapproachbybeinghighlygeneric—wedosobyusingaverybroadfamilyφ(x;θ).Thisapproachcanalsocapturethebeneﬁtofthesecondapproach.Humanpractitionerscanencodetheirknowledgetohelpgeneralizationbydesigningfamiliesφ(x;θ)thattheyexpectwillperformwell.Theadvantageisthatthehumandesigneronlyneedstoﬁndtherightgeneralfunctionfamilyratherthanﬁndingpreciselytherightfunction.Thisgeneralprincipleofimprovingmodelsbylearningfeaturesextendsbeyondthefeedforwardnetworksdescribedinthischapter.Itisarecurringthemeofdeeplearningthatappliestoallofthekindsofmodelsdescribedthroughoutthisbook.Feedforwardnetworksaretheapplicationofthisprincipletolearningdeterministic170 CHAPTER6.DEEPFEEDFORWARDNETWORKSmappingsfromxtoythatlackfeedbackconnections. Othermodelspresentedlaterwillapplytheseprinciplestolearningstochasticmappings,learningfunctionswithfeedback,andlearningprobabilitydistributionsoverasinglevector.Webeginthischapterwithasimpleexampleofafeedforwardnetwork.Next,weaddresseachofthedesigndecisionsneededtodeployafeedforwardnetwork.First,trainingafeedforwardnetworkrequiresmakingmanyofthesamedesigndecisionsasarenecessaryforalinearmodel:choosingtheoptimizer,thecostfunction,andtheformoftheoutputunits.Wereviewthesebasicsofgradient-basedlearning,thenproceedtoconfrontsomeofthedesigndecisionsthatareuniquetofeedforwardnetworks.Feedforwardnetworkshaveintroducedtheconceptofahiddenlayer,andthisrequiresustochoosetheactivationfunctionsthatwillbeusedtocomputethehiddenlayervalues.Wemustalsodesignthearchitectureofthenetwork,includinghowmanylayersthenetworkshouldcontain,howtheselayersshould beconnectedto each other, andhowmanyunitsshould beineachlayer.Learningindeepneuralnetworksrequirescomputingthegradientsofcomplicatedfunctions.Wepresenttheback-propagationalgorithmanditsmoderngeneralizations,whichcanbeusedtoeﬃcientlycomputethesegradients.Finally,weclosewithsomehistoricalperspective.6.1Example:LearningXORTomaketheideaofafeedforwardnetworkmoreconcrete,webeginwithanexampleofafullyfunctioningfeedforwardnetworkonaverysimpletask:learningtheXORfunction.TheXORfunction(“exclusiveor”)isanoperationontwobinaryvalues,x1andx2.Whenexactlyoneofthesebinaryvaluesisequalto,theXORfunction1returns.Otherwise,itreturns0.TheXORfunctionprovidesthetargetfunction1y=f∗(x)thatwewanttolearn.Ourmodelprovidesafunctiony=f(x;θ)andourlearningalgorithmwilladapttheparametersθtomakefassimilaraspossibletof∗.Inthissimpleexample,wewillnotbeconcernedwithstatisticalgeneralization.WewantournetworktoperformcorrectlyonthefourpointsX={[0,0],[0,1],[1,0],and[1,1]}. Wewilltrainthenetworkonallfourofthesepoints. Theonlychallengeistoﬁtthetrainingset.Wecantreatthisproblemasaregressionproblemanduseameansquarederrorlossfunction.Wechoosethislossfunctiontosimplifythemathforthisexampleasmuchaspossible.Inpracticalapplications,MSEisusuallynotan171 CHAPTER6.DEEPFEEDFORWARDNETWORKSappropriatecostfunctionformodelingbinarydata.Moreappropriateapproachesaredescribedinsection.6.2.2.2Evaluatedonourwholetrainingset,theMSElossfunctionisJ() =θ14x∈X(f∗()(;))x−fxθ2.(6.1)Nowwemustchoosetheformofourmodel,f(x;θ).Supposethatwechoosealinearmodel,withconsistingofand.Ourmodelisdeﬁnedtobeθwbf,b(;xw) = xw+b.(6.2)WecanminimizeJ(θ)inclosedformwithrespecttowandbusingthenormalequations.Aftersolvingthenormalequations,weobtainw=0andb=12. Thelinearmodelsimplyoutputs0.5everywhere.Whydoesthishappen?Figureshows6.1howalinearmodelisnotabletorepresenttheXORfunction.Onewaytosolvethisproblemistouseamodelthatlearnsadiﬀerentfeaturespaceinwhichalinearmodelisabletorepresentthesolution.Speciﬁcally,wewillintroduceaverysimplefeedforwardnetworkwithonehiddenlayercontainingtwohiddenunits.Seeﬁgureforanillustrationof6.2thismodel.Thisfeedforwardnetworkhasavectorofhiddenunitshthatarecomputedbyafunctionf(1)(x;Wc,).Thevaluesofthesehiddenunitsarethenusedastheinputforasecondlayer.Thesecondlayeristheoutputlayerofthenetwork.Theoutputlayerisstilljustalinearregressionmodel,butnowitisappliedtohratherthantox.Thenetworknowcontainstwofunctionschainedtogether:h=f(1)(x;Wc,)andy=f(2)(h;w,b),withthecompletemodelbeingf,,,bf(;xWcw) = (2)(f(1)())x.Whatfunctionshouldf(1)compute?Linearmodelshaveserveduswellsofar,anditmaybetemptingtomakef(1)belinearaswell.Unfortunately,iff(1)werelinear,thenthefeedforwardnetworkasawholewouldremainalinearfunctionofitsinput.Ignoringtheintercepttermsforthemoment,supposef(1)(x) =Wxandf(2)(h) =hw.Thenf(x) =wWx.Wecouldrepresentthisfunctionasf() = xxwwherew= Ww.Clearly,wemustuseanonlinearfunctiontodescribethefeatures.Mostneuralnetworksdosousinganaﬃnetransformationcontrolledbylearnedparameters,followedbyaﬁxed,nonlinearfunctioncalledanactivationfunction.Weusethatstrategyhere,bydeﬁningh=g(Wx+c),whereWprovidestheweightsofalineartransformationandcthebiases.Previously,todescribealinearregression172 CHAPTER6.DEEPFEEDFORWARDNETWORKS 01x101x2Originalspacex 012h101h2Learnedspaceh Figure6.1:SolvingtheXORproblembylearningarepresentation.Theboldnumbersprintedontheplotindicatethevaluethatthelearnedfunctionmustoutputateachpoint.(Left)AlinearmodelapplieddirectlytotheoriginalinputcannotimplementtheXORfunction.Whenx1= 0,themodel’soutputmustincreaseasx2increases.Whenx1= 1,themodel’soutputmustdecreaseasx2increases.Alinearmodelmustapplyaﬁxedcoeﬃcientw2tox2.Thelinearmodelthereforecannotusethevalueofx1tochangethecoeﬃcientonx2andcannotsolvethisproblem.(Right)Inthetransformedspacerepresentedbythefeaturesextractedbyaneuralnetwork,alinearmodelcannowsolvetheproblem.Inourexamplesolution,thetwopointsthatmusthaveoutputhavebeen1collapsedintoasinglepointinfeaturespace.Inotherwords,thenonlinearfeatureshavemappedbothx= [1,0]andx= [0,1]toasinglepointinfeaturespace,h= [1,0].Thelinearmodelcannowdescribethefunctionasincreasinginh1anddecreasinginh2.Inthisexample,themotivationforlearningthefeaturespaceisonlytomakethemodelcapacitygreatersothatitcanﬁtthetrainingset.Inmorerealisticapplications,learnedrepresentationscanalsohelpthemodeltogeneralize. 173 CHAPTER6.DEEPFEEDFORWARDNETWORKSy yh hx xWwy yh1h1x1x1h2h2x2x2Figure6.2:Anexampleofafeedforwardnetwork,drawnintwodiﬀerentstyles.Speciﬁcally,thisisthefeedforwardnetworkweusetosolvetheXORexample.Ithasasinglehiddenlayercontainingtwounits.(Left)Inthisstyle,wedraweveryunitasanodeinthegraph.Thisstyleisveryexplicitandunambiguousbutfornetworkslargerthanthisexampleitcanconsumetoomuchspace.Inthisstyle,wedrawanodeinthegraphfor(Right)eachentirevectorrepresentingalayer’sactivations. Thisstyleismuchmorecompact.Sometimesweannotatetheedgesinthisgraphwiththenameoftheparametersthatdescribetherelationshipbetweentwolayers.Here,weindicatethatamatrixWdescribesthemappingfromxtoh,andavectorwdescribesthemappingfromhtoy.Wetypicallyomittheinterceptparametersassociatedwitheachlayerwhenlabelingthiskindofdrawing.model,weusedavectorofweightsandascalarbiasparametertodescribeanaﬃnetransformationfromaninputvectortoanoutputscalar.Now,wedescribeanaﬃnetransformationfromavectorxtoavectorh,soanentirevectorofbiasparametersisneeded.Theactivationfunctiongistypicallychosentobeafunctionthatisappliedelement-wise,withhi=g(xW:,i+ci).Inmodernneuralnetworks,thedefaultrecommendationistousetherectiﬁedlinearunitorReLU(Jarrettetal.etal.,;,;2009NairandHinton2010Glorot,)deﬁnedbytheactivation2011afunctiondepictedinﬁgure.gz,z() = max0{}6.3Wecannowspecifyourcompletenetworkasf,,,b(;xWcw) = wmax0{,Wxc+}+b.(6.3)WecannowspecifyasolutiontotheXORproblem.LetW=1111,(6.4)c=0−1,(6.5)174 CHAPTER6.DEEPFEEDFORWARDNETWORKS 0z0gz()=max0{,z} Figure6.3:Therectiﬁedlinearactivationfunction.Thisactivationfunctionisthedefaultactivationfunctionrecommendedforusewithmostfeedforwardneuralnetworks.Applyingthisfunctiontotheoutputofalineartransformationyieldsanonlineartransformation.However,thefunctionremainsveryclosetolinear,inthesensethatisapiecewiselinearfunctionwithtwolinearpieces.Becauserectiﬁedlinearunitsarenearlylinear,theypreservemanyofthepropertiesthatmakelinearmodelseasytooptimizewithgradient-basedmethods.Theyalsopreservemanyofthepropertiesthatmakelinearmodelsgeneralizewell.Acommonprinciplethroughoutcomputerscienceisthatwecanbuildcomplicatedsystemsfromminimalcomponents. MuchasaTuringmachine’smemoryneedsonlytobeabletostore0or1states,wecanbuildauniversalfunctionapproximatorfromrectiﬁedlinearfunctions. 175 CHAPTER6.DEEPFEEDFORWARDNETWORKSw=1−2,(6.6)and.b= 0Wecannowwalkthroughthewaythatthemodelprocessesabatchofinputs.LetXbethedesignmatrixcontainingallfourpointsinthebinaryinputspace,withoneexampleperrow:X=00011011.(6.7)Theﬁrststepintheneuralnetworkistomultiplytheinputmatrixbytheﬁrstlayer’sweightmatrix:XW=00111122.(6.8)Next,weaddthebiasvector,toobtainc01−101021.(6.9)Inthisspace,alloftheexamplesliealongalinewithslope.Aswemovealong1thisline,theoutputneedstobeginat,thenriseto,thendropbackdownto.010Alinearmodelcannotimplementsuchafunction.Toﬁnishcomputingthevalueofforeachexample,weapplytherectiﬁedlineartransformation:h00101021.(6.10)Thistransformationhaschangedtherelationshipbetweentheexamples.Theynolongerlieonasingleline.Asshowninﬁgure,theynowlieinaspacewherea6.1linearmodelcansolvetheproblem.Weﬁnishbymultiplyingbytheweightvector:w0110.(6.11)176 CHAPTER6.DEEPFEEDFORWARDNETWORKSTheneuralnetworkhasobtainedthecorrectanswerforeveryexampleinthebatch.Inthisexample,wesimplyspeciﬁedthesolution,thenshowedthatitobtainedzeroerror. Inarealsituation,theremightbebillionsofmodelparametersandbillionsoftrainingexamples,soonecannotsimplyguessthesolutionaswedidhere.Instead,agradient-basedoptimizationalgorithmcanﬁndparametersthatproduceverylittleerror.ThesolutionwedescribedtotheXORproblemisataglobalminimumofthelossfunction,sogradientdescentcouldconvergetothispoint.ThereareotherequivalentsolutionstotheXORproblemthatgradientdescentcouldalsoﬁnd.Theconvergencepointofgradientdescentdependsontheinitialvaluesoftheparameters.Inpractice,gradientdescentwouldusuallynotﬁndclean,easilyunderstood,integer-valuedsolutionsliketheonewepresentedhere.6.2Gradient-BasedLearningDesigningandtraininganeuralnetworkisnotmuchdiﬀerentfromtraininganyothermachinelearningmodelwithgradientdescent.Insection,wedescribed5.10howtobuildamachinelearningalgorithmbyspecifyinganoptimizationprocedure,acostfunction,andamodelfamily.Thelargestdiﬀerencebetweenthelinearmodelswehaveseensofarandneuralnetworksisthatthenonlinearityofaneuralnetworkcausesmostinterestinglossfunctionstobecomenon-convex.Thismeansthatneuralnetworksareusuallytrainedbyusingiterative,gradient-basedoptimizersthatmerelydrivethecostfunctiontoaverylowvalue,ratherthanthelinearequationsolversusedtotrainlinearregressionmodelsortheconvexoptimizationalgorithmswithglobalconver-genceguaranteesusedtotrainlogisticregressionorSVMs.Convexoptimizationconvergesstartingfromanyinitialparameters(intheory—inpracticeitisveryrobustbutcanencounternumericalproblems).Stochasticgradientdescentappliedtonon-convexlossfunctionshasnosuchconvergenceguarantee,andissensitivetothevaluesoftheinitialparameters.Forfeedforwardneuralnetworks,itisimportanttoinitializeallweightstosmallrandomvalues.Thebiasesmaybeinitializedtozeroortosmallpositivevalues.Theiterativegradient-basedopti-mizationalgorithmsusedtotrainfeedforwardnetworksandalmostallotherdeepmodelswillbedescribedindetailinchapter,withparameterinitializationin8particulardiscussedinsection.Forthemoment,itsuﬃcestounderstandthat8.4thetrainingalgorithmisalmostalwaysbasedonusingthegradienttodescendthecostfunctioninonewayoranother. Thespeciﬁcalgorithmsareimprovementsandreﬁnementsontheideasofgradientdescent,introducedinsection,and,4.3177 CHAPTER6.DEEPFEEDFORWARDNETWORKSmorespeciﬁcally,aremostoftenimprovementsofthestochasticgradientdescentalgorithm,introducedinsection.5.9Wecanofcourse,trainmodelssuchaslinearregressionandsupportvectormachineswithgradientdescenttoo,andinfactthisiscommonwhenthetrainingsetisextremelylarge.Fromthispointofview,traininganeuralnetworkisnotmuchdiﬀerentfromtraininganyothermodel.Computingthegradientisslightlymorecomplicatedforaneuralnetwork,butcanstillbedoneeﬃcientlyandexactly.Sectionwilldescribehowtoobtainthegradientusingtheback-propagation6.5algorithmandmoderngeneralizationsoftheback-propagationalgorithm.Aswithothermachinelearningmodels,toapplygradient-basedlearningwemustchooseacostfunction,andwemustchoosehowtorepresenttheoutputofthemodel.Wenowrevisitthesedesignconsiderationswithspecialemphasisontheneuralnetworksscenario.6.2.1CostFunctionsAnimportantaspectofthedesignofadeepneuralnetworkisthechoiceofthecostfunction.Fortunately,thecostfunctionsforneuralnetworksaremoreorlessthesameasthoseforotherparametricmodels,suchaslinearmodels.Inmostcases,ourparametricmodeldeﬁnesadistributionp(yx|;θ)andwesimplyuse theprinciple ofmaximumlikelihood.Thismeansweusethecross-entropybetweenthetrainingdataandthemodel’spredictionsasthecostfunction.Sometimes,wetakeasimplerapproach,whereratherthanpredictingacompleteprobabilitydistributionovery,wemerelypredictsomestatisticofyconditionedon.Specializedlossfunctionsallowustotrainapredictoroftheseestimates.xThetotalcostfunctionusedtotrainaneuralnetworkwilloftencombineoneoftheprimarycostfunctionsdescribedherewitharegularizationterm.Wehavealreadyseensomesimpleexamplesofregularizationappliedtolinearmodelsinsection.Theweightdecayapproachusedforlinearmodelsisalsodirectly5.2.2applicabletodeepneuralnetworksandisamongthemostpopularregularizationstrategies.Moreadvancedregularizationstrategiesforneuralnetworkswillbedescribedinchapter.76.2.1.1LearningConditionalDistributionswithMaximumLikelihoodMostmodernneuralnetworksaretrainedusingmaximumlikelihood.Thismeansthatthecostfunctionissimplythenegativelog-likelihood,equivalentlydescribed178 CHAPTER6.DEEPFEEDFORWARDNETWORKSasthecross-entropybetweenthetrainingdataandthemodeldistribution.ThiscostfunctionisgivenbyJ() = θ−Exy,∼ˆpdatalogpmodel()yx|.(6.12)Thespeciﬁcformofthecostfunctionchangesfrommodeltomodel,dependingonthespeciﬁcformoflogpmodel.Theexpansionoftheaboveequationtypicallyyieldssometermsthatdonotdependonthemodelparametersandmaybedis-carded.Forexample,aswesawinsection,if5.5.1pmodel(yx|) =N(y;f(x;θ),I),thenwerecoverthemeansquarederrorcost,Jθ() =12Exy,∼ˆpdata||−||yf(;)xθ2+const,(6.13)uptoascalingfactorof12andatermthatdoesnotdependon.ThediscardedθconstantisbasedonthevarianceoftheGaussiandistribution,whichinthiscasewechosenottoparametrize.Previously,wesawthattheequivalencebetweenmaximumlikelihoodestimationwithanoutputdistributionandminimizationofmeansquarederrorholdsforalinearmodel,butinfact,theequivalenceholdsregardlessoftheusedtopredictthemeanoftheGaussian.f(;)xθAnadvantageofthisapproachofderivingthecostfunctionfrommaximumlikelihoodisthatitremovestheburdenofdesigningcostfunctionsforeachmodel.Specifyingamodelp(yx|)automaticallydeterminesacostfunctionlogp(yx|).Onerecurringthemethroughoutneuralnetworkdesignisthatthegradientofthecostfunctionmustbelargeandpredictableenoughtoserveasagoodguideforthelearningalgorithm.Functionsthatsaturate(becomeveryﬂat)underminethisobjectivebecausetheymakethegradientbecomeverysmall.Inmanycasesthishappensbecausetheactivationfunctionsusedtoproducetheoutputofthehiddenunitsortheoutputunitssaturate. Thenegativelog-likelihoodhelpstoavoidthisproblemformanymodels.Manyoutputunitsinvolveanexpfunctionthatcansaturatewhenitsargumentisverynegative.Thelogfunctioninthenegativelog-likelihoodcostfunctionundoestheexpofsomeoutputunits.Wewilldiscusstheinteractionbetweenthecostfunctionandthechoiceofoutputunitinsection.6.2.2Oneunusualpropertyofthecross-entropycostusedtoperformmaximumlikelihoodestimationisthatitusuallydoesnothaveaminimumvaluewhenappliedtothemodelscommonlyusedinpractice.Fordiscreteoutputvariables,mostmodelsareparametrizedinsuchawaythattheycannotrepresentaprobabilityofzeroorone,butcancomearbitrarilyclosetodoingso.Logisticregressionisanexampleofsuchamodel.Forreal-valuedoutputvariables,ifthemodel179 CHAPTER6.DEEPFEEDFORWARDNETWORKScancontrolthedensityoftheoutputdistribution(forexample,bylearningthevarianceparameterofaGaussianoutputdistribution)thenitbecomespossibletoassignextremelyhighdensitytothecorrecttrainingsetoutputs,resultingincross-entropyapproachingnegativeinﬁnity.Regularizationtechniquesdescribedinchapterprovideseveraldiﬀerentwaysofmodifyingthelearningproblemso7thatthemodelcannotreapunlimitedrewardinthisway.6.2.1.2LearningConditionalStatisticsInsteadoflearningafullprobabilitydistributionp(yx|;θ)weoftenwanttolearnjustoneconditionalstatisticofgiven.yxForexample,wemayhaveapredictorf(x;θ) thatwewishtopredictthemeanof.yIfweuseasuﬃcientlypowerfulneuralnetwork,wecanthinkoftheneuralnetworkasbeingabletorepresentanyfunctionffromawideclassoffunctions,withthisclassbeinglimitedonlybyfeaturessuchascontinuityandboundednessratherthanbyhavingaspeciﬁcparametricform.Fromthispointofview,wecanviewthecostfunctionasbeingafunctionalratherthanjustafunction.Afunctionalisamappingfromfunctionstorealnumbers.Wecanthusthinkoflearningaschoosingafunctionratherthanmerelychoosingasetofparameters.Wecandesignourcostfunctionaltohaveitsminimumoccuratsomespeciﬁcfunctionwedesire.Forexample,wecandesignthecostfunctionaltohaveitsminimumlieonthefunctionthatmapsxtotheexpectedvalueofygivenx.Solvinganoptimizationproblemwithrespecttoafunctionrequiresamathematicaltoolcalledcalculusofvariations,describedinsection.Itisnotnecessary19.4.2tounderstandcalculusofvariationstounderstandthecontentofthischapter.Atthemoment,itisonlynecessarytounderstandthatcalculusofvariationsmaybeusedtoderivethefollowingtworesults.Ourﬁrstresultderivedusingcalculusofvariationsisthatsolvingtheoptimiza-tionproblemf∗= argminfExy,∼pdata||−||yf()x2(6.14)yieldsf∗() = xEy∼pdata()yx|[]y,(6.15)solongasthisfunctionlieswithintheclassweoptimizeover.Inotherwords,ifwecouldtrainoninﬁnitelymanysamplesfromthetruedatageneratingdistribution,minimizingthemeansquarederrorcostfunctiongivesafunctionthatpredictsthemeanofforeachvalueof.yx180 CHAPTER6.DEEPFEEDFORWARDNETWORKSDiﬀerentcostfunctionsgivediﬀerentstatistics.Asecondresultderivedusingcalculusofvariationsisthatf∗= argminfExy,∼pdata||−||yf()x1(6.16)yieldsafunctionthatpredictsthemedianvalueofyforeachx,solongassuchafunctionmaybedescribedbythefamilyoffunctionsweoptimizeover.Thiscostfunctioniscommonlycalled.meanabsoluteerrorUnfortunately,meansquarederrorandmeanabsoluteerroroftenleadtopoorresultswhenusedwithgradient-basedoptimization.Someoutputunitsthatsaturateproduceverysmallgradientswhencombinedwiththesecostfunctions.Thisisonereasonthatthecross-entropycostfunctionismorepopularthanmeansquarederrorormeanabsoluteerror,evenwhenitisnotnecessarytoestimateanentiredistribution.p()yx|6.2.2OutputUnitsThechoiceofcostfunctionistightlycoupledwiththechoiceofoutputunit.Mostofthetime,wesimplyusethecross-entropybetweenthedatadistributionandthemodeldistribution. Thechoiceofhowtorepresenttheoutputthendeterminestheformofthecross-entropyfunction.Anykindofneuralnetworkunitthatmaybeusedasanoutputcanalsobeusedasahiddenunit.Here,wefocusontheuseoftheseunitsasoutputsofthemodel,butinprincipletheycanbeusedinternallyaswell.Werevisittheseunitswithadditionaldetailabouttheiruseashiddenunitsinsection.6.3Throughoutthissection,wesupposethatthefeedforwardnetworkprovidesasetofhiddenfeaturesdeﬁnedbyh=f(x;θ).Theroleoftheoutputlayeristhentoprovidesomeadditionaltransformationfromthefeaturestocompletethetaskthatthenetworkmustperform.6.2.2.1LinearUnitsforGaussianOutputDistributionsOnesimplekindofoutputunitisanoutputunitbasedonanaﬃnetransformationwithnononlinearity.Theseareoftenjustcalledlinearunits.Givenfeaturesh,alayeroflinearoutputunitsproducesavectorˆy=Wh+b.LinearoutputlayersareoftenusedtoproducethemeanofaconditionalGaussiandistribution:p() = (;yx|NyˆyI,).(6.17)181 CHAPTER6.DEEPFEEDFORWARDNETWORKSMaximizingthelog-likelihoodisthenequivalenttominimizingthemeansquarederror.ThemaximumlikelihoodframeworkmakesitstraightforwardtolearnthecovarianceoftheGaussiantoo,ortomakethecovarianceoftheGaussianbeafunctionoftheinput.However,thecovariancemustbeconstrainedtobeapositivedeﬁnitematrixforallinputs.Itisdiﬃculttosatisfysuchconstraintswithalinearoutputlayer,sotypicallyotheroutputunitsareusedtoparametrizethecovariance.Approachestomodelingthecovariancearedescribedshortly,insection.6.2.2.4Becauselinearunitsdonotsaturate,theyposelittlediﬃcultyforgradient-basedoptimizationalgorithmsandmaybeusedwithawidevarietyofoptimizationalgorithms.6.2.2.2SigmoidUnitsforBernoulliOutputDistributionsManytasksrequirepredictingthevalueofabinaryvariabley.Classiﬁcationproblemswithtwoclassescanbecastinthisform.Themaximum-likelihoodapproachistodeﬁneaBernoullidistributionoveryconditionedon.xABernoullidistributionisdeﬁnedbyjustasinglenumber.TheneuralnetneedstopredictonlyP(y= 1|x).Forthisnumbertobeavalidprobability,itmustlieintheinterval[0,1].Satisfyingthisconstraintrequiressomecarefuldesigneﬀort.Supposeweweretousealinearunit,andthresholditsvaluetoobtainavalidprobability:Py(= 1 ) = max|x0min,1,wh+b.(6.18)Thiswouldindeeddeﬁneavalidconditionaldistribution,butwewouldnotbeabletotrainitveryeﬀectivelywithgradientdescent.Anytimethatwh+bstrayedoutsidetheunitinterval,thegradientoftheoutputofthemodelwithrespecttoitsparameterswouldbe0.Agradientof0istypicallyproblematicbecausethelearningalgorithmnolongerhasaguideforhowtoimprovethecorrespondingparameters.Instead,itisbettertouseadiﬀerentapproachthatensuresthereisalwaysastronggradientwheneverthemodelhasthewronganswer.Thisapproachisbasedonusingsigmoidoutputunitscombinedwithmaximumlikelihood.Asigmoidoutputunitisdeﬁnedbyˆyσ= wh+b(6.19)182 CHAPTER6.DEEPFEEDFORWARDNETWORKSwhereisthelogisticsigmoidfunctiondescribedinsection.σ3.10Wecanthinkofthesigmoidoutputunitashavingtwocomponents.First,itusesalinearlayertocomputez=wh+b.Next,itusesthesigmoidactivationfunctiontoconvertintoaprobability.zWeomitthedependenceonxforthemomenttodiscusshowtodeﬁneaprobabilitydistributionoveryusingthevaluez.Thesigmoidcanbemotivatedbyconstructinganunnormalizedprobabilitydistribution˜P(y),whichdoesnotsumto1.Wecanthendividebyanappropriateconstanttoobtainavalidprobabilitydistribution.Ifwebeginwiththeassumptionthattheunnormalizedlogprobabilitiesarelinearinyandz,wecanexponentiatetoobtaintheunnormalizedprobabilities.WethennormalizetoseethatthisyieldsaBernoullidistributioncontrolledbyasigmoidaltransformationof:zlog˜Pyyz() = (6.20)˜Pyyz() = exp()(6.21)Py() =exp()yz1y=0exp(yz)(6.22)Pyσyz.() = ((2−1))(6.23)Probabilitydistributionsbasedonexponentiationandnormalizationarecommonthroughoutthestatisticalmodelingliterature.Thezvariabledeﬁningsuchadistributionoverbinaryvariablesiscalleda.logitThisapproachtopredictingtheprobabilitiesinlog-spaceisnaturaltousewithmaximumlikelihoodlearning.Becausethecostfunctionusedwithmaximumlikelihoodis−logP(y|x),theloginthecostfunctionundoestheexpofthesigmoid.Withoutthiseﬀect,thesaturationofthesigmoidcouldpreventgradient-based learningfrom makinggoodprogress.Theloss functionfor maximumlikelihoodlearningofaBernoulliparametrizedbyasigmoidisJPy() = logθ−(|x)(6.24)= log((21))−σy−z(6.25)= ((12))ζ−yz.(6.26)Thisderivationmakesuseofsomepropertiesfromsection.Byrewriting3.10thelossintermsofthesoftplusfunction,wecanseethatitsaturatesonlywhen(1−2y)zisverynegative.Saturationthusoccursonlywhenthemodelalreadyhastherightanswer—wheny= 1andzisverypositive,ory= 0andzisverynegative.Whenzhasthewrongsign,theargumenttothesoftplusfunction,183 CHAPTER6.DEEPFEEDFORWARDNETWORKS(1−2y)z,maybesimpliﬁedto||z.As||zbecomeslargewhilezhasthewrongsign,thesoftplusfunctionasymptotestowardsimplyreturningitsargument||z.Thederivativewithrespecttozasymptotestosign(z),so,inthelimitofextremelyincorrectz,thesoftplusfunctiondoesnotshrinkthegradientatall.Thispropertyisveryusefulbecauseitmeansthatgradient-basedlearningcanacttoquicklycorrectamistaken.zWhenweuseotherlossfunctions,suchasmeansquarederror,thelosscansaturateanytimeσ(z)saturates.Thesigmoidactivationfunctionsaturatesto0whenzbecomesverynegativeandsaturatestowhen1zbecomesverypositive.Thegradientcanshrinktoosmalltobeusefulforlearningwheneverthishappens,whetherthemodelhasthecorrectanswerortheincorrectanswer.Forthisreason,maximumlikelihoodisalmostalwaysthepreferredapproachtotrainingsigmoidoutputunits.Analytically,thelogarithmofthesigmoidisalwaysdeﬁnedandﬁnite,becausethesigmoidreturnsvaluesrestrictedtotheopeninterval(0,1),ratherthanusingtheentireclosedintervalofvalidprobabilities[0,1].Insoftwareimplementations,toavoidnumericalproblems,itisbesttowritethenegativelog-likelihoodasafunctionofz,ratherthanasafunctionofˆy=σ(z).Ifthesigmoidfunctionunderﬂowstozero,thentakingthelogarithmofˆyyieldsnegativeinﬁnity.6.2.2.3SoftmaxUnitsforMultinoulliOutputDistributionsAnytimewewishtorepresentaprobabilitydistributionoveradiscretevariablewithnpossiblevalues,wemayusethesoftmaxfunction.Thiscanbeseenasageneralizationofthesigmoidfunctionwhichwasusedtorepresentaprobabilitydistributionoverabinaryvariable.Softmaxfunctionsaremostoftenusedastheoutputofaclassiﬁer,torepresenttheprobabilitydistributionoverndiﬀerentclasses.Morerarely,softmaxfunctionscanbeusedinsidethemodelitself,ifwewishthemodeltochoosebetweenoneofndiﬀerentoptionsforsomeinternalvariable.Inthecaseofbinaryvariables,wewishedtoproduceasinglenumberˆyPy.= (= 1 )|x(6.27)Becausethisnumberneededtoliebetweenand,andbecausewewantedthe01logarithmofthenumbertobewell-behavedforgradient-basedoptimizationofthelog-likelihood,wechosetoinsteadpredictanumberz=log˜P(y=1|x).ExponentiatingandnormalizinggaveusaBernoullidistributioncontrolledbythesigmoidfunction.184 CHAPTER6.DEEPFEEDFORWARDNETWORKSTogeneralizetothecaseofadiscretevariablewithnvalues,wenowneedtoproduceavectorˆy,withˆyi=P(y=i|x).Werequirenotonlythateachelementofˆyibebetweenand,butalsothattheentirevectorsumstosothat011itrepresentsavalidprobabilitydistribution.ThesameapproachthatworkedfortheBernoullidistributiongeneralizestothemultinoullidistribution.First,alinearlayerpredictsunnormalizedlogprobabilities:zW= hb+,(6.28)wherezi=log˜P(y=i|x).Thesoftmaxfunctioncanthenexponentiateandnormalizetoobtainthedesiredzˆy.Formally,thesoftmaxfunctionisgivenbysoftmax()zi=exp(zi)jexp(zj).(6.29)Aswiththelogisticsigmoid,theuseoftheexpfunctionworksverywellwhentrainingthesoftmaxtooutputatargetvalueyusingmaximumlog-likelihood.Inthiscase,wewishtomaximizelogP(y=i;z)=logsoftmax(z)i.Deﬁningthesoftmaxintermsofexpisnaturalbecausetheloginthelog-likelihoodcanundotheofthesoftmax:explogsoftmax()zi= zi−logjexp(zj).(6.30)Theﬁrsttermofequationshowsthattheinput6.30zialwayshasadirectcontributiontothecostfunction.Becausethistermcannotsaturate,weknowthatlearningcanproceed,evenifthecontributionofzitothesecondtermofequationbecomesverysmall.Whenmaximizingthelog-likelihood,theﬁrst6.30termencourageszitobepushedup,whilethesecondtermencouragesallofztobepusheddown.Togainsomeintuitionforthesecondterm,logjexp(zj),observethatthistermcanberoughlyapproximatedbymaxjzj.Thisapproximationisbasedontheideathatexp(zk) isinsigniﬁcantforanyzkthatisnoticeablylessthanmaxjzj.Theintuitionwecangainfromthisapproximationisthatthenegativelog-likelihoodcostfunctionalwaysstronglypenalizesthemostactiveincorrectprediction.Ifthecorrectansweralreadyhasthelargestinputtothesoftmax,thenthe−zitermandthelogjexp(zj)≈maxjzj=zitermswillroughlycancel.Thisexamplewillthencontributelittletotheoveralltrainingcost,whichwillbedominatedbyotherexamplesthatarenotyetcorrectlyclassiﬁed.Sofarwehavediscussedonlyasingleexample.Overall,unregularizedmaximumlikelihoodwilldrivethemodeltolearnparametersthatdrivethesoftmaxtopredict185 CHAPTER6.DEEPFEEDFORWARDNETWORKSthefractionofcountsofeachoutcomeobservedinthetrainingset:softmax((;))zxθi≈mj=11y()j=i,x()j=xmj=11x()j=x.(6.31)Becausemaximumlikelihoodisaconsistentestimator,thisisguaranteedtohappensolongasthemodelfamilyiscapableofrepresentingthetrainingdistribution.Inpractice,limitedmodelcapacityandimperfectoptimizationwillmeanthatthemodelisonlyabletoapproximatethesefractions.Manyobjectivefunctionsotherthanthelog-likelihooddonotworkaswellwiththesoftmaxfunction.Speciﬁcally,objectivefunctionsthatdonotusealogtoundotheexpofthesoftmaxfailtolearnwhentheargumenttotheexpbecomesverynegative,causingthegradienttovanish.Inparticular,squarederrorisapoorlossfunctionforsoftmaxunits,andcanfailtotrainthemodeltochangeitsoutput,evenwhenthemodelmakeshighlyconﬁdentincorrectpredictions(,Bridle1990).Tounderstandwhytheseotherlossfunctionscanfail,weneedtoexaminethesoftmaxfunctionitself.Likethesigmoid,thesoftmaxactivationcansaturate.Thesigmoidfunctionhasasingleoutputthatsaturateswhenitsinputisextremelynegativeorextremelypositive.Inthecaseofthesoftmax,therearemultipleoutputvalues.Theseoutputvaluescansaturatewhenthediﬀerencesbetweeninputvaluesbecomeextreme.Whenthesoftmaxsaturates,manycostfunctionsbasedonthesoftmaxalsosaturate,unlesstheyareabletoinvertthesaturatingactivatingfunction.Toseethatthesoftmaxfunctionrespondstothediﬀerencebetweenitsinputs,observethatthesoftmaxoutputisinvarianttoaddingthesamescalartoallofitsinputs:softmax() = softmax(+)zzc.(6.32)Usingthisproperty,wecanderiveanumericallystablevariantofthesoftmax:softmax() = softmax(maxzz−izi).(6.33)Thereformulatedversionallowsustoevaluatesoftmaxwithonlysmallnumericalerrorsevenwhenzcontainsextremelylargeorextremelynegativenumbers.Ex-aminingthenumericallystablevariant,weseethatthesoftmaxfunctionisdrivenbytheamountthatitsargumentsdeviatefrommaxizi.Anoutputsoftmax(z)isaturatestowhenthecorrespondinginputismaximal1(zi=maxizi)andziismuchgreaterthanalloftheotherinputs.Theoutputsoftmax(z)icanalsosaturatetowhen0ziisnotmaximalandthemaximumismuchgreater.Thisisageneralizationofthewaythatsigmoidunitssaturate,and186 CHAPTER6.DEEPFEEDFORWARDNETWORKScancausesimilardiﬃcultiesforlearningifthelossfunctionisnotdesignedtocompensateforit.Theargumentztothesoftmaxfunctioncanbeproducedintwodiﬀerentways.Themostcommonissimplytohaveanearlierlayeroftheneuralnetworkoutputeveryelementofz,asdescribedaboveusingthelinearlayerz=Wh+b.Whilestraightforward,thisapproachactuallyoverparametrizesthedistribution.Theconstraintthatthenoutputsmustsumtomeansthatonly1n−1parametersarenecessary;theprobabilityofthen-thvaluemaybeobtainedbysubtractingtheﬁrstn−1 1probabilitiesfrom.Wecanthusimposearequirementthatoneelementofzbeﬁxed.Forexample,wecanrequirethatzn=0.Indeed,thisisexactlywhatthesigmoidunitdoes.DeﬁningP(y= 1|x) =σ(z)isequivalenttodeﬁningP(y= 1|x) =softmax(z)1withatwo-dimensionalzandz1= 0.Boththen−1argumentandthenargumentapproachestothesoftmaxcandescribethesamesetofprobabilitydistributions,buthavediﬀerentlearningdynamics.Inpractice,thereisrarelymuchdiﬀerencebetweenusingtheoverparametrizedversionortherestrictedversion,anditissimplertoimplementtheoverparametrizedversion.Fromaneuroscientiﬁcpointofview,itisinterestingtothinkofthesoftmaxasawaytocreateaformofcompetitionbetweentheunitsthatparticipateinit:thesoftmaxoutputsalwayssumto1soanincreaseinthevalueofoneunitnecessarilycorrespondstoadecreaseinthevalueofothers.Thisisanalogoustothelateralinhibitionthatisbelievedtoexistbetweennearbyneuronsinthecortex.Attheextreme(whenthediﬀerencebetweenthemaximalaiandtheothersislargeinmagnitude)itbecomesaformofwinner-take-all(oneoftheoutputsisnearly1andtheothersarenearly0).Thename“softmax”canbesomewhatconfusing.Thefunctionismorecloselyrelatedtotheargmaxfunctionthanthemaxfunction. Theterm“soft”derivesfromthefactthatthesoftmaxfunctioniscontinuousanddiﬀerentiable.Theargmaxfunction,withitsresultrepresentedasaone-hotvector,isnotcontinuousordiﬀerentiable.Thesoftmaxfunctionthusprovidesa“softened”versionoftheargmax.Thecorrespondingsoftversionofthemaximumfunctionissoftmax(z)z.Itwouldperhapsbebettertocallthesoftmaxfunction“softargmax,” butthecurrentnameisanentrenchedconvention.6.2.2.4OtherOutputTypesThelinear, sigmoid, andsoftmaxoutputunitsdescribedabovearethemostcommon.Neuralnetworkscangeneralizetoalmostanykindofoutputlayerthatwewish.Theprincipleofmaximumlikelihoodprovidesaguideforhowtodesign187 CHAPTER6.DEEPFEEDFORWARDNETWORKSagoodcostfunctionfornearlyanykindofoutputlayer.Ingeneral,ifwedeﬁneaconditionaldistributionp(yx|;θ),theprincipleofmaximumlikelihoodsuggestsweuseasourcostfunction.−|log(pyxθ;)Ingeneral,wecanthinkoftheneuralnetworkasrepresentingafunctionf(x;θ).Theoutputsofthisfunctionarenotdirectpredictionsofthevaluey.Instead,f(x;θ) =ωprovidestheparametersforadistributionovery.Ourlossfunctioncanthenbeinterpretedas.−log(;())pyωxForexample,wemaywishtolearnthevarianceofaconditionalGaussianfory,givenx.Inthesimplecase,wherethevarianceσ2isaconstant,thereisaclosedformexpressionbecausethemaximumlikelihoodestimatorofvarianceissimplytheempiricalmeanofthesquareddiﬀerencebetweenobservationsyandtheirexpectedvalue.Acomputationallymoreexpensiveapproachthatdoesnotrequirewritingspecial-casecodeistosimplyincludethevarianceasoneofthepropertiesofthedistributionp(y|x)thatiscontrolledbyω=f(x;θ).Thenegativelog-likelihood−logp(y;ω(x))willthenprovideacostfunctionwiththeappropriatetermsnecessarytomakeouroptimizationprocedureincrementallylearnthevariance.Inthesimplecasewherethestandarddeviationdoesnotdependontheinput,wecanmakeanewparameterinthenetworkthatiscopieddirectlyintoω.Thisnewparametermightbeσitselforcouldbeaparametervrepresentingσ2oritcouldbeaparameterβrepresenting1σ2,dependingonhowwechoosetoparametrizethedistribution.Wemaywishourmodeltopredictadiﬀerentamountofvarianceinyfordiﬀerentvaluesofx.Thisiscalledaheteroscedasticmodel.Intheheteroscedasticcase,wesimplymakethespeciﬁcationofthevariancebeoneofthevaluesoutputbyf(x;θ).AtypicalwaytodothisistoformulatetheGaussiandistributionusingprecision,ratherthanvariance,asdescribedinequation.3.22Inthemultivariatecaseitismostcommontouseadiagonalprecisionmatrixdiag(6.34)()β.Thisformulationworkswellwithgradientdescentbecausetheformulaforthelog-likelihoodoftheGaussiandistributionparametrizedbyβinvolvesonlymul-tiplicationbyβiandadditionoflogβi.Thegradientofmultiplication,addition,andlogarithmoperationsiswell-behaved.Bycomparison,ifweparametrizedtheoutputintermsofvariance,wewouldneedtousedivision.Thedivisionfunctionbecomesarbitrarilysteepnearzero.Whilelargegradientscanhelplearning,arbitrarilylargegradientsusuallyresultininstability.Ifweparametrizedtheoutputintermsofstandarddeviation,thelog-likelihoodwouldstillinvolvedivision,andwouldalsoinvolvesquaring.Thegradientthroughthesquaringoperationcanvanishnearzero,makingitdiﬃculttolearnparametersthataresquared.188 CHAPTER6.DEEPFEEDFORWARDNETWORKSRegardlessofwhetherweusestandarddeviation,variance,orprecision,wemustensurethatthecovariancematrixoftheGaussianispositivedeﬁnite. Becausetheeigenvaluesoftheprecisionmatrixarethereciprocalsoftheeigenvaluesofthecovariancematrix,thisisequivalenttoensuringthattheprecisionmatrixispositivedeﬁnite.Ifweuseadiagonalmatrix,orascalartimesthediagonalmatrix,thentheonlyconditionweneedtoenforceontheoutputofthemodelispositivity.Ifwesupposethataistherawactivationofthemodelusedtodeterminethediagonalprecision,wecanusethesoftplusfunctiontoobtainapositiveprecisionvector:β=ζ(a).Thissamestrategyappliesequallyifusingvarianceorstandarddeviationratherthanprecisionorifusingascalartimesidentityratherthandiagonalmatrix.Itisraretolearnacovarianceorprecisionmatrixwithricherstructurethandiagonal. Ifthecovarianceisfullandconditional,thenaparametrizationmustbechosenthatguaranteespositive-deﬁnitenessofthepredictedcovariancematrix.ThiscanbeachievedbywritingΣ() = ()xBxB()x,whereBisanunconstrainedsquarematrix.Onepracticalissueifthematrixisfullrankisthatcomputingthelikelihoodisexpensive,withadd×matrixrequiringO(d3)computationforthedeterminantandinverseofΣ(x)(orequivalently,andmorecommonlydone,itseigendecompositionorthatof).Bx()Weoftenwanttoperformmultimodalregression,thatis,topredictrealvaluesthatcomefromaconditionaldistributionp(yx|)thatcanhaveseveraldiﬀerentpeaksinyspaceforthesamevalueofx.Inthiscase,aGaussianmixtureisanaturalrepresentationfortheoutput(,;,).Jacobsetal.1991Bishop1994NeuralnetworkswithGaussianmixturesastheiroutputareoftencalledmixturedensitynetworks.AGaussianmixtureoutputwithncomponentsisdeﬁnedbytheconditionalprobabilitydistributionp() =yx|ni=1pi(= c|Nx)(;yµ()i()x,Σ()i())x.(6.35)Theneuralnetworkmusthavethreeoutputs:avectordeﬁningp(c=i|x),amatrixprovidingµ()i(x)foralli,andatensorprovidingΣ()i(x)foralli.Theseoutputsmustsatisfydiﬀerentconstraints:1.Mixturecomponentsp(c=i|x):theseformamultinoullidistributionoverthendiﬀerentcomponentsassociatedwithlatentvariable1c,andcan1Weconsiderctobelatentbecausewedonotobserveitinthedata:giveninputxandtargety,itisnotpossibletoknowwithcertaintywhichGaussiancomponentwasresponsiblefory,butwecanimaginethatywasgeneratedbypickingoneofthem,andmakethatunobservedchoicearandomvariable.189 CHAPTER6.DEEPFEEDFORWARDNETWORKStypicallybeobtainedbyasoftmaxoverann-dimensionalvector,toguaranteethattheseoutputsarepositiveandsumto1.2.Meansµ()i(x):theseindicatethecenterormeanassociatedwiththei-thGaussiancomponent,andareunconstrained(typicallywithnononlinearityatallfortheseoutputunits).Ifyisad-vector,thenthenetworkmustoutputannd×matrixcontainingallnofthesed-dimensionalvectors. Learningthesemeanswithmaximumlikelihoodisslightlymorecomplicatedthanlearningthemeansofadistributionwithonlyoneoutputmode.Weonlywanttoupdatethemeanforthecomponentthatactuallyproducedtheobservation.Inpractice,wedonotknowwhichcomponentproducedeachobservation.Theexpressionforthenegativelog-likelihoodnaturallyweightseachexample’scontributiontothelossforeachcomponentbytheprobabilitythatthecomponentproducedtheexample.3.CovariancesΣ()i(x):thesespecifythecovariancematrixforeachcomponenti.AswhenlearningasingleGaussiancomponent,wetypicallyuseadiagonalmatrixtoavoidneedingtocomputedeterminants.Aswithlearningthemeansofthemixture,maximumlikelihoodiscomplicatedbyneedingtoassignpartialresponsibilityforeachpointtoeachmixturecomponent.Gradientdescentwillautomaticallyfollowthecorrectprocessifgiventhecorrectspeciﬁcationofthenegativelog-likelihoodunderthemixturemodel.Ithasbeenreportedthatgradient-basedoptimizationofconditionalGaussianmixtures(ontheoutputofneuralnetworks)canbeunreliable,inpartbecauseonegetsdivisions(bythevariance)whichcanbenumericallyunstable(whensomevariancegetstobesmallforaparticularexample,yieldingverylargegradients).Onesolutionistoclipgradients(seesection)whileanotheristoscale10.11.1thegradientsheuristically(,).MurrayandLarochelle2014Gaussianmixtureoutputsareparticularlyeﬀectiveingenerativemodelsofspeech(Schuster1999,)ormovementsofphysicalobjects(Graves2013,).Themixturedensitystrategygivesawayforthenetworktorepresentmultipleoutputmodesandtocontrolthevarianceofitsoutput,whichiscrucialforobtainingahighdegreeofqualityinthesereal-valueddomains.Anexampleofamixturedensitynetworkisshowninﬁgure.6.4Ingeneral,wemaywishtocontinuetomodellargervectorsycontainingmorevariables,andtoimposericherandricherstructuresontheseoutputvariables.Forexample,wemaywishforourneuralnetworktooutputasequenceofcharactersthatformsasentence.Inthese cases,wemaycontinuetousetheprincipleofmaximumlikelihoodappliedtoourmodelp(y;ω(x)),butthemodelweuse190 CHAPTER6.DEEPFEEDFORWARDNETWORKS xy Figure6.4:Samplesdrawnfromaneuralnetworkwithamixturedensityoutputlayer.Theinputxissampledfromauniformdistributionandtheoutputyissampledfrompmodel(yx|).Theneuralnetworkisabletolearnnonlinearmappingsfromtheinputtotheparametersoftheoutputdistribution.Theseparametersincludetheprobabilitiesgoverningwhichofthreemixturecomponentswillgeneratetheoutputaswellastheparametersforeachmixturecomponent.EachmixturecomponentisGaussianwithpredictedmeanandvariance.Alloftheseaspectsoftheoutputdistributionareabletovarywithrespecttotheinput,andtodosoinnonlinearways.xtodescribeybecomescomplexenoughtobebeyondthescopeofthischapter.Chapterdescribeshowtouserecurrentneuralnetworkstodeﬁnesuchmodels10oversequences,andpartdescribesadvancedtechniquesformodelingarbitraryIIIprobabilitydistributions.6.3HiddenUnitsSofarwehavefocusedourdiscussionondesignchoicesforneuralnetworksthatarecommontomostparametricmachinelearningmodelstrainedwithgradient-basedoptimization.Nowweturntoanissuethatisuniquetofeedforwardneuralnetworks:howtochoosethetypeofhiddenunittouseinthehiddenlayersofthemodel.Thedesignofhiddenunitsisanextremelyactiveareaofresearchanddoesnotyethavemanydeﬁnitiveguidingtheoreticalprinciples.Rectiﬁedlinearunitsareanexcellentdefaultchoiceofhiddenunit.Manyothertypesofhiddenunitsareavailable.Itcanbediﬃculttodeterminewhentousewhichkind(thoughrectiﬁedlinearunitsareusuallyanacceptablechoice). We191 CHAPTER6.DEEPFEEDFORWARDNETWORKSdescribeheresomeofthebasicintuitionsmotivatingeachtypeofhiddenunits.Theseintuitionscanhelpdecidewhentotryouteachoftheseunits.Itisusuallyimpossibletopredictinadvancewhichwillworkbest.Thedesignprocessconsistsoftrialanderror,intuitingthatakindofhiddenunitmayworkwell,andthentraininganetworkwiththatkindofhiddenunitandevaluatingitsperformanceonavalidationset.Someofthehiddenunitsincludedinthislistarenotactuallydiﬀerentiableatallinputpoints.Forexample,therectiﬁedlinearfunctiong(z) =max{0,z}isnotdiﬀerentiableatz= 0.Thismayseemlikeitinvalidatesgforusewithagradient-basedlearningalgorithm.Inpractice,gradientdescentstillperformswellenoughforthesemodelstobeusedformachinelearningtasks. Thisisinpartbecauseneuralnetworktrainingalgorithmsdonotusuallyarriveatalocalminimumofthecostfunction,butinsteadmerelyreduceitsvaluesigniﬁcantly,asshowninﬁgure.Theseideaswillbedescribedfurtherinchapter.Becausewedonot4.38expecttrainingtoactuallyreachapointwherethegradientis0,itisacceptablefortheminimaofthecostfunctiontocorrespondtopointswithundeﬁnedgradient.Hiddenunitsthatarenotdiﬀerentiableareusuallynon-diﬀerentiableatonlyasmallnumberofpoints.Ingeneral,afunctiong(z)hasaleftderivativedeﬁnedbytheslopeofthefunctionimmediatelytotheleftofzandarightderivativedeﬁnedbytheslopeofthefunctionimmediatelytotherightofz.Afunctionisdiﬀerentiableatzonlyifboththeleftderivativeandtherightderivativearedeﬁnedandequaltoeachother.Thefunctionsusedinthecontextofneuralnetworksusuallyhavedeﬁnedleftderivativesanddeﬁnedrightderivatives.Inthecaseofg(z) =max{0,z},theleftderivativeatz= 00isandtherightderivativeis.Softwareimplementationsofneuralnetworktrainingusuallyreturnoneof1theone-sidedderivativesratherthanreportingthatthederivativeisundeﬁnedorraisinganerror. Thismaybeheuristicallyjustiﬁedbyobservingthatgradient-basedoptimizationonadigitalcomputerissubjecttonumericalerroranyway.Whenafunctionisaskedtoevaluateg(0),itisveryunlikelythattheunderlyingvaluetrulywas.Instead,itwaslikelytobesomesmallvalue0thatwasroundedto.Insomecontexts,moretheoreticallypleasingjustiﬁcationsareavailable,but0theseusuallydonotapplytoneuralnetworktraining.Theimportantpointisthatinpracticeonecansafelydisregardthenon-diﬀerentiabilityofthehiddenunitactivationfunctionsdescribedbelow.Unlessindicatedotherwise,mosthiddenunitscanbedescribedasacceptingavectorofinputsx,computinganaﬃnetransformationz=Wx+b,andthenapplyinganelement-wisenonlinearfunctiong(z).Mosthiddenunitsaredistinguishedfromeachotheronlybythechoiceoftheformoftheactivationfunction.g()z192 CHAPTER6.DEEPFEEDFORWARDNETWORKS6.3.1RectiﬁedLinearUnitsandTheirGeneralizationsRectiﬁedlinearunitsusetheactivationfunction.gz,z() = max0{}Rectiﬁedlinearunitsareeasytooptimizebecausetheyaresosimilartolinearunits.Theonlydiﬀerencebetweenalinearunitandarectiﬁedlinearunitisthatarectiﬁedlinearunitoutputszeroacrosshalfitsdomain. Thismakesthederivativesthrougharectiﬁedlinearunitremainlargewhenevertheunitisactive.Thegradientsarenotonlylargebutalsoconsistent.Thesecondderivativeoftherectifyingoperationisalmosteverywhere,andthederivativeoftherectifying0operationiseverywherethattheunitisactive.Thismeansthatthegradient1directionisfarmoreusefulforlearningthanitwouldbewithactivationfunctionsthatintroducesecond-ordereﬀects.Rectiﬁedlinearunitsaretypicallyusedontopofanaﬃnetransformation:hW= (gxb+).(6.36)Wheninitializingtheparametersoftheaﬃnetransformation,itcanbeagoodpracticetosetallelementsofbtoasmall,positivevalue,suchas0.1.Thismakesitverylikelythattherectiﬁedlinearunitswillbeinitiallyactiveformostinputsinthetrainingsetandallowthederivativestopassthrough.Severalgeneralizationsofrectiﬁedlinearunitsexist.Mostofthesegeneral-izationsperformcomparablytorectiﬁedlinearunitsandoccasionallyperformbetter.Onedrawbacktorectiﬁedlinearunitsisthattheycannotlearnviagradient-based methods onexamples for which their activation iszero.Avariety ofgeneralizationsofrectiﬁedlinearunitsguaranteethattheyreceivegradientevery-where.Threegeneralizationsofrectiﬁedlinearunitsarebasedonusinganon-zeroslopeαiwhenzi<0:hi=g(zα,)i=max(0,zi)+αimin(0,zi).Absolutevaluerectiﬁcationﬁxesαi=−1toobtaing(z) =||z.Itisusedforobjectrecognitionfromimages(,),whereitmakessensetoseekfeaturesthatareJarrettetal.2009invariantunderapolarityreversaloftheinputillumination.Othergeneralizationsofrectiﬁedlinearunitsaremorebroadlyapplicable.AleakyReLU(,Maasetal.2013)ﬁxesαitoasmallvaluelike0.01whileaparametricReLUorPReLUtreatsαiasalearnableparameter(,).Heetal.2015Maxoutunits(,)generalizerectiﬁedlinearunitsGoodfellowetal.2013afurther.Insteadofapplyinganelement-wisefunctiong(z),maxoutunitsdividezintogroupsofkvalues.Eachmaxoutunitthenoutputsthemaximumelementof193 CHAPTER6.DEEPFEEDFORWARDNETWORKSoneofthesegroups:g()zi=maxj∈G()izj(6.37)whereG()iisthesetofindicesintotheinputsforgroupi,{(i−1)k+1,...,ik}.Thisprovidesawayoflearningapiecewiselinearfunctionthatrespondstomultipledirectionsintheinputspace.xAmaxoutunitcanlearnapiecewiselinear,convexfunctionwithuptokpieces.Maxoutunitscanthusbeseenaslearningtheactivationfunctionitselfratherthanjusttherelationshipbetweenunits.Withlargeenoughk,amaxoutunitcanlearntoapproximateanyconvexfunctionwitharbitraryﬁdelity.Inparticular,amaxoutlayerwithtwopiecescanlearntoimplementthesamefunctionoftheinputxasatraditionallayerusingtherectiﬁedlinearactivationfunction,absolutevaluerectiﬁcationfunction,ortheleakyorparametricReLU,orcanlearntoimplementatotallydiﬀerentfunctionaltogether.Themaxoutlayerwillofcoursebeparametrizeddiﬀerentlyfromanyoftheseotherlayertypes,sothelearningdynamicswillbediﬀerenteveninthecaseswheremaxoutlearnstoimplementthesamefunctionofasoneoftheotherlayertypes.xEachmaxoutunitisnowparametrizedbykweightvectorsinsteadofjustone,somaxoutunitstypicallyneedmoreregularizationthanrectiﬁedlinearunits.Theycanworkwellwithoutregularizationifthetrainingsetislargeandthenumberofpiecesperunitiskeptlow(,).Caietal.2013Maxoutunitshaveafewotherbeneﬁts.Insomecases,onecangainsomesta-tisticalandcomputationaladvantagesbyrequiringfewerparameters.Speciﬁcally,ifthefeaturescapturedbyndiﬀerentlinearﬁlterscanbesummarizedwithoutlosinginformationbytakingthemaxovereachgroupofkfeatures,thenthenextlayercangetbywithtimesfewerweights.kBecauseeachunitisdrivenbymultipleﬁlters,maxoutunitshavesomeredun-dancythathelpsthemtoresistaphenomenoncalledcatastrophicforgettinginwhichneuralnetworksforgethowtoperformtasksthattheyweretrainedoninthepast(,).Goodfellowetal.2014aRectiﬁedlinearunitsandallofthesegeneralizationsofthemarebasedontheprinciplethatmodelsareeasiertooptimizeiftheirbehaviorisclosertolinear.Thissamegeneralprincipleofusinglinearbehaviortoobtaineasieroptimizationalsoappliesinothercontextsbesidesdeeplinearnetworks.Recurrentnetworkscanlearnfromsequencesandproduceasequenceofstatesandoutputs.Whentrainingthem,oneneedstopropagateinformationthroughseveraltimesteps,whichismucheasierwhensomelinearcomputations(withsomedirectionalderivativesbeingofmagnitudenear1)areinvolved.Oneofthebest-performingrecurrentnetwork194 CHAPTER6.DEEPFEEDFORWARDNETWORKSarchitectures,theLSTM,propagatesinformationthroughtimeviasummation—aparticularstraightforwardkindofsuchlinearactivation.Thisisdiscussedfurtherinsection.10.106.3.2LogisticSigmoidandHyperbolicTangentPriortotheintroductionofrectiﬁedlinearunits,mostneuralnetworksusedthelogisticsigmoidactivationfunctiongzσz() = ()(6.38)orthehyperbolictangentactivationfunctiongzz.() = tanh()(6.39)Theseactivationfunctionsarecloselyrelatedbecause.tanh() = 2(2)1zσz−We havealready seensigmoid unitsasoutput units, usedto predicttheprobabilitythatabinaryvariableis.Unlikepiecewiselinearunits,sigmoidal1unitssaturateacrossmostoftheirdomain—theysaturatetoahighvaluewhenzisverypositive,saturatetoalowvaluewhenzisverynegative,andareonlystronglysensitivetotheirinputwhenzisnear0.Thewidespreadsaturationofsigmoidalunitscanmakegradient-basedlearningverydiﬃcult.Forthisreason,theiruseashiddenunitsinfeedforwardnetworksisnowdiscouraged.Theiruseasoutputunitsiscompatiblewiththeuseofgradient-basedlearningwhenanappropriatecostfunctioncanundothesaturationofthesigmoidintheoutputlayer.Whenasigmoidalactivationfunctionmustbeused,thehyperbolictangentactivationfunctiontypicallyperformsbetterthanthelogisticsigmoid.Itresemblestheidentityfunctionmoreclosely,inthesensethattanh(0) = 0whileσ(0) =12.Becausetanhissimilartotheidentityfunctionnear,trainingadeepneural0networkˆy=wtanh(Utanh(Vx))resemblestrainingalinearmodelˆy=wUVxsolongastheactivationsofthenetworkcanbekeptsmall.Thismakestrainingthenetworkeasier.tanhSigmoidalactivationfunctionsaremorecommoninsettingsotherthanfeed-forwardnetworks.Recurrentnetworks,manyprobabilisticmodels,andsomeautoencodershaveadditionalrequirementsthatruleouttheuseofpiecewiselinearactivationfunctionsandmakesigmoidalunitsmoreappealingdespitethedrawbacksofsaturation.195 CHAPTER6.DEEPFEEDFORWARDNETWORKS6.3.3OtherHiddenUnitsManyothertypesofhiddenunitsarepossible,butareusedlessfrequently.Ingeneral,awidevarietyofdiﬀerentiablefunctionsperformperfectlywell.Manyunpublishedactivationfunctionsperformjustaswellasthepopularones.Toprovideaconcreteexample,theauthorstestedafeedforwardnetworkusingh=cos(Wx+b)ontheMNISTdatasetandobtainedanerrorrateoflessthan1%,whichiscompetitivewithresultsobtainedusingmoreconventionalactivationfunctions.Duringresearchanddevelopmentofnewtechniques,itiscommontotestmanydiﬀerentactivationfunctionsandﬁndthatseveralvariationsonstandardpracticeperformcomparably.Thismeansthatusuallynewhiddenunittypesarepublishedonlyiftheyareclearlydemonstratedtoprovideasigniﬁcantimprovement.Newhiddenunittypesthatperformroughlycomparablytoknowntypesaresocommonastobeuninteresting.Itwouldbeimpracticaltolistallofthehiddenunittypesthathaveappearedintheliterature.Wehighlightafewespeciallyusefulanddistinctiveones.Onepossibilityistonothaveanactivationg(z)atall.Onecanalsothinkofthisasusingtheidentityfunctionastheactivationfunction.Wehavealreadyseenthatalinearunitcanbeusefulastheoutputofaneuralnetwork.Itmayalsobeusedasahiddenunit.Ifeverylayeroftheneuralnetworkconsistsofonlylineartransformations,thenthenetworkasawholewillbelinear.However,itisacceptableforsomelayersoftheneuralnetworktobepurelylinear.Consideraneuralnetworklayerwithninputsandpoutputs,h=g(Wx+b).Wemayreplacethiswithtwolayers,withonelayerusingweightmatrixUandtheotherusingweightmatrixV.Iftheﬁrstlayerhasnoactivationfunction,thenwehaveessentiallyfactoredtheweightmatrixoftheoriginallayerbasedonW.Thefactoredapproachistocomputeh=g(VUx+b).IfUproducesqoutputs,thenUandVtogethercontainonly(n+p)qparameters,whileWcontainsnpparameters.Forsmallq,thiscanbeaconsiderablesavinginparameters.Itcomesatthecostofconstrainingthelineartransformationtobelow-rank,buttheselow-rankrelationshipsareoftensuﬃcient.Linearhiddenunitsthusoﬀeraneﬀectivewayofreducingthenumberofparametersinanetwork.Softmaxunitsareanotherkindofunitthatisusuallyusedasanoutput(asdescribedinsection)butmaysometimesbeusedasahiddenunit.Softmax6.2.2.3unitsnaturallyrepresentaprobabilitydistributionoveradiscretevariablewithkpossiblevalues,sotheymaybeusedasakindofswitch.Thesekindsofhiddenunitsareusuallyonlyusedinmoreadvancedarchitecturesthatexplicitlylearntomanipulatememory,describedinsection.10.12196 CHAPTER6.DEEPFEEDFORWARDNETWORKSAfewotherreasonablycommonhiddenunittypesinclude:•RadialbasisfunctionorRBFunit:hi=exp−1σ2i||W:,i−||x2.ThisfunctionbecomesmoreactiveasxapproachesatemplateW:,i.Becauseitsaturatestoformost,itcanbediﬃculttooptimize.0x•Softplus:g(a) =ζ(a) =log(1+ea).Thisisasmoothversionoftherectiﬁer,introducedby()forfunctionapproximationandbyDugasetal.2001NairandHinton2010()fortheconditionaldistributionsofundirectedprobabilisticmodels.()comparedthesoftplusandrectiﬁerandfoundGlorotetal.2011abetterresultswiththelatter.Theuseofthesoftplusisgenerallydiscouraged.Thesoftplusdemonstratesthattheperformanceofhiddenunittypescanbeverycounterintuitive—onemightexpectittohaveanadvantageovertherectiﬁerduetobeingdiﬀerentiableeverywhereorduetosaturatinglesscompletely,butempiricallyitdoesnot.•Hardtanh:thisisshapedsimilarlytothetanhandtherectiﬁerbutunlikethelatter,itisbounded,g(a)=max(−1,min(1,a)).Itwasintroducedby().Collobert2004Hiddenunitdesignremainsanactiveareaofresearchandmanyusefulhiddenunittypesremaintobediscovered.6.4ArchitectureDesignAnotherkeydesignconsiderationforneuralnetworksisdeterminingthearchitecture.Thewordarchitecturereferstotheoverallstructureofthenetwork:howmanyunitsitshouldhaveandhowtheseunitsshouldbeconnectedtoeachother.Mostneuralnetworksareorganizedintogroupsofunitscalledlayers. Mostneuralnetworkarchitecturesarrangetheselayersinachainstructure,witheachlayerbeingafunctionofthelayerthatprecededit.Inthisstructure,theﬁrstlayerisgivenbyh(1)= g(1)W(1)xb+(1),(6.40)thesecondlayerisgivenbyh(2)= g(2)W(2)h(1)+b(2),(6.41)andsoon.197 CHAPTER6.DEEPFEEDFORWARDNETWORKSInthesechain-basedarchitectures,themainarchitecturalconsiderationsaretochoosethedepthofthenetworkandthewidthofeachlayer.Aswewillsee,anetworkwithevenonehiddenlayerissuﬃcienttoﬁtthetrainingset.Deepernetworksoftenareabletousefarfewerunitsperlayerandfarfewerparametersandoftengeneralizetothetestset,butarealsooftenhardertooptimize. Theidealnetworkarchitectureforataskmustbefoundviaexperimentationguidedbymonitoringthevalidationseterror.6.4.1UniversalApproximationPropertiesandDepthAlinearmodel,mappingfromfeaturestooutputsviamatrixmultiplication,canbydeﬁnitionrepresentonlylinearfunctions.Ithastheadvantageofbeingeasytotrainbecausemanylossfunctionsresultinconvexoptimizationproblemswhenappliedtolinearmodels.Unfortunately,weoftenwanttolearnnonlinearfunctions.Atﬁrstglance,wemightpresumethatlearninganonlinearfunctionrequiresdesigningaspecializedmodelfamilyforthekindofnonlinearitywewanttolearn.Fortunately,feedforwardnetworkswithhiddenlayersprovideauniversalapproxi-mationframework.Speciﬁcally,theuniversalapproximationtheorem(Horniketal.,;,)statesthatafeedforwardnetworkwithalinearoutput1989Cybenko1989layerandatleastonehiddenlayerwithany“squashing”activationfunction(suchasthelogisticsigmoidactivationfunction)canapproximateanyBorelmeasurablefunctionfromoneﬁnite-dimensionalspacetoanotherwithanydesirednon-zeroamountoferror,providedthatthenetworkisgivenenoughhiddenunits.Thederivativesofthefeedforwardnetworkcanalsoapproximatethederivativesofthefunctionarbitrarilywell(,).TheconceptofBorelmeasurabilityHorniketal.1990isbeyondthescopeofthisbook; forourpurposesitsuﬃcestosaythatanycontinuousfunctiononaclosedandboundedsubsetofRnisBorelmeasurableandthereforemaybeapproximatedbyaneuralnetwork.Aneuralnetworkmayalsoapproximateanyfunctionmappingfromanyﬁnitedimensionaldiscretespacetoanother.Whiletheoriginaltheoremswereﬁrststatedintermsofunitswithactivationfunctionsthatsaturatebothforverynegativeandforverypositivearguments,universalapproximationtheoremshavealsobeenprovedforawiderclassofactivationfunctions,whichincludesthenowcommonlyusedrectiﬁedlinearunit(,).Leshnoetal.1993Theuniversalapproximationtheoremmeansthatregardlessofwhatfunctionwearetryingtolearn,weknowthatalargeMLPwillbeabletorepresentthisfunction.However,wearenotguaranteedthatthetrainingalgorithmwillbeabletolearnthatfunction.EveniftheMLPisabletorepresentthefunction,learningcanfailfortwodiﬀerentreasons.First,theoptimizationalgorithmusedfortraining198 CHAPTER6.DEEPFEEDFORWARDNETWORKSmaynotbeabletoﬁndthevalueoftheparametersthatcorrespondstothedesiredfunction.Second,thetrainingalgorithmmightchoosethewrongfunctionduetooverﬁtting.Recallfromsectionthatthe“nofreelunch”theoremshowsthat5.2.1thereisnouniversallysuperiormachinelearningalgorithm.Feedforwardnetworksprovideauniversalsystemforrepresentingfunctions,inthesensethat,givenafunction,thereexistsafeedforwardnetworkthatapproximatesthefunction.Thereisnouniversalprocedureforexaminingatrainingsetofspeciﬁcexamplesandchoosingafunctionthatwillgeneralizetopointsnotinthetrainingset.Theuniversalapproximationtheoremsaysthatthereexistsanetworklargeenoughtoachieveanydegreeofaccuracywedesire,butthetheoremdoesnotsayhowlargethisnetworkwillbe.()providessomeboundsontheBarron1993sizeofasingle-layernetworkneededtoapproximateabroadclassoffunctions.Unfortunately,intheworsecase,anexponentialnumberofhiddenunits(possiblywithonehiddenunitcorrespondingtoeachinputconﬁgurationthatneedstobedistinguished)mayberequired.Thisiseasiesttoseeinthebinarycase:thenumberofpossiblebinaryfunctionsonvectorsv∈{0,1}nis22nandselectingonesuchfunctionrequires2nbits,whichwillingeneralrequireO(2n)degreesoffreedom.Insummary,afeedforwardnetworkwithasinglelayerissuﬃcienttorepresentanyfunction,butthelayermaybeinfeasiblylargeandmayfailtolearnandgeneralizecorrectly.Inmanycircumstances,usingdeepermodelscanreducethenumberofunitsrequiredtorepresentthedesiredfunctionandcanreducetheamountofgeneralizationerror.Thereexistfamiliesoffunctionswhichcanbeapproximatedeﬃcientlybyanarchitecturewithdepthgreaterthansomevalued,butwhichrequireamuchlargermodelifdepthisrestrictedtobelessthanorequaltod.Inmanycases,thenumberofhiddenunitsrequiredbytheshallowmodelisexponentialinn. Suchresultswereﬁrstprovedformodelsthatdonotresemblethecontinuous,diﬀerentiableneuralnetworksusedformachinelearning,buthavesincebeenextendedtothesemodels.Theﬁrstresultswereforcircuitsoflogicgates(,).LaterHåstad1986workextendedtheseresultstolinearthresholdunitswithnon-negativeweights(,;,),andthentonetworkswithHåstadandGoldmann1991Hajnaletal.1993continuous-valuedactivations(,;,). ManymodernMaass1992Maassetal.1994neuralnetworksuserectiﬁedlinearunits.()demonstratedLeshnoetal.1993thatshallownetworkswithabroadfamilyofnon-polynomialactivationfunctions,includingrectiﬁedlinearunits,haveuniversalapproximationproperties,buttheseresultsdonotaddressthequestionsofdepthoreﬃciency—theyspecifyonlythatasuﬃcientlywiderectiﬁernetworkcouldrepresentanyfunction.Montufaretal.199 CHAPTER6.DEEPFEEDFORWARDNETWORKS()showedthatfunctionsrepresentablewithadeeprectiﬁernetcanrequire2014anexponentialnumberofhiddenunitswithashallow(onehiddenlayer)network.Moreprecisely,theyshowedthatpiecewiselinearnetworks(whichcanbeobtainedfromrectiﬁernonlinearitiesormaxoutunits)canrepresentfunctionswithanumberofregionsthatisexponentialinthedepthofthenetwork.Figureillustrateshow6.5anetworkwithabsolutevaluerectiﬁcationcreatesmirrorimagesofthefunctioncomputedontopofsomehiddenunit,withrespecttotheinputofthathiddenunit.Eachhiddenunitspeciﬁeswheretofoldtheinputspaceinordertocreatemirrorresponses(onbothsidesoftheabsolutevaluenonlinearity).Bycomposingthesefoldingoperations,weobtainanexponentiallylargenumberofpiecewiselinearregionswhichcancaptureallkindsofregular(e.g.,repeating)patterns. Figure6.5:Anintuitive,geometricexplanationoftheexponentialadvantageofdeeperrectiﬁernetworksformallyby().Montufaretal.2014(Left)Anabsolutevaluerectiﬁcationunithasthesameoutputforeverypairofmirrorpointsinitsinput.Themirroraxisofsymmetryisgivenbythehyperplanedeﬁnedbytheweightsandbiasoftheunit.Afunctioncomputedontopofthatunit(thegreendecisionsurface)willbeamirrorimageofasimplerpatternacrossthataxisofsymmetry.Thefunctioncanbeobtained(Center)byfoldingthespacearoundtheaxisofsymmetry.Anotherrepeatingpatterncan(Right)befoldedontopoftheﬁrst(byanotherdownstreamunit)toobtainanothersymmetry(whichisnowrepeatedfourtimes,withtwohiddenlayers).Figurereproducedwithpermissionfrom().Montufaretal.2014Moreprecisely,themaintheoremin()statesthattheMontufaretal.2014numberoflinearregionscarvedoutbyadeeprectiﬁernetworkwithdinputs,depth,andunitsperhiddenlayer,islnOnddl(−1)nd,(6.42)i.e.,exponentialinthedepth.Inthecaseofmaxoutnetworkswithﬁltersperlkunit,thenumberoflinearregionsisOk(1)+l−d.(6.43)200 CHAPTER6.DEEPFEEDFORWARDNETWORKSOfcourse,thereisnoguaranteethatthekindsoffunctionswewanttolearninapplicationsofmachinelearning(andinparticularforAI)sharesuchaproperty.Wemayalsowanttochooseadeepmodelforstatisticalreasons. Anytimewechooseaspeciﬁcmachinelearningalgorithm,weareimplicitlystatingsomesetofpriorbeliefswehaveaboutwhatkindoffunctionthealgorithmshouldlearn.Choosingadeepmodelencodesaverygeneralbeliefthatthefunctionwewanttolearnshouldinvolvecompositionofseveralsimplerfunctions.Thiscanbeinterpretedfromarepresentationlearningpointofviewassayingthatwebelievethelearningproblemconsistsofdiscoveringasetofunderlyingfactorsofvariationthatcaninturnbedescribedintermsofother,simplerunderlyingfactorsofvariation.Alternately,wecaninterprettheuseofadeeparchitectureasexpressingabeliefthatthefunctionwewanttolearnisacomputerprogramconsistingofmultiplesteps,whereeachstepmakesuseofthepreviousstep’soutput. Theseintermediateoutputsarenotnecessarilyfactorsofvariation,butcaninsteadbeanalogoustocountersorpointersthatthenetworkusestoorganizeitsinternalprocessing.Empirically,greaterdepthdoesseemtoresultinbettergeneralizationforawidevarietyoftasks(,;,;,;Bengioetal.2007Erhanetal.2009Bengio2009Mesnil2011Ciresan2012Krizhevsky2012Sermanetetal.,;etal.,;etal.,;etal.,2013Farabet2013Couprie2013Kahou2013Goodfellow;etal.,;etal.,;etal.,;etal.etal.,;2014dSzegedy,).Seeﬁgureandﬁgureforexamplesof2014a6.66.7someoftheseempiricalresults.Thissuggeststhatusingdeeparchitecturesdoesindeedexpressausefulprioroverthespaceoffunctionsthemodellearns.6.4.2OtherArchitecturalConsiderationsSofarwehavedescribedneuralnetworksasbeingsimplechainsoflayers,withthemainconsiderationsbeingthedepthofthenetworkandthewidthofeachlayer.Inpractice,neuralnetworksshowconsiderablymorediversity.Manyneuralnetworkarchitectureshavebeendevelopedforspeciﬁctasks.Specializedarchitecturesforcomputervisioncalledconvolutionalnetworksaredescribedinchapter.Feedforwardnetworksmayalsobegeneralizedtothe9recurrentneuralnetworksforsequenceprocessing,describedinchapter,which10havetheirownarchitecturalconsiderations.Ingeneral,thelayersneednotbeconnectedinachain,eventhoughthisisthemostcommonpractice.Manyarchitecturesbuildamainchainbutthenaddextraarchitecturalfeaturestoit,suchasskipconnectionsgoingfromlayeritolayeri+2orhigher.Theseskipconnectionsmakeiteasierforthegradienttoﬂowfromoutputlayerstolayersnearertheinput.201 CHAPTER6.DEEPFEEDFORWARDNETWORKS 34567891011920.925.930.935.940.945.950.955.960.965.Testaccuracy(percent) Figure6.6:Empiricalresultsshowingthatdeepernetworksgeneralizebetterwhenusedtotranscribemulti-digitnumbersfromphotographsofaddresses.DatafromGoodfellowetal.(). Thetestsetaccuracyconsistentlyincreaseswithincreasingdepth. See2014dﬁgureforacontrolexperimentdemonstratingthatotherincreasestothemodelsize6.7donotyieldthesameeﬀect.Anotherkeyconsiderationofarchitecturedesignisexactlyhowtoconnectapairoflayerstoeachother.InthedefaultneuralnetworklayerdescribedbyalineartransformationviaamatrixW,everyinputunitisconnectedtoeveryoutputunit.Manyspecializednetworksinthechaptersaheadhavefewerconnections,sothateachunitintheinputlayerisconnectedtoonlyasmallsubsetofunitsintheoutputlayer.Thesestrategiesforreducingthenumberofconnectionsreducethenumberofparametersandtheamountofcomputationrequiredtoevaluatethenetwork,butareoftenhighlyproblem-dependent.Forexample,convolutionalnetworks,describedinchapter,usespecializedpatternsofsparseconnections9thatareveryeﬀectiveforcomputervisionproblems.Inthischapter,itisdiﬃculttogivemuchmorespeciﬁcadviceconcerningthearchitectureofagenericneuralnetwork.Subsequentchaptersdeveloptheparticulararchitecturalstrategiesthathavebeenfoundtoworkwellfordiﬀerentapplicationdomains. 202 CHAPTER6.DEEPFEEDFORWARDNETWORKS 000204060810......Numberofparameters×10891929394959697Testaccuracy(percent)3,convolutional3,fullyconnected11,convolutional Figure6.7:Deepermodelstendtoperformbetter.Thisisnotmerelybecausethemodelislarger.ThisexperimentfromGoodfellow2014detal.()showsthatincreasingthenumberofparametersinlayersofconvolutionalnetworkswithoutincreasingtheirdepthisnotnearlyaseﬀectiveatincreasingtestsetperformance.Thelegendindicatesthedepthofnetworkusedtomakeeachcurveandwhetherthecurverepresentsvariationinthesizeoftheconvolutionalorthefullyconnectedlayers.Weobservethatshallowmodelsinthiscontextoverﬁtataround20millionparameterswhiledeeponescanbeneﬁtfromhavingover60million.Thissuggeststhatusingadeepmodelexpressesausefulpreferenceoverthespaceoffunctionsthemodelcanlearn.Speciﬁcally,itexpressesabeliefthatthefunctionshouldconsistofmanysimplerfunctionscomposedtogether.Thiscouldresulteitherinlearningarepresentationthatiscomposedinturnofsimplerrepresentations(e.g.,cornersdeﬁnedintermsofedges)orinlearningaprogramwithsequentiallydependentsteps(e.g.,ﬁrstlocateasetofobjects,thensegmentthemfromeachother,thenrecognizethem). 203 CHAPTER6.DEEPFEEDFORWARDNETWORKS6.5Back-PropagationandOtherDiﬀerentiationAlgo-rithmsWhenweuseafeedforwardneuralnetworktoacceptaninputxandproduceanoutputˆy,informationﬂowsforwardthroughthenetwork.Theinputsxprovidetheinitialinformationthatthenpropagatesuptothehiddenunitsateachlayerandﬁnallyproducesˆy.Thisiscalledforwardpropagation.Duringtraining,forwardpropagationcancontinueonwarduntilitproducesascalarcostJ(θ).Theback-propagationalgorithm(,),oftensimplycalledRumelhartetal.1986abackprop,allowstheinformationfromthecosttothenﬂowbackwardsthroughthenetwork,inordertocomputethegradient.Computingananalyticalexpressionforthegradientisstraightforward,butnumericallyevaluatingsuchanexpressioncanbecomputationallyexpensive.Theback-propagationalgorithmdoessousingasimpleandinexpensiveprocedure.Thetermback-propagationisoften misunderstoodasmeaningthewholelearningalgorithmformulti-layerneuralnetworks.Actually,back-propagationrefersonlytothemethodforcomputingthegradient,whileanotheralgorithm,suchasstochasticgradientdescent,isusedtoperformlearningusingthisgradient.Furthermore,back-propagationisoftenmisunderstoodasbeingspeciﬁctomulti-layerneuralnetworks,butinprincipleitcancomputederivativesofanyfunction(forsomefunctions,thecorrectresponseistoreportthatthederivativeofthefunctionisundeﬁned).Speciﬁcally,wewilldescribehowtocomputethegradient∇xf(xy,)foranarbitraryfunctionf,wherexisasetofvariableswhosederivativesaredesired,andyisanadditionalsetofvariablesthatareinputstothefunctionbutwhosederivativesarenotrequired.Inlearningalgorithms,thegradientwemostoftenrequireisthegradientofthecostfunctionwithrespecttotheparameters,∇θJ(θ).Manymachinelearningtasksinvolvecomputingotherderivatives,eitheraspartof thelearning process, or toanalyzethelearned model.The back-propagationalgorithmcanbeappliedtothesetasksaswell,andisnotrestrictedtocomputingthegradientofthecostfunctionwithrespecttotheparameters.Theideaofcomputingderivativesbypropagatinginformationthroughanetworkisverygeneral,andcanbeusedtocomputevaluessuchastheJacobianofafunctionfwithmultipleoutputs.Werestrictourdescriptionheretothemostcommonlyusedcasewherehasasingleoutput.f204 CHAPTER6.DEEPFEEDFORWARDNETWORKS6.5.1ComputationalGraphsSofarwehavediscussedneuralnetworkswitharelativelyinformalgraphlanguage.Todescribetheback-propagationalgorithmmoreprecisely,itishelpfultohaveamorepreciselanguage.computationalgraphManywaysofformalizingcomputationasgraphsarepossible.Here,weuseeachnodeinthegraphtoindicateavariable.Thevariablemaybeascalar,vector,matrix,tensor,orevenavariableofanothertype.Toformalizeourgraphs,wealsoneedtointroducetheideaofanoperation.Anoperationisasimplefunctionofoneormorevariables.Ourgraphlanguageisaccompaniedbyasetofallowableoperations.Functionsmorecomplicatedthantheoperationsinthissetmaybedescribedbycomposingmanyoperationstogether.Withoutlossofgenerality, wedeﬁneanoperationtoreturnonlyasingleoutputvariable.Thisdoesnotlosegeneralitybecausetheoutputvariablecanhavemultipleentries,suchasavector.Softwareimplementationsofback-propagationusuallysupportoperationswithmultipleoutputs,butweavoidthiscaseinourdescriptionbecauseitintroducesmanyextradetailsthatarenotimportanttoconceptualunderstanding.Ifavariableyiscomputedbyapplyinganoperationtoavariablex,thenwedrawadirectededgefromxtoy. Wesometimesannotatetheoutputnodewiththenameoftheoperationapplied,andothertimesomitthislabelwhentheoperationisclearfromcontext.Examplesofcomputationalgraphsareshowninﬁgure.6.86.5.2ChainRuleofCalculusThechainruleofcalculus(nottobeconfusedwiththechainruleofprobability)isusedtocomputethederivativesoffunctionsformedbycomposingotherfunctionswhosederivativesareknown.Back-propagationisanalgorithmthatcomputesthechainrule,withaspeciﬁcorderofoperationsthatishighlyeﬃcient.Letxbearealnumber,andletfandgbothbefunctionsmappingfromarealnumbertoarealnumber.Supposethaty=g(x)andz=f(g(x)) =f(y).Thenthechainrulestatesthatdzdx=dzdydydx.(6.44)Wecangeneralizethisbeyondthescalarcase.Supposethatx∈Rm,y∈Rn,205 CHAPTER6.DEEPFEEDFORWARDNETWORKS z zx xy y(a)×x xw w(b)u(1)u(1)dotb bu(2)u(2)+ˆyˆyσ (c)X XW WU(1)U(1)matmulb bU(2)U(2)+H Hrelu x xw w(d)ˆyˆydotλ λu(1)u(1)sqru(2)u(2)sumu(3)u(3)× Figure6.8:Examplesofcomputationalgraphs.Thegraphusingthe(a)×operationtocomputez=xy.Thegraphforthelogisticregressionprediction(b)ˆy=σxw+b.Someoftheintermediateexpressionsdonothavenamesinthealgebraicexpressionbutneednamesinthegraph.Wesimplynamethei-thsuchvariableu()i.The(c)computationalgraphfortheexpressionH=max{0,XW+b},whichcomputesadesignmatrixofrectiﬁedlinearunitactivationsHgivenadesignmatrixcontainingaminibatchofinputsX.Examplesa–cappliedatmostoneoperationtoeachvariable,butit(d)ispossibletoapplymorethanoneoperation.Hereweshowacomputationgraphthatappliesmorethanoneoperationtotheweightswofalinearregressionmodel.Theweightsareusedtomakeboththepredictionˆyandtheweightdecaypenaltyλiw2i.206 CHAPTER6.DEEPFEEDFORWARDNETWORKSgmapsfromRmtoRn,andfmapsfromRntoR.Ify=g(x) andz=f(y),then∂z∂xi=j∂z∂yj∂yj∂xi.(6.45)Invectornotation,thismaybeequivalentlywrittenas∇xz=∂y∂x∇yz,(6.46)where∂y∂xistheJacobianmatrixof.nm×gFromthisweseethatthegradientofavariablexcanbeobtainedbymultiplyingaJacobianmatrix∂y∂xbyagradient∇yz.Theback-propagationalgorithmconsistsofperformingsuchaJacobian-gradientproductforeachoperationinthegraph.Usuallywedonotapplytheback-propagationalgorithmmerelytovectors,butrathertotensorsofarbitrarydimensionality.Conceptually,thisisexactlythesameasback-propagationwithvectors.Theonlydiﬀerenceishowthenumbersarearrangedinagridtoformatensor.Wecouldimagineﬂatteningeachtensorintoavectorbeforewerunback-propagation,computingavector-valuedgradient,andthenreshapingthegradientbackintoatensor.Inthisrearrangedview,back-propagationisstilljustmultiplyingJacobiansbygradients.TodenotethegradientofavaluezwithrespecttoatensorX,wewrite∇Xz,justasifXwereavector.TheindicesintoXnowhavemultiplecoordinates—forexample,a3-Dtensorisindexedbythreecoordinates.Wecanabstractthisawaybyusingasinglevariableitorepresentthecompletetupleofindices.Forallpossibleindextuplesi,(∇Xz)igives∂z∂Xi.Thisisexactlythesameashowforallpossibleintegerindicesiintoavector,(∇xz)igives∂z∂xi.Usingthisnotation,wecanwritethechainruleasitappliestotensors.Ifand,thenYX= (g)zf= ()Y∇Xz=j(∇XYj)∂z∂Yj.(6.47)6.5.3RecursivelyApplyingtheChainRuletoObtainBackpropUsingthechainrule,itisstraightforwardtowritedownanalgebraicexpressionforthegradientofascalarwithrespecttoanynodeinthecomputationalgraphthatproducedthatscalar.However,actuallyevaluatingthatexpressioninacomputerintroducessomeextraconsiderations.Speciﬁcally,manysubexpressionsmayberepeatedseveraltimeswithintheoverallexpressionforthegradient.Anyprocedurethatcomputesthegradient207 CHAPTER6.DEEPFEEDFORWARDNETWORKSwillneedtochoosewhethertostorethesesubexpressionsortorecomputethemseveraltimes.Anexampleofhowtheserepeatedsubexpressionsariseisgiveninﬁgure.Insomecases,computingthesamesubexpressiontwicewouldsimply6.9bewasteful. Forcomplicatedgraphs,therecanbeexponentiallymanyofthesewastedcomputations,makinganaiveimplementationofthechainruleinfeasible.Inothercases,computingthesamesubexpressiontwicecouldbeavalidwaytoreducememoryconsumptionatthecostofhigherruntime.Weﬁrstbeginbyaversionoftheback-propagationalgorithmthatspeciﬁestheactualgradientcomputationdirectly(algorithmalongwithalgorithmforthe6.26.1associatedforwardcomputation),intheorderitwillactuallybedoneandaccordingtotherecursiveapplicationofchainrule.Onecouldeitherdirectlyperformthesecomputationsorviewthedescriptionofthealgorithmasasymbolicspeciﬁcationofthecomputationalgraphforcomputingtheback-propagation.However,thisformulationdoesnotmakeexplicitthemanipulationandtheconstructionofthesymbolicgraphthatperformsthegradientcomputation. Suchaformulationispresentedbelowinsection,withalgorithm,wherewealsogeneralizeto6.5.66.5nodesthatcontainarbitrarytensors.Firstconsideracomputationalgraphdescribinghowtocomputeasinglescalaru()n(saythelossonatrainingexample).Thisscalaristhequantitywhosegradientwewanttoobtain,withrespecttotheniinputnodesu(1)tou(ni). Inotherwordswewishtocompute∂u()n∂u()iforalli∈{1,2,...,ni}.Intheapplicationofback-propagationtocomputinggradientsforgradientdescentoverparameters,u()nwillbethecostassociatedwithanexampleoraminibatch,whileu(1)tou(ni)correspondtotheparametersofthemodel.Wewillassumethatthenodesofthegraphhavebeenorderedinsuchawaythatwecancomputetheiroutputoneaftertheother,startingatu(ni+1)andgoinguptou()n.Asdeﬁnedinalgorithm,eachnode6.1u()iisassociatedwithanoperationf()iandiscomputedbyevaluatingthefunctionu()i= (fA()i)(6.48)whereA()iisthesetofallnodesthatareparentsofu()i.Thatalgorithmspeciﬁestheforwardpropagationcomputation,whichwecouldputinagraphG.Inordertoperformback-propagation,wecanconstructacomputationalgraphthatdependsonGandaddstoitanextrasetofnodes.TheseformasubgraphBwithonenodepernodeofG.ComputationinBproceedsinexactlythereverseoftheorderofcomputationinG,andeachnodeofBcomputesthederivative∂u()n∂u()iassociatedwiththeforwardgraphnodeu()i.Thisisdone208 CHAPTER6.DEEPFEEDFORWARDNETWORKSAlgorithm6.1Aprocedurethatperformsthecomputationsmappingniinputsu(1)tou(ni)toanoutputu()n.Thisdeﬁnesacomputationalgraphwhereeachnodecomputesnumericalvalueu()ibyapplyingafunctionf()itothesetofargumentsA()ithatcomprisesthevaluesofpreviousnodesu()j,j<i,withjPa∈(u()i).Theinputtothecomputationalgraphisthevectorx,andissetintotheﬁrstninodesu(1)tou(ni).Theoutputofthecomputationalgraphisreadoﬀthelast(output)nodeu()n.fori,...,n= 1idou()i←xiendforforin= i+1,...,ndoA()i←{u()j|∈jPau(()i)}u()i←f()i(A()i)endforreturnu()nusingthechainrulewithrespecttoscalaroutputu()n:∂u()n∂u()j=ijPau:∈(()i)∂u()n∂u()i∂u()i∂u()j(6.49)asspeciﬁedbyalgorithm.Thesubgraph6.2Bcontainsexactlyoneedgeforeachedgefromnodeu()jtonodeu()iofG.Theedgefromu()jtou()iisassociatedwiththecomputationof∂u()i∂u()j.Inaddition,adotproductisperformedforeachnode,betweenthegradientalreadycomputedwithrespecttonodesu()ithatarechildrenofu()jandthevectorcontainingthepartialderivatives∂u()i∂u()jforthesamechildrennodesu()i.Tosummarize,theamountofcomputationrequiredforperformingtheback-propagationscaleslinearlywiththenumberofedgesinG,wherethecomputationforeachedgecorrespondstocomputingapartialderivative(ofonenodewithrespecttooneofitsparents)aswellasperformingonemultiplicationandoneaddition.Below,wegeneralizethisanalysistotensor-valuednodes,whichisjustawaytogroupmultiplescalarvaluesinthesamenodeandenablemoreeﬃcientimplementations.Theback-propagationalgorithmisdesignedtoreducethenumberofcommonsubexpressionswithoutregardtomemory.Speciﬁcally,itperformsontheorderofoneJacobianproductpernodeinthegraph. Thiscanbeseenfromthefactthatbackprop(algorithm)visitseachedgefromnode6.2u()jtonodeu()iofthegraphexactlyonceinordertoobtaintheassociatedpartialderivative∂u()i∂u()j.209 CHAPTER6.DEEPFEEDFORWARDNETWORKSAlgorithm6.2Simpliﬁedversionoftheback-propagationalgorithmforcomputingthederivativesofu()nwithrespecttothevariablesinthegraph.Thisexampleisintendedtofurtherunderstandingbyshowingasimpliﬁedcasewhereallvariablesarescalars,andwewishtocomputethederivativeswithrespecttou(1),...,u(ni).Thissimpliﬁedversioncomputesthederivativesofallnodesinthegraph. Thecomputationalcostofthisalgorithmisproportionaltothenumberofedgesinthegraph,assumingthatthepartialderivativeassociatedwitheachedgerequiresaconstanttime.Thisisofthesameorderasthenumberofcomputationsfortheforwardpropagation.Each∂u()i∂u()jisafunctionoftheparentsu()jofu()i,thuslinkingthenodesoftheforwardgraphtothoseaddedfortheback-propagationgraph.Runforwardpropagation(algorithmforthisexample)toobtaintheactiva-6.1tionsofthenetworkInitializegrad_table,adatastructurethatwillstorethederivativesthathavebeencomputed.Theentrygradtable_[u()i]willstorethecomputedvalueof∂u()n∂u()i.gradtable_[u()n] 1←fordojn= −1downto1Thenextlinecomputes∂u()n∂u()j=ijPau:∈(()i)∂u()n∂u()i∂u()i∂u()jusingstoredvalues:gradtable_[u()j] ←ijPau:∈(()i)gradtable_[u()i]∂u()i∂u()jendforreturn{gradtable_[u()i] = 1|i,...,ni}Back-propagationthusavoidstheexponentialexplosioninrepeatedsubexpressions.However,otheralgorithmsmaybeabletoavoidmoresubexpressionsbyperformingsimpliﬁcationsonthecomputationalgraph,ormaybeabletoconservememorybyrecomputingratherthanstoringsomesubexpressions.Wewillrevisittheseideasafterdescribingtheback-propagationalgorithmitself.6.5.4Back-PropagationComputationinFully-ConnectedMLPToclarifytheabovedeﬁnitionoftheback-propagationcomputation,letusconsiderthespeciﬁcgraphassociatedwithafully-connectedmulti-layerMLP.Algorithmﬁrstshowstheforwardpropagation,whichmapsparametersto6.3thesupervisedlossL(ˆyy,)associatedwithasingle(input,target)trainingexample()xy,,withˆytheoutputoftheneuralnetworkwhenisprovidedininput.xAlgorithm then shows thecorresponding computation tobe donefor6.4210 CHAPTER6.DEEPFEEDFORWARDNETWORKS z z x xy y w wfff Figure6.9:Acomputationalgraphthatresultsinrepeatedsubexpressionswhencomputingthegradient.Letw∈Rbetheinputtothegraph.Weusethesamefunctionf:RR→astheoperationthatweapplyateverystepofachain:x=f(w),y=f(x),z=f(y).Tocompute∂z∂w,weapplyequationandobtain:6.44∂z∂w(6.50)=∂z∂y∂y∂x∂x∂w(6.51)=f()yf()xf()w(6.52)=f((()))ffwf(())fwf()w(6.53)Equationsuggestsanimplementationinwhichwecomputethevalueof6.52f(w)onlyonceandstoreitinthevariablex.Thisistheapproachtakenbytheback-propagationalgorithm.Analternativeapproachissuggestedbyequation,wherethesubexpression6.53f(w)appearsmorethanonce.Inthealternativeapproach,f(w)isrecomputedeachtimeitisneeded.Whenthememoryrequiredtostorethevalueoftheseexpressionsislow,theback-propagationapproachofequationisclearlypreferablebecauseofitsreduced6.52runtime.However,equationisalsoavalidimplementationofthechainrule,andis6.53usefulwhenmemoryislimited.211 CHAPTER6.DEEPFEEDFORWARDNETWORKSapplyingtheback-propagationalgorithmtothisgraph.Algorithmsandaredemonstrationsthatarechosentobesimpleand6.36.4straightforwardtounderstand.However, theyarespecializedtoonespeciﬁcproblem.Modernsoftwareimplementationsarebasedonthegeneralizedformofback-propagationdescribedinsectionbelow,whichcanaccommodateanycompu-6.5.6tationalgraphbyexplicitlymanipulatingadatastructureforrepresentingsymboliccomputation.Algorithm6.3Forwardpropagationthroughatypicaldeepneuralnetworkandthecomputationofthecostfunction.ThelossL(ˆyy,)dependsontheoutputˆyandonthetargety(seesectionforexamplesoflossfunctions).To6.2.1.1obtainthetotalcostJ,thelossmaybeaddedtoaregularizerΩ(θ),whereθcontainsalltheparameters(weightsandbiases).Algorithmshowshowto6.4computegradientsofJwithrespecttoparametersWandb.Forsimplicity,thisdemonstrationusesonlyasingleinputexamplex.Practicalapplicationsshoulduseaminibatch.Seesectionforamorerealisticdemonstration.6.5.7Require:Networkdepth,lRequire:W()i,i,...,l,∈{1}theweightmatricesofthemodelRequire:b()i,i,...,l,∈{1}thebiasparametersofthemodelRequire:x,theinputtoprocessRequire:y,thetargetoutputh(0)= xfordok,...,l= 1a()k= b()k+W()kh(1)k−h()k= (fa()k)endforˆyh= ()lJL= (ˆyy,)+Ω()λθ6.5.5Symbol-to-SymbolDerivativesAlgebraicexpressionsandcomputationalgraphsbothoperateonsymbols,orvariables thatdo not havespeciﬁc values.Thesealgebraic andgraph-basedrepresentationsarecalledsymbolicrepresentations.Whenweactuallyuseortrainaneuralnetwork,wemustassignspeciﬁcvaluestothesesymbols.Wereplaceasymbolicinputtothenetworkxwithaspeciﬁcnumericvalue,suchas[12376518].,.,−..212 CHAPTER6.DEEPFEEDFORWARDNETWORKS Algorithm6.4Backwardcomputationforthedeepneuralnetworkofalgo-rithm,whichusesinadditiontotheinput6.3xatargety.Thiscomputationyieldsthegradientsontheactivationsa()kforeachlayerk,startingfromtheoutputlayerandgoingbackwardstotheﬁrsthiddenlayer.Fromthesegradients,whichcanbeinterpretedasanindicationofhoweachlayer’soutputshouldchangetoreduceerror,onecanobtainthegradientontheparametersofeachlayer.Thegradientsonweightsandbiasescanbeimmediatelyusedaspartofastochas-ticgradientupdate(performingtheupdaterightafterthegradientshavebeencomputed)orusedwithothergradient-basedoptimizationmethods.Aftertheforwardcomputation,computethegradientontheoutputlayer:g←∇ˆyJ= ∇ˆyL(ˆyy,)fordokl,l,...,= −11Convert thegradienton thelayer’s output into a gradient into thepre-nonlinearityactivation(element-wisemultiplicationifiselement-wise):fg←∇a()kJf= g(a()k)Computegradientsonweightsandbiases(includingtheregularizationterm,whereneeded):∇b()kJλ= +g∇b()kΩ()θ∇W()kJ= gh(1)k−+λ∇W()kΩ()θPropagatethegradientsw.r.t.thenextlower-levelhiddenlayer’sactivations:g←∇h(1)k−J= W()kgendfor 213 CHAPTER6.DEEPFEEDFORWARDNETWORKSzz x xy y w wfffzz x xy y w wfffdzdydzdyfdydxdydxfdzdxdzdx×dxdwdxdwfdzdwdzdw×Figure6.10:Anexampleofthesymbol-to-symbolapproachtocomputingderivatives.Inthisapproach,theback-propagationalgorithmdoesnotneedtoeveraccessanyactualspeciﬁcnumericvalues.Instead,itaddsnodestoacomputationalgraphdescribinghowtocomputethesederivatives.Agenericgraphevaluationenginecanlatercomputethederivativesforanyspeciﬁcnumericvalues.(Left)Inthisexample,webeginwithagraphrepresentingz=f(f(f(w))).Weruntheback-propagationalgorithm,instructing(Right)ittoconstructthegraphfortheexpressioncorrespondingtodzdw.Inthisexample,wedonotexplainhowtheback-propagationalgorithmworks.Thepurposeisonlytoillustratewhatthedesiredresultis:acomputationalgraphwithasymbolicdescriptionofthederivative.Someapproachestoback-propagationtakeacomputationalgraphandasetofnumericalvaluesfortheinputstothegraph,thenreturnasetofnumericalvaluesdescribingthegradientatthoseinputvalues.Wecallthisapproach“symbol-to-number”diﬀerentiation.ThisistheapproachusedbylibrariessuchasTorch(,)andCaﬀe(,).Collobertetal.2011bJia2013Anotherapproachistotakeacomputationalgraphandaddadditionalnodestothegraphthatprovideasymbolicdescriptionofthedesiredderivatives.ThisistheapproachtakenbyTheano(,;,)Bergstraetal.2010Bastienetal.2012andTensorFlow(,).AnexampleofhowthisapproachworksAbadietal.2015isillustratedinﬁgure.Theprimaryadvantageofthisapproachisthat6.10thederivativesaredescribedinthesamelanguageastheoriginalexpression.Becausethederivativesarejustanothercomputationalgraph,itispossibletorunback-propagationagain,diﬀerentiatingthederivativesinordertoobtainhigherderivatives.Computationofhigher-orderderivativesisdescribedinsection.6.5.10Wewillusethelatterapproachanddescribetheback-propagationalgorithmin214 CHAPTER6.DEEPFEEDFORWARDNETWORKStermsofconstructingacomputationalgraphforthederivatives.Anysubsetofthegraphmaythenbeevaluatedusingspeciﬁcnumericalvaluesatalatertime.Thisallowsustoavoidspecifyingexactlywheneachoperationshouldbecomputed.Instead,agenericgraphevaluationenginecanevaluateeverynodeassoonasitsparents’valuesareavailable.Thedescriptionofthesymbol-to-symbolbasedapproachsubsumesthesymbol-to-numberapproach.Thesymbol-to-numberapproachcanbeunderstoodasperformingexactlythesamecomputationsasaredoneinthegraphbuiltbythesymbol-to-symbolapproach.Thekeydiﬀerenceisthatthesymbol-to-numberapproachdoesnotexposethegraph.6.5.6GeneralBack-PropagationTheback-propagationalgorithmisverysimple.Tocomputethegradientofsomescalarzwithrespecttooneofitsancestorsxinthegraph,webeginbyobservingthatthegradientwithrespecttozisgivenbydzdz=1.WecanthencomputethegradientwithrespecttoeachparentofzinthegraphbymultiplyingthecurrentgradientbytheJacobianoftheoperationthatproducedz.WecontinuemultiplyingbyJacobianstravelingbackwardsthroughthegraphinthiswayuntilwereachx.Foranynodethatmaybereachedbygoingbackwardsfromzthroughtwoormorepaths,wesimplysumthegradientsarrivingfromdiﬀerentpathsatthatnode.Moreformally,eachnodeinthegraphGcorrespondstoavariable.Toachievemaximumgenerality,wedescribethisvariableasbeingatensorV. Tensorcaningeneralhaveanynumberofdimensions. Theysubsumescalars,vectors,andmatrices.Weassumethateachvariableisassociatedwiththefollowingsubroutines:V•getoperation_(V):ThisreturnstheoperationthatcomputesV,repre-sentedbytheedgescomingintoVinthecomputationalgraph.Forexample,theremaybeaPythonorC++classrepresentingthematrixmultiplicationoperation,andtheget_operationfunction.Supposewehaveavariablethatiscreatedbymatrixmultiplication,C=AB.Thengetoperation_(V)returnsapointertoaninstanceofthecorrespondingC++class.•getconsumers_(V,G):ThisreturnsthelistofvariablesthatarechildrenofVinthecomputationalgraph.G•Ggetinputs_(V,):ThisreturnsthelistofvariablesthatareparentsofVinthecomputationalgraph.G215 CHAPTER6.DEEPFEEDFORWARDNETWORKSEachoperationopisalsoassociatedwithabpropoperation.ThisbpropoperationcancomputeaJacobian-vectorproductasdescribedbyequation.6.47Thisishowtheback-propagationalgorithmisabletoachievegreatgenerality.Eachoperationisresponsibleforknowinghowtoback-propagatethroughtheedgesinthegraphthatitparticipatesin.Forexample,wemightuseamatrixmultiplicationoperationtocreateavariableC=AB.SupposethatthegradientofascalarzwithrespecttoCisgivenbyG.Thematrixmultiplicationoperationisresponsiblefordeﬁningtwoback-propagationrules,oneforeachofitsinputarguments.IfwecallthebpropmethodtorequestthegradientwithrespecttoAgiventhatthegradientontheoutputisG,thenthebpropmethodofthematrixmultiplicationoperationmuststatethatthegradientwithrespecttoAisgivenbyGB.Likewise,ifwecallthebpropmethodtorequestthegradientwithrespecttoB,thenthematrixoperationisresponsibleforimplementingthebpropmethodandspecifyingthatthedesiredgradientisgivenbyAG.Theback-propagationalgorithmitselfdoesnotneedtoknowanydiﬀerentiationrules.Itonlyneedstocalleachoperation’sbpropruleswiththerightarguments.Formally,opbpropinputs.(,,XG)mustreturni(∇Xopfinputs.()i)Gi,(6.54)whichisjustanimplementationofthechainruleasexpressedinequation.6.47Here,inputsisalistofinputsthataresuppliedtotheoperation,op.fisthemathematicalfunctionthattheoperationimplements,Xistheinputwhosegradientwewishtocompute,andisthegradientontheoutputoftheoperation.GTheop.bpropmethodshouldalwayspretendthatallofitsinputsaredistinctfromeachother,eveniftheyarenot.Forexample,ifthemuloperatorispassedtwocopiesofxtocomputex2,theop.bpropmethodshouldstillreturnxasthederivativewithrespecttobothinputs.Theback-propagationalgorithmwilllateraddbothoftheseargumentstogethertoobtain2x,whichisthecorrecttotalderivativeon.xSoftwareimplementationsofback-propagationusuallyprovideboththeopera-tionsandtheirbpropmethods,sothatusersofdeeplearningsoftwarelibrariesareabletoback-propagatethroughgraphsbuiltusingcommonoperationslikematrixmultiplication,exponents,logarithms,andsoon.Softwareengineerswhobuildanewimplementationofback-propagationoradvanceduserswhoneedtoaddtheirownoperationtoanexistinglibrarymustusuallyderivetheop.bpropmethodforanynewoperationsmanually.Theback-propagationalgorithmisformallydescribedinalgorithm.6.5216 CHAPTER6.DEEPFEEDFORWARDNETWORKSAlgorithm6.5Theoutermostskeletonoftheback-propagationalgorithm.Thisportiondoessimplesetupandcleanupwork.Mostoftheimportantworkhappensinthesubroutineofalgorithmbuild_grad6.6.Require:T,thetargetsetofvariableswhosegradientsmustbecomputed.Require:G,thecomputationalgraphRequire:z,thevariabletobediﬀerentiatedLetGbeGprunedtocontainonlynodesthatareancestorsofzanddescendentsofnodesin.TInitialize,adatastructureassociatingtensorstotheirgradientsgrad_tablegradtable_[] 1z←fordoVinTbuildgrad_(V,,GG,gradtable_)endforReturnrestrictedtograd_tableTInsection,weexplainedthatback-propagationwasdevelopedinorderto6.5.2avoidcomputingthesamesubexpressioninthechainrulemultipletimes.Thenaivealgorithmcouldhaveexponentialruntimeduetotheserepeatedsubexpressions.Nowthatwehavespeciﬁedtheback-propagationalgorithm,wecanunderstanditscomputationalcost.Ifweassumethateachoperationevaluationhasroughlythesamecost,thenwemayanalyzethecomputationalcostintermsofthenumberofoperationsexecuted.Keepinmindherethatwerefertoanoperationasthefundamentalunitofourcomputationalgraph,whichmightactuallyconsistofverymanyarithmeticoperations(forexample,wemighthaveagraphthattreatsmatrixmultiplicationasasingleoperation).ComputingagradientinagraphwithnnodeswillneverexecutemorethanO(n2)operationsorstoretheoutputofmorethanO(n2) operations.Herewearecountingoperationsinthecomputationalgraph,notindividualoperationsexecutedbytheunderlyinghardware,soitisimportanttorememberthattheruntimeofeachoperationmaybehighlyvariable.Forexample,multiplyingtwomatricesthateachcontainmillionsofentriesmightcorrespondtoasingleoperationinthegraph.WecanseethatcomputingthegradientrequiresasmostO(n2) operationsbecausetheforwardpropagationstagewillatworstexecuteallnnodesintheoriginalgraph(dependingonwhichvalueswewanttocompute,wemaynotneedtoexecutetheentiregraph).Theback-propagationalgorithmaddsoneJacobian-vectorproduct,whichshouldbeexpressedwithO(1)nodes,peredgeintheoriginalgraph.BecausethecomputationalgraphisadirectedacyclicgraphithasatmostO(n2)edges.Forthekindsofgraphsthatarecommonlyusedinpractice,thesituationisevenbetter.Mostneuralnetworkcostfunctionsare217 CHAPTER6.DEEPFEEDFORWARDNETWORKSAlgorithm6.6Theinnerloopsubroutinebuildgrad_(V,,GG,gradtable_)oftheback-propagationalgorithm,calledbytheback-propagationalgorithmdeﬁnedinalgorithm.6.5Require:V,thevariablewhosegradientshouldbeaddedtoand.Ggrad_tableRequire:G,thegraphtomodify.Require:G,therestrictionoftonodesthatparticipateinthegradient.GRequire:grad_table,adatastructuremappingnodestotheirgradientsifthenVisingrad_tableReturn_gradtable[]Vendifi←1forCVin_getconsumers(,G)doopgetoperation←_()CDC←buildgrad_(,,GG,gradtable_)G()i←Gopbpropgetinputs.(_(C,)),,VDii←+1endforG←iG()igradtable_[] = VGInsertandtheoperationscreatingitintoGGReturnGroughlychain-structured,causingback-propagationtohaveO(n)cost.Thisisfarbetterthanthenaiveapproach,whichmightneedtoexecuteexponentiallymanynodes.Thispotentiallyexponentialcostcanbeseenbyexpandingandrewritingtherecursivechainrule(equation)non-recursively:6.49∂u()n∂u()j=path(u(π1),u(π2),...,u(πt)),fromπ1=tojπt=ntk=2∂u(πk)∂u(πk−1).(6.55)Sincethenumberofpathsfromnodejtonodencangrowexponentiallyinthelengthofthesepaths,thenumberoftermsintheabovesum,whichisthenumberofsuchpaths,cangrowexponentiallywiththedepthoftheforwardpropagationgraph.Thislargecostwouldbeincurredbecausethesamecomputationfor∂u()i∂u()jwouldberedonemanytimes. Toavoidsuchrecomputation,wecanthinkofback-propagationasatable-ﬁllingalgorithmthattakesadvantageofstoringintermediateresults∂u()n∂u()i.Eachnodeinthegraphhasacorrespondingslotinatabletostorethegradientforthatnode.Byﬁllinginthesetableentriesinorder,218 CHAPTER6.DEEPFEEDFORWARDNETWORKSback-propagationavoidsrepeatingmanycommonsubexpressions.Thistable-ﬁllingstrategyissometimescalled.dynamicprogramming6.5.7Example:Back-PropagationforMLPTrainingAsanexample,wewalkthroughtheback-propagationalgorithmasitisusedtotrainamultilayerperceptron.Herewedevelopaverysimplemultilayerperceptionwithasinglehiddenlayer.Totrainthismodel,wewilluseminibatchstochasticgradientdescent.Theback-propagationalgorithmisusedtocomputethegradientofthecostonasingleminibatch.Speciﬁcally,weuseaminibatchofexamplesfromthetrainingsetformattedasadesignmatrixXandavectorofassociatedclasslabelsy.ThenetworkcomputesalayerofhiddenfeaturesH=max{0,XW(1)}.Tosimplifythepresentationwedonotusebiasesinthismodel.Weassumethatourgraphlanguageincludesareluoperationthatcancomputemax{0,Z}element-wise.ThepredictionsoftheunnormalizedlogprobabilitiesoverclassesarethengivenbyHW(2).Weassumethatourgraphlanguageincludesacross_entropyoperationthatcomputesthecross-entropybetweenthetargetsyandtheprobabilitydistributiondeﬁnedbytheseunnormalizedlogprobabilities.Theresultingcross-entropydeﬁnesthecostJMLE.Minimizingthiscross-entropyperformsmaximumlikelihoodestimationoftheclassiﬁer.However,tomakethisexamplemorerealistic,wealsoincludearegularizationterm.ThetotalcostJJ= MLE+λi,jW(1)i,j2+i,jW(2)i,j2(6.56)consistsofthecross-entropyandaweightdecaytermwithcoeﬃcientλ.Thecomputationalgraphisillustratedinﬁgure.6.11Thecomputationalgraphforthegradientofthisexampleislargeenoughthatitwouldbetedioustodrawortoread.Thisdemonstratesoneofthebeneﬁtsoftheback-propagationalgorithm,whichisthatitcanautomaticallygenerategradientsthatwouldbestraightforwardbuttediousforasoftwareengineertoderivemanually.Wecanroughlytraceoutthebehavioroftheback-propagationalgorithmbylookingattheforwardpropagationgraphinﬁgure.Totrain,wewish6.11tocomputeboth∇W(1)Jand∇W(2)J.TherearetwodiﬀerentpathsleadingbackwardfromJtotheweights:onethroughthecross-entropycost,andonethroughtheweightdecaycost.Theweightdecaycostisrelativelysimple;itwillalwayscontribute2λW()itothegradientonW()i.219 CHAPTER6.DEEPFEEDFORWARDNETWORKS X XW(1)W(1)U(1)U(1)matmulH Hrelu U(3)U(3)sqru(4)u(4)sumλ λu(7)u(7)W(2)W(2)U(2)U(2)matmuly yJMLEJMLEcross_entropy U(5)U(5)sqru(6)u(6)sumu(8)u(8)J J+×+ Figure6.11:Thecomputationalgraphusedtocomputethecostusedtotrainourexampleofasingle-layerMLPusingthecross-entropylossandweightdecay.Theotherpaththroughthecross-entropycostisslightlymorecomplicated.LetGbethegradientontheunnormalizedlogprobabilitiesU(2)providedbythecross_entropyoperation.Theback-propagationalgorithmnowneedstoexploretwodiﬀerentbranches.Ontheshorterbranch,itaddsHGtothegradientonW(2),usingtheback-propagationruleforthesecondargumenttothematrixmultiplicationoperation.Theotherbranchcorrespondstothelongerchaindescendingfurtheralongthenetwork.First,theback-propagationalgorithmcomputes∇HJ=GW(2)usingtheback-propagationrulefortheﬁrstargumenttothematrixmultiplicationoperation.Next,thereluoperationusesitsback-propagationruletozerooutcomponentsofthegradientcorrespondingtoentriesofU(1)thatwerelessthan.Lettheresultbecalled0G.Thelaststepoftheback-propagationalgorithmistousetheback-propagationruleforthesecondargumentoftheoperationtoaddmatmulXGtothegradientonW(1).Afterthesegradientshavebeencomputed,itistheresponsibilityofthegradientdescentalgorithm,oranotheroptimizationalgorithm,tousethesegradientstoupdatetheparameters.FortheMLP,thecomputationalcostisdominatedbythecostofmatrixmultiplication.Duringtheforwardpropagationstage,wemultiplybyeachweight220 CHAPTER6.DEEPFEEDFORWARDNETWORKSmatrix,resultinginO(w) multiply-adds,wherewisthenumberofweights.Duringthebackwardpropagationstage,wemultiplybythetransposeofeachweightmatrix,whichhasthesamecomputationalcost.Themainmemorycostofthealgorithmisthatweneedtostoretheinputtothenonlinearityofthehiddenlayer.Thisvalueisstoredfromthetimeitiscomputeduntilthebackwardpasshasreturnedtothesamepoint.ThememorycostisthusO(mnh),wheremisthenumberofexamplesintheminibatchandnhisthenumberofhiddenunits.6.5.8ComplicationsOurdescriptionoftheback-propagationalgorithmhereissimplerthantheimple-mentationsactuallyusedinpractice.Asnotedabove,wehaverestrictedthedeﬁnitionofanoperationtobeafunctionthatreturnsasingletensor.Mostsoftwareimplementationsneedtosupportoperationsthatcanreturnmorethanonetensor.Forexample,ifwewishtocomputeboththemaximumvalueinatensorandtheindexofthatvalue,itisbesttocomputebothinasinglepassthroughmemory,soitismosteﬃcienttoimplementthisprocedureasasingleoperationwithtwooutputs.We havenot described how tocontrolthememoryconsumption ofback-propagation.Back-propagationofteninvolvessummationofmanytensorstogether.Inthenaiveapproach,eachofthesetensorswouldbecomputedseparately,thenallofthemwouldbeaddedinasecondstep.Thenaiveapproachhasanoverlyhighmemorybottleneckthatcanbeavoidedbymaintainingasinglebuﬀerandaddingeachvaluetothatbuﬀerasitiscomputed.Real-worldimplementationsofback-propagationalsoneedtohandlevariousdatatypes,suchas32-bitﬂoatingpoint,64-bitﬂoatingpoint,andintegervalues.Thepolicyforhandlingeachofthesetypestakesspecialcaretodesign.Someoperationshaveundeﬁnedgradients,anditisimportanttotrackthesecasesanddeterminewhetherthegradientrequestedbytheuserisundeﬁned.Variousothertechnicalitiesmakereal-worlddiﬀerentiationmorecomplicated.Thesetechnicalitiesarenotinsurmountable,andthischapterhasdescribedthekeyintellectualtoolsneededtocomputederivatives,butitisimportanttobeawarethatmanymoresubtletiesexist.6.5.9DiﬀerentiationoutsidetheDeepLearningCommunityThe deeplearning communityhas beensomewhat isolatedfrom the broadercomputersciencecommunityandhaslargelydevelopeditsownculturalattitudes221 CHAPTER6.DEEPFEEDFORWARDNETWORKSconcerninghowtoperformdiﬀerentiation.Moregenerally,theﬁeldofautomaticdiﬀerentiationisconcernedwithhowtocomputederivativesalgorithmically.Theback-propagationalgorithmdescribedhereisonlyoneapproachtoautomaticdiﬀerentiation.Itisaspecialcaseofabroaderclassoftechniquescalledreversemodeaccumulation.Otherapproachesevaluatethesubexpressionsofthechainruleindiﬀerentorders.Ingeneral, determiningtheorderofevaluationthatresultsinthelowestcomputationalcostisadiﬃcultproblem.FindingtheoptimalsequenceofoperationstocomputethegradientisNP-complete(,),Naumann2008inthesensethatitmayrequiresimplifyingalgebraicexpressionsintotheirleastexpensiveform.Forexample,supposewehavevariablesp1,p2,...,pnrepresentingprobabilitiesandvariablesz1,z2,...,znrepresentingunnormalizedlogprobabilities.Supposewedeﬁneqi=exp(zi)iexp(zi),(6.57)wherewebuildthesoftmaxfunctionoutofexponentiation,summationanddivisionoperations, and construct across-entropylossJ=−ipilogqi.AhumanmathematiciancanobservethatthederivativeofJwithrespecttozitakesaverysimpleform:qi−pi.Theback-propagationalgorithmisnotcapableofsimplifyingthegradientthisway,andwillinsteadexplicitlypropagategradientsthroughallofthelogarithmandexponentiationoperationsintheoriginalgraph.SomesoftwarelibrariessuchasTheano(,;,)areabletoBergstraetal.2010Bastienetal.2012performsomekindsofalgebraicsubstitutiontoimproveoverthegraphproposedbythepureback-propagationalgorithm.WhentheforwardgraphGhasasingleoutputnodeandeachpartialderivative∂u()i∂u()jcanbecomputedwithaconstantamountofcomputation,back-propagationguaranteesthatthenumberofcomputationsforthegradientcomputationisofthesameorderasthenumberofcomputationsfortheforwardcomputation:thiscanbeseeninalgorithmbecauseeachlocalpartialderivative6.2∂u()i∂u()jneedstobecomputedonlyoncealongwithanassociatedmultiplicationandadditionfortherecursivechain-ruleformulation(equation).Theoverallcomputationis6.49thereforeO(#edges).However,itcanpotentiallybereducedbysimplifyingthecomputationalgraphconstructedbyback-propagation,andthisisanNP-completetask. ImplementationssuchasTheanoandTensorFlowuseheuristicsbasedonmatchingknownsimpliﬁcationpatternsinordertoiterativelyattempttosimplifythegraph.Wedeﬁnedback-propagationonlyforthecomputationofagradientofascalaroutputbutback-propagationcanbeextendedtocomputeaJacobian(eitherofkdiﬀerentscalarnodesinthegraph,orofatensor-valuednodecontainingkvalues).Anaiveimplementationmaythenneedktimesmorecomputation:for222 CHAPTER6.DEEPFEEDFORWARDNETWORKSeachscalarinternalnodeintheoriginalforwardgraph,thenaiveimplementationcomputeskgradientsinsteadofasinglegradient.Whenthenumberofoutputsofthegraphislargerthanthenumberofinputs,itissometimespreferabletouseanotherformofautomaticdiﬀerentiationcalledforwardmodeaccumulation.Forwardmodecomputationhasbeenproposedforobtainingreal-timecomputationofgradientsinrecurrentnetworks,forexample(,).ThisWilliamsandZipser1989alsoavoidstheneedtostorethevaluesandgradientsforthewholegraph,tradingoﬀcomputationaleﬃciencyformemory.Therelationshipbetweenforwardmodeandbackwardmodeisanalogoustotherelationshipbetweenleft-multiplyingversusright-multiplyingasequenceofmatrices,suchasABCD,(6.58)wherethematricescanbethoughtofasJacobianmatrices.Forexample,ifDisacolumnvectorwhileAhasmanyrows,thiscorrespondstoagraphwithasingleoutputandmanyinputs,andstartingthemultiplicationsfromtheendandgoingbackwardsonlyrequiresmatrix-vectorproducts.Thiscorrespondstothebackwardmode.Instead,startingtomultiplyfromtheleftwouldinvolveaseriesofmatrix-matrixproducts,whichmakesthewholecomputationmuchmoreexpensive.However,ifAhasfewerrowsthanDhascolumns,itischeapertorunthemultiplicationsleft-to-right,correspondingtotheforwardmode.Inmanycommunitiesoutsideofmachinelearning,itismorecommontoim-plementdiﬀerentiationsoftwarethatactsdirectlyontraditionalprogramminglanguagecode,suchasPythonorCcode,andautomaticallygeneratesprogramsthatdiﬀerentiatefunctionswrittenintheselanguages.Inthedeeplearningcom-munity,computationalgraphsareusuallyrepresentedbyexplicitdatastructurescreatedbyspecializedlibraries.Thespecializedapproachhasthedrawbackofrequiringthelibrarydevelopertodeﬁnethebpropmethodsforeveryoperationandlimitingtheuserofthelibrarytoonlythoseoperationsthathavebeendeﬁned.However,thespecializedapproachalsohasthebeneﬁtofallowingcustomizedback-propagationrulestobedevelopedforeachoperation,allowingthedevelopertoimprovespeedorstabilityinnon-obviouswaysthatanautomaticprocedurewouldpresumablybeunabletoreplicate.Back-propagationisthereforenottheonlywayortheoptimalwayofcomputingthegradient,butitisaverypracticalmethodthatcontinuestoservethedeeplearningcommunityverywell.Inthefuture,diﬀerentiationtechnologyfordeepnetworksmayimproveasdeeplearningpractitionersbecomemoreawareofadvancesinthebroaderﬁeldofautomaticdiﬀerentiation.223 CHAPTER6.DEEPFEEDFORWARDNETWORKS6.5.10Higher-OrderDerivativesSomesoftwareframeworkssupporttheuseofhigher-orderderivatives.Amongthedeeplearningsoftwareframeworks,thisincludesatleastTheanoandTensorFlow.Theselibrariesusethesamekindofdatastructuretodescribetheexpressionsforderivativesastheyusetodescribetheoriginalfunctionbeingdiﬀerentiated.Thismeansthatthesymbolicdiﬀerentiationmachinerycanbeappliedtoderivatives.Inthecontextofdeeplearning,itisraretocomputeasinglesecondderivativeofascalarfunction.Instead,weareusuallyinterestedinpropertiesoftheHessianmatrix.Ifwehaveafunctionf:Rn→R,thentheHessianmatrixisofsizenn×.Intypicaldeeplearningapplications,nwillbethenumberofparametersinthemodel,whichcouldeasilynumberinthebillions.TheentireHessianmatrixisthusinfeasibletoevenrepresent.InsteadofexplicitlycomputingtheHessian,thetypicaldeeplearningapproachistouseKrylovmethods.Krylovmethodsareasetofiterativetechniquesforperformingvariousoperationslikeapproximatelyinvertingamatrixorﬁndingapproximationstoitseigenvectorsoreigenvalues,withoutusinganyoperationotherthanmatrix-vectorproducts.InordertouseKrylovmethodsontheHessian,weonlyneedtobeabletocomputetheproductbetweentheHessianmatrixHandanarbitraryvectorv.Astraightforwardtechnique(,)fordoingsoistocomputeChristianson1992Hv= ∇x(∇xfx())v.(6.59)Bothofthegradientcomputationsinthisexpressionmaybecomputedautomati-callybytheappropriatesoftwarelibrary.Notethattheoutergradientexpressiontakesthegradientofafunctionoftheinnergradientexpression.Ifvisitselfavectorproducedbyacomputationalgraph,itisimportanttospecifythattheautomaticdiﬀerentiationsoftwareshouldnotdiﬀerentiatethroughthegraphthatproduced.vWhilecomputingtheHessianisusuallynotadvisable,itispossibletodowithHessianvectorproducts.OnesimplycomputesHe()iforalli= 1,...,n,wheree()iistheone-hotvectorwithe()ii= 1andallotherentriesequalto0.6.6HistoricalNotesFeedforwardnetworkscanbeseenaseﬃcientnonlinearfunctionapproximatorsbasedonusinggradientdescenttominimizetheerrorinafunctionapproximation.224 CHAPTER6.DEEPFEEDFORWARDNETWORKSFromthispointofview,themodernfeedforwardnetworkistheculminationofcenturiesofprogressonthegeneralfunctionapproximationtask.Thechainrulethatunderliestheback-propagationalgorithmwasinventedinthe17thcentury(,;,).CalculusandalgebrahaveLeibniz1676L’Hôpital1696longbeenusedtosolveoptimizationproblemsinclosedform,butgradientdescentwasnotintroducedasatechniqueforiterativelyapproximatingthesolutiontooptimizationproblemsuntilthe19thcentury(Cauchy1847,).Beginninginthe1940s,thesefunctionapproximationtechniqueswereusedtomotivatemachinelearningmodelssuchastheperceptron.However,theearliestmodelswerebasedonlinearmodels.CriticsincludingMarvinMinskypointedoutseveraloftheﬂawsofthelinearmodelfamily,suchasitsinabilitytolearntheXORfunction,whichledtoabacklashagainsttheentireneuralnetworkapproach.Learningnonlinearfunctionsrequiredthedevelopmentofamultilayerper-ceptronandameansofcomputingthegradientthroughsuchamodel.Eﬃcientapplicationsofthechainrulebasedondynamicprogrammingbegantoappearinthe1960sand1970s,mostlyforcontrolapplications(,;Kelley1960BrysonandDenham1961Dreyfus1962BrysonandHo1969Dreyfus1973,;,;,;,)butalsoforsensitivityanalysis(,).Linnainmaa1976Werbos1981()proposedapplyingthesetechniquestotrainingartiﬁcialneuralnetworks.Theideawasﬁnallydevelopedinpracticeafterbeingindependentlyrediscoveredindiﬀerentways(,;LeCun1985Parker1985Rumelhart1986a,;etal.,).ThebookParallelDistributedPro-cessingpresentedtheresultsofsomeoftheﬁrstsuccessfulexperimentswithback-propagationinachapter(,)thatcontributedgreatlyRumelhartetal.1986btothepopularizationofback-propagationandinitiatedaveryactiveperiodofresearchinmulti-layerneuralnetworks. However,theideasputforwardbytheauthorsofthatbookandinparticularbyRumelhartandHintongomuchbeyondback-propagation. Theyincludecrucialideasaboutthepossiblecomputationalimplementationofseveralcentralaspectsofcognitionandlearning,whichcameunderthenameof“connectionism”becauseoftheimportancethisschoolofthoughtplacesontheconnectionsbetweenneuronsasthelocusoflearningandmemory.Inparticular,theseideasincludethenotionofdistributedrepresentation(Hintonetal.,).1986Followingthesuccessofback-propagation,neuralnetworkresearchgainedpop-ularityandreachedapeakintheearly1990s.Afterwards,othermachinelearningtechniquesbecamemorepopularuntilthemoderndeeplearningrenaissancethatbeganin2006.Thecoreideasbehindmodernfeedforwardnetworkshavenotchangedsub-stantiallysincethe1980s. Thesameback-propagationalgorithmandthesame225 CHAPTER6.DEEPFEEDFORWARDNETWORKSapproachestogradientdescentarestillinuse.Mostoftheimprovementinneuralnetworkperformancefrom1986to2015canbeattributedtotwofactors.First,largerdatasetshavereducedthedegreetowhichstatisticalgeneralizationisachallengeforneuralnetworks.Second,neuralnetworkshavebecomemuchlarger,duetomorepowerfulcomputers,andbettersoftwareinfrastructure.However,asmallnumberofalgorithmicchangeshaveimprovedtheperformanceofneuralnetworksnoticeably.Oneofthesealgorithmicchangeswasthereplacementofmeansquarederrorwiththecross-entropyfamilyoflossfunctions.Meansquarederrorwaspopularinthe1980sand1990s,butwasgraduallyreplacedbycross-entropylossesandtheprincipleofmaximumlikelihoodasideasspreadbetweenthestatisticscommunityandthemachinelearningcommunity.Theuseofcross-entropylossesgreatlyimprovedtheperformanceofmodelswithsigmoidandsoftmaxoutputs,whichhadpreviouslysuﬀeredfromsaturationandslowlearningwhenusingthemeansquarederrorloss.Theothermajoralgorithmicchangethathasgreatlyimprovedtheperformanceoffeedforwardnetworkswasthereplacementofsigmoidhiddenunitswithpiecewiselinearhiddenunits,suchasrectiﬁedlinearunits.Rectiﬁcationusingthemax{0,z}functionwasintroducedinearlyneuralnetworkmodelsanddatesbackatleastasfarastheCognitronandNeocognitron(Fukushima19751980,,).Theseearlymodelsdid notuserectiﬁed linearunits, but insteadappliedrectiﬁcation tononlinearfunctions.Despitetheearlypopularityofrectiﬁcation,rectiﬁcationwaslargelyreplacedbysigmoidsinthe1980s,perhapsbecausesigmoidsperformbetterwhenneuralnetworksareverysmall.Asoftheearly2000s,rectiﬁedlinearunitswereavoidedduetoasomewhatsuperstitiousbeliefthatactivationfunctionswithnon-diﬀerentiablepointsmustbeavoided.Thisbegantochangeinabout2009.Jarrett2009etal.()observedthat“usingarectifyingnonlinearityisthesinglemostimportantfactorinimprovingtheperformanceofarecognitionsystem”amongseveraldiﬀerentfactorsofneuralnetworkarchitecturedesign.Forsmalldatasets,()observedthatusingrectifyingnon-Jarrettetal.2009linearitiesisevenmoreimportantthanlearningtheweightsofthehiddenlayers.Randomweightsaresuﬃcienttopropagateusefulinformationthrougharectiﬁedlinearnetwork,allowingtheclassiﬁerlayeratthetoptolearnhowtomapdiﬀerentfeaturevectorstoclassidentities.Whenmoredataisavailable,learningbeginstoextractenoughusefulknowledgetoexceedtheperformanceofrandomlychosenparameters.()Glorotetal.2011ashowedthatlearningisfareasierindeeprectiﬁedlinearnetworksthanindeepnetworksthathavecurvatureortwo-sidedsaturationintheiractivationfunctions.226 CHAPTER6.DEEPFEEDFORWARDNETWORKSRectiﬁedlinearunitsarealsoofhistoricalinterestbecausetheyshowthatneurosciencehascontinuedtohave aninﬂuenceonthe developmentofdeeplearningalgorithms.()motivaterectiﬁedlinearunitsfromGlorotetal.2011abiologicalconsiderations.Thehalf-rectifyingnonlinearitywasintendedtocapturethesepropertiesofbiologicalneurons:1)Forsomeinputs,biologicalneuronsarecompletelyinactive.2)Forsomeinputs,abiologicalneuron’soutputisproportionaltoitsinput.3)Mostofthetime,biologicalneuronsoperateintheregimewheretheyareinactive(i.e.,theyshouldhavesparseactivations).Whenthemodernresurgenceofdeeplearningbeganin2006,feedforwardnetworkscontinuedtohaveabadreputation.Fromabout2006-2012,itwaswidelybelievedthatfeedforwardnetworkswouldnotperformwellunlesstheywereassistedbyothermodels,suchasprobabilisticmodels.Today,itisnowknownthatwiththerightresourcesandengineeringpractices,feedforwardnetworksperformverywell.Today,gradient-basedlearninginfeedforwardnetworksisusedasatooltodevelopprobabilisticmodels,suchasthevariationalautoencoderandgenerativeadversarialnetworks,describedinchapter.Ratherthanbeingviewedasanunreliable20technologythatmustbesupportedbyothertechniques,gradient-basedlearninginfeedforwardnetworkshasbeenviewedsince2012asapowerfultechnologythatmaybeappliedtomanyothermachinelearningtasks.In2006,thecommunityusedunsupervisedlearningtosupportsupervisedlearning,andnow,ironically,itismorecommontousesupervisedlearningtosupportunsupervisedlearning.Feedforwardnetworkscontinuetohaveunfulﬁlledpotential.Inthefuture,weexpecttheywillbeappliedtomanymoretasks,andthatadvancesinoptimizationalgorithmsandmodeldesignwillimprovetheirperformanceevenfurther.Thischapterhasprimarilydescribedtheneuralnetworkfamilyofmodels.Inthesubsequentchapters,weturntohowtousethesemodels—howtoregularizeandtrainthem. 227 Chapter7RegularizationforDeepLearningAcentralprobleminmachinelearningishowtomakeanalgorithmthatwillperformwellnotjustonthetrainingdata,butalsoonnewinputs.Manystrategiesusedinmachinelearningareexplicitlydesignedtoreducethetesterror,possiblyattheexpenseofincreasedtrainingerror.Thesestrategiesareknowncollectivelyasregularization. Aswewillseethereareagreatmanyformsofregularizationavailabletothedeeplearningpractitioner.Infact, developingmoreeﬀectiveregularizationstrategieshasbeenoneofthemajorresearcheﬀortsintheﬁeld.Chapterintroducedthebasicconceptsofgeneralization,underﬁtting,overﬁt-5ting,bias,varianceandregularization.Ifyouarenotalreadyfamiliarwiththesenotions,pleaserefertothatchapterbeforecontinuingwiththisone.Inthischapter,wedescriberegularizationinmoredetail,focusingonregular-izationstrategiesfordeepmodelsormodelsthatmaybeusedasbuildingblockstoformdeepmodels.Somesectionsofthischapterdealwithstandardconceptsinmachinelearning.Ifyouarealreadyfamiliarwiththeseconcepts, feelfreetoskiptherelevantsections.However,mostofthischapterisconcernedwiththeextensionofthesebasicconceptstotheparticularcaseofneuralnetworks.Insection,wedeﬁnedregularizationas“anymodiﬁcationwemaketo5.2.2alearningalgorithmthatisintendedtoreduceitsgeneralizationerrorbutnotitstrainingerror.”Therearemanyregularizationstrategies.Someputextraconstraints ona machine learning model,such asadding restrictionson theparametervalues.Someaddextratermsintheobjectivefunctionthatcanbethoughtofascorrespondingtoasoftconstraintontheparametervalues.Ifchosencarefully,theseextraconstraintsandpenaltiescanleadtoimprovedperformance228 CHAPTER7.REGULARIZATIONFORDEEPLEARNINGonthetestset.Sometimestheseconstraintsandpenaltiesaredesignedtoencodespeciﬁckindsofpriorknowledge.Othertimes,theseconstraintsandpenaltiesaredesignedtoexpressagenericpreferenceforasimplermodelclassinordertopromotegeneralization.Sometimespenaltiesandconstraintsarenecessarytomakeanunderdeterminedproblemdetermined.Otherformsofregularization,knownasensemblemethods,combinemultiplehypothesesthatexplainthetrainingdata.Inthecontextofdeeplearning,mostregularizationstrategiesarebasedonregularizingestimators.Regularizationofanestimatorworksbytradingincreasedbiasforreducedvariance.Aneﬀectiveregularizerisonethatmakesaproﬁtabletrade,reducingvariancesigniﬁcantlywhilenotoverlyincreasingthebias.Whenwediscussedgeneralizationandoverﬁttinginchapter,wefocusedonthreesituations,5wherethemodelfamilybeingtrainedeither(1)excludedthetruedatageneratingprocess—correspondingtounderﬁttingandinducingbias,or(2)matchedthetruedatageneratingprocess,or(3)includedthegeneratingprocessbutalsomanyotherpossiblegeneratingprocesses—theoverﬁttingregimewherevarianceratherthanbiasdominatestheestimationerror.Thegoalofregularizationistotakeamodelfromthethirdregimeintothesecondregime.Inpractice,anoverlycomplexmodelfamilydoesnotnecessarilyincludethetargetfunctionorthetruedatageneratingprocess,orevenacloseapproximationofeither.Wealmostneverhaveaccesstothetruedatageneratingprocesssowecanneverknowforsureifthemodelfamilybeingestimatedincludesthegeneratingprocessornot.However,mostapplicationsofdeeplearningalgorithmsaretodomainswherethetruedatageneratingprocessisalmostcertainlyoutsidethemodelfamily.Deeplearningalgorithmsaretypicallyappliedtoextremelycomplicateddomainssuchasimages,audiosequencesandtext,forwhichthetruegenerationprocessessentiallyinvolvessimulatingtheentireuniverse.Tosomeextent,wearealwaystryingtoﬁtasquarepeg(thedatageneratingprocess)intoaroundhole(ourmodelfamily).Whatthismeansisthatcontrollingthecomplexityofthemodelisnotasimplematterofﬁndingthemodeloftherightsize,withtherightnumberofparameters.Instead,wemightﬁnd—andindeedinpracticaldeeplearningscenarios,wealmostalwaysdoﬁnd—thatthebestﬁttingmodel(inthesenseofminimizinggeneralizationerror)isalargemodelthathasbeenregularizedappropriately.Wenowreviewseveralstrategiesforhowtocreatesuchalarge,deep,regularizedmodel.229 CHAPTER7.REGULARIZATIONFORDEEPLEARNING7.1ParameterNormPenaltiesRegularizationhasbeenusedfordecadespriortotheadventofdeeplearning.Linearmodelssuchaslinearregressionandlogisticregressionallowsimple,straightforward,andeﬀectiveregularizationstrategies.Manyregularizationapproachesarebasedonlimitingthecapacityofmodels,suchasneuralnetworks,linearregression,orlogisticregression,byaddingapa-rameternormpenaltyΩ(θ)totheobjectivefunctionJ.Wedenotetheregularizedobjectivefunctionby˜J:˜J,J,α(;θXy) = (;θXy)+Ω()θ(7.1)whereα∈[0,∞)isahyperparameterthatweightstherelativecontributionofthenormpenaltyterm,,relativetothestandardobjectivefunctionΩJ.Settingαto0resultsinnoregularization.Largervaluesofαcorrespondtomoreregularization.Whenourtrainingalgorithmminimizestheregularizedobjectivefunction˜JitwilldecreaseboththeoriginalobjectiveJonthetrainingdataandsomemeasureofthesizeoftheparametersθ(orsomesubsetoftheparameters).Diﬀerentchoicesfortheparameternormcanresultindiﬀerentsolutionsbeingpreferred.ΩInthissection,wediscusstheeﬀectsofthevariousnormswhenusedaspenaltiesonthemodelparameters.Beforedelvingintotheregularizationbehaviorofdiﬀerentnorms,wenotethatforneuralnetworks,wetypicallychoosetouseaparameternormpenaltythatΩpenalizesoftheaﬃnetransformationateachlayerandleavesonlytheweightsthebiasesunregularized.Thebiasestypicallyrequirelessdatatoﬁtaccuratelythantheweights. Eachweightspeciﬁeshowtwovariablesinteract. Fittingtheweightwellrequiresobservingbothvariablesinavarietyofconditions.Eachbiascontrolsonlyasinglevariable.Thismeansthatwedonotinducetoomuchvariancebyleavingthebiasesunregularized.Also,regularizingthebiasparameterscanintroduceasigniﬁcantamountofunderﬁtting.Wethereforeusethevectorwtoindicatealloftheweightsthatshouldbeaﬀectedbyanormpenalty,whilethevectorθdenotesalloftheparameters,includingbothwandtheunregularizedparameters.Inthecontextofneuralnetworks,itissometimesdesirabletouseaseparatepenaltywithadiﬀerentαcoeﬃcientforeachlayerofthenetwork.Becauseitcanbeexpensivetosearchforthecorrectvalueofmultiplehyperparameters,itisstillreasonabletousethesameweightdecayatalllayersjusttoreducethesizeofsearchspace.230 CHAPTER7.REGULARIZATIONFORDEEPLEARNING7.1.1L2ParameterRegularizationWehavealreadyseen,insection,oneofthesimplestandmostcommonkinds5.2.2ofparameternormpenalty:theL2parameternormpenaltycommonlyknownasweightdecay.Thisregularizationstrategydrivestheweightsclosertotheorigin1byaddingaregularizationtermΩ(θ) =12w22totheobjectivefunction.Inotheracademiccommunities,L2regularizationisalsoknownasridgeregressionorTikhonovregularization.Wecangainsomeinsightintothebehaviorofweightdecayregularizationbystudyingthegradientoftheregularizedobjectivefunction.Tosimplifythepresentation,weassumenobiasparameter,soθisjustw.Suchamodelhasthefollowingtotalobjectivefunction:˜J,(;wXy) =α2wwwXy+(J;,),(7.2)withthecorrespondingparametergradient∇w˜J,α(;wXy) = w+∇wJ,.(;wXy)(7.3)Totakeasinglegradientsteptoupdatetheweights,weperformthisupdate:www←−α(+∇wJ,.(;wXy))(7.4)Writtenanotherway,theupdateis:ww←−(1α)−∇wJ,.(;wXy)(7.5)Wecanseethattheadditionoftheweightdecaytermhasmodiﬁedthelearningruletomultiplicativelyshrinktheweightvectorbyaconstantfactoroneachstep,justbeforeperformingtheusualgradientupdate.Thisdescribeswhathappensinasinglestep.Butwhathappensovertheentirecourseoftraining?Wewillfurthersimplifytheanalysisbymakingaquadraticapproximationtotheobjectivefunctionintheneighborhoodofthevalueoftheweightsthatobtainsminimalunregularizedtrainingcost,w∗=argminwJ(w).Iftheobjectivefunctionistrulyquadratic,asinthecaseofﬁttingalinearregressionmodelwith1Moregenerally,wecouldregularizetheparameterstobenearanyspeciﬁcpointinspaceand,surprisingly,stillgetaregularizationeﬀect,butbetterresultswillbeobtainedforavalueclosertothetrueone,withzerobeingadefaultvaluethatmakessensewhenwedonotknowifthecorrectvalueshouldbepositiveornegative.Sinceitisfarmorecommontoregularizethemodelparameterstowardszero,wewillfocusonthisspecialcaseinourexposition.231 CHAPTER7.REGULARIZATIONFORDEEPLEARNINGmeansquarederror,thentheapproximationisperfect.TheapproximationˆJisgivenbyˆJJ() = θ(w∗)+12(ww−∗)Hww(−∗),(7.6)whereHistheHessianmatrixofJwithrespecttowevaluatedatw∗.Thereisnoﬁrst-orderterminthisquadraticapproximation,becausew∗isdeﬁnedtobeaminimum,wherethegradientvanishes.Likewise,becausew∗isthelocationofaminimumof,wecanconcludethatispositivesemideﬁnite.JHTheminimumofˆJoccurswhereitsgradient∇wˆJ() = (wHww−∗)(7.7)isequalto.0Tostudytheeﬀectofweightdecay,wemodifyequationbyaddingthe7.7weightdecaygradient.WecannowsolvefortheminimumoftheregularizedversionofˆJ.Weusethevariable˜wtorepresentthelocationoftheminimum.α˜wH+(˜ww−∗) = 0(7.8)(+)HαI˜wHw= ∗(7.9)˜wHI= (+α)−1Hw∗.(7.10)Asαapproaches0,theregularizedsolution˜wapproachesw∗.Butwhathappensasαgrows?BecauseHisrealandsymmetric,wecandecomposeitintoadiagonalmatrixΛandanorthonormalbasisofeigenvectors,Q,suchthatHQQ= Λ.Applyingthedecompositiontoequation,weobtain:7.10˜wQQ= (Λ+)αI−1QQΛw∗(7.11)=QIQ(+Λα)−1QQΛw∗(7.12)= (+)QΛαI−1ΛQw∗.(7.13)Weseethattheeﬀectofweightdecayistorescalew∗alongtheaxesdeﬁnedbytheeigenvectorsofH.Speciﬁcally,thecomponentofw∗thatisalignedwiththei-theigenvectorofHisrescaledbyafactorofλiλi+α.(Youmaywishtoreviewhowthiskindofscalingworks,ﬁrstexplainedinﬁgure).2.3AlongthedirectionswheretheeigenvaluesofHarerelativelylarge,forexample,whereλiα,theeﬀectofregularizationisrelativelysmall.However,componentswithλiαwillbeshrunktohavenearlyzeromagnitude.Thiseﬀectisillustratedinﬁgure.7.1232 CHAPTER7.REGULARIZATIONFORDEEPLEARNING w1w2w∗˜w Figure7.1:AnillustrationoftheeﬀectofL2(orweightdecay)regularizationonthevalueoftheoptimalw.Thesolidellipsesrepresentcontoursofequalvalueoftheunregularizedobjective.ThedottedcirclesrepresentcontoursofequalvalueoftheL2regularizer.Atthepoint˜w,thesecompetingobjectivesreachanequilibrium.Intheﬁrstdimension,theeigenvalueoftheHessianofJissmall. Theobjectivefunctiondoesnotincreasemuchwhenmovinghorizontallyawayfromw∗.Becausetheobjectivefunctiondoesnotexpressastrongpreferencealongthisdirection,theregularizerhasastrongeﬀectonthisaxis.Theregularizerpullsw1closetozero.Intheseconddimension,theobjectivefunctionisverysensitivetomovementsawayfromw∗.Thecorrespondingeigenvalueislarge,indicatinghighcurvature.Asaresult,weightdecayaﬀectsthepositionofw2relativelylittle.Onlydirectionsalongwhichtheparameterscontributesigniﬁcantlytoreducingtheobjectivefunctionarepreservedrelativelyintact.Indirectionsthatdonotcontributetoreducingtheobjectivefunction,asmalleigenvalueoftheHessiantellsusthatmovementinthisdirectionwillnotsigniﬁcantlyincreasethegradient.Componentsoftheweightvectorcorrespondingtosuchunimportantdirectionsaredecayedawaythroughtheuseoftheregularizationthroughouttraining.Sofarwehavediscussedweightdecayintermsofitseﬀectontheoptimizationofanabstract,general,quadraticcostfunction.Howdotheseeﬀectsrelatetomachinelearninginparticular?Wecanﬁndoutbystudyinglinearregression,amodelforwhichthetruecostfunctionisquadraticandthereforeamenabletothesamekindofanalysiswehaveusedsofar.Applyingtheanalysisagain,wewillbeabletoobtainaspecialcaseofthesameresults,butwiththesolutionnowphrasedintermsofthetrainingdata.Forlinearregression,thecostfunctionis233 CHAPTER7.REGULARIZATIONFORDEEPLEARNINGthesumofsquarederrors:()Xwy−()Xwy−.(7.14)WhenweaddL2regularization,theobjectivefunctionchangesto()Xwy−()+Xwy−12αww.(7.15)ThischangesthenormalequationsforthesolutionfromwX= (X)−1Xy(7.16)towX= (XI+α)−1Xy.(7.17)ThematrixXXinequationisproportionaltothecovariancematrix7.161mXX.UsingL2regularizationreplacesthismatrixwithXXI+α−1inequation.7.17Thenewmatrixisthesameastheoriginalone,butwiththeadditionofαtothediagonal.Thediagonalentriesofthismatrixcorrespondtothevarianceofeachinputfeature.WecanseethatL2regularizationcausesthelearningalgorithmto“perceive”theinputXashavinghighervariance,whichmakesitshrinktheweightsonfeatureswhosecovariancewiththeoutputtargetislowcomparedtothisaddedvariance.7.1.2L1RegularizationWhileL2weightdecayisthemostcommonformofweightdecay,thereareotherwaystopenalizethesizeofthemodelparameters. AnotheroptionistouseL1regularization.Formally,L1regularizationonthemodelparameterisdeﬁnedas:wΩ() = θ||||w1=i|wi|,(7.18)thatis,asthesumofabsolutevaluesoftheindividualparameters.2WewillnowdiscusstheeﬀectofL1regularizationonthesimplelinearregressionmodel,withnobiasparameter,thatwestudiedinouranalysisofL2regularization.Inparticular,weareinterestedindelineatingthediﬀerencesbetweenL1andL2forms2AswithL2regularization,wecouldregularizetheparameterstowardsavaluethatisnotzero,butinsteadtowardssomeparametervaluew()o.InthatcasetheL1regularizationwouldintroducethetermΩ() = θ||−ww()o||1=i|wi−w()oi|.234 CHAPTER7.REGULARIZATIONFORDEEPLEARNINGofregularization.AswithL2weightdecay,L1weightdecaycontrolsthestrengthoftheregularizationbyscalingthepenaltyusingapositivehyperparameterΩα.Thus,theregularizedobjectivefunction˜J,(;wXy)isgivenby˜J,α(;wXy) = ||||w1+(;)JwXy,,(7.19)withthecorrespondinggradient(actually,sub-gradient):∇w˜J,α(;wXy) = sign()+w∇wJ,(Xyw;)(7.20)whereissimplythesignofappliedelement-wise.sign()wwByinspectingequation,wecanseeimmediatelythattheeﬀectof7.20L1regularizationisquitediﬀerentfromthatofL2regularization.Speciﬁcally,wecanseethattheregularizationcontributiontothegradientnolongerscaleslinearlywitheachwi;insteaditisaconstantfactorwithasignequaltosign(wi).OneconsequenceofthisformofthegradientisthatwewillnotnecessarilyseecleanalgebraicsolutionstoquadraticapproximationsofJ(Xy,;w)aswedidforL2regularization.OursimplelinearmodelhasaquadraticcostfunctionthatwecanrepresentviaitsTaylorseries.Alternately,wecouldimaginethatthisisatruncatedTaylorseriesapproximatingthecostfunctionofamoresophisticatedmodel.Thegradientinthissettingisgivenby∇wˆJ() = (wHww−∗),(7.21)where,again,istheHessianmatrixofwithrespecttoevaluatedatHJww∗.BecausetheL1penaltydoesnotadmitcleanalgebraicexpressionsinthecaseofafullygeneralHessian,wewillalsomakethefurthersimplifyingassumptionthattheHessianisdiagonal,H=diag([H11,,...,Hn,n]),whereeachHi,i>0.Thisassumptionholdsifthedataforthelinearregressionproblemhasbeenpreprocessedtoremoveallcorrelationbetweentheinputfeatures,whichmaybeaccomplishedusingPCA.OurquadraticapproximationoftheL1regularizedobjectivefunctiondecom-posesintoasumovertheparameters:ˆJ,J(;wXy) = (w∗;)+Xy,i12Hi,i(wi−w∗i)2+αw|i|.(7.22)Theproblemofminimizingthisapproximatecostfunctionhasananalyticalsolution(foreachdimension),withthefollowingform:iwi= sign(w∗i)max|w∗i|−αHi,i,0.(7.23)235 CHAPTER7.REGULARIZATIONFORDEEPLEARNINGConsiderthesituationwherew∗i>i0forall.Therearetwopossibleoutcomes:1.Thecasewherew∗i≤αHi,i.Heretheoptimalvalueofwiundertheregularizedobjectiveissimplywi= 0.ThisoccursbecausethecontributionofJ(w;Xy,)totheregularizedobjective˜J(w;Xy,)isoverwhelmed—indirectioni—bytheL1regularizationwhichpushesthevalueofwitozero.2.Thecasewherew∗i>αHi,i.Inthiscase,theregularizationdoesnotmovetheoptimalvalueofwitozerobutinsteaditjustshiftsitinthatdirectionbyadistanceequaltoαHi,i.Asimilarprocesshappenswhenw∗i<0,butwiththeL1penaltymakingwilessnegativebyαHi,i,or0.IncomparisontoL2regularization,L1regularizationresultsinasolutionthatismoresparse.Sparsityinthiscontextreferstothefactthatsomeparametershaveanoptimalvalueofzero.ThesparsityofL1regularizationisaqualitativelydiﬀerentbehaviorthanariseswithL2regularization.Equationgavethe7.13solution˜wforL2regularization.IfwerevisitthatequationusingtheassumptionofadiagonalandpositivedeﬁniteHessianHthatweintroducedforouranalysisofL1regularization,weﬁndthat˜wi=Hi,iHi,i+αw∗i.Ifw∗iwasnonzero,then˜wiremainsnonzero.ThisdemonstratesthatL2regularizationdoesnotcausetheparameterstobecomesparse,whileL1regularizationmaydosoforlargeenough.αThesparsitypropertyinducedbyL1regularizationhasbeenusedextensivelyasafeatureselectionmechanism.Featureselectionsimpliﬁesamachinelearningproblembychoosingwhichsubsetoftheavailablefeaturesshouldbeused.Inparticular,thewellknownLASSO(,)(leastabsoluteshrinkageandTibshirani1995selectionoperator)modelintegratesanL1penaltywithalinearmodelandaleastsquarescostfunction.TheL1penaltycausesasubsetoftheweightstobecomezero,suggestingthatthecorrespondingfeaturesmaysafelybediscarded.Insection,wesawthatmanyregularizationstrategiescanbeinterpreted5.6.1asMAPBayesianinference,andthatinparticular,L2regularizationisequivalenttoMAPBayesianinferencewithaGaussianpriorontheweights. ForL1regu-larization,thepenaltyαΩ(w)=αi|wi|usedtoregularizeacostfunctionisequivalenttothelog-priortermthatismaximizedbyMAPBayesianinferencewhenthepriorisanisotropicLaplacedistribution(equation)over3.26w∈Rn:log() =pwilogLaplace(wi;0,1α) = −||||αw1+loglog2nαn−.(7.24)236 CHAPTER7.REGULARIZATIONFORDEEPLEARNINGFromthepointofviewoflearningviamaximizationwithrespecttow,wecanignorethetermsbecausetheydonotdependon.loglog2α−w7.2NormPenaltiesasConstrainedOptimizationConsiderthecostfunctionregularizedbyaparameternormpenalty:˜J,J,α.(;θXy) = (;θXy)+Ω()θ(7.25)Recallfromsectionthatwecanminimizeafunctionsubjecttoconstraints4.4byconstructingageneralizedLagrangefunction,consistingoftheoriginalobjectivefunctionplusasetofpenalties.Eachpenaltyisaproductbetweenacoeﬃcient,calledaKarush–Kuhn–Tucker(KKT)multiplier,andafunctionrepresentingwhethertheconstraintissatisﬁed.IfwewantedtoconstrainΩ(θ)tobelessthansomeconstant,wecouldconstructageneralizedLagrangefunctionkL−(;) = (;)+(Ω()θ,αXy,JθXy,αθk.)(7.26)Thesolutiontotheconstrainedproblemisgivenbyθ∗= argminθmaxα,α≥0L()θ,α.(7.27)Asdescribedinsection,solvingthisproblemrequiresmodifyingboth4.4θandα.Sectionprovidesaworkedexampleoflinearregressionwithan4.5L2constraint.Manydiﬀerentproceduresarepossible—somemayusegradientdescent,whileothersmayuseanalyticalsolutionsforwherethegradientiszero—butinallproceduresαmustincreasewheneverΩ(θ)>kanddecreasewheneverΩ(θ)<k.AllpositiveαencourageΩ(θ)toshrink.Theoptimalvalueα∗willencourageΩ(θ)toshrink,butnotsostronglytomakebecomelessthan.Ω()θkTogainsomeinsightintotheeﬀectoftheconstraint,wecanﬁxα∗andviewtheproblemasjustafunctionof:θθ∗= argminθL(θ,α∗) = argminθJ,α(;θXy)+∗Ω()θ.(7.28)Thisisexactlythesameastheregularizedtrainingproblemofminimizing˜J.Wecanthusthinkofaparameternormpenaltyasimposingaconstraintontheweights.IfistheΩL2norm,thentheweightsareconstrainedtolieinanL2ball. IfistheΩL1norm,thentheweightsareconstrainedtolieinaregionof237 CHAPTER7.REGULARIZATIONFORDEEPLEARNINGlimitedL1norm.Usuallywedonotknowthesizeoftheconstraintregionthatweimposebyusingweightdecaywithcoeﬃcientα∗becausethevalueofα∗doesnotdirectlytellusthevalueofk.Inprinciple,onecansolvefork,buttherelationshipbetweenkandα∗dependsontheformofJ.Whilewedonotknowtheexactsizeoftheconstraintregion,wecancontrolitroughlybyincreasingordecreasingαinordertogroworshrinktheconstraintregion.Largerαwillresultinasmallerconstraintregion.Smallerwillresultinalargerconstraintregion.αSometimeswemaywishtouseexplicitconstraintsratherthanpenalties.Asdescribedinsection,wecanmodifyalgorithmssuchasstochasticgradient4.4descenttotakeastepdownhillonJ(θ)andthenprojectθbacktothenearestpointthatsatisﬁesΩ(θ)<k.Thiscanbeusefulifwehaveanideaofwhatvalueofkisappropriateanddonotwanttospendtimesearchingforthevalueofαthatcorrespondstothis.kAnotherreasontouseexplicitconstraintsandreprojectionratherthanenforcingconstraintswithpenaltiesisthatpenaltiescancausenon-convexoptimizationprocedurestogetstuckinlocalminimacorrespondingtosmallθ.Whentrainingneuralnetworks,thisusuallymanifestsasneuralnetworksthattrainwithseveral“deadunits.”Theseareunitsthatdonotcontributemuchtothebehaviorofthefunctionlearnedbythenetworkbecausetheweightsgoingintooroutofthemareallverysmall. Whentrainingwithapenaltyonthenormoftheweights,theseconﬁgurationscanbelocallyoptimal,evenifitispossibletosigniﬁcantlyreduceJbymakingtheweightslarger.Explicitconstraintsimplementedbyre-projectioncanworkmuchbetterinthesecasesbecausetheydonotencouragetheweightstoapproachtheorigin.Explicitconstraintsimplementedbyre-projectiononlyhaveaneﬀectwhentheweightsbecomelargeandattempttoleavetheconstraintregion.Finally,explicitconstraintswithreprojectioncanbeusefulbecausetheyimposesomestabilityontheoptimizationprocedure.Whenusinghighlearningrates,itispossibletoencounterapositivefeedbackloopinwhichlargeweightsinducelargegradientswhichtheninducealargeupdatetotheweights.Iftheseupdatesconsistentlyincreasethesizeoftheweights,thenθrapidlymovesawayfromtheoriginuntilnumericaloverﬂowoccurs.Explicitconstraintswithreprojectionpreventthisfeedbackloopfromcontinuingtoincreasethemagnitudeoftheweightswithoutbound.()recommendusingconstraintscombinedwithHintonetal.2012cahighlearningratetoallowrapidexplorationofparameterspacewhilemaintainingsomestability.Inparticular,Hinton2012cetal.()recommendastrategyintroducedbySrebroandShraibman2005():constrainingthenormofeachcolumnoftheweightmatrix238 CHAPTER7.REGULARIZATIONFORDEEPLEARNINGofaneuralnetlayer,ratherthanconstrainingtheFrobeniusnormoftheentireweightmatrix.Constrainingthenormofeachcolumnseparatelypreventsanyonehiddenunitfromhavingverylargeweights.IfweconvertedthisconstraintintoapenaltyinaLagrangefunction,itwouldbesimilartoL2weightdecaybutwithaseparateKKTmultiplierfortheweightsofeachhiddenunit.EachoftheseKKTmultiplierswouldbedynamicallyupdatedseparatelytomakeeachhiddenunitobeytheconstraint.Inpractice,columnnormlimitationisalwaysimplementedasanexplicitconstraintwithreprojection.7.3RegularizationandUnder-ConstrainedProblemsInsomecases,regularizationisnecessaryformachinelearningproblemstobeprop-erlydeﬁned.Manylinearmodelsinmachinelearning,includinglinearregressionandPCA,dependoninvertingthematrixXX.ThisisnotpossiblewheneverXXissingular.Thismatrixcanbesingularwheneverthedatageneratingdistri-butiontrulyhasnovarianceinsomedirection,orwhennovarianceisobservedinsomedirectionbecausetherearefewerexamples(rowsofX)thaninputfeatures(columnsofX).Inthiscase,manyformsofregularizationcorrespondtoinvertingXXI+αinstead.Thisregularizedmatrixisguaranteedtobeinvertible.Theselinearproblemshaveclosedformsolutionswhentherelevantmatrixisinvertible.Itisalsopossibleforaproblemwithnoclosedformsolutiontobeunderdetermined.Anexampleislogisticregressionappliedtoaproblemwheretheclassesarelinearlyseparable.Ifaweightvectorwisabletoachieveperfectclassiﬁcation,then2wwillalsoachieveperfectclassiﬁcationandhigherlikelihood.Aniterativeoptimizationprocedurelikestochasticgradientdescentwillcontinuallyincreasethemagnitudeofwand,intheory,willneverhalt.Inpractice,anumericalimplementationofgradientdescentwilleventuallyreachsuﬃcientlylargeweightstocausenumericaloverﬂow,atwhichpointitsbehaviorwilldependonhowtheprogrammerhasdecidedtohandlevaluesthatarenotrealnumbers.Mostformsofregularizationareabletoguaranteetheconvergenceofiterativemethodsappliedtounderdeterminedproblems. Forexample,weightdecaywillcausegradientdescenttoquitincreasingthemagnitudeoftheweightswhentheslopeofthelikelihoodisequaltotheweightdecaycoeﬃcient.Theideaofusingregularizationtosolveunderdeterminedproblemsextendsbeyondmachinelearning.Thesameideaisusefulforseveralbasiclinearalgebraproblems.Aswesawinsection,wecansolveunderdeterminedlinearequationsusing2.9239 CHAPTER7.REGULARIZATIONFORDEEPLEARNINGtheMoore-Penrosepseudoinverse.RecallthatonedeﬁnitionofthepseudoinverseX+ofamatrixisXX+=limα0(XXI+α)−1X.(7.29)Wecannowrecognizeequationasperforminglinearregressionwithweight7.29decay.Speciﬁcally,equationisthelimitofequationastheregularization7.297.17coeﬃcientshrinkstozero.Wecanthusinterpretthepseudoinverseasstabilizingunderdeterminedproblemsusingregularization.7.4DatasetAugmentationThebestwaytomakeamachinelearningmodelgeneralizebetteristotrainitonmoredata.Ofcourse,inpractice,theamountofdatawehaveislimited.Onewaytogetaroundthisproblemistocreatefakedataandaddittothetrainingset.Forsomemachinelearningtasks,itisreasonablystraightforwardtocreatenewfakedata.Thisapproachiseasiestforclassiﬁcation.Aclassiﬁerneedstotakeacompli-cated,highdimensionalinputxandsummarizeitwithasinglecategoryidentityy.Thismeansthatthemaintaskfacingaclassiﬁeristobeinvarianttoawidevarietyoftransformations.Wecangeneratenew(x,y)pairseasilyjustbytransformingtheinputsinourtrainingset.xThisapproachisnotasreadilyapplicabletomanyothertasks.Forexample,itisdiﬃculttogeneratenewfakedataforadensityestimationtaskunlesswehavealreadysolvedthedensityestimationproblem.Datasetaugmentationhasbeenaparticularlyeﬀectivetechniqueforaspeciﬁcclassiﬁcationproblem:objectrecognition.Imagesarehighdimensionalandincludeanenormousvarietyoffactorsofvariation,manyofwhichcanbeeasilysimulated.Operationsliketranslatingthetrainingimagesafewpixelsineachdirectioncanoftengreatlyimprovegeneralization,evenifthemodelhasalreadybeendesignedtobepartiallytranslationinvariantbyusingtheconvolutionandpoolingtechniquesdescribedinchapter.Manyotheroperationssuchasrotatingtheimageorscaling9theimagehavealsoprovenquiteeﬀective.Onemustbecarefulnottoapplytransformationsthatwouldchangethecorrectclass.Forexample,opticalcharacterrecognitiontasksrequirerecognizingthediﬀerencebetween‘b’and‘d’andthediﬀerencebetween‘6’and‘9’,sohorizontalﬂipsand180◦rotationsarenotappropriatewaysofaugmentingdatasetsforthesetasks.240 CHAPTER7.REGULARIZATIONFORDEEPLEARNINGTherearealsotransformationsthatwewouldlikeourclassiﬁerstobeinvariantto,butwhicharenoteasytoperform.Forexample,out-of-planerotationcannotbeimplementedasasimplegeometricoperationontheinputpixels.Datasetaugmentationiseﬀectiveforspeechrecognitiontasksaswell(JaitlyandHinton2013,).Injectingnoiseintheinputtoaneuralnetwork(SietsmaandDow1991,)canalsobeseenasaformofdataaugmentation.Formanyclassiﬁcationandevensomeregressiontasks,thetaskshouldstillbepossibletosolveevenifsmallrandomnoiseisaddedtotheinput.Neuralnetworksprovenottobeveryrobusttonoise,however(TangandEliasmith2010,).Onewaytoimprovetherobustnessofneuralnetworksissimplytotrainthemwithrandomnoiseappliedtotheirinputs.Inputnoiseinjectionispartofsomeunsupervisedlearningalgorithmssuchasthedenoisingautoencoder(Vincent2008etal.,).Noiseinjectionalsoworkswhenthenoiseisappliedtothehiddenunits,whichcanbeseenasdoingdatasetaugmentationatmultiplelevelsofabstraction.Poole2014etal.()recentlyshowedthatthisapproachcanbehighlyeﬀectiveprovidedthatthemagnitudeofthenoiseiscarefullytuned.Dropout,apowerfulregularizationstrategythatwillbedescribedinsection,canbeseenasaprocessofconstructingnewinputsby7.12multiplyingbynoise.Whencomparingmachinelearningbenchmarkresults,itisimportanttotaketheeﬀectofdatasetaugmentationintoaccount.Often,hand-designeddatasetaugmentationschemescandramaticallyreducethegeneralizationerrorofamachinelearningtechnique.Tocomparetheperformanceofonemachinelearningalgorithmtoanother,itisnecessarytoperformcontrolledexperiments.WhencomparingmachinelearningalgorithmAandmachinelearningalgorithmB,itisnecessarytomakesurethatbothalgorithmswereevaluatedusingthesamehand-designeddatasetaugmentationschemes.SupposethatalgorithmAperformspoorlywithnodatasetaugmentationandalgorithmBperformswellwhencombinedwithnumeroussynthetictransformationsoftheinput.Insuchacaseitislikelythesynthetictransformationscausedtheimprovedperformance,ratherthantheuseofmachinelearningalgorithmB.Sometimesdecidingwhetheranexperimenthasbeenproperlycontrolledrequiressubjectivejudgment.Forexample,machinelearningalgorithmsthatinjectnoiseintotheinputareperformingaformofdatasetaugmentation.Usually,operationsthataregenerallyapplicable(suchasaddingGaussiannoisetotheinput)areconsideredpartofthemachinelearningalgorithm,whileoperationsthatarespeciﬁctooneapplicationdomain(suchasrandomlycroppinganimage)areconsideredtobeseparatepre-processingsteps.241 CHAPTER7.REGULARIZATIONFORDEEPLEARNING7.5NoiseRobustnessSectionhasmotivatedtheuseofnoiseappliedtotheinputsasadataset7.4augmentationstrategy.Forsomemodels,theadditionofnoisewithinﬁnitesimalvarianceattheinputofthemodelisequivalenttoimposingapenaltyonthenormoftheweights(,,).Inthegeneralcase,itisimportanttoBishop1995abrememberthatnoiseinjectioncanbemuchmorepowerfulthansimplyshrinkingtheparameters,especiallywhenthenoiseisaddedtothehiddenunits.Noiseappliedtothehiddenunitsissuchanimportanttopicthatitmerititsownseparatediscussion;thedropoutalgorithmdescribedinsectionisthemaindevelopment7.12ofthatapproach.Anotherwaythatnoisehasbeenusedintheserviceofregularizingmodelsisbyaddingittotheweights.Thistechniquehasbeenusedprimarilyinthecontextofrecurrentneuralnetworks(,;Jimetal.1996Graves2011,). Thiscanbeinterpretedasa stochasticimplementationof Bayesianinference overtheweights. TheBayesiantreatmentoflearningwouldconsiderthemodelweightstobeuncertainandrepresentableviaaprobabilitydistributionthatreﬂectsthisuncertainty.Addingnoisetotheweightsisapractical,stochasticwaytoreﬂectthisuncertainty.Noiseappliedtotheweightscanalsobeinterpretedasequivalent(undersomeassumptions)toamoretraditionalformofregularization,encouragingstabilityofthefunctiontobelearned.Considertheregressionsetting,wherewewishtotrainafunctionˆy(x)thatmapsasetoffeaturesxtoascalarusingtheleast-squarescostfunctionbetweenthemodelpredictionsˆy()xandthetruevalues:yJ= Epx,y()(ˆyy()x−)2.(7.30)Thetrainingsetconsistsoflabeledexamplesm{(x(1),y(1))(,...,x()m,y()m)}.WenowassumethatwitheachinputpresentationwealsoincludearandomperturbationW∼N(;0,ηI)ofthenetworkweights.Letusimaginethatwehaveastandardl-layerMLP.WedenotetheperturbedmodelasˆyW(x).Despitetheinjectionofnoise,wearestillinterestedinminimizingthesquarederroroftheoutputofthenetwork.Theobjectivefunctionthusbecomes:˜JW= Ep,y,(xW)(ˆyW())x−y2(7.31)= Ep,y,(xW)ˆy2W()2ˆx−yyW()+xy2.(7.32)Forsmallη,theminimizationofJwithaddedweightnoise(withcovarianceηI)isequivalenttominimizationofJwithanadditionalregularizationterm:242 CHAPTER7.REGULARIZATIONFORDEEPLEARNINGηEp,y(x)∇Wˆy()x2.Thisformofregularizationencouragestheparameterstogotoregionsofparameterspacewheresmallperturbationsoftheweightshavearelativelysmallinﬂuenceontheoutput.Inotherwords,itpushesthemodelintoregionswherethemodelisrelativelyinsensitivetosmallvariationsintheweights,ﬁndingpointsthatarenotmerelyminima,butminimasurroundedbyﬂatregions(HochreiterandSchmidhuber1995,).Inthesimpliﬁedcaseoflinearregression(where,forinstance,ˆy(x) =wx+b),thisregularizationtermcollapsesintoηEp()xx2,whichisnotafunctionofparametersandthereforedoesnotcontributetothegradientof˜JWwithrespecttothemodelparameters.7.5.1InjectingNoiseattheOutputTargetsMostdatasetshavesomeamountofmistakesintheylabels.Itcanbeharmfultomaximizelogp(y|x)whenyisamistake.Onewaytopreventthisistoexplicitlymodelthenoiseonthelabels.Forexample,wecanassumethatforsomesmallconstant,thetrainingsetlabelyiscorrectwithprobability1−,andotherwiseanyoftheotherpossiblelabelsmightbecorrect.Thisassumptioniseasytoincorporateintothecostfunctionanalytically,ratherthanbyexplicitlydrawingnoisesamples.Forexample,labelsmoothingregularizesamodelbasedonasoftmaxwithkoutputvaluesbyreplacingthehardandclassiﬁcationtargets01withtargetsofk−1and1−,respectively.Thestandardcross-entropylossmaythenbeusedwiththesesofttargets.Maximumlikelihoodlearningwithasoftmaxclassiﬁerandhardtargetsmayactuallyneverconverge—thesoftmaxcanneverpredictaprobabilityofexactlyorexactly,soitwillcontinuetolearnlarger01andlargerweights,makingmoreextremepredictionsforever.Itispossibletopreventthisscenariousingotherregularizationstrategieslikeweightdecay.Labelsmoothinghastheadvantageofpreventingthepursuitofhardprobabilitieswithoutdiscouragingcorrectclassiﬁcation.Thisstrategyhasbeenusedsincethe1980sandcontinuestobefeaturedprominentlyinmodernneuralnetworks(Szegedyetal.,).20157.6Semi-SupervisedLearningIntheparadigmofsemi-supervisedlearning,bothunlabeledexamplesfromP(x)andlabeledexamplesfromP(xy,)areusedtoestimateP(yx|)orpredictyfromx.Inthecontextofdeeplearning,semi-supervisedlearningusuallyreferstolearningarepresentationh=f(x).Thegoalistolearnarepresentationso243 CHAPTER7.REGULARIZATIONFORDEEPLEARNINGthatexamplesfromthesameclasshavesimilarrepresentations.Unsupervisedlearningcanprovideusefulcuesforhowtogroupexamplesinrepresentationspace.Examplesthatclustertightlyintheinputspaceshouldbemappedtosimilarrepresentations.Alinearclassiﬁerinthenewspacemayachievebettergeneralizationinmanycases(BelkinandNiyogi2002Chapelle2003,;etal.,).Along-standingvariantofthisapproachistheapplicationofprincipalcomponentsanalysisasapre-processingstepbeforeapplyingaclassiﬁer(ontheprojecteddata).Insteadofhavingseparateunsupervisedandsupervisedcomponentsinthemodel,onecanconstructmodelsinwhichagenerativemodelofeitherP(x)orP(xy,)sharesparameterswithadiscriminativemodelofP(yx|).Onecanthentrade-oﬀthesupervisedcriterion−logP(yx|)withtheunsupervisedorgenerativeone(suchas−logP(x)or−logP(xy,)).Thegenerativecriterionthenexpressesaparticularformofpriorbeliefaboutthesolutiontothesupervisedlearningproblem(,),namelythatthestructureofLasserreetal.2006P(x)isconnectedtothestructureofP(yx|)inawaythatiscapturedbythesharedparametrization.Bycontrollinghowmuchofthegenerativecriterionisincludedinthetotalcriterion,onecanﬁndabettertrade-oﬀthanwithapurelygenerativeorapurelydiscriminativetrainingcriterion(,;Lasserreetal.2006LarochelleandBengio2008,).SalakhutdinovandHinton2008()describeamethodforlearningthekernelfunctionofakernelmachineusedforregression,inwhichtheusageofunlabeledexamplesformodelingimprovesquitesigniﬁcantly.P()xP()yx|See()formoreinformationaboutsemi-supervisedlearning.Chapelleetal.20067.7Multi-TaskLearningMulti-tasklearning(,)isawaytoimprovegeneralizationbypoolingCaruana1993theexamples(whichcanbeseenassoftconstraintsimposedontheparameters)arisingoutofseveraltasks. Inthesamewaythatadditionaltrainingexamplesputmorepressureontheparametersofthemodeltowardsvaluesthatgeneralizewell,whenpartofamodelissharedacrosstasks,thatpartofthemodelismoreconstrainedtowardsgoodvalues(assumingthesharingisjustiﬁed),oftenyieldingbettergeneralization.Figureillustratesaverycommonformofmulti-tasklearning,inwhich7.2diﬀerentsupervisedtasks(predictingy()igivenx)sharethesameinputx,aswellassomeintermediate-levelrepresentationh(shared)capturingacommonpoolof244 CHAPTER7.REGULARIZATIONFORDEEPLEARNINGfactors.Themodelcangenerallybedividedintotwokindsofpartsandassociatedparameters:1.Task-speciﬁcparameters(whichonlybeneﬁtfromtheexamplesoftheirtasktoachievegoodgeneralization).Thesearetheupperlayersoftheneuralnetworkinﬁgure.7.22.Genericparameters,sharedacrossallthetasks(whichbeneﬁtfromthepooleddataofallthetasks).Thesearethelowerlayersoftheneuralnetworkinﬁgure.7.2 h(1)h(1)h(2)h(2)h(3)h(3)y(1)y(1)y(2)y(2) h(shared)h(shared)x xFigure7.2:Multi-tasklearningcanbecastinseveralwaysindeeplearningframeworksandthisﬁgureillustratesthecommonsituationwherethetasksshareacommoninputbutinvolvediﬀerenttargetrandomvariables.Thelowerlayersofadeepnetwork(whetheritissupervisedandfeedforwardorincludesagenerativecomponentwithdownwardarrows)canbesharedacrosssuchtasks,whiletask-speciﬁcparameters(associatedrespectivelywiththeweightsintoandfromh(1)andh(2))canbelearnedontopofthoseyieldingasharedrepresentationh(shared).Theunderlyingassumptionisthatthereexistsacommonpooloffactorsthatexplainthevariationsintheinputx,whileeachtaskisassociatedwithasubsetofthesefactors.Inthisexample,itisadditionallyassumedthattop-levelhiddenunitsh(1)andh(2)arespecializedtoeachtask(respectivelypredictingy(1)andy(2))whilesomeintermediate-levelrepresentationh(shared)issharedacrossalltasks.Intheunsupervisedlearningcontext,itmakessenseforsomeofthetop-levelfactorstobeassociatedwithnoneoftheoutputtasks(h(3)):thesearethefactorsthatexplainsomeoftheinputvariationsbutarenotrelevantforpredictingy(1)ory(2).Improvedgeneralizationandgeneralizationerrorbounds(,)canbeBaxter1995achievedbecauseofthesharedparameters,forwhichstatisticalstrengthcanbe245 CHAPTER7.REGULARIZATIONFORDEEPLEARNING 050100150200250Time(epochs)000.005.010.015.020.Loss(negative log-likelihood)TrainingsetlossValidationsetloss Figure7.3:Learningcurvesshowinghowthenegativelog-likelihoodlosschangesovertime(indicatedasnumberoftrainingiterationsoverthedataset,orepochs).Inthisexample,wetrainamaxoutnetworkonMNIST.Observethatthetrainingobjectivedecreasesconsistentlyovertime,butthevalidationsetaveragelosseventuallybeginstoincreaseagain,forminganasymmetricU-shapedcurve.greatlyimproved(inproportionwiththeincreasednumberofexamplesforthesharedparameters,comparedtothescenarioofsingle-taskmodels).Ofcoursethiswillhappenonlyifsomeassumptionsaboutthestatisticalrelationshipbetweenthediﬀerenttasksarevalid,meaningthatthereissomethingsharedacrosssomeofthetasks.Fromthepointofviewofdeeplearning,theunderlyingpriorbeliefisthefollowing:amongthefactorsthat explainthevariations observed inthedataassociatedwiththediﬀerenttasks,somearesharedacrosstwoormoretasks.7.8EarlyStoppingWhentraininglargemodelswithsuﬃcientrepresentationalcapacitytooverﬁtthetask,weoftenobservethattrainingerrordecreasessteadilyovertime,butvalidationseterrorbeginstoriseagain.Seeﬁgureforanexampleofthis7.3behavior.Thisbehavioroccursveryreliably.Thismeanswecanobtainamodelwithbettervalidationseterror(andthus,hopefullybettertestseterror)byreturningtotheparametersettingatthepointintimewiththelowestvalidationseterror.Everytimetheerroronthevalidationsetimproves,westoreacopyofthemodelparameters.Whenthetrainingalgorithmterminates,wereturntheseparameters,ratherthanthelatestparameters.The246 CHAPTER7.REGULARIZATIONFORDEEPLEARNINGalgorithmterminateswhennoparametershaveimprovedoverthebestrecordedvalidationerrorforsomepre-speciﬁednumberofiterations.Thisprocedureisspeciﬁedmoreformallyinalgorithm.7.1Algorithm 7.1Theearlystopping meta-algorithmfor determiningthe bestamountoftimetotrain.Thismeta-algorithmisageneralstrategythatworkswellwithavarietyoftrainingalgorithmsandwaysofquantifyingerroronthevalidationset.Letbethenumberofstepsbetweenevaluations.nLetpbethe“patience,”thenumberoftimestoobserveworseningvalidationseterrorbeforegivingup.Letθobetheinitialparameters.θθ←oi←0j←0v←∞θ∗←θi∗←iwhiledoj<pUpdatebyrunningthetrainingalgorithmforsteps.θniin←+v←ValidationSetError()θifv<vthenj←0θ∗←θi∗←ivv←elsejj←+1endifendwhileBestparametersareθ∗,bestnumberoftrainingstepsisi∗Thisstrategyisknownasearlystopping.Itisprobablythemostcommonlyusedformofregularizationindeeplearning.Itspopularityisduebothtoitseﬀectivenessanditssimplicity.Onewaytothinkofearlystoppingisasaveryeﬃcienthyperparameterselectionalgorithm.Inthisview,thenumberoftrainingstepsisjustanotherhyperparameter.WecanseeinﬁgurethatthishyperparameterhasaU-shapedvalidationset7.3247 CHAPTER7.REGULARIZATIONFORDEEPLEARNINGperformancecurve.MosthyperparametersthatcontrolmodelcapacityhavesuchaU-shapedvalidationsetperformancecurve,asillustratedinﬁgure.Inthecaseof5.3earlystopping,wearecontrollingtheeﬀectivecapacityofthemodelbydetermininghowmanystepsitcantaketoﬁtthetrainingset.Mosthyperparametersmustbechosenusinganexpensiveguessandcheckprocess,wherewesetahyperparameteratthestartoftraining,thenruntrainingforseveralstepstoseeitseﬀect.The“trainingtime” hyperparameterisuniqueinthatbydeﬁnitionasinglerunoftrainingtriesoutmanyvaluesofthehyperparameter.Theonlysigniﬁcantcosttochoosingthishyperparameterautomaticallyviaearlystoppingisrunningthevalidationsetevaluationperiodicallyduringtraining.Ideally,thisisdoneinparalleltothetrainingprocessonaseparatemachine,separateCPU,orseparateGPUfromthemaintrainingprocess.Ifsuchresourcesarenotavailable,thenthecostoftheseperiodicevaluationsmaybereducedbyusingavalidationsetthatissmallcomparedtothetrainingsetorbyevaluatingthevalidationseterrorlessfrequentlyandobtainingalowerresolutionestimateoftheoptimaltrainingtime.Anadditionalcosttoearlystoppingistheneedtomaintainacopyofthebestparameters.Thiscostisgenerallynegligible,becauseitisacceptabletostoretheseparametersinaslowerandlargerformofmemory(forexample,traininginGPUmemory,butstoringtheoptimalparametersinhostmemoryoronadiskdrive).Sincethebestparametersarewrittentoinfrequentlyandneverreadduringtraining,theseoccasionalslowwriteshavelittleeﬀectonthetotaltrainingtime.Earlystoppingisaveryunobtrusiveformofregularization,inthatitrequiresalmostnochangeintheunderlyingtrainingprocedure,theobjectivefunction,orthesetofallowableparametervalues.Thismeansthatitiseasytouseearlystoppingwithoutdamagingthelearningdynamics.Thisisincontrasttoweightdecay,whereonemustbecarefulnottousetoomuchweightdecayandtrapthenetworkinabadlocalminimumcorrespondingtoasolutionwithpathologicallysmallweights.Earlystoppingmaybeusedeitheraloneorinconjunctionwithotherregulariza-tionstrategies.Evenwhenusingregularizationstrategiesthatmodifytheobjectivefunctiontoencouragebettergeneralization,itisrareforthebestgeneralizationtooccuratalocalminimumofthetrainingobjective.Earlystoppingrequiresavalidationset,whichmeanssometrainingdataisnotfedtothemodel.Tobestexploitthisextradata,onecanperformextratrainingaftertheinitialtrainingwithearlystoppinghascompleted.Inthesecond,extratrainingstep,allofthetrainingdataisincluded.Therearetwobasicstrategiesonecanuseforthissecondtrainingprocedure.Onestrategy(algorithm)istoinitializethemodelagainandretrainonall7.2248 CHAPTER7.REGULARIZATIONFORDEEPLEARNINGofthedata.Inthissecondtrainingpass,wetrainforthesamenumberofstepsastheearlystoppingproceduredeterminedwasoptimalintheﬁrstpass.Therearesomesubtletiesassociatedwiththisprocedure.Forexample,thereisnotagoodwayofknowingwhethertoretrainforthesamenumberofparameterupdatesorthesamenumberofpassesthroughthedataset.Onthesecondroundoftraining,eachpassthroughthedatasetwillrequiremoreparameterupdatesbecausethetrainingsetisbigger.Algorithm7.2Ameta-algorithmforusingearlystoppingtodeterminehowlongtotrain,thenretrainingonallthedata.LetX()trainandy()trainbethetrainingset.SplitX()trainandy()traininto(X()subtrain,X(valid))(andy()subtrain,y(valid))respectively.Runearlystopping(algorithm)startingfromrandom7.1θusingX()subtrainandy()subtrainfortrainingdataandX(valid)andy(valid)forvalidationdata.Thisreturnsi∗,theoptimalnumberofsteps.Settorandomvaluesagain.θTrainonX()trainandy()trainfori∗steps.Anotherstrategyforusingallofthedataistokeeptheparametersobtainedfromtheﬁrstroundoftrainingandthencontinuetrainingbutnowusingallofthedata.Atthisstage,wenownolongerhaveaguideforwhentostopintermsofanumberofsteps. Instead,wecanmonitortheaveragelossfunctiononthevalidationset,andcontinuetraininguntilitfallsbelowthevalueofthetrainingsetobjectiveatwhichtheearlystoppingprocedurehalted.Thisstrategyavoidsthehighcostofretrainingthemodelfromscratch,butisnotaswell-behaved.Forexample,thereisnotanyguaranteethattheobjectiveonthevalidationsetwilleverreachthetargetvalue,sothisstrategyisnotevenguaranteedtoterminate.Thisprocedureispresentedmoreformallyinalgorithm.7.3Earlystoppingisalsousefulbecauseitreducesthecomputationalcostofthetrainingprocedure.Besidestheobviousreductionincostduetolimitingthenumberoftrainingiterations,italsohasthebeneﬁtofprovidingregularizationwithoutrequiringtheadditionofpenaltytermstothecostfunctionorthecomputationofthegradientsofsuchadditionalterms.Howearlystoppingactsasaregularizer:Sofarwehavestatedthatearlystoppingaregularizationstrategy,butwehavesupportedthisclaimonlybyisshowinglearningcurveswherethevalidationseterrorhasaU-shapedcurve.What249 CHAPTER7.REGULARIZATIONFORDEEPLEARNINGAlgorithm7.3Meta-algorithmusingearlystoppingtodetermineatwhatobjec-tivevaluewestarttooverﬁt,thencontinuetraininguntilthatvalueisreached.LetX()trainandy()trainbethetrainingset.SplitX()trainandy()traininto(X()subtrain,X(valid))(andy()subtrain,y(valid))respectively.Runearlystopping(algorithm)startingfromrandom7.1θusingX()subtrainandy()subtrainfortrainingdataandX(valid)andy(valid)forvalidationdata.Thisupdates.θJ,←(θX()subtrain,y()subtrain)whileJ,(θX(valid),y(valid)) >doTrainonX()trainandy()trainforsteps.nendwhileistheactualmechanismbywhichearlystoppingregularizesthemodel?Bishop()and()arguedthatearlystoppinghastheeﬀectof1995aSjöbergandLjung1995restrictingtheoptimizationproceduretoarelativelysmallvolumeofparameterspaceintheneighborhoodoftheinitialparametervalueθo,asillustratedinﬁgure.Morespeciﬁcally,imaginetaking7.4τoptimizationsteps(correspondingtoτtrainingiterations)andwithlearningrate.Wecanviewtheproductτasameasureofeﬀectivecapacity.Assumingthegradientisbounded,restrictingboththenumberofiterationsandthelearningratelimitsthevolumeofparameterspacereachablefromθo.Inthissense,τbehavesasifitwerethereciprocalofthecoeﬃcientusedforweightdecay.Indeed,wecanshowhow—inthecaseofasimplelinearmodelwithaquadraticerrorfunctionandsimplegradientdescent—earlystoppingisequivalenttoL2regularization.InordertocomparewithclassicalL2regularization,weexamineasimplesettingwheretheonlyparametersarelinearweights(θ=w).WecanmodelthecostfunctionJwithaquadraticapproximationintheneighborhoodoftheempiricallyoptimalvalueoftheweightsw∗:ˆJJ() = θ(w∗)+12(ww−∗)Hww(−∗),(7.33)whereHistheHessianmatrixofJwithrespecttowevaluatedatw∗.Giventheassumptionthatw∗isaminimumofJ(w),weknowthatHispositivesemideﬁnite.UnderalocalTaylorseriesapproximation,thegradientisgivenby:∇wˆJ() = (wHww−∗).(7.34)250 CHAPTER7.REGULARIZATIONFORDEEPLEARNING w1w2w∗˜ww1w2w∗˜wFigure7.4:Anillustrationoftheeﬀectofearlystopping.(Left)Thesolidcontourlinesindicatethecontoursofthenegativelog-likelihood.ThedashedlineindicatesthetrajectorytakenbySGDbeginningfromtheorigin.Ratherthanstoppingatthepointw∗thatminimizesthecost,earlystoppingresultsinthetrajectorystoppingatanearlierpoint˜w.(Right)AnillustrationoftheeﬀectofL2regularizationforcomparison.ThedashedcirclesindicatethecontoursoftheL2penalty,whichcausestheminimumofthetotalcosttolienearertheoriginthantheminimumoftheunregularizedcost.Wearegoingtostudythetrajectoryfollowedbytheparametervectorduringtraining.Forsimplicity,letussettheinitialparametervectortotheorigin,3thatisw(0)=0.LetusstudytheapproximatebehaviorofgradientdescentonJbyanalyzinggradientdescentonˆJ:w()τ= w(1)τ−−∇wˆJ(w(1)τ−)(7.35)= w(1)τ−−Hw((1)τ−−w∗)(7.36)w()τ−w∗= ()(IH−w(1)τ−−w∗).(7.37)LetusnowrewritethisexpressioninthespaceoftheeigenvectorsofH,exploitingtheeigendecompositionofH:H=QQΛ,whereΛisadiagonalmatrixandQisanorthonormalbasisofeigenvectors.w()τ−w∗= (IQQ−Λ)(w(1)τ−−w∗)(7.38)Q(w()τ−w∗) = ()I−ΛQ(w(1)τ−−w∗)(7.39)3Forneuralnetworks,toobtainsymmetrybreakingbetweenhiddenunits,wecannotinitializealltheparametersto0,asdiscussedinsection.However,theargumentholdsforanyother6.2initialvaluew(0).251 CHAPTER7.REGULARIZATIONFORDEEPLEARNINGAssumingthatw(0)=0andthatischosentobesmallenoughtoguarantee|1−λi|<1,theparametertrajectoryduringtrainingafterτparameterupdatesisasfollows:Qw()τ= [()I−I−Λτ]Qw∗.(7.40)Now,theexpressionforQ˜winequationfor7.13L2regularizationcanberear-rangedas:Q˜wI= (+Λα)−1ΛQw∗(7.41)Q˜wII= [−(+Λα)−1α]Qw∗(7.42)Comparingequationandequation,weseethatifthehyperparameters7.407.42,ατ,andarechosensuchthat()I−Λτ= (+)ΛαI−1α,(7.43)thenL2regularizationandearlystoppingcanbeseentobeequivalent(atleastunderthequadraticapproximationoftheobjectivefunction).Goingevenfurther,bytakinglogarithmsandusingtheseriesexpansionforlog(1+x),wecanconcludethatifallλiaresmall(thatis,λi1andλi/α1)thenτ≈1α,(7.44)α≈1τ.(7.45)Thatis,undertheseassumptions,thenumberoftrainingiterationsτplaysaroleinverselyproportionaltotheL2regularizationparameter,andtheinverseofτplaystheroleoftheweightdecaycoeﬃcient.Parametervaluescorrespondingtodirectionsofsigniﬁcantcurvature(oftheobjectivefunction)areregularizedlessthandirectionsoflesscurvature.Ofcourse,inthecontextofearlystopping,thisreallymeansthatparametersthatcorrespondtodirectionsofsigniﬁcantcurvaturetendtolearnearlyrelativetoparameterscorrespondingtodirectionsoflesscurvature.ThederivationsinthissectionhaveshownthatatrajectoryoflengthτendsatapointthatcorrespondstoaminimumoftheL2-regularizedobjective.Earlystoppingisofcoursemorethanthemererestrictionofthetrajectorylength;instead,earlystoppingtypicallyinvolvesmonitoringthevalidationseterrorinordertostopthetrajectoryataparticularlygoodpointinspace.Earlystoppingthereforehastheadvantageoverweightdecaythatearlystoppingautomaticallydeterminesthecorrectamountofregularizationwhileweightdecayrequiresmanytrainingexperimentswithdiﬀerentvaluesofitshyperparameter.252 CHAPTER7.REGULARIZATIONFORDEEPLEARNING7.9ParameterTyingandParameterSharingThusfar,inthischapter,whenwehavediscussedaddingconstraintsorpenaltiestotheparameters,wehavealwaysdonesowithrespecttoaﬁxedregionorpoint.Forexample,L2regularization(orweightdecay)penalizesmodelparametersfordeviatingfromtheﬁxedvalueofzero.However,sometimeswemayneedotherwaystoexpressourpriorknowledgeaboutsuitablevaluesofthemodelparameters.Sometimeswemightnotknowpreciselywhatvaluestheparametersshouldtakebutweknow,fromknowledgeofthedomainandmodelarchitecture,thatthereshouldbesomedependenciesbetweenthemodelparameters.Acommontypeofdependencythatweoftenwanttoexpressisthatcertainparametersshouldbeclosetooneanother.Considerthefollowingscenario:wehavetwomodelsperformingthesameclassiﬁcationtask(withthesamesetofclasses)butwithsomewhatdiﬀerentinputdistributions.Formally,wehavemodelAwithparametersw()AandmodelBwithparametersw()B.Thetwomodelsmaptheinput totwo diﬀerent, but relatedoutputs:ˆy()A=f(w()A,x)andˆy()B= (gw()B,x).Letusimaginethatthetasksaresimilarenough(perhapswithsimilarinputandoutputdistributions)thatwebelievethemodelparametersshouldbeclosetoeachother:∀i,w()Aishouldbeclosetow()Bi.Wecanleveragethisinformationthroughregularization.Speciﬁcally,wecanuseaparameternormpenaltyoftheform:Ω(w()A,w()B)=w()A−w()B22. HereweusedanL2penalty,butotherchoicesarealsopossible.Thiskindofapproachwasproposedby(),whoregularizedLasserreetal.2006theparametersofonemodel,trainedasaclassiﬁerinasupervisedparadigm,tobeclosetotheparametersofanothermodel,trainedinanunsupervisedparadigm(tocapturethedistributionoftheobservedinputdata).Thearchitectureswereconstructedsuchthatmanyoftheparametersintheclassiﬁermodelcouldbepairedtocorrespondingparametersintheunsupervisedmodel.Whileaparameternormpenaltyisonewaytoregularizeparameterstobeclosetooneanother,themorepopularwayistouseconstraints:toforcesetsofparameterstobeequal.Thismethodofregularizationisoftenreferredtoasparametersharing,becauseweinterpretthevariousmodelsormodelcomponentsassharingauniquesetofparameters.Asigniﬁcantadvantageofparametersharingoverregularizingtheparameterstobeclose(viaanormpenalty)isthatonlyasubsetoftheparameters(theuniqueset)needtobestoredinmemory.Incertainmodels—suchastheconvolutionalneuralnetwork—thiscanleadtosigniﬁcantreductioninthememoryfootprintofthemodel.253 CHAPTER7.REGULARIZATIONFORDEEPLEARNINGConvolutionalNeuralNetworksByfarthemostpopularandextensiveuseofparametersharingoccursinconvolutionalneuralnetworks(CNNs)appliedtocomputervision.Naturalimageshavemanystatisticalpropertiesthatareinvarianttotranslation.Forexample,aphotoofacatremainsaphotoofacatifitistranslatedonepixeltotheright.CNNstakethispropertyintoaccountbysharingparametersacrossmultipleimagelocations.Thesamefeature(ahiddenunitwiththesameweights)iscomputedoverdiﬀerentlocationsintheinput.Thismeansthatwecanﬁndacatwiththesamecatdetectorwhetherthecatappearsatcolumniorcolumni+1intheimage.ParametersharinghasallowedCNNstodramaticallylowerthenumberofuniquemodelparametersandtosigniﬁcantlyincreasenetworksizeswithoutrequiringacorrespondingincreaseintrainingdata. Itremainsoneofthebestexamplesofhowtoeﬀectivelyincorporatedomainknowledgeintothenetworkarchitecture.CNNswillbediscussedinmoredetailinchapter.97.10SparseRepresentationsWeightdecayactsbyplacingapenaltydirectlyonthemodelparameters.Anotherstrategyistoplaceapenaltyontheactivationsoftheunitsinaneuralnetwork,encouragingtheiractivationstobesparse.Thisindirectlyimposesacomplicatedpenaltyonthemodelparameters.Wehave alreadydiscussed (insection)how7.1.2L1penalizationinducesasparseparametrization—meaningthatmanyoftheparametersbecomezero(orcloseto zero).Representationalsparsity, on theother hand, describesarepresentationwheremanyoftheelementsoftherepresentationarezero(orclosetozero).Asimpliﬁedviewofthisdistinctioncanbeillustratedinthecontextoflinearregression:18515−9−3=400200−001030−050000100104−−100050−23−2−514y∈RmA∈Rmn×x∈Rn(7.46) 254 CHAPTER7.REGULARIZATIONFORDEEPLEARNING−14119223=312541−−423113−−−−−154232312303−−−−−−5422510200−30y∈RmB∈Rmn×h∈Rn(7.47)Intheﬁrstexpression,wehaveanexampleofasparselyparametrizedlinearregressionmodel.Inthesecond,wehavelinearregressionwithasparserepresenta-tionhofthedatax.Thatis,hisafunctionofxthat,insomesense,representstheinformationpresentin,butdoessowithasparsevector.xRepresentationalregularizationisaccomplishedbythesamesortsofmechanismsthatwehaveusedinparameterregularization.NormpenaltyregularizationofrepresentationsisperformedbyaddingtothelossfunctionJanormpenaltyontherepresentation.ThispenaltyisdenotedΩ()h.Asbefore,wedenotetheregularizedlossfunctionby˜J:˜J,J,α(;θXy) = (;θXy)+Ω()h(7.48)whereα∈[0,∞)weightstherelativecontributionofthenormpenaltyterm,withlargervaluesofcorrespondingtomoreregularization.αJustasanL1penaltyontheparametersinducesparametersparsity,anL1penaltyontheelementsoftherepresentationinducesrepresentationalsparsity:Ω(h) =||||h1=i|hi|. Ofcourse,theL1penaltyisonlyonechoiceofpenaltythatcanresultinasparserepresentation.OthersincludethepenaltyderivedfromaStudent-tpriorontherepresentation(,;,)OlshausenandField1996Bergstra2011andKLdivergencepenalties(,)thatareespeciallyLarochelleandBengio2008usefulforrepresentationswithelementsconstrainedtolieontheunitinterval.Lee2008Goodfellow2009etal.()andetal.()bothprovideexamplesofstrategiesbasedonregularizingtheaverageactivationacrossseveralexamples,1mih()i,tobenearsometargetvalue,suchasavectorwith.01foreachentry.Otherapproachesobtainrepresentationalsparsitywithahardconstraintontheactivationvalues.Forexample,orthogonalmatchingpursuit(Patietal.,1993)encodesaninputxwiththerepresentationhthatsolvestheconstrainedoptimizationproblemargminhh,0<k−xWh2,(7.49)whereh0isthenumberofnon-zeroentriesofh. ThisproblemcanbesolvedeﬃcientlywhenWisconstrainedtobeorthogonal.Thismethodisoftencalled255 CHAPTER7.REGULARIZATIONFORDEEPLEARNINGOMP-kwiththevalueofkspeciﬁedtoindicatethenumberofnon-zerofeaturesallowed.()demonstratedthatOMP-canbeaveryeﬀectiveCoatesandNg20111featureextractorfordeeparchitectures.Essentiallyanymodelthathashiddenunitscanbemadesparse.Throughoutthisbook,wewillseemanyexamplesofsparsityregularizationusedinavarietyofcontexts.7.11BaggingandOtherEnsembleMethodsBagging(shortforbootstrapaggregating)isatechniqueforreducinggen-eralizationerrorbycombiningseveralmodels(,).TheideaistoBreiman1994trainseveraldiﬀerentmodelsseparately,thenhaveallofthemodelsvoteontheoutputfortestexamples.Thisisanexampleofageneralstrategyinmachinelearningcalledmodelaveraging.Techniquesemployingthisstrategyareknownasensemblemethods.Thereasonthatmodelaveragingworksisthatdiﬀerentmodelswillusuallynotmakeallthesameerrorsonthetestset.Considerforexampleasetofkregressionmodels.Supposethateachmodelmakesanerrorioneachexample, withtheerrorsdrawnfromazero-meanmultivariatenormaldistributionwithvariancesE[2i] =vandcovariancesE[ij] =c. Thentheerrormadebytheaveragepredictionofalltheensemblemodelsis1kii.TheexpectedsquarederroroftheensemblepredictorisE1kii2=1k2Ei2i+ji=ij(7.50)=1kv+k−1kc.(7.51)Inthecasewheretheerrorsareperfectlycorrelatedandc=v,themeansquarederrorreducestov,sothemodelaveragingdoesnothelpatall.Inthecasewheretheerrorsareperfectlyuncorrelatedandc= 0,theexpectedsquarederroroftheensembleisonly1kv.Thismeansthattheexpectedsquarederroroftheensembledecreaseslinearlywiththeensemblesize.Inotherwords,onaverage,theensemblewillperformatleastaswellasanyofitsmembers,andifthemembersmakeindependenterrors,theensemblewillperformsigniﬁcantlybetterthanitsmembers.Diﬀerentensemblemethodsconstructtheensembleofmodelsindiﬀerentways.Forexample,eachmemberoftheensemblecouldbeformedbytrainingacompletely256 CHAPTER7.REGULARIZATIONFORDEEPLEARNING 88First ensemble memberSecond ensemble memberOriginal datasetFirst resampled datasetSecond resampled datasetFigure7.5:Acartoondepictionofhowbaggingworks.Supposewetrainan8detectoronthedatasetdepictedabove,containingan8,a6anda9.Supposewemaketwodiﬀerentresampleddatasets.Thebaggingtrainingprocedureistoconstructeachofthesedatasetsbysamplingwithreplacement.Theﬁrstdatasetomitsthe9andrepeatsthe8.Onthisdataset,thedetectorlearnsthataloopontopofthedigitcorrespondstoan8.Ontheseconddataset,werepeatthe9andomitthe6.Inthiscase,thedetectorlearnsthatalooponthebottomofthedigitcorrespondstoan8.Eachoftheseindividualclassiﬁcationrulesisbrittle,butifweaveragetheiroutputthenthedetectorisrobust,achievingmaximalconﬁdenceonlywhenbothloopsofthe8arepresent.diﬀerentkindofmodelusingadiﬀerentalgorithmorobjectivefunction.Baggingisamethodthatallowsthesamekindofmodel,trainingalgorithmandobjectivefunctiontobereusedseveraltimes.Speciﬁcally,bagginginvolvesconstructingkdiﬀerentdatasets.Eachdatasethasthesamenumberofexamplesastheoriginaldataset,buteachdatasetisconstructedbysamplingwithreplacementfromtheoriginaldataset.Thismeansthat,withhighprobability,eachdatasetismissingsomeoftheexamplesfromtheoriginaldatasetandalsocontainsseveralduplicateexamples(onaveragearound2/3oftheexamplesfromtheoriginaldatasetarefoundintheresultingtrainingset,ifithasthesamesizeastheoriginal).Modeliisthentrainedondataseti.Thediﬀerencesbetweenwhichexamplesareincludedineachdatasetresultindiﬀerencesbetweenthetrainedmodels.Seeﬁgureforanexample.7.5Neuralnetworksreachawideenoughvarietyofsolutionpointsthattheycanoftenbeneﬁtfrommodelaveragingevenifallofthemodelsaretrainedonthesamedataset.Diﬀerencesinrandominitialization,randomselectionofminibatches,diﬀerencesinhyperparameters,ordiﬀerentoutcomesofnon-deterministicimple-mentationsofneuralnetworksareoftenenoughtocausediﬀerentmembersofthe257 CHAPTER7.REGULARIZATIONFORDEEPLEARNINGensembletomakepartiallyindependenterrors.Modelaveragingisanextremelypowerfulandreliablemethodforreducinggeneralizationerror.Itsuseisusuallydiscouragedwhenbenchmarkingalgorithmsforscientiﬁcpapers,becauseanymachinelearningalgorithmcanbeneﬁtsubstan-tiallyfrommodelaveragingatthepriceofincreasedcomputationandmemory.Forthisreason,benchmarkcomparisonsareusuallymadeusingasinglemodel.Machinelearningcontestsareusuallywonbymethodsusingmodelaverag-ingoverdozensofmodels.ArecentprominentexampleistheNetﬂixGrandPrize(Koren2009,).Notalltechniquesforconstructingensemblesaredesignedtomaketheensemblemoreregularizedthantheindividualmodels.Forexample,atechniquecalledboosting(FreundandSchapire1996ba,,)constructsanensemblewithhighercapacitythantheindividualmodels.Boostinghasbeenappliedtobuildensemblesofneuralnetworks(SchwenkandBengio1998,)byincrementallyaddingneuralnetworkstotheensemble.Boostinghasalsobeenappliedinterpretinganindividualneuralnetworkasanensemble(,),incrementallyaddinghiddenBengioetal.2006aunitstotheneuralnetwork.7.12DropoutDropout(Srivastava2014etal.,)providesacomputationallyinexpensivebutpowerfulmethodofregularizingabroadfamilyofmodels.Toaﬁrstapproximation,dropoutcanbethoughtofasamethodofmakingbaggingpracticalforensemblesofverymanylargeneuralnetworks.Bagginginvolvestrainingmultiplemodels,andevaluatingmultiplemodelsoneachtestexample.Thisseemsimpracticalwheneachmodelisalargeneuralnetwork,sincetrainingandevaluatingsuchnetworksiscostlyintermsofruntimeandmemory.Itiscommontouseensemblesofﬁvetotenneuralnetworks—()usedsixtowintheILSVRC—Szegedyetal.2014abutmorethanthisrapidlybecomesunwieldy.Dropoutprovidesaninexpensiveapproximationtotrainingandevaluatingabaggedensembleofexponentiallymanyneuralnetworks.Speciﬁcally,dropouttrainstheensembleconsistingofallsub-networksthatcanbeformedbyremovingnon-outputunitsfromanunderlyingbasenetwork,asillustratedinﬁgure.Inmostmodernneuralnetworks,basedonaseriesof7.6aﬃnetransformationsandnonlinearities,wecaneﬀectivelyremoveaunitfromanetworkbymultiplyingitsoutputvaluebyzero. Thisprocedurerequiressomeslightmodiﬁcationformodelssuchasradialbasisfunctionnetworks,whichtake258 CHAPTER7.REGULARIZATIONFORDEEPLEARNINGthediﬀerencebetweentheunit’sstateandsomereferencevalue.Here,wepresentthedropoutalgorithmintermsofmultiplicationbyzeroforsimplicity,butitcanbetriviallymodiﬁedtoworkwithotheroperationsthatremoveaunitfromthenetwork.Recallthattolearnwithbagging,wedeﬁnekdiﬀerentmodels,constructkdiﬀerentdatasetsbysamplingfromthetrainingsetwithreplacement,andthentrainmodeliondataseti.Dropoutaimstoapproximatethisprocess,butwithanexponentiallylargenumberofneuralnetworks.Speciﬁcally,totrainwithdropout,weuseaminibatch-basedlearningalgorithmthatmakessmallsteps,suchasstochasticgradientdescent.Eachtimeweloadanexampleintoaminibatch,werandomlysampleadiﬀerentbinarymasktoapplytoalloftheinputandhiddenunitsinthenetwork.Themaskforeachunitissampledindependentlyfromalloftheothers.Theprobabilityofsamplingamaskvalueofone(causingaunittobeincluded)isahyperparameterﬁxedbeforetrainingbegins. Itisnotafunctionofthecurrentvalueofthemodelparametersortheinputexample.Typically,aninputunitisincludedwithprobability0.8andahiddenunitisincludedwithprobability0.5.Wethenrunforwardpropagation,back-propagation,andthelearningupdateasusual.Figureillustrateshowtorunforwardpropagation7.7withdropout.Moreformally,supposethatamaskvectorµspeciﬁeswhichunitstoinclude,andJ(θµ,)deﬁnesthecostofthemodeldeﬁnedbyparametersθandmaskµ.ThendropouttrainingconsistsinminimizingEµJ(θµ,).Theexpectationcontainsexponentiallymanytermsbutwecanobtainanunbiasedestimateofitsgradientbysamplingvaluesof.µDropouttrainingisnotquitethesameasbaggingtraining.Inthecaseofbagging,themodelsareallindependent.Inthecaseofdropout,themodelsshareparameters,witheachmodelinheritingadiﬀerentsubsetofparametersfromtheparentneuralnetwork.Thisparametersharingmakesitpossibletorepresentanexponentialnumberofmodelswithatractableamountofmemory.Inthecaseofbagging,eachmodelistrainedtoconvergenceonitsrespectivetrainingset.Inthecaseofdropout,typicallymostmodelsarenotexplicitlytrainedatall—usually,themodelislargeenoughthatitwouldbeinfeasibletosampleallpossiblesub-networkswithinthelifetimeoftheuniverse.Instead,atinyfractionofthepossiblesub-networksareeachtrainedforasinglestep,andtheparametersharingcausestheremainingsub-networkstoarriveatgoodsettingsoftheparameters.Thesearetheonlydiﬀerences.Beyondthese,dropoutfollowsthebaggingalgorithm.Forexample,thetrainingsetencounteredbyeachsub-networkisindeedasubsetoftheoriginaltrainingsetsampledwithreplacement.259 CHAPTER7.REGULARIZATIONFORDEEPLEARNING y yh1h1h2h2x1x1x2x2y yh1h1h2h2x1x1x2x2y yh1h1h2h2x2x2y yh1h1h2h2x1x1y yh2h2x1x1x2x2y yh1h1x1x1x2x2y yh1h1h2h2y yx1x1x2x2y yh2h2x2x2y yh1h1x1x1y yh1h1x2x2y yh2h2x1x1y yx1x1y yx2x2y yh2h2y yh1h1y yBase network Ensemble of subnetworksFigure 7.6:Dropout trainsan ensemble consistingof allsub-networks that canbeconstructedbyremovingnon-outputunitsfromanunderlyingbasenetwork.Here,webeginwithabasenetworkwithtwovisibleunitsandtwohiddenunits.Therearesixteenpossiblesubsetsofthesefourunits.Weshowallsixteensubnetworksthatmaybeformedbydroppingoutdiﬀerentsubsetsofunitsfromtheoriginalnetwork.Inthissmallexample,alargeproportionoftheresultingnetworkshavenoinputunitsornopathconnectingtheinputtotheoutput.Thisproblembecomesinsigniﬁcantfornetworkswithwiderlayers,wheretheprobabilityofdroppingallpossiblepathsfrominputstooutputsbecomessmaller.260 CHAPTER7.REGULARIZATIONFORDEEPLEARNING ˆx1ˆx1µx1µx1x1x1ˆx2ˆx2x2x2µx2µx2h1h1h2h2µh1µh1µh2µh2ˆh1ˆh1ˆh2ˆh2y yy yh1h1h2h2x1x1x2x2 Figure7.7:Anexampleofforwardpropagationthroughafeedforwardnetworkusingdropout.(Top)Inthisexample,weuseafeedforwardnetworkwithtwoinputunits,onehiddenlayerwithtwohiddenunits,andoneoutputunit.Toperformforward(Bottom)propagationwithdropout,werandomlysampleavectorµwithoneentryforeachinputorhiddenunitinthenetwork.Theentriesofµarebinaryandaresampledindependentlyfromeachother.Theprobabilityofeachentrybeingisahyperparameter,usually10.5forthehiddenlayersand0.8fortheinput.Eachunitinthenetworkismultipliedbythecorrespondingmask,andthenforwardpropagationcontinuesthroughtherestofthenetworkasusual.Thisisequivalenttorandomlyselectingoneofthesub-networksfromﬁgureandrunningforwardpropagationthroughit.7.6261 CHAPTER7.REGULARIZATIONFORDEEPLEARNINGTomakeaprediction,abaggedensemblemustaccumulatevotesfromallofitsmembers.Werefertothisprocessasinferenceinthiscontext. Sofar,ourdescriptionofbagginganddropouthasnotrequiredthatthemodelbeexplicitlyprobabilistic.Now,weassumethatthemodel’sroleistooutputaprobabilitydistribution.Inthecaseofbagging,eachmodeliproducesaprobabilitydistributionp()i(y|x).Thepredictionoftheensembleisgivenbythearithmeticmeanofallofthesedistributions,1kki=1p()i()y|x.(7.52)Inthecaseofdropout,eachsub-modeldeﬁnedbymaskvectorµdeﬁnesaprob-abilitydistributionp(y,|xµ).Thearithmeticmeanoverallmasksisgivenbyµppy,()µ(|xµ)(7.53)wherep(µ)istheprobabilitydistributionthatwasusedtosampleµattrainingtime.Becausethissumincludesanexponentialnumberofterms,itisintractabletoevaluateexceptincaseswherethestructureofthemodelpermitssomeformofsimpliﬁcation.Sofar,deepneuralnetsarenotknowntopermitanytractablesimpliﬁcation.Instead, wecan approximatetheinferencewithsampling, byaveragingtogethertheoutputfrommanymasks.Even10-20masksareoftensuﬃcienttoobtaingoodperformance.However,thereisanevenbetterapproach,thatallowsustoobtainagoodapproximationtothepredictionsoftheentireensemble,atthecostofonlyoneforwardpropagation.Todoso,wechangetousingthegeometricmeanratherthanthearithmeticmeanoftheensemblemembers’predicteddistributions.Warde-Farley2014etal.()presentargumentsandempiricalevidencethatthegeometricmeanperformscomparablytothearithmeticmeaninthiscontext.Thegeometricmeanofmultipleprobabilitydistributionsisnotguaranteedtobeaprobabilitydistribution.Toguaranteethattheresultisaprobabilitydistribution,weimposetherequirementthatnoneofthesub-modelsassignsprobability0toanyevent,andwerenormalizetheresultingdistribution.Theunnormalizedprobabilitydistributiondeﬁneddirectlybythegeometricmeanisgivenby˜pensemble() =y|x2dµpy,(|xµ)(7.54)wheredisthenumberofunitsthatmaybedropped.Hereweuseauniformdistributionoverµtosimplifythepresentation,butnon-uniformdistributionsare262 CHAPTER7.REGULARIZATIONFORDEEPLEARNINGalsopossible.Tomakepredictionswemustre-normalizetheensemble:pensemble() =y|x˜pensemble()y|xy˜pensemble(y|x).(7.55)Akeyinsight(,)involvedindropoutisthatwecanapproxi-Hintonetal.2012cmatepensemblebyevaluatingp(y|x)inonemodel:themodelwithallunits,butwiththeweightsgoingoutofunitimultipliedbytheprobabilityofincludinguniti.Themotivationforthismodiﬁcationistocapturetherightexpectedvalueoftheoutputfromthatunit.Wecallthisapproachtheweightscalinginferencerule.Thereisnotyetanytheoreticalargumentfortheaccuracyofthisapproximateinferenceruleindeepnonlinearnetworks,butempiricallyitperformsverywell.Becauseweusuallyuseaninclusionprobabilityof12,theweightscalingruleusuallyamountstodividingtheweightsbyattheendoftraining,andthenusing2 themodelasusual.Anotherwaytoachievethesameresultistomultiplythestatesoftheunitsbyduringtraining.Eitherway,thegoalistomakesurethat2theexpectedtotalinputtoaunitattesttimeisroughlythesameastheexpectedtotalinputtothatunitattraintime,eventhoughhalftheunitsattraintimearemissingonaverage.Formanyclassesofmodelsthatdonothavenonlinearhiddenunits,theweightscalinginferenceruleisexact.Forasimpleexample,considerasoftmaxregressionclassiﬁerwithinputvariablesrepresentedbythevector:nvPy(= y|v) = softmaxWv+by.(7.56)Wecanindexintothefamilyofsub-modelsbyelement-wisemultiplicationoftheinputwithabinaryvector:dPy(= y|v;) = dsoftmaxW()+dvby.(7.57)Theensemblepredictorisdeﬁnedbyre-normalizingthegeometricmeanoverallensemblemembers’predictions:Pensemble(= ) =yy|v˜Pensemble(= )yy|vy˜Pensemble(= yy|v)(7.58)where˜Pensemble(= ) =yy|v2nd∈{}01,nPy.(= y|v;)d(7.59)263 CHAPTER7.REGULARIZATIONFORDEEPLEARNINGToseethattheweightscalingruleisexact,wecansimplify˜Pensemble:˜Pensemble(= ) =yy|v2nd∈{}01,nPy(= y|v;)d(7.60)=2nd∈{}01,nsoftmax(W()+)dvby(7.61)=2nd∈{}01,nexpWy,:()+dvbyyexpWy,:()+dvby(7.62)=2nd∈{}01,nexpWy,:()+dvby2nd∈{}01,nyexpWy,:()+dvby(7.63)Because˜Pwillbenormalized,wecansafelyignoremultiplicationbyfactorsthatareconstantwithrespectto:y˜Pensemble(= ) yy|v∝2nd∈{}01,nexpWy,:()+dvby(7.64)= exp12nd∈{}01,nWy,:()+dvby(7.65)= exp12Wy,:v+by.(7.66)Substitutingthisbackintoequationweobtainasoftmaxclassiﬁerwithweights7.5812W.Theweightscalingruleisalsoexactinothersettings,includingregressionnetworkswithconditionallynormaloutputs,anddeepnetworksthathavehiddenlayerswithoutnonlinearities.However,theweightscalingruleisonlyanapproxi-mationfordeepmodelsthathavenonlinearities.Thoughtheapproximationhasnotbeentheoreticallycharacterized,itoftenworkswell,empirically.Goodfellowetal.()foundexperimentallythattheweightscalingapproximationcanwork2013abetter(intermsofclassiﬁcationaccuracy)thanMonteCarloapproximationstotheensemblepredictor.ThisheldtrueevenwhentheMonteCarloapproximationwasallowedtosampleupto1,000sub-networks.()foundGalandGhahramani2015thatsomemodelsobtainbetterclassiﬁcationaccuracyusingtwentysamplesand264 CHAPTER7.REGULARIZATIONFORDEEPLEARNINGtheMonteCarloapproximation.Itappearsthattheoptimalchoiceofinferenceapproximationisproblem-dependent.Srivastava2014etal.()showedthatdropoutismoreeﬀectivethanotherstandardcomputationallyinexpensiveregularizers,suchasweightdecay,ﬁlternormconstraintsandsparseactivityregularization.Dropoutmayalsobecombinedwithotherformsofregularizationtoyieldafurtherimprovement.Oneadvantageofdropoutisthatitisverycomputationallycheap.UsingdropoutduringtrainingrequiresonlyO(n)computationperexampleperupdate,togeneratenrandombinarynumbersandmultiplythembythestate.Dependingontheimplementation,itmayalsorequireO(n)memorytostorethesebinarynumbersuntiltheback-propagationstage.Runninginferenceinthetrainedmodelhasthesamecostper-exampleasifdropoutwerenotused,thoughwemustpaythecostofdividingtheweightsby2oncebeforebeginningtoruninferenceonexamples.Anothersigniﬁcantadvantageofdropoutisthatitdoesnotsigniﬁcantlylimitthetypeofmodelortrainingprocedurethatcanbeused.Itworkswellwithnearlyanymodelthatusesadistributedrepresentationandcanbetrainedwithstochasticgradientdescent.Thisincludesfeedforwardneuralnetworks,probabilisticmodelssuchasrestrictedBoltzmannmachines(Srivastava2014etal.,),andrecurrentneuralnetworks(BayerandOsendorfer2014Pascanu2014a,;etal.,).Manyotherregularizationstrategiesofcomparablepowerimposemoresevererestrictionsonthearchitectureofthemodel.Thoughthecostper-stepofapplyingdropouttoaspeciﬁcmodelisnegligible,thecostofusingdropoutinacompletesystemcanbesigniﬁcant.Becausedropoutisaregularizationtechnique,itreducestheeﬀectivecapacityofamodel.Tooﬀsetthiseﬀect,wemustincreasethesizeofthemodel.Typicallytheoptimalvalidationseterrorismuchlowerwhenusingdropout,butthiscomesatthecostofamuchlargermodelandmanymoreiterationsofthetrainingalgorithm.Forverylargedatasets,regularizationconferslittlereductioningeneralizationerror. Inthesecases,thecomputationalcostofusingdropoutandlargermodelsmayoutweighthebeneﬁtofregularization.Whenextremelyfewlabeledtrainingexamplesareavailable,dropoutislesseﬀective.Bayesian neuralnetworks(, )outperform dropout ontheNeal1996AlternativeSplicingDataset(,)wherefewerthan5,000examplesXiongetal.2011areavailable(Srivastava2014etal.,).Whenadditionalunlabeleddataisavailable,unsupervisedfeaturelearningcangainanadvantageoverdropout.Wager2013etal.()showedthat,whenappliedtolinearregression,dropoutisequivalenttoL2weightdecay,withadiﬀerentweightdecaycoeﬃcientfor265 CHAPTER7.REGULARIZATIONFORDEEPLEARNINGeachinputfeature.Themagnitudeofeachfeature’sweightdecaycoeﬃcientisdeterminedbyitsvariance.Similarresultsholdforotherlinearmodels.Fordeepmodels,dropoutisnotequivalenttoweightdecay.Thestochasticityusedwhiletrainingwithdropoutisnotnecessaryfortheapproach’ssuccess.Itisjustameansofapproximatingthesumoverallsub-models.WangandManning2013()derivedanalyticalapproximationstothismarginalization.Theirapproximation,knownasfastdropoutresultedinfasterconvergencetimeduetothereducedstochasticityinthecomputationofthegradient.Thismethodcanalsobeappliedattesttime,asamoreprincipled(butalsomorecomputationallyexpensive)approximationtotheaverageoverallsub-networksthantheweightscalingapproximation.Fastdropouthasbeenusedtonearlymatchtheperformanceofstandarddropoutonsmallneuralnetworkproblems,buthasnotyetyieldedasigniﬁcantimprovementorbeenappliedtoalargeproblem.Justasstochasticityisnotnecessarytoachievetheregularizing eﬀectofdropout,itisalsonotsuﬃcient.Todemonstratethis,Warde-Farley2014etal.()designedcontrolexperimentsusingamethodcalleddropoutboostingthattheydesignedtouseexactlythesamemasknoiseastraditionaldropoutbutlackitsregularizingeﬀect.Dropoutboostingtrainstheentireensembletojointlymaximizethelog-likelihoodonthetrainingset.Inthesamesensethattraditionaldropoutisanalogoustobagging, thisapproachisanalogoustoboosting.Asintended,experimentswithdropoutboostingshowalmostnoregularizationeﬀectcomparedtotrainingtheentirenetworkasasinglemodel.Thisdemonstratesthattheinterpretationofdropoutasbagginghasvaluebeyondtheinterpretationofdropoutasrobustnesstonoise.Theregularizationeﬀectofthebaggedensembleisonlyachievedwhenthestochasticallysampledensemblemembersaretrainedtoperformwellindependentlyofeachother.Dropouthasinspiredotherstochasticapproachestotrainingexponentiallylargeensemblesofmodelsthatshareweights. DropConnectisaspecialcaseofdropoutwhereeachproductbetweenasinglescalarweightandasinglehiddenunitstateisconsideredaunitthatcanbedropped(Wan2013etal.,).Stochasticpoolingisaformofrandomizedpooling(seesection)forbuildingensembles9.3ofconvolutionalnetworkswitheachconvolutionalnetworkattendingtodiﬀerentspatiallocationsofeachfeaturemap. Sofar,dropoutremainsthemostwidelyusedimplicitensemblemethod.Oneofthekeyinsightsofdropoutisthattraininganetworkwithstochasticbehaviorandmakingpredictionsbyaveragingovermultiplestochasticdecisionsimplementsaformofbaggingwithparametersharing.Earlier, wedescribed266 CHAPTER7.REGULARIZATIONFORDEEPLEARNINGdropoutas bagginganensembleofmodelsformedbyincludingor excludingunits.However,thereisnoneedforthismodelaveragingstrategytobebasedoninclusionandexclusion.Inprinciple,anykindofrandommodiﬁcationisadmissible.Inpractice,wemustchoosemodiﬁcationfamiliesthatneuralnetworksareabletolearntoresist.Ideally,weshouldalsousemodelfamiliesthatallowafastapproximateinferencerule.Wecanthinkofanyformofmodiﬁcationparametrizedbyavectorµastraininganensembleconsistingofp(y,|xµ)forallpossiblevaluesofµ.Thereisnorequirementthatµhaveaﬁnitenumberofvalues.Forexample,µcanbereal-valued.Srivastava2014etal.()showedthatmultiplyingtheweightsbyµ∼N(1,I)canoutperformdropoutbasedonbinarymasks.BecauseE[µ] =1thestandardnetworkautomaticallyimplementsapproximateinferenceintheensemble,withoutneedinganyweightscaling.Sofarwehavedescribeddropoutpurelyasameansofperformingeﬃcient,approximatebagging.However,thereisanotherviewofdropoutthatgoesfurtherthanthis.Dropouttrainsnotjustabaggedensembleofmodels,butanensembleofmodelsthatsharehiddenunits.Thismeanseachhiddenunitmustbeabletoperformwellregardlessofwhichotherhiddenunitsareinthemodel.Hiddenunitsmustbepreparedtobeswappedandinterchangedbetweenmodels.Hintonetal.()wereinspiredbyanideafrombiology:sexualreproduction,whichinvolves2012cswappinggenesbetweentwodiﬀerentorganisms,createsevolutionarypressureforgenestobecomenotjustgood,buttobecomereadilyswappedbetweendiﬀerentorganisms.Suchgenesandsuchfeaturesareveryrobusttochangesintheirenvironmentbecausetheyarenotabletoincorrectlyadapttounusualfeaturesofanyoneorganismormodel.Dropoutthusregularizeseachhiddenunittobenotmerelyagoodfeaturebutafeaturethatisgoodinmanycontexts. Warde-Farley2014etal.()compareddropouttrainingtotrainingoflargeensemblesandconcludedthatdropoutoﬀersadditionalimprovementstogeneralizationerrorbeyondthoseobtainedbyensemblesofindependentmodels.Itisimportanttounderstandthatalargeportionofthepowerofdropoutarisesfromthefactthatthemaskingnoiseisappliedtothehiddenunits.Thiscanbeseenasaformofhighlyintelligent,adaptivedestructionoftheinformationcontentoftheinputratherthandestructionoftherawvaluesoftheinput.Forexample,ifthemodellearnsahiddenunithithatdetectsafacebyﬁndingthenose,thendroppinghicorrespondstoerasingtheinformationthatthereisanoseintheimage.Themodelmustlearnanotherhi,eitherthatredundantlyencodesthepresenceofanose,orthatdetectsthefacebyanotherfeature,suchasthemouth.Traditionalnoiseinjectiontechniquesthataddunstructurednoiseattheinputarenotabletorandomlyerasetheinformationaboutanosefromanimageofafaceunlessthemagnitudeofthenoiseissogreatthatnearlyalloftheinformationin267 CHAPTER7.REGULARIZATIONFORDEEPLEARNINGtheimageisremoved.Destroyingextractedfeaturesratherthanoriginalvaluesallowsthedestructionprocesstomakeuseofalloftheknowledgeabouttheinputdistributionthatthemodelhasacquiredsofar.Anotherimportantaspectofdropoutisthatthenoiseismultiplicative.Ifthenoisewereadditivewithﬁxedscale,thenarectiﬁedlinearhiddenunithiwithaddednoisecouldsimplylearntohavehibecomeverylargeinordertomaketheaddednoiseinsigniﬁcantbycomparison.Multiplicativenoisedoesnotallowsuchapathologicalsolutiontothenoiserobustnessproblem.Anotherdeeplearningalgorithm,batchnormalization,reparametrizesthemodelinawaythatintroducesbothadditiveandmultiplicativenoiseonthehiddenunitsattrainingtime.Theprimarypurposeofbatchnormalizationistoimproveoptimization,butthenoisecanhavearegularizingeﬀect,andsometimesmakesdropoutunnecessary.Batchnormalizationisdescribedfurtherinsection.8.7.17.13AdversarialTrainingInmanycases,neuralnetworkshavebeguntoreachhumanperformancewhenevaluatedonani.i.d.testset.Itisnaturalthereforetowonderwhetherthesemodelshaveobtainedatruehuman-levelunderstandingofthesetasks.Inordertoprobethelevelofunderstandinganetworkhasoftheunderlyingtask,wecansearchforexamplesthatthemodelmisclassiﬁes.()foundthatSzegedyetal.2014bevenneuralnetworksthatperformathumanlevelaccuracyhaveanearly100%errorrateonexamplesthatareintentionallyconstructedbyusinganoptimizationproceduretosearchforaninputxnearadatapointxsuchthatthemodeloutputisverydiﬀerentatx.Inmanycases,xcanbesosimilartoxthatahumanobservercannottellthediﬀerencebetweentheoriginalexampleandtheadversarialexample,butthenetworkcanmakehighlydiﬀerentpredictions.Seeﬁgureforanexample.7.8Adversarialexampleshavemanyimplications,forexample,incomputersecurity,thatarebeyondthescopeofthischapter. However,theyareinterestinginthecontextofregularizationbecauseonecanreducetheerrorrateontheoriginali.i.d.testsetviaadversarialtraining—trainingonadversariallyperturbedexamplesfromthetrainingset(,;Szegedyetal.2014bGoodfellow2014betal.,).Goodfellow2014betal.()showedthatoneoftheprimarycausesoftheseadversarial examplesis excessive linearity.Neural networks arebuilt out ofprimarilylinearbuildingblocks. Insomeexperimentstheoverallfunctiontheyimplementprovestobehighlylinearasaresult.Theselinearfunctionsareeasy268 CHAPTER7.REGULARIZATIONFORDEEPLEARNING+.007×=xsign(∇xJ(θx,,y))x+sign(∇xJ(θx,,y))y=“panda”“nematode”“gibbon”w/57.7%conﬁdencew/8.2%conﬁdencew/99.3%conﬁdenceFigure7.8: AdemonstrationofadversarialexamplegenerationappliedtoGoogLeNet(,)onImageNet.ByaddinganimperceptiblysmallvectorwhoseSzegedyetal.2014aelementsareequaltothesignoftheelementsofthegradientofthecostfunctionwithrespecttotheinput,wecanchangeGoogLeNet’sclassiﬁcationoftheimage.Reproducedwithpermissionfrom().Goodfellowetal.2014btooptimize.Unfortunately,thevalueofalinearfunctioncanchangeveryrapidlyifithasnumerousinputs.Ifwechangeeachinputby,thenalinearfunctionwithweightswcanchangebyasmuchas||||w1,whichcanbeaverylargeamountifwishigh-dimensional.Adversarialtrainingdiscouragesthishighlysensitivelocallylinearbehaviorbyencouragingthenetworktobelocallyconstantintheneighborhoodofthetrainingdata.Thiscanbeseenasawayofexplicitlyintroducingalocalconstancypriorintosupervisedneuralnets.Adversarialtraininghelpstoillustratethepowerofusingalargefunctionfamilyincombinationwithaggressiveregularization.Purelylinearmodels,likelogisticregression,arenotabletoresistadversarialexamplesbecausetheyareforcedtobelinear.Neuralnetworksareabletorepresentfunctionsthatcanrangefromnearlylineartonearlylocallyconstantandthushavetheﬂexibilitytocapturelineartrendsinthetrainingdatawhilestilllearningtoresistlocalperturbation.Adversarialexamplesalsoprovideameansofaccomplishingsemi-supervisedlearning.Atapointxthatisnotassociatedwithalabelinthedataset,themodelitselfassignssomelabelˆy.Themodel’slabelˆymaynotbethetruelabel,butifthemodelishighquality,thenˆyhasahighprobabilityofprovidingthetruelabel.Wecanseekanadversarialexamplexthatcausestheclassiﬁertooutputalabelywithy=ˆy.Adversarialexamplesgeneratedusingnotthetruelabelbutalabelprovidedbyatrainedmodelarecalledvirtualadversarialexamples(Miyato2015etal.,).Theclassiﬁermaythenbetrainedtoassignthesamelabeltoxandx.Thisencouragestheclassiﬁertolearnafunctionthatis269 CHAPTER7.REGULARIZATIONFORDEEPLEARNINGrobusttosmallchangesanywherealongthemanifoldwheretheunlabeleddatalies.Theassumptionmotivatingthisapproachisthatdiﬀerentclassesusuallylieondisconnectedmanifolds,andasmallperturbationshouldnotbeabletojumpfromoneclassmanifoldtoanotherclassmanifold.7.14Tangent Distance, TangentProp,and ManifoldTangentClassiﬁerManymachinelearningalgorithmsaimtoovercomethecurseofdimensionalitybyassumingthatthedataliesnearalow-dimensionalmanifold,asdescribedinsection.5.11.3Oneoftheearlyattemptstotakeadvantageofthemanifoldhypothesisisthetangentdistancealgorithm(,,).Itisanon-parametricSimardetal.19931998nearest-neighboralgorithminwhichthemetricusedisnotthegenericEuclideandistancebutonethatisderivedfromknowledgeofthemanifoldsnearwhichprobabilityconcentrates.Itisassumedthatwearetryingtoclassifyexamplesandthatexamplesonthesamemanifoldsharethesamecategory.Sincetheclassiﬁershouldbeinvarianttothelocalfactorsofvariationthatcorrespondtomovementonthemanifold,itwouldmakesensetouseasnearest-neighbordistancebetweenpointsx1andx2thedistancebetweenthemanifoldsM1andM2towhichtheyrespectivelybelong.Althoughthatmaybecomputationallydiﬃcult(itwouldrequiresolvinganoptimizationproblem,toﬁndthenearestpairofpointsonM1andM2),acheapalternativethatmakessenselocallyistoapproximateMibyitstangentplaneatxiandmeasurethedistancebetweenthetwotangents,orbetweenatangentplaneandapoint.Thatcanbeachievedbysolvingalow-dimensionallinearsystem(inthedimensionofthemanifolds).Ofcourse,thisalgorithmrequiresonetospecifythetangentvectors.Inarelatedspirit,thetangentpropalgorithm(,)(ﬁgure)Simardetal.19927.9trainsaneuralnetclassiﬁerwithanextrapenaltytomakeeachoutputf(x)oftheneuralnetlocallyinvarianttoknownfactorsofvariation.Thesefactorsofvariationcorrespondtomovementalongthemanifoldnearwhichexamplesofthesameclassconcentrate.Localinvarianceisachievedbyrequiring∇xf(x)tobeorthogonaltotheknownmanifoldtangentvectorsv()iatx,orequivalentlythatthedirectionalderivativeoffatxinthedirectionsv()ibesmallbyaddingaregularizationpenalty:ΩΩ() =fi(∇xf())xv()i2.(7.67)270 CHAPTER7.REGULARIZATIONFORDEEPLEARNINGThisregularizercanofcoursebescaledbyanappropriatehyperparameter,and,formostneuralnetworks,wewouldneedtosumovermanyoutputsratherthantheloneoutputf(x)describedhereforsimplicity.Aswiththetangentdistancealgorithm,thetangentvectorsarederivedapriori,usuallyfromtheformalknowledgeoftheeﬀectoftransformationssuchastranslation,rotation,andscalinginimages.Tangentprophasbeenusednotjustforsupervisedlearning(,)Simardetal.1992butalsointhecontextofreinforcementlearning(,).Thrun1995Tangentpropagationis closelyrelated todataset augmentation.In bothcases,theuserofthealgorithmencodeshisorherpriorknowledgeofthetaskbyspecifyingasetoftransformationsthatshouldnotaltertheoutputofthenetwork.Thediﬀerenceisthatinthecaseofdatasetaugmentation,thenetworkisexplicitlytrainedtocorrectlyclassifydistinctinputsthatwerecreatedbyapplyingmorethananinﬁnitesimalamountofthesetransformations.Tangentpropagationdoesnotrequireexplicitlyvisitinganewinputpoint.Instead,itanalyticallyregularizesthemodeltoresistperturbationinthedirectionscorrespondingtothe speciﬁed transformation.While thisanalytical approach isintellectuallyelegant,ithastwomajordrawbacks.First,itonlyregularizesthemodeltoresistinﬁnitesimalperturbation.Explicitdatasetaugmentationconfersresistancetolargerperturbations.Second,theinﬁnitesimalapproachposesdiﬃcultiesformodelsbasedonrectiﬁedlinearunits.Thesemodelscanonlyshrinktheirderivativesbyturningunitsoﬀorshrinkingtheirweights.Theyarenotabletoshrinktheirderivativesbysaturatingatahighvaluewithlargeweights,assigmoidortanhunitscan.Datasetaugmentationworkswellwithrectiﬁedlinearunitsbecausediﬀerentsubsetsofrectiﬁedunitscanactivatefordiﬀerenttransformedversionsofeachoriginalinput.Tangentpropagationisalsorelatedtodoublebackprop(DruckerandLeCun,1992)andadversarialtraining(,;,).Szegedyetal.2014bGoodfellowetal.2014bDoublebackpropregularizestheJacobiantobesmall,whileadversarialtrainingﬁndsinputsneartheoriginalinputsandtrainsthemodeltoproducethesameoutputontheseasontheoriginalinputs.Tangentpropagationanddatasetaugmentationusingmanuallyspeciﬁedtransformationsbothrequirethatthemodelshouldbeinvarianttocertainspeciﬁeddirectionsofchangeintheinput.Doublebackpropandadversarialtrainingbothrequirethatthemodelshouldbeinvarianttodirectionsofchangeintheinputsolongasthechangeissmall.Justallasdatasetaugmentationisthenon-inﬁnitesimalversionoftangentpropagation,adversarialtrainingisthenon-inﬁnitesimalversionofdoublebackprop.Themanifoldtangentclassiﬁer(,),eliminatestheneedtoRifaietal.2011cknowthetangentvectorsapriori.Aswewillseeinchapter,autoencoderscan14271 CHAPTER7.REGULARIZATIONFORDEEPLEARNING x1x2NormalTangent Figure7.9: Illustrationofthemainideaofthetangentpropalgorithm(,Simardetal.1992Rifai2011c)andmanifoldtangentclassiﬁer(etal.,),whichbothregularizetheclassiﬁeroutputfunctionf(x).Eachcurverepresentsthemanifoldforadiﬀerentclass,illustratedhereasaone-dimensionalmanifoldembeddedinatwo-dimensionalspace.Ononecurve,wehavechosenasinglepointanddrawnavectorthatistangenttotheclassmanifold(paralleltoandtouchingthemanifold)andavectorthatisnormaltotheclassmanifold(orthogonaltothemanifold).Inmultipledimensionstheremaybemanytangentdirectionsandmanynormaldirections.Weexpecttheclassiﬁcationfunctiontochangerapidlyasitmovesinthedirectionnormaltothemanifold,andnottochangeasitmovesalongtheclassmanifold.Bothtangentpropagationandthemanifoldtangentclassiﬁerregularizef(x) tonotchangeverymuchasxmovesalongthemanifold.Tangentpropagationrequirestheusertomanuallyspecifyfunctionsthatcomputethetangentdirections(suchasspecifyingthatsmalltranslationsofimagesremaininthesameclassmanifold)whilethemanifoldtangentclassiﬁerestimatesthemanifoldtangentdirectionsbytraininganautoencodertoﬁtthetrainingdata.Theuseofautoencoderstoestimatemanifoldswillbedescribedinchapter.14estimatethemanifoldtangentvectors.Themanifoldtangentclassiﬁermakesuseofthistechniquetoavoidneedinguser-speciﬁedtangentvectors. Asillustratedinﬁgure,theseestimatedtangentvectorsgobeyondtheclassicalinvariants14.10thatariseoutofthegeometryofimages(suchastranslation,rotationandscaling)andincludefactorsthatmustbelearnedbecausetheyareobject-speciﬁc(suchasmovingbodyparts).Thealgorithmproposedwiththemanifoldtangentclassiﬁeristhereforesimple:(1)useanautoencodertolearnthemanifoldstructurebyunsupervisedlearning,and(2)usethesetangentstoregularizeaneuralnetclassiﬁerasintangentprop(equation).7.67Thischapterhasdescribedmostofthegeneralstrategiesusedtoregularizeneuralnetworks.Regularizationisacentralthemeofmachinelearningandassuch272 CHAPTER7.REGULARIZATIONFORDEEPLEARNINGwillberevisitedperiodicallybymostoftheremainingchapters.Anothercentralthemeofmachinelearningisoptimization,describednext. 273 Chapter8OptimizationforTrainingDeepModelsDeeplearningalgorithmsinvolveoptimizationinmanycontexts.Forexample,performinginferenceinmodelssuchasPCAinvolvessolvinganoptimizationproblem.Weoftenuseanalyticaloptimizationtowriteproofsordesignalgorithms.Ofallofthemanyoptimizationproblemsinvolvedindeeplearning,themostdiﬃcultisneuralnetworktraining.Itisquitecommontoinvestdaystomonthsoftimeonhundredsofmachinesinordertosolveevenasingleinstanceoftheneuralnetworktrainingproblem.Becausethisproblemissoimportantandsoexpensive,aspecializedsetofoptimizationtechniqueshavebeendevelopedforsolvingit.Thischapterpresentstheseoptimizationtechniquesforneuralnetworktraining.Ifyouareunfamiliarwiththebasicprinciplesofgradient-basedoptimization,wesuggestreviewingchapter.Thatchapterincludesabriefoverviewofnumerical4optimizationingeneral.Thischapterfocusesononeparticularcaseofoptimization:ﬁndingtheparam-etersθofaneuralnetworkthatsigniﬁcantlyreduceacostfunctionJ(θ),whichtypicallyincludesaperformancemeasureevaluatedontheentiretrainingsetaswellasadditionalregularizationterms.Webeginwithadescriptionofhowoptimizationusedasatrainingalgorithmforamachinelearningtaskdiﬀersfrompureoptimization.Next,wepresentseveraloftheconcretechallengesthatmakeoptimizationofneuralnetworksdiﬃcult.Wethendeﬁneseveralpracticalalgorithms,includingbothoptimizationalgorithmsthemselvesandstrategiesforinitializingtheparameters.Moreadvancedalgorithmsadapttheirlearningratesduringtrainingorleverageinformationcontainedin274 CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSthesecondderivativesofthecostfunction.Finally,weconcludewithareviewofseveraloptimizationstrategiesthatareformedbycombiningsimpleoptimizationalgorithmsintohigher-levelprocedures.8.1HowLearningDiﬀersfromPureOptimizationOptimizationalgorithmsusedfortrainingofdeepmodelsdiﬀerfromtraditionaloptimizationalgorithmsinseveralways.Machinelearningusuallyactsindirectly.Inmostmachinelearningscenarios,wecareaboutsomeperformancemeasureP,thatisdeﬁnedwithrespecttothetestsetandmayalsobeintractable.WethereforeoptimizePonlyindirectly.WereduceadiﬀerentcostfunctionJ(θ)inthehopethatdoingsowillimproveP.Thisisincontrasttopureoptimization,whereminimizingJisagoalinandofitself.Optimizationalgorithmsfortrainingdeepmodelsalsotypicallyincludesomespecializationonthespeciﬁcstructureofmachinelearningobjectivefunctions.Typically,thecostfunctioncanbewrittenasanaverageoverthetrainingset,suchasJ() = θE()ˆx,y∼pdataLf,y,((;)xθ)(8.1)whereListheper-examplelossfunction,f(x;θ)isthepredictedoutputwhentheinputisx,ˆpdataistheempiricaldistribution.Inthesupervisedlearningcase,yisthetargetoutput.Throughoutthischapter,wedeveloptheunregularizedsupervisedcase,wheretheargumentstoLaref(x;θ)andy.However,itistrivialtoextendthisdevelopment,forexample,toincludeθorxasarguments,ortoexcludeyasarguments,inordertodevelopvariousformsofregularizationorunsupervisedlearning.Equationdeﬁnesanobjectivefunctionwithrespecttothetrainingset.We8.1wouldusuallyprefertominimizethecorrespondingobjectivefunctionwheretheexpectationistakenacrossthedatageneratingdistributionpdataratherthanjustovertheﬁnitetrainingset:J∗() = θE()x,y∼pdataLf,y.((;)xθ)(8.2)8.1.1EmpiricalRiskMinimizationThegoalofamachinelearningalgorithmistoreducetheexpectedgeneralizationerrorgivenbyequation.Thisquantityisknownasthe8.2risk.Weemphasizeherethattheexpectationistakenoverthetrueunderlyingdistributionpdata.Ifweknewthetruedistributionpdata(x,y),riskminimizationwouldbeanoptimizationtask275 CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSsolvablebyanoptimizationalgorithm.However,whenwedonotknowpdata(x,y)butonlyhaveatrainingsetofsamples,wehaveamachinelearningproblem.Thesimplestwaytoconvertamachinelearningproblembackintoanop-timizationproblemistominimizetheexpectedlossonthetrainingset.Thismeansreplacingthetruedistributionp(x,y) withtheempiricaldistributionˆp(x,y)deﬁnedbythetrainingset.WenowminimizetheempiricalriskEx,y∼ˆpdata()x,y[((;))] =Lfxθ,y1mmi=1Lf((x()i;)θ,y()i)(8.3)whereisthenumberoftrainingexamples.mThetrainingprocessbasedonminimizingthisaveragetrainingerrorisknownasempiricalriskminimization.Inthissetting,machinelearningisstillverysimilartostraightforwardoptimization.Ratherthanoptimizingtheriskdirectly,weoptimizetheempiricalrisk,andhopethattheriskdecreasessigniﬁcantlyaswell.Avarietyoftheoreticalresultsestablishconditionsunderwhichthetrueriskcanbeexpectedtodecreasebyvariousamounts.However,empiricalriskminimizationispronetooverﬁtting.Modelswithhighcapacitycansimplymemorizethetrainingset.Inmanycases,empiricalriskminimizationisnotreallyfeasible.Themosteﬀectivemodernoptimizationalgorithmsarebasedongradientdescent,butmanyusefullossfunctions,suchas0-1loss,havenousefulderivatives(thederivativeiseitherzeroorundeﬁnedeverywhere).Thesetwoproblemsmeanthat,inthecontextofdeeplearning,werarelyuseempiricalriskminimization.Instead,wemustuseaslightlydiﬀerentapproach,inwhichthequantitythatweactuallyoptimizeisevenmorediﬀerentfromthequantitythatwetrulywanttooptimize.8.1.2SurrogateLossFunctionsandEarlyStoppingSometimes,thelossfunctionweactuallycareabout(sayclassiﬁcationerror)isnotonethatcanbeoptimizedeﬃciently.Forexample,exactlyminimizingexpected0-1lossistypicallyintractable(exponentialintheinputdimension),evenforalinearclassiﬁer(MarcotteandSavard1992,).Insuchsituations,onetypicallyoptimizesasurrogatelossfunctioninstead,whichactsasaproxybuthasadvantages.Forexample,thenegativelog-likelihoodofthecorrectclassistypicallyusedasasurrogateforthe0-1loss.Thenegativelog-likelihoodallowsthemodeltoestimatetheconditionalprobabilityoftheclasses,giventheinput,andifthemodelcandothatwell,thenitcanpicktheclassesthatyieldtheleastclassiﬁcationerrorinexpectation.276 CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSInsomecases,asurrogatelossfunctionactuallyresultsinbeingabletolearnmore.Forexample,thetestset0-1lossoftencontinuestodecreaseforalongtimeafterthetrainingset0-1losshasreachedzero,whentrainingusingthelog-likelihoodsurrogate.Thisisbecauseevenwhentheexpected0-1lossiszero,onecanimprovetherobustnessoftheclassiﬁerbyfurtherpushingtheclassesapartfromeachother,obtainingamoreconﬁdentandreliableclassiﬁer,thusextractingmoreinformationfromthetrainingdatathanwouldhavebeenpossiblebysimplyminimizingtheaverage0-1lossonthetrainingset.Averyimportantdiﬀerencebetweenoptimizationingeneralandoptimizationasweuseitfortrainingalgorithmsisthattrainingalgorithmsdonotusuallyhaltatalocalminimum.Instead,amachinelearningalgorithmusuallyminimizesasurrogatelossfunctionbuthaltswhenaconvergencecriterionbasedonearlystopping(section)issatisﬁed.Typicallytheearlystoppingcriterionisbased7.8onthetrueunderlyinglossfunction,suchas0-1lossmeasuredonavalidationset,andisdesignedtocausethealgorithmtohaltwheneveroverﬁttingbeginstooccur.Trainingoftenhaltswhilethesurrogatelossfunctionstillhaslargederivatives,whichisverydiﬀerentfromthepureoptimizationsetting,whereanoptimizationalgorithmisconsideredtohaveconvergedwhenthegradientbecomesverysmall.8.1.3BatchandMinibatchAlgorithmsOneaspectofmachinelearningalgorithmsthatseparatesthemfromgeneraloptimizationalgorithmsisthattheobjectivefunctionusuallydecomposesasasumoverthetrainingexamples.Optimizationalgorithmsformachinelearningtypicallycomputeeachupdatetotheparametersbasedonanexpectedvalueofthecostfunctionestimatedusingonlyasubsetofthetermsofthefullcostfunction.Forexample,maximumlikelihoodestimationproblems,whenviewedinlogspace,decomposeintoasumovereachexample:θML= argmaxθmi=1logpmodel(x()i,y()i;)θ.(8.4)Maximizingthissumisequivalenttomaximizingtheexpectationovertheempiricaldistributiondeﬁnedbythetrainingset:J() = θEx,y∼ˆpdatalogpmodel(;)x,yθ.(8.5)MostofthepropertiesoftheobjectivefunctionJusedbymostofouropti-mizationalgorithmsarealsoexpectationsoverthetrainingset.Forexample,the277 CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSmostcommonlyusedpropertyisthegradient:∇θJ() = θEx,y∼ˆpdata∇θlogpmodel(;)x,yθ.(8.6)Computing thisexpectation exactly isvery expensive because it requiresevaluatingthemodeloneveryexampleintheentiredataset.Inpractice,wecancomputetheseexpectationsbyrandomlysamplingasmallnumberofexamplesfromthedataset,thentakingtheaverageoveronlythoseexamples.Recallthatthestandarderrorofthemean(equation)estimatedfrom5.46nsamplesisgivenbyσ/√n,whereσisthetruestandarddeviationofthevalueofthesamples.Thedenominatorof√nshowsthattherearelessthanlinearreturnstousingmoreexamplestoestimatethegradient.Comparetwohypotheticalestimatesofthegradient,onebasedon100examplesandanotherbasedon10,000examples.Thelatterrequires100timesmorecomputationthantheformer,butreducesthestandarderrorofthemeanonlybyafactorof10.Mostoptimizationalgorithmsconvergemuchfaster(intermsoftotalcomputation,notintermsofnumberofupdates)iftheyareallowedtorapidlycomputeapproximateestimatesofthegradientratherthanslowlycomputingtheexactgradient.Anotherconsiderationmotivatingstatisticalestimationofthegradientfromasmallnumberofsamplesisredundancyinthetrainingset.Intheworstcase,allmsamplesinthetrainingsetcouldbeidenticalcopiesofeachother.Asampling-basedestimateofthegradientcouldcomputethecorrectgradientwithasinglesample,usingmtimeslesscomputationthanthenaiveapproach.Inpractice,weareunlikelytotrulyencounterthisworst-casesituation,butwemayﬁndlargenumbersofexamplesthatallmakeverysimilarcontributionstothegradient.Optimizationalgorithmsthatusetheentiretrainingsetarecalledbatchordeterministicgradientmethods,becausetheyprocessallofthetrainingexamplessimultaneouslyinalargebatch.Thisterminologycanbesomewhatconfusingbecausetheword“batch”isalsooftenusedtodescribetheminibatchusedbyminibatchstochasticgradientdescent.Typicallytheterm“batchgradientdescent”impliestheuseofthefulltrainingset,whiletheuseoftheterm“batch”todescribeagroupofexamplesdoesnot. Forexample,itisverycommontousetheterm“batchsize”todescribethesizeofaminibatch.Optimizationalgorithmsthatuseonlyasingleexampleatatimearesometimescalledstochasticorsometimesonlinemethods.Thetermonlineisusuallyreservedforthecasewheretheexamplesaredrawnfromastreamofcontinuallycreatedexamplesratherthanfromaﬁxed-sizetrainingsetoverwhichseveralpassesaremade.Mostalgorithmsusedfordeeplearningfallsomewhereinbetween,usingmore278 CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSthanonebutlessthanallofthetrainingexamples.Theseweretraditionallycalledminibatchorminibatchstochasticmethodsanditisnowcommontosimplycallthemstochasticmethods.Thecanonicalexampleofastochasticmethodisstochasticgradientdescent,presentedindetailinsection.8.3.1Minibatchsizesaregenerallydrivenbythefollowingfactors:•Largerbatchesprovideamoreaccurateestimateofthegradient,butwithlessthanlinearreturns.•Multicorearchitecturesareusuallyunderutilizedbyextremelysmallbatches.Thismotivatesusingsomeabsoluteminimumbatchsize,belowwhichthereisnoreductioninthetimetoprocessaminibatch.•Ifallexamplesinthebatcharetobeprocessedinparallel(asistypicallythecase),thentheamountofmemoryscaleswiththebatchsize.Formanyhardwaresetupsthisisthelimitingfactorinbatchsize.•Somekindsofhardwareachievebetterruntimewithspeciﬁcsizesofarrays.EspeciallywhenusingGPUs,itiscommonforpowerof2batchsizestooﬀerbetterruntime.Typicalpowerof2batchsizesrangefrom32to256,with16sometimesbeingattemptedforlargemodels.•Smallbatchescanoﬀeraregularizingeﬀect(,),WilsonandMartinez2003perhapsduetothenoisetheyaddtothelearningprocess.Generalizationerrorisoftenbestforabatchsizeof1. Trainingwithsuchasmallbatchsizemightrequireasmalllearningratetomaintainstabilityduetothehighvarianceintheestimateofthegradient.Thetotalruntimecanbeveryhighduetotheneedtomakemoresteps,bothbecauseofthereducedlearningrateandbecauseittakesmorestepstoobservetheentiretrainingset.Diﬀerentkindsofalgorithmsusediﬀerentkindsofinformationfromthemini-batchindiﬀerentways.Somealgorithmsaremoresensitivetosamplingerrorthanothers,eitherbecausetheyuseinformationthatisdiﬃculttoestimateaccuratelywithfewsamples,orbecausetheyuseinformationinwaysthatamplifysamplingerrorsmore.Methodsthatcomputeupdatesbasedonlyonthegradientgareusuallyrelativelyrobustandcanhandlesmallerbatchsizeslike100.Second-ordermethods,whichusealsotheHessianmatrixHandcomputeupdatessuchasH−1g,typicallyrequiremuchlargerbatchsizeslike10,000.TheselargebatchsizesarerequiredtominimizeﬂuctuationsintheestimatesofH−1g.SupposethatHisestimatedperfectlybuthasapoorconditionnumber.Multiplicationby279 CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSHoritsinverseampliﬁespre-existingerrors,inthiscase,estimationerrorsing.VerysmallchangesintheestimateofgcanthuscauselargechangesintheupdateH−1g,evenifHwereestimatedperfectly.Ofcourse,Hwillbeestimatedonlyapproximately,sotheupdateH−1gwillcontainevenmoreerrorthanwewouldpredictfromapplyingapoorlyconditionedoperationtotheestimateof.gItisalsocrucialthattheminibatchesbeselectedrandomly.Computinganunbiasedestimateoftheexpectedgradientfromasetofsamplesrequiresthatthosesamplesbeindependent.Wealsowishfortwosubsequentgradientestimatestobeindependentfromeachother,sotwosubsequentminibatchesofexamplesshouldalsobeindependentfromeachother.Manydatasetsaremostnaturallyarrangedinawaywheresuccessiveexamplesarehighlycorrelated.Forexample,wemighthaveadatasetofmedicaldatawithalonglistofbloodsampletestresults.Thislistmightbearrangedsothatﬁrstwehaveﬁvebloodsamplestakenatdiﬀerenttimesfromtheﬁrstpatient,thenwehavethreebloodsamplestakenfromthesecondpatient,thenthebloodsamplesfromthethirdpatient,andsoon.Ifweweretodrawexamplesinorderfromthislist,theneachofourminibatcheswouldbeextremelybiased,becauseitwouldrepresentprimarilyonepatientoutofthemanypatientsinthedataset.Incasessuchasthesewheretheorderofthedatasetholdssomesigniﬁcance,itisnecessarytoshuﬄetheexamplesbeforeselectingminibatches.Forverylargedatasets,forexampledatasetscontainingbillionsofexamplesinadatacenter,itcanbeimpracticaltosampleexamplestrulyuniformlyatrandomeverytimewewanttoconstructaminibatch.Fortunately,inpracticeitisusuallysuﬃcienttoshuﬄetheorderofthedatasetonceandthenstoreitinshuﬄedfashion.Thiswillimposeaﬁxedsetofpossibleminibatchesofconsecutiveexamplesthatallmodelstrainedthereafterwilluse,andeachindividualmodelwillbeforcedtoreusethisorderingeverytimeitpassesthroughthetrainingdata.However,thisdeviationfromtruerandomselectiondoesnotseemtohaveasigniﬁcantdetrimentaleﬀect.Failingtoevershuﬄetheexamplesinanywaycanseriouslyreducetheeﬀectivenessofthealgorithm.Manyoptimizationproblemsinmachinelearningdecomposeoverexampleswellenoughthatwecancomputeentireseparateupdatesoverdiﬀerentexamplesinparallel.Inotherwords,wecancomputetheupdatethatminimizesJ(X)foroneminibatchofexamplesXatthesametimethatwecomputetheupdateforseveralotherminibatches.Suchasynchronousparalleldistributedapproachesarediscussedfurtherinsection.12.1.3Aninterestingmotivationforminibatchstochasticgradientdescentisthatitfollowsthegradientofthetruegeneralizationerror(equation)solongasno8.2examplesarerepeated.Mostimplementationsofminibatchstochasticgradient280 CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSdescentshuﬄethedatasetonceandthenpassthroughitmultipletimes.Ontheﬁrstpass,eachminibatchisusedtocomputeanunbiasedestimateofthetruegeneralizationerror.Onthesecondpass,theestimatebecomesbiasedbecauseitisformedbyre-samplingvaluesthathavealreadybeenused,ratherthanobtainingnewfairsamplesfromthedatageneratingdistribution.Thefactthatstochasticgradientdescentminimizesgeneralizationerroriseasiesttoseeintheonlinelearningcase,whereexamplesorminibatchesaredrawnfromastreamofdata.Inotherwords,insteadofreceivingaﬁxed-sizetrainingset,thelearnerissimilartoalivingbeingwhoseesanewexampleateachinstant,witheveryexample(x,y)comingfromthedatageneratingdistributionpdata(x,y).Inthisscenario,examplesareneverrepeated;everyexperienceisafairsamplefrompdata.Theequivalenceiseasiesttoderivewhenbothxandyarediscrete. Inthiscase,thegeneralizationerror(equation)canbewrittenasasum8.2J∗() =θxypdata()((;))x,yLfxθ,y,(8.7)withtheexactgradientg= ∇θJ∗() =θxypdata()x,y∇θLf,y.((;)xθ)(8.8)Wehavealreadyseenthesamefactdemonstratedforthelog-likelihoodinequa-tionandequation;weobservenowthatthisholdsforotherfunctions8.58.6Lbesidesthelikelihood.Asimilarresultcanbederivedwhenxandyarecontinuous,undermildassumptionsregardingpdataand.LHence, wecanobtainanunbiasedestimatoroftheexactgradientof thegeneralizationerrorbysamplingaminibatchofexamples{x(1),...x()m}withcor-respondingtargetsy()ifromthedatageneratingdistributionpdata,andcomputingthegradientofthelosswithrespecttotheparametersforthatminibatch:ˆg=1m∇θiLf((x()i;)θ,y()i).(8.9)UpdatinginthedirectionofθˆgperformsSGDonthegeneralizationerror.Ofcourse, thisinterpretationonly applieswhenexamplesarenotreused.Nonetheless,itisusuallybesttomakeseveralpassesthroughthetrainingset,unlessthetrainingsetisextremelylarge. Whenmultiplesuchepochsareused,onlytheﬁrstepochfollowstheunbiasedgradientofthegeneralizationerror,but281 CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSofcourse,theadditionalepochsusuallyprovideenoughbeneﬁtduetodecreasedtrainingerrortooﬀsettheharmtheycausebyincreasingthegapbetweentrainingerrorandtesterror.Withsomedatasetsgrowingrapidlyinsize,fasterthancomputingpower,itisbecomingmorecommonformachinelearningapplicationstouseeachtrainingexampleonlyonceoreventomakeanincompletepassthroughthetrainingset.Whenusinganextremelylargetrainingset,overﬁttingisnotanissue,sounderﬁttingandcomputationaleﬃciencybecomethepredominantconcerns.Seealso()foradiscussionoftheeﬀectofcomputationalBottouandBousquet2008bottlenecksongeneralizationerror,asthenumberoftrainingexamplesgrows.8.2ChallengesinNeuralNetworkOptimizationOptimizationingeneralisanextremelydiﬃculttask.Traditionally,machinelearninghasavoidedthediﬃcultyofgeneraloptimizationbycarefullydesigningtheobjectivefunctionandconstraintstoensurethattheoptimizationproblemisconvex.Whentrainingneuralnetworks,wemustconfrontthegeneralnon-convexcase.Evenconvexoptimizationisnotwithoutitscomplications.Inthissection,wesummarizeseveralofthemostprominentchallengesinvolvedinoptimizationfortrainingdeepmodels.8.2.1Ill-ConditioningSomechallengesariseevenwhenoptimizingconvexfunctions.Ofthese,themostprominentisill-conditioningoftheHessianmatrixH.Thisisaverygeneralprobleminmostnumericaloptimization,convexorotherwise,andisdescribedinmoredetailinsection.4.3.1Theill-conditioningproblemisgenerallybelievedtobepresentinneuralnetworktrainingproblems.Ill-conditioningcanmanifestbycausingSGDtoget“stuck”inthesensethatevenverysmallstepsincreasethecostfunction.Recallfromequationthatasecond-orderTaylorseriesexpansionofthe4.9costfunctionpredictsthatagradientdescentstepofwilladd−g122gHgg−g(8.10)tothecost.Ill-conditioningofthegradientbecomesaproblemwhen122gHgexceedsgg. Todeterminewhetherill-conditioningisdetrimentaltoaneuralnetwork trainingtask, one canmonitorthe squaredgradientnormggand282 CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELS −50050100150200250Trainingtime(epochs)−20246810121416Gradient norm050100150200250Trainingtime(epochs)01.02.03.04.05.06.07.08.09.10.Classiﬁcationerrorrate Figure8.1:Gradientdescentoftendoesnotarriveatacriticalpointofanykind.Inthisexample,thegradientnormincreasesthroughouttrainingofaconvolutionalnetworkusedforobjectdetection.(Left)Ascatterplotshowinghowthenormsofindividualgradientevaluationsaredistributedovertime.Toimprovelegibility,onlyonegradientnormisplottedperepoch.Therunningaverageofallgradientnormsisplottedasasolidcurve.Thegradientnormclearlyincreasesovertime,ratherthandecreasingaswewouldexpectifthetrainingprocessconvergedtoacriticalpoint.Despitetheincreasing(Right)gradient,thetrainingprocessisreasonablysuccessful.Thevalidationsetclassiﬁcationerrordecreasestoalowlevel.thegHgterm.Inmanycases,thegradientnormdoesnotshrinksigniﬁcantlythroughoutlearning,butthegHgtermgrowsbymorethananorderofmagnitude.Theresultisthatlearningbecomesveryslowdespitethepresenceofastronggradientbecausethelearningratemustbeshrunktocompensateforevenstrongercurvature.Figureshowsanexampleofthegradientincreasingsigniﬁcantly8.1duringthesuccessfultrainingofaneuralnetwork.Thoughill-conditioningispresentinothersettingsbesidesneuralnetworktraining,someofthetechniquesusedtocombatitinothercontextsarelessapplicabletoneuralnetworks.Forexample,Newton’smethodisanexcellenttoolforminimizingconvexfunctionswithpoorlyconditionedHessianmatrices,butinthesubsequentsectionswewillarguethatNewton’smethodrequiressigniﬁcantmodiﬁcationbeforeitcanbeappliedtoneuralnetworks.8.2.2LocalMinimaOneofthemostprominentfeaturesofaconvexoptimizationproblemisthatitcanbereducedtotheproblemofﬁndingalocalminimum.Anylocalminimumis283 CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSguaranteedtobeaglobalminimum.Someconvexfunctionshaveaﬂatregionatthebottomratherthanasingleglobalminimumpoint,butanypointwithinsuchaﬂatregionisanacceptablesolution.Whenoptimizingaconvexfunction,weknowthatwehavereachedagoodsolutionifweﬁndacriticalpointofanykind.Withnon-convexfunctions,suchasneuralnets,itispossibletohavemanylocalminima.Indeed,nearlyanydeepmodelisessentiallyguaranteedtohaveanextremelylargenumberoflocalminima.However,aswewillsee,thisisnotnecessarilyamajorproblem.Neuralnetworksandanymodelswithmultipleequivalentlyparametrizedlatentvariablesallhavemultiplelocalminimabecauseofthemodelidentiﬁabilityproblem.Amodelissaidtobeidentiﬁableifasuﬃcientlylargetrainingsetcanruleoutallbutonesettingofthemodel’sparameters.Modelswithlatentvariablesareoftennotidentiﬁablebecausewecanobtainequivalentmodelsbyexchanginglatentvariableswitheachother.Forexample,wecouldtakeaneuralnetworkandmodifylayer1byswappingtheincomingweightvectorforunitiwiththeincomingweightvectorforunitj,thendoingthesamefortheoutgoingweightvectors.Ifwehavemlayerswithnunitseach,thentherearen!mwaysofarrangingthehiddenunits.Thiskindofnon-identiﬁabilityisknownasweightspacesymmetry.Inadditiontoweightspacesymmetry,manykindsofneuralnetworkshaveadditionalcausesofnon-identiﬁability.Forexample,inanyrectiﬁedlinearormaxoutnetwork,wecanscalealloftheincomingweightsandbiasesofaunitbyαifwealsoscaleallofitsoutgoingweightsby1α.Thismeansthat—ifthecostfunctiondoesnotincludetermssuchasweightdecaythatdependdirectlyontheweightsratherthanthemodels’outputs—everylocalminimumofarectiﬁedlinearormaxoutnetworkliesonan(mn×)-dimensionalhyperbolaofequivalentlocalminima.Thesemodelidentiﬁabilityissuesmeanthattherecanbeanextremelylargeorevenuncountablyinﬁniteamountoflocalminimainaneuralnetworkcostfunction.However,alloftheselocalminimaarisingfromnon-identiﬁabilityareequivalenttoeachotherincostfunctionvalue.Asaresult,theselocalminimaarenotaproblematicformofnon-convexity.Localminimacanbeproblematiciftheyhavehighcostincomparisontotheglobalminimum.Onecanconstructsmallneuralnetworks,evenwithouthiddenunits,thathavelocalminimawithhighercostthantheglobalminimum(SontagandSussman1989Brady1989GoriandTesi1992,;etal.,;,).Iflocalminimawithhighcostarecommon,thiscouldposeaseriousproblemforgradient-basedoptimizationalgorithms.Itremainsanopenquestionwhethertherearemanylocalminimaofhighcost284 CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSfornetworksofpracticalinterestandwhetheroptimizationalgorithmsencounterthem.Formanyyears,mostpractitionersbelievedthatlocalminimawereacommonproblemplaguingneuralnetworkoptimization.Today,thatdoesnotappeartobethecase.Theproblemremainsanactiveareaofresearch,butexpertsnowsuspectthat,forsuﬃcientlylargeneuralnetworks,mostlocalminimahavealowcostfunctionvalue,andthatitisnotimportanttoﬁndatrueglobalminimumratherthantoﬁndapointinparameterspacethathaslowbutnotminimalcost(,;,;,;Saxeetal.2013Dauphinetal.2014Goodfellowetal.2015Choromanskaetal.,).2014Manypractitionersattributenearlyalldiﬃcultywithneuralnetworkoptimiza-tiontolocalminima.Weencouragepractitionerstocarefullytestforspeciﬁcproblems.Atestthatcanruleoutlocalminimaastheproblemistoplotthenormofthegradientovertime.Ifthenormofthegradientdoesnotshrinktoinsigniﬁcantsize,theproblemisneitherlocalminimanoranyotherkindofcriticalpoint.Thiskindofnegativetestcanruleoutlocalminima.Inhighdimensionalspaces,itcanbeverydiﬃculttopositivelyestablishthatlocalminimaaretheproblem.Manystructuresotherthanlocalminimaalsohavesmallgradients.8.2.3Plateaus,SaddlePointsandOtherFlatRegionsFormanyhigh-dimensionalnon-convexfunctions,localminima(andmaxima)areinfactrarecomparedtoanotherkindofpointwithzerogradient:asaddlepoint.Somepointsaroundasaddlepointhavegreatercostthanthesaddlepoint,whileothershavealowercost. Atasaddlepoint,theHessianmatrixhasbothpositiveandnegativeeigenvalues.Pointslyingalongeigenvectorsassociatedwithpositiveeigenvalueshavegreatercostthanthesaddlepoint,whilepointslyingalongnegativeeigenvalueshavelowervalue.Wecanthinkofasaddlepointasbeingalocalminimumalongonecross-sectionofthecostfunctionandalocalmaximumalonganothercross-section.Seeﬁgureforanillustration.4.5Manyclasses ofrandomfunctionsexhibitthefollowingbehavior:inlow-dimensionalspaces,localminimaarecommon.Inhigherdimensionalspaces,localminimaarerareandsaddlepointsaremorecommon.Forafunctionf:Rn→Rofthistype,theexpectedratioofthenumberofsaddlepointstolocalminimagrowsexponentiallywithn.Tounderstandtheintuitionbehindthisbehavior,observethattheHessianmatrixatalocalminimumhasonlypositiveeigenvalues. TheHessianmatrixatasaddlepointhasamixtureofpositiveandnegativeeigenvalues.Imaginethatthesignofeacheigenvalueisgeneratedbyﬂippingacoin.Inasingledimension,itiseasytoobtainalocalminimumbytossingacoinandgettingheadsonce.Inn-dimensionalspace,itisexponentiallyunlikelythatallncointosseswill285 CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSbeheads.See()forareviewoftherelevanttheoreticalwork.Dauphinetal.2014AnamazingpropertyofmanyrandomfunctionsisthattheeigenvaluesoftheHessianbecomemorelikelytobepositiveaswereachregionsoflowercost. Inourcointossinganalogy,thismeanswearemorelikelytohaveourcoincomeupheadsntimesifweareatacriticalpointwithlowcost. Thismeansthatlocalminimaaremuchmorelikelytohavelowcostthanhighcost.Criticalpointswithhighcostarefarmorelikelytobesaddlepoints.Criticalpointswithextremelyhighcostaremorelikelytobelocalmaxima.Thishappensformanyclassesofrandomfunctions.Doesithappenforneuralnetworks?()showedtheoreticallythatshallowautoencodersBaldiandHornik1989(feedforwardnetworkstrainedtocopytheirinputtotheiroutput,describedinchapter)withnononlinearitieshaveglobalminimaandsaddlepointsbutno14localminimawithhighercostthantheglobalminimum.Theyobservedwithoutproofthattheseresultsextendtodeepernetworkswithoutnonlinearities.Theoutputofsuchnetworksisalinearfunctionoftheirinput,buttheyareusefultostudyasamodelofnonlinearneuralnetworksbecausetheirlossfunctionisanon-convexfunctionoftheirparameters.Suchnetworksareessentiallyjustmultiplematricescomposedtogether.()providedexactsolutionsSaxeetal.2013tothecompletelearningdynamicsinsuchnetworksandshowedthatlearninginthesemodelscapturesmanyofthequalitativefeaturesobservedinthetrainingofdeepmodelswithnonlinearactivationfunctions.()showedDauphinetal.2014experimentallythatrealneuralnetworksalsohavelossfunctionsthatcontainverymanyhigh-costsaddlepoints.Choromanska2014etal.()providedadditionaltheoreticalarguments,showingthatanotherclassofhigh-dimensionalrandomfunctionsrelatedtoneuralnetworksdoessoaswell.Whataretheimplicationsoftheproliferationofsaddlepointsfortrainingalgo-rithms?Forﬁrst-orderoptimizationalgorithmsthatuseonlygradientinformation,thesituationisunclear.Thegradientcanoftenbecomeverysmallnearasaddlepoint.Ontheotherhand,gradientdescentempiricallyseemstobeabletoescapesaddlepointsinmanycases.()providedvisualizationsofGoodfellowetal.2015severallearningtrajectoriesofstate-of-the-artneuralnetworks,withanexamplegiveninﬁgure.Thesevisualizationsshowaﬂatteningofthecostfunctionnear8.2aprominentsaddlepointwheretheweightsareallzero,buttheyalsoshowthegradientdescenttrajectoryrapidlyescapingthisregion.()Goodfellowetal.2015alsoarguethatcontinuous-timegradientdescentmaybeshownanalyticallytoberepelledfrom,ratherthanattractedto,anearbysaddlepoint,butthesituationmaybediﬀerentformorerealisticusesofgradientdescent.ForNewton’smethod, itisclearthatsaddlepointsconstituteaproblem.286 CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELS Projection2ofθProjection1ofθJ()θFigure8.2:Avisualizationofthecostfunctionofaneuralnetwork.ImageadaptedwithpermissionfromGoodfellow2015etal.(). Thesevisualizationsappearsimilarforfeedforwardneuralnetworks,convolutionalnetworks,andrecurrentnetworksappliedtorealobjectrecognitionandnaturallanguageprocessingtasks.Surprisingly,thesevisualizationsusuallydonotshowmanyconspicuousobstacles. Priortothesuccessofstochasticgradientdescentfortrainingverylargemodelsbeginninginroughly2012,neuralnetcostfunctionsurfacesweregenerallybelievedtohavemuchmorenon-convexstructurethanisrevealedbytheseprojections. Theprimaryobstaclerevealedbythisprojectionisasaddlepointofhighcostnearwheretheparametersareinitialized,but,asindicatedbythebluepath,theSGDtrainingtrajectoryescapesthissaddlepointreadily.Mostoftrainingtimeisspenttraversingtherelativelyﬂatvalleyofthecostfunction,whichmaybeduetohighnoiseinthegradient,poorconditioningoftheHessianmatrixinthisregion,orsimplytheneedtocircumnavigatethetall“mountain”visibleintheﬁgureviaanindirectarcingpath. 287 CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSGradientdescentisdesignedtomove“downhill”andisnotexplicitlydesignedtoseekacriticalpoint.Newton’smethod,however,isdesignedtosolveforapointwherethegradientiszero.Withoutappropriatemodiﬁcation,itcanjumptoasaddlepoint.Theproliferationofsaddlepointsinhighdimensionalspacespresumablyexplainswhysecond-ordermethodshavenotsucceededinreplacinggradientdescentforneuralnetworktraining.()introducedaDauphinetal.2014saddle-freeNewtonmethodforsecond-orderoptimizationandshowedthatitimprovessigniﬁcantlyoverthetraditionalversion.Second-ordermethodsremaindiﬃculttoscaletolargeneuralnetworks,butthissaddle-freeapproachholdspromiseifitcouldbescaled.Thereareotherkindsofpointswithzerogradientbesidesminimaandsaddlepoints.Therearealsomaxima, whicharemuchlikesaddlepointsfromtheperspectiveofoptimization—manyalgorithmsarenotattractedtothem, butunmodiﬁedNewton’smethodis.Maximaofmanyclassesofrandomfunctionsbecomeexponentiallyrareinhighdimensionalspace,justlikeminimado.Theremayalsobewide,ﬂatregionsofconstantvalue.Intheselocations,thegradientandalsotheHessianareallzero.Suchdegeneratelocationsposemajorproblemsforallnumericaloptimizationalgorithms.Inaconvexproblem,awide,ﬂatregionmustconsistentirelyofglobalminima,butinageneraloptimizationproblem,sucharegioncouldcorrespondtoahighvalueoftheobjectivefunction.8.2.4CliﬀsandExplodingGradientsNeuralnetworkswithmanylayersoftenhaveextremelysteepregionsresemblingcliﬀs,asillustratedinﬁgure.Theseresultfromthemultiplicationofseveral8.3largeweightstogether.Onthefaceofanextremelysteepcliﬀstructure,thegradientupdatestepcanmovetheparametersextremelyfar,usuallyjumpingoﬀofthecliﬀstructurealtogether. 288 CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELS Figure8.3:Theobjectivefunctionforhighlynonlineardeepneuralnetworksorforrecurrentneuralnetworksoftencontainssharpnonlinearitiesinparameterspaceresultingfromthemultiplicationofseveralparameters.Thesenonlinearitiesgiverisetoveryhighderivativesinsomeplaces.Whentheparametersgetclosetosuchacliﬀregion,agradientdescentupdatecancatapulttheparametersveryfar,possiblylosingmostoftheoptimizationworkthathadbeendone. FigureadaptedwithpermissionfromPascanuetal.().2013Thecliﬀcanbedangerouswhetherweapproachitfromaboveorfrombelow,butfortunatelyitsmostseriousconsequencescanbeavoidedusingthegradientclippingheuristicdescribedinsection.Thebasicideaistorecallthat10.11.1thegradientdoesnotspecifytheoptimalstepsize,butonlytheoptimaldirectionwithinaninﬁnitesimalregion.Whenthetraditionalgradientdescentalgorithmproposestomakeaverylargestep,thegradientclippingheuristicintervenestoreducethestepsizetobesmallenoughthatitislesslikelytogooutsidetheregionwherethegradientindicatesthedirectionofapproximatelysteepestdescent.Cliﬀstructuresaremostcommoninthecostfunctionsforrecurrentneuralnetworks,becausesuchmodelsinvolveamultiplicationofmanyfactors,withonefactorforeachtimestep.Longtemporalsequencesthusincuranextremeamountofmultiplication.8.2.5Long-TermDependenciesAnotherdiﬃcultythatneuralnetworkoptimizationalgorithmsmustovercomearises when thecomputational graph becomes extremely deep.Feedforwardnetworkswithmanylayershavesuchdeepcomputationalgraphs.Sodorecurrentnetworks,describedinchapter,whichconstructverydeepcomputationalgraphs10289 CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSbyrepeatedlyapplyingthesameoperationateachtimestepofalongtemporalsequence.Repeatedapplicationofthesameparametersgivesrisetoespeciallypronounceddiﬃculties.Forexample,supposethatacomputationalgraphcontainsapaththatconsistsofrepeatedlymultiplyingbyamatrixW.Aftertsteps,thisisequivalenttomul-tiplyingbyWt.SupposethatWhasaneigendecompositionW=Vdiag(λ)V−1.Inthissimplecase,itisstraightforwardtoseethatWt=VλVdiag()−1t= ()VdiagλtV−1.(8.11)Anyeigenvaluesλithatarenotnearanabsolutevalueofwilleitherexplodeifthey1aregreaterthaninmagnitudeorvanishiftheyarelessthaninmagnitude.The11vanishingandexplodinggradientproblemreferstothefactthatgradientsthroughsuchagrapharealsoscaledaccordingtodiag(λ)t.Vanishinggradientsmakeitdiﬃculttoknowwhichdirectiontheparametersshouldmovetoimprovethecostfunction,whileexplodinggradientscanmakelearningunstable.Thecliﬀstructuresdescribedearlierthatmotivategradientclippingareanexampleoftheexplodinggradientphenomenon.TherepeatedmultiplicationbyWateachtimestepdescribedhereisverysimilartothepowermethodalgorithmusedtoﬁndthelargesteigenvalueofamatrixWandthecorrespondingeigenvector.FromthispointofviewitisnotsurprisingthatxWtwilleventuallydiscardallcomponentsofxthatareorthogonaltotheprincipaleigenvectorof.WRecurrentnetworksusethesamematrixWateachtimestep,butfeedforwardnetworksdonot,soevenverydeepfeedforwardnetworkscanlargelyavoidthevanishingandexplodinggradientproblem(,).Sussillo2014Wedeferafurtherdiscussionofthechallengesoftrainingrecurrentnetworksuntilsection,afterrecurrentnetworkshavebeendescribedinmoredetail.10.78.2.6InexactGradientsMostoptimizationalgorithmsaredesignedwiththeassumptionthatwehaveaccesstotheexactgradientorHessianmatrix.Inpractice,weusuallyonlyhaveanoisyorevenbiasedestimateofthesequantities. Nearlyeverydeeplearningalgorithmreliesonsampling-basedestimatesatleastinsofarasusingaminibatchoftrainingexamplestocomputethegradient.Inothercases,theobjectivefunctionwewanttominimizeisactuallyintractable.Whentheobjectivefunctionisintractable,typicallyitsgradientisintractableaswell.Insuchcaseswecanonlyapproximatethegradient.Theseissuesmostlyarise290 CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSwiththemoreadvancedmodelsinpart.Forexample,contrastivedivergenceIIIgivesatechniqueforapproximatingthegradientoftheintractablelog-likelihoodofaBoltzmannmachine.Variousneuralnetworkoptimizationalgorithmsaredesignedtoaccountforimperfectionsinthegradientestimate.Onecanalsoavoidtheproblembychoosingasurrogatelossfunctionthatiseasiertoapproximatethanthetrueloss.8.2.7PoorCorrespondencebetweenLocalandGlobalStructureManyoftheproblemswehavediscussedsofarcorrespondtopropertiesofthelossfunctionatasinglepoint—itcanbediﬃculttomakeasinglestepifJ(θ)ispoorlyconditionedatthecurrentpointθ,orifθliesonacliﬀ,orifθisasaddlepointhidingtheopportunitytomakeprogressdownhillfromthegradient.Itispossibletoovercomealloftheseproblemsatasinglepointandstillperformpoorlyifthedirectionthatresultsinthemostimprovementlocallydoesnotpointtowarddistantregionsofmuchlowercost.Goodfellow2015etal.()arguethatmuchoftheruntimeoftrainingisduetothelengthofthetrajectoryneededtoarriveatthesolution.Figureshowsthat8.2thelearningtrajectoryspendsmostofitstimetracingoutawidearcaroundamountain-shapedstructure.Muchofresearchintothediﬃcultiesofoptimizationhasfocusedonwhethertrainingarrivesataglobalminimum,alocalminimum,orasaddlepoint,butinpracticeneuralnetworksdonotarriveatacriticalpointofanykind.Figure8.1showsthatneuralnetworksoftendonotarriveataregionofsmallgradient.Indeed,suchcriticalpointsdonotevennecessarilyexist.Forexample,thelossfunction−logp(y|x;θ)canlackaglobalminimumpointandinsteadasymptoticallyapproachsomevalueasthemodelbecomesmoreconﬁdent.Foraclassiﬁerwithdiscreteyandp(y|x)providedbyasoftmax,thenegativelog-likelihoodcanbecomearbitrarilyclosetozeroifthemodelisabletocorrectlyclassifyeveryexampleinthetrainingset,butitisimpossibletoactuallyreachthevalueofzero.Likewise,amodelofrealvaluesp(y|x) =N(y;f(θ),β−1)canhavenegativelog-likelihoodthatasymptotestonegativeinﬁnity—iff(θ)isabletocorrectlypredictthevalueofalltrainingsetytargets,thelearningalgorithmwillincreaseβwithoutbound.Seeﬁgureforanexampleofafailureoflocaloptimizationto8.4ﬁndagoodcostfunctionvalueevenintheabsenceofanylocalminimaorsaddlepoints.Futureresearchwillneedtodevelopfurtherunderstandingofthefactorsthatinﬂuencethelengthofthelearningtrajectoryandbettercharacterizetheoutcome291 CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELS θJ()θ Figure8.4:Optimizationbasedonlocaldownhillmovescanfailifthelocalsurfacedoesnotpointtowardtheglobalsolution.Hereweprovideanexampleofhowthiscanoccur,eveniftherearenosaddlepointsandnolocalminima.Thisexamplecostfunctioncontainsonlyasymptotestowardlowvalues,notminima.Themaincauseofdiﬃcultyinthiscaseisbeinginitializedonthewrongsideofthe“mountain”andnotbeingabletotraverseit. Inhigherdimensionalspace,learningalgorithmscanoftencircumnavigatesuchmountainsbutthetrajectoryassociatedwithdoingsomaybelongandresultinexcessivetrainingtime,asillustratedinﬁgure.8.2oftheprocess.Manyexistingresearchdirectionsareaimedatﬁndinggoodinitialpointsforproblemsthathavediﬃcultglobalstructure,ratherthandevelopingalgorithmsthatusenon-localmoves.Gradientdescentandessentiallyalllearningalgorithmsthatareeﬀectivefortrainingneuralnetworksarebasedonmakingsmall,localmoves.Theprevioussectionshaveprimarilyfocusedonhowthecorrectdirectionoftheselocalmovescanbediﬃculttocompute.Wemaybeabletocomputesomepropertiesoftheobjectivefunction,suchasitsgradient,onlyapproximately,withbiasorvarianceinourestimateofthecorrectdirection.Inthesecases,localdescentmayormaynotdeﬁneareasonablyshortpathtoavalidsolution,butwearenotactuallyabletofollowthelocaldescentpath.Theobjectivefunctionmayhaveissuessuchaspoorconditioningordiscontinuousgradients,causingtheregionwherethegradientprovidesagoodmodeloftheobjectivefunctiontobeverysmall.Inthesecases,localdescentwithstepsofsizemaydeﬁneareasonablyshortpathtothesolution,butweareonlyabletocomputethelocaldescentdirectionwithstepsofsizeδ.Inthesecases,localdescentmayormaynotdeﬁneapathtothesolution,butthepathcontainsmanysteps,sofollowingthepathincursa292 CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELShighcomputationalcost.Sometimeslocalinformationprovidesusnoguide,whenthefunctionhasawideﬂatregion,orifwemanagetolandexactlyonacriticalpoint(usuallythislatterscenarioonlyhappenstomethodsthatsolveexplicitlyforcriticalpoints,suchasNewton’smethod).Inthesecases,localdescentdoesnotdeﬁneapathtoasolutionatall.Inothercases,localmovescanbetoogreedyandleadusalongapaththatmovesdownhillbutawayfromanysolution,asinﬁgure,oralonganunnecessarilylongtrajectorytothesolution,asinﬁgure.8.48.2Currently,wedonotunderstandwhichoftheseproblemsaremostrelevanttomakingneuralnetworkoptimizationdiﬃcult,andthisisanactiveareaofresearch.Regardlessofwhichoftheseproblemsaremostsigniﬁcant,allofthemmightbeavoidedifthereexistsaregionofspaceconnectedreasonablydirectlytoasolutionbyapaththatlocaldescentcanfollow,andifweareabletoinitializelearningwithinthatwell-behavedregion. Thislastviewsuggestsresearchintochoosinggoodinitialpointsfortraditionaloptimizationalgorithmstouse.8.2.8TheoreticalLimitsofOptimizationSeveraltheoreticalresultsshowthattherearelimitsontheperformanceofanyoptimizationalgorithmwemightdesignforneuralnetworks(BlumandRivest,1992Judd1989WolpertandMacReady1997;,;,).Typicallytheseresultshavelittlebearingontheuseofneuralnetworksinpractice.Sometheoreticalresultsapplyonlytothecasewheretheunitsofaneuralnetworkoutput discretevalues.However, most neuralnetworkunitsoutputsmoothlyincreasingvaluesthatmakeoptimizationvialocalsearchfeasible.Sometheoreticalresultsshowthatthereexistproblemclassesthatareintractable,butitcanbediﬃculttotellwhetheraparticularproblemfallsintothatclass.Otherresultsshowthatﬁndingasolutionforanetworkofagivensizeisintractable,butinpracticewecanﬁndasolutioneasilybyusingalargernetworkforwhichmanymoreparametersettingscorrespondtoanacceptablesolution.Moreover,inthecontextofneuralnetworktraining,weusuallydonotcareaboutﬁndingtheexactminimumofafunction,butseekonlytoreduceitsvaluesuﬃcientlytoobtaingoodgeneralizationerror. Theoreticalanalysisofwhetheranoptimizationalgorithmcanaccomplishthisgoalisextremelydiﬃcult.Developingmorerealisticboundsontheperformanceofoptimizationalgorithmsthereforeremainsanimportantgoalformachinelearningresearch.293 CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELS8.3BasicAlgorithmsWehavepreviouslyintroducedthegradientdescent(section)algorithmthat4.3followsthegradientofanentiretrainingsetdownhill.Thismaybeacceleratedconsiderablybyusingstochasticgradientdescenttofollowthegradientofrandomlyselectedminibatchesdownhill,asdiscussedinsectionandsection.5.98.1.38.3.1StochasticGradientDescentStochasticgradientdescent(SGD)anditsvariantsareprobablythemostusedoptimizationalgorithmsformachinelearningingeneralandfordeeplearninginparticular. Asdiscussedinsection,itispossibletoobtainanunbiased8.1.3estimateofthegradientbytakingtheaveragegradientonaminibatchofmexamplesdrawni.i.dfromthedatageneratingdistribution.Algorithmshowshowtofollowthisestimateofthegradientdownhill.8.1Algorithm8.1Stochasticgradientdescent(SGD)updateattrainingiterationkRequire:Learningratek.Require:InitialparameterθwhiledostoppingcriterionnotmetSampleaminibatchofmexamplesfromthetrainingset{x(1),...,x()m}withcorrespondingtargetsy()i.Computegradientestimate:ˆg←+1m∇θiLf((x()i;)θ,y()i)Applyupdate:θθ←−ˆgendwhileAcrucialparameterfortheSGDalgorithmisthelearningrate.Previously,wehavedescribedSGDasusingaﬁxedlearningrate.Inpractice,itisnecessarytograduallydecreasethelearningrateovertime,sowenowdenotethelearningrateatiterationaskk.ThisisbecausetheSGDgradientestimatorintroducesasourceofnoise(therandomsamplingofmtrainingexamples)thatdoesnotvanishevenwhenwearriveataminimum.Bycomparison,thetruegradientofthetotalcostfunctionbecomessmallandthen0whenweapproachandreachaminimumusingbatchgradientdescent,sobatchgradientdescentcanuseaﬁxedlearningrate.AsuﬃcientconditiontoguaranteeconvergenceofSGDisthat∞k=1k= and∞,(8.12)294 CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELS∞k=12k<.∞(8.13)Inpractice,itiscommontodecaythelearningratelinearlyuntiliteration:τk= (1)−α0+ατ(8.14)withα=kτ.Afteriteration,itiscommontoleaveconstant.τThelearningratemaybechosenbytrialanderror,butitisusuallybesttochooseitbymonitoringlearningcurvesthatplottheobjectivefunctionasafunctionoftime.Thisismoreofanartthanascience,andmostguidanceonthissubjectshouldberegardedwithsomeskepticism.Whenusingthelinearschedule,theparameterstochooseare0,τ,andτ.Usuallyτmaybesettothenumberofiterationsrequiredtomakeafewhundredpassesthroughthetrainingset.Usuallyτshouldbesettoroughlythevalueof1%0.Themainquestionishowtoset0.Ifitistoolarge,thelearningcurvewillshowviolentoscillations,withthecostfunctionoftenincreasingsigniﬁcantly.Gentleoscillationsareﬁne,especiallyiftrainingwithastochasticcostfunctionsuchasthecostfunctionarisingfromtheuseofdropout.Ifthelearningrateistoolow,learningproceedsslowly,andiftheinitiallearningrateistoolow,learningmaybecomestuckwithahighcostvalue.Typically,theoptimalinitiallearningrate,intermsoftotaltrainingtimeandtheﬁnalcostvalue,ishigherthanthelearningratethatyieldsthebestperformanceaftertheﬁrst100iterationsorso.Therefore,itisusuallybesttomonitortheﬁrstseveraliterationsandusealearningratethatishigherthanthebest-performinglearningrateatthistime,butnotsohighthatitcausessevereinstability.ThemostimportantpropertyofSGDandrelatedminibatchoronlinegradient-basedoptimizationisthatcomputationtimeperupdatedoesnotgrowwiththenumberoftrainingexamples.Thisallowsconvergenceevenwhenthenumberoftrainingexamplesbecomesverylarge.Foralargeenoughdataset,SGDmayconvergetowithinsomeﬁxedtoleranceofitsﬁnaltestseterrorbeforeithasprocessedtheentiretrainingset.TostudytheconvergencerateofanoptimizationalgorithmitiscommontomeasuretheexcesserrorJ(θ)−minθJ(θ),whichistheamountthatthecurrentcostfunctionexceedstheminimumpossiblecost.WhenSGDisappliedtoaconvexproblem,theexcesserrorisO(1√k)afterkiterations,whileinthestronglyconvexcaseitisO(1k).Theseboundscannotbeimprovedunlessextraconditionsareassumed.Batchgradientdescentenjoysbetterconvergenceratesthanstochasticgradientdescentintheory.However,theCramér-Raobound(,;,Cramér1946Rao1945)statesthatgeneralizationerrorcannotdecreasefasterthanO(1k).Bottou295 CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSandBousquet2008()arguethatitthereforemaynotbeworthwhiletopursueanoptimizationalgorithmthatconvergesfasterthanO(1k)formachinelearningtasks—fasterconvergencepresumablycorrespondstooverﬁtting.Moreover,theasymptoticanalysisobscuresmanyadvantagesthatstochasticgradientdescenthasafterasmallnumberofsteps.Withlargedatasets,theabilityofSGDtomakerapidinitialprogresswhileevaluatingthegradientforonlyveryfewexamplesoutweighsitsslowasymptoticconvergence.MostofthealgorithmsdescribedintheremainderofthischapterachievebeneﬁtsthatmatterinpracticebutarelostintheconstantfactorsobscuredbytheO(1k)asymptoticanalysis.Onecanalsotradeoﬀthebeneﬁtsofbothbatchandstochasticgradientdescentbygraduallyincreasingtheminibatchsizeduringthecourseoflearning.FormoreinformationonSGD,see().Bottou19988.3.2MomentumWhilestochasticgradientdescentremainsaverypopularoptimizationstrategy,learningwithitcansometimesbeslow.Themethodofmomentum(Polyak1964,)isdesignedtoacceleratelearning,especiallyinthefaceofhighcurvature,smallbutconsistentgradients,ornoisygradients.Themomentumalgorithmaccumulatesanexponentiallydecayingmovingaverageofpastgradientsandcontinuestomoveintheirdirection.Theeﬀectofmomentumisillustratedinﬁgure.8.5Formally,themomentumalgorithmintroducesavariablevthatplaystheroleofvelocity—itisthedirectionandspeedatwhichtheparametersmovethroughparameterspace.Thevelocityissettoanexponentiallydecayingaverageofthenegativegradient.Thenamemomentumderivesfromaphysicalanalogy,inwhichthenegativegradientisaforcemovingaparticlethroughparameterspace,accordingtoNewton’slawsofmotion.Momentuminphysicsismasstimesvelocity.Inthemomentumlearningalgorithm,weassumeunitmass,sothevelocityvectorvmayalsoberegardedasthemomentumoftheparticle.Ahyperparameterα∈[0,1)determineshowquicklythecontributionsofpreviousgradientsexponentiallydecay.Theupdateruleisgivenby:vv←α−∇θ1mmi=1L((fx()i;)θ,y()i),(8.15)θθv←+.(8.16)Thevelocityvaccumulatesthegradientelements∇θ1mmi=1L((fx()i;)θ,y()i).Thelargerαisrelativeto,themorepreviousgradientsaﬀectthecurrentdirection.TheSGDalgorithmwithmomentumisgiveninalgorithm.8.2296 CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELS −−−30201001020−30−20−1001020 Figure8.5:Momentumaimsprimarilytosolvetwoproblems:poorconditioningoftheHessianmatrixandvarianceinthestochasticgradient.Here,weillustratehowmomentumovercomestheﬁrstofthesetwoproblems.ThecontourlinesdepictaquadraticlossfunctionwithapoorlyconditionedHessianmatrix.Theredpathcuttingacrossthecontoursindicatesthepathfollowedbythemomentumlearningruleasitminimizesthisfunction.Ateachstepalongtheway,wedrawanarrowindicatingthestepthatgradientdescentwouldtakeatthatpoint.Wecanseethatapoorlyconditionedquadraticobjectivelookslikealong,narrowvalleyorcanyonwithsteepsides.Momentumcorrectlytraversesthecanyonlengthwise,whilegradientstepswastetimemovingbackandforthacrossthenarrowaxisofthecanyon.Comparealsoﬁgure,whichshowsthebehaviorofgradient4.6descentwithoutmomentum. 297 CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSPreviously,thesizeofthestepwassimplythenormofthegradientmultipliedbythelearningrate.Now,thesizeofthestepdependsonhowlargeandhowalignedasequenceofgradientsare.Thestepsizeislargestwhenmanysuccessivegradientspointinexactlythesamedirection.Ifthemomentumalgorithmalwaysobservesgradientg,thenitwillaccelerateinthedirectionof−g,untilreachingaterminalvelocitywherethesizeofeachstepis||||g1−α.(8.17)Itisthushelpfultothinkofthemomentumhyperparameterintermsof11−α.Forexample,α=.9correspondstomultiplyingthemaximumspeedbyrelativeto10thegradientdescentalgorithm.Commonvaluesofαusedinpracticeinclude.5,.9,and.99.Likethelearningrate,αmayalsobeadaptedovertime.Typicallyitbeginswithasmallvalueandislaterraised.Itislessimportanttoadaptαovertimethantoshrinkovertime.Algorithm8.2Stochasticgradientdescent(SGD)withmomentumRequire:Learningrate,momentumparameter.αRequire:Initialparameter,initialvelocity.θvwhiledostoppingcriterionnotmetSampleaminibatchofmexamplesfromthetrainingset{x(1),...,x()m}withcorrespondingtargetsy()i.Computegradientestimate:g←1m∇θiLf((x()i;)θ,y()i)Computevelocityupdate:vvg←α−Applyupdate:θθv←+endwhileWecanviewthemomentumalgorithmassimulatingaparticlesubjecttocontinuous-timeNewtoniandynamics.Thephysicalanalogycanhelptobuildintuitionforhowthemomentumandgradientdescentalgorithmsbehave.Thepositionoftheparticleatanypointintimeisgivenbyθ(t).Theparticleexperiencesnetforce.Thisforcecausestheparticletoaccelerate:f()tf() =t∂2∂t2θ()t.(8.18)Ratherthanviewingthisasasecond-orderdiﬀerentialequationoftheposition,wecanintroducethevariablev(t)representingthevelocityoftheparticleattimetandrewritetheNewtoniandynamicsasaﬁrst-orderdiﬀerentialequation:v() =t∂∂tθ()t,(8.19)298 CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSf() =t∂∂tv()t.(8.20)Themomentumalgorithmthenconsistsofsolvingthediﬀerentialequationsvianumericalsimulation.AsimplenumericalmethodforsolvingdiﬀerentialequationsisEuler’smethod,whichsimplyconsistsofsimulatingthedynamicsdeﬁnedbytheequationbytakingsmall,ﬁnitestepsinthedirectionofeachgradient.Thisexplainsthebasicformofthemomentumupdate,butwhatspeciﬁcallyaretheforces?Oneforceisproportionaltothenegativegradientofthecostfunction:−∇θJ(θ).Thisforcepushestheparticledownhillalongthecostfunctionsurface.Thegradientdescentalgorithmwouldsimplytakeasinglestepbasedoneachgradient,buttheNewtonianscenariousedbythemomentumalgorithminsteadusesthisforcetoalterthevelocityoftheparticle.Wecanthinkoftheparticleasbeinglikeahockeypuckslidingdownanicysurface.Wheneveritdescendsasteeppartofthesurface,itgathersspeedandcontinuesslidinginthatdirectionuntilitbeginstogouphillagain.Oneotherforceisnecessary.Iftheonlyforceisthegradientofthecostfunction,thentheparticlemightnevercometorest.Imagineahockeypuckslidingdownonesideofavalleyandstraightuptheotherside,oscillatingbackandforthforever,assumingtheiceisperfectlyfrictionless.Toresolvethisproblem,weaddoneotherforce,proportionalto−v(t).Inphysicsterminology,thisforcecorrespondstoviscousdrag,asiftheparticlemustpushthrougharesistantmediumsuchassyrup.Thiscausestheparticletograduallyloseenergyovertimeandeventuallyconvergetoalocalminimum.Whydoweuse−v(t)andviscousdraginparticular? Partofthereasontouse−v(t)ismathematicalconvenience—anintegerpowerofthevelocityiseasytoworkwith.However,otherphysicalsystemshaveotherkindsofdragbasedonotherintegerpowersofthevelocity.Forexample,aparticletravelingthroughtheairexperiencesturbulentdrag,withforceproportionaltothesquareofthevelocity,whileaparticlemovingalongthegroundexperiencesdryfriction,withaforceofconstantmagnitude.Wecanrejecteachoftheseoptions.Turbulentdrag,proportionaltothesquareofthevelocity,becomesveryweakwhenthevelocityissmall.Itisnotpowerfulenoughtoforcetheparticletocometorest.Aparticlewithanon-zeroinitialvelocitythatexperiencesonlytheforceofturbulentdragwillmoveawayfromitsinitialpositionforever,withthedistancefromthestartingpointgrowinglikeO(logt).Wemustthereforeusealowerpowerofthevelocity.Ifweuseapowerofzero,representingdryfriction,thentheforceistoostrong.Whentheforceduetothegradientofthecostfunctionissmallbutnon-zero,theconstantforceduetofrictioncancausetheparticletocometorestbeforereachingalocalminimum.Viscousdragavoidsbothoftheseproblems—itisweakenough299 CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSthatthegradientcancontinuetocausemotionuntilaminimumisreached,butstrongenoughtopreventmotionifthegradientdoesnotjustifymoving.8.3.3NesterovMomentumSutskever2013etal.()introducedavariantofthemomentumalgorithmthatwasinspiredbyNesterov’sacceleratedgradientmethod(,,).TheNesterov19832004updaterulesinthiscasearegivenby:vv←α−∇θ1mmi=1Lfx(()i;+)θαv,y()i,(8.21)θθv←+,(8.22)wheretheparametersαandplayasimilarroleasinthestandardmomentummethod.ThediﬀerencebetweenNesterovmomentumandstandardmomentumiswherethegradientisevaluated.WithNesterovmomentumthegradientisevaluatedafterthecurrentvelocityisapplied.ThusonecaninterpretNesterovmomentumasattemptingtoaddacorrectionfactortothestandardmethodofmomentum.ThecompleteNesterovmomentumalgorithmispresentedinalgorithm.8.3Intheconvexbatchgradientcase,NesterovmomentumbringstherateofconvergenceoftheexcesserrorfromO(1/k)(afterksteps)toO(1/k2)asshownbyNesterov1983().Unfortunately, inthestochasticgradientcase,Nesterovmomentumdoesnotimprovetherateofconvergence.Algorithm8.3Stochasticgradientdescent(SGD)withNesterovmomentumRequire:Learningrate,momentumparameter.αRequire:Initialparameter,initialvelocity.θvwhiledostoppingcriterionnotmetSampleaminibatchofmexamplesfromthetrainingset{x(1),...,x()m}withcorrespondinglabelsy()i.Applyinterimupdate:˜θθv←+αComputegradient(atinterimpoint):g←1m∇˜θiLf((x()i;˜θy),()i)Computevelocityupdate:vvg←α−Applyupdate:θθv←+endwhile300 CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELS8.4ParameterInitializationStrategiesSomeoptimizationalgorithmsarenotiterativebynatureandsimplysolveforasolutionpoint.Otheroptimizationalgorithmsareiterativebynaturebut,whenappliedtotherightclassofoptimizationproblems,convergetoacceptablesolutionsinanacceptableamountoftimeregardlessofinitialization.Deeplearningtrainingalgorithmsusuallydonothaveeitheroftheseluxuries.Trainingalgorithmsfordeeplearningmodelsareusuallyiterativeinnatureandthusrequiretheusertospecifysomeinitialpointfromwhichtobegintheiterations.Moreover,trainingdeepmodelsisasuﬃcientlydiﬃculttaskthatmostalgorithmsarestronglyaﬀectedbythechoiceofinitialization.Theinitialpointcandeterminewhetherthealgorithmconvergesatall,withsomeinitialpointsbeingsounstablethatthealgorithmencountersnumericaldiﬃcultiesandfailsaltogether.Whenlearningdoesconverge,theinitialpointcandeterminehowquicklylearningconvergesandwhetheritconvergestoapointwithhigh orlowcost.Also, pointsofcomparablecostcanhavewildlyvaryinggeneralizationerror,andtheinitialpointcanaﬀectthegeneralizationaswell.Moderninitializationstrategiesaresimpleandheuristic.Designingimprovedinitializationstrategiesisadiﬃculttaskbecauseneuralnetworkoptimizationisnotyetwellunderstood.Mostinitializationstrategiesarebasedonachievingsomenicepropertieswhenthenetworkisinitialized.However,wedonothaveagoodunderstandingofwhichofthesepropertiesarepreservedunderwhichcircumstancesafterlearningbeginstoproceed.Afurtherdiﬃcultyisthatsomeinitialpointsmaybebeneﬁcialfromtheviewpointofoptimizationbutdetrimentalfromtheviewpointofgeneralization.Ourunderstandingofhowtheinitialpointaﬀectsgeneralizationisespeciallyprimitive,oﬀeringlittletonoguidanceforhowtoselecttheinitialpoint.Perhapstheonlypropertyknownwithcompletecertaintyisthattheinitialparametersneedto“breaksymmetry” betweendiﬀerentunits.Iftwohiddenunitswiththesameactivationfunctionareconnectedtothesameinputs,thentheseunitsmusthavediﬀerentinitialparameters. Iftheyhavethesameinitialparameters,thenadeterministiclearningalgorithmappliedtoadeterministiccostandmodelwillconstantlyupdatebothoftheseunitsinthesameway.Evenifthemodelortrainingalgorithmiscapableofusingstochasticitytocomputediﬀerentupdatesfordiﬀerentunits(forexample,ifonetrainswithdropout),itisusuallybesttoinitializeeachunittocomputeadiﬀerentfunctionfromalloftheotherunits.Thismayhelptomakesurethatnoinputpatternsarelostinthenullspaceofforwardpropagationandnogradientpatternsarelostinthenullspaceofback-propagation.Thegoalofhavingeachunitcomputeadiﬀerentfunction301 CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSmotivatesrandominitializationoftheparameters.Wecouldexplicitlysearchforalargesetofbasisfunctionsthatareallmutuallydiﬀerentfromeachother,butthisoftenincursanoticeablecomputationalcost.Forexample,ifwehaveatmostasmanyoutputsasinputs,wecoulduseGram-Schmidtorthogonalizationonaninitialweightmatrix,andbeguaranteedthateachunitcomputesaverydiﬀerentfunctionfromeachotherunit.Randominitializationfromahigh-entropydistributionoverahigh-dimensionalspaceiscomputationallycheaperandunlikelytoassignanyunitstocomputethesamefunctionaseachother.Typically,wesetthebiasesforeachunittoheuristicallychosenconstants,andinitializeonlytheweightsrandomly.Extraparameters,forexample,parametersencodingtheconditionalvarianceofaprediction,areusuallysettoheuristicallychosenconstantsmuchlikethebiasesare.Wealmostalwaysinitializealltheweightsin themodel tovalues drawnrandomly froma Gaussian oruniform distribution.Thechoiceof Gaussianoruniformdistributiondoesnotseemtomatterverymuch,buthasnotbeenexhaustivelystudied.Thescaleoftheinitialdistribution,however,doeshavealargeeﬀectonboththeoutcomeoftheoptimizationprocedureandontheabilityofthenetworktogeneralize.Largerinitialweightswillyieldastrongersymmetrybreakingeﬀect,helpingtoavoidredundantunits.Theyalsohelptoavoidlosingsignalduringforwardorback-propagationthroughthelinearcomponentofeachlayer—largervaluesinthematrixresultinlargeroutputsofmatrixmultiplication.Initialweightsthataretoolargemay,however,resultinexplodingvaluesduringforwardpropagationorback-propagation.Inrecurrentnetworks,largeweightscanalsoresultinchaos(suchextremesensitivitytosmallperturbationsoftheinputthatthebehaviorofthedeterministicforwardpropagationprocedureappearsrandom). Tosomeextent,theexplodinggradientproblemcanbemitigatedbygradientclipping(thresholdingthevaluesofthegradientsbeforeperformingagradientdescentstep).Largeweightsmayalsoresultinextremevaluesthatcausetheactivationfunctiontosaturate,causingcompletelossofgradientthroughsaturatedunits.Thesecompetingfactorsdeterminetheidealinitialscaleoftheweights.Theperspectivesofregularizationandoptimizationcangiveverydiﬀerentinsightsintohowweshouldinitializeanetwork.Theoptimizationperspectivesuggeststhattheweightsshouldbelargeenoughtopropagateinformationsuccess-fully,butsomeregularizationconcernsencouragemakingthemsmaller.Theuseofanoptimizationalgorithmsuchasstochasticgradientdescentthatmakessmallincrementalchangestotheweightsandtendstohaltinareasthatarenearertotheinitialparameters(whetherduetogettingstuckinaregionoflowgradient,or302 CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSduetotriggeringsomeearlystoppingcriterionbasedonoverﬁtting)expressesapriorthattheﬁnalparametersshouldbeclosetotheinitialparameters.Recallfromsectionthatgradientdescentwithearlystoppingisequivalenttoweight7.8decayforsomemodels.Inthegeneralcase,gradientdescentwithearlystoppingisnotthesameasweightdecay,butdoesprovidealooseanalogyforthinkingabouttheeﬀectofinitialization.Wecanthinkofinitializingtheparametersθtoθ0asbeingsimilartoimposingaGaussianpriorp(θ)withmeanθ0.Fromthispointofview,itmakessensetochooseθ0tobenear0.Thispriorsaysthatitismorelikelythatunitsdonotinteractwitheachotherthanthattheydointeract.Unitsinteractonlyifthelikelihoodtermoftheobjectivefunctionexpressesastrongpreferenceforthemtointeract.Ontheotherhand,ifweinitializeθ0tolargevalues,thenourpriorspeciﬁeswhichunitsshouldinteractwitheachother,andhowtheyshouldinteract.Someheuristicsareavailableforchoosingtheinitialscaleoftheweights.OneheuristicistoinitializetheweightsofafullyconnectedlayerwithminputsandnoutputsbysamplingeachweightfromU(−1√m,1√m),whileGlorotandBengio()suggestusingthe2010normalizedinitializationWi,j∼U−6mn+,6mn+.(8.23)Thislatterheuristicisdesignedtocompromisebetweenthegoalofinitializingalllayerstohavethesameactivationvarianceandthegoalofinitializingalllayerstohavethesamegradientvariance.Theformulaisderivedusingtheassumptionthatthenetworkconsistsonlyofachainofmatrixmultiplications,withnononlinearities.Realneuralnetworksobviouslyviolatethisassumption,butmanystrategiesdesignedforthelinearmodelperformreasonablywellonitsnonlinearcounterparts.Saxe2013etal.()recommendinitializingtorandomorthogonalmatrices,withacarefullychosenscalingorgainfactorgthataccountsforthenonlinearityappliedateachlayer.Theyderivespeciﬁcvaluesofthescalingfactorfordiﬀerenttypesofnonlinearactivationfunctions.Thisinitializationschemeisalsomotivatedbyamodelofadeepnetworkasasequenceofmatrixmultiplieswithoutnonlinearities.Undersuchamodel,thisinitializationschemeguaranteesthatthetotalnumberoftrainingiterationsrequiredtoreachconvergenceisindependentofdepth.Increasingthescalingfactorgpushesthenetworktowardtheregimewhereactivationsincreaseinnormastheypropagateforwardthroughthenetworkandgradientsincreaseinnormastheypropagatebackward. ()showedSussillo2014thatsettingthegainfactorcorrectlyissuﬃcienttotrainnetworksasdeepas303 CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELS1,000layers,withoutneedingtouseorthogonalinitializations. Akeyinsightofthisapproachisthatinfeedforwardnetworks,activationsandgradientscangroworshrinkoneachstepofforwardorback-propagation,followingarandomwalkbehavior.Thisisbecausefeedforwardnetworksuseadiﬀerentweightmatrixateachlayer.Ifthisrandomwalkistunedtopreservenorms,thenfeedforwardnetworkscanmostlyavoidthevanishingandexplodinggradientsproblemthatariseswhenthesameweightmatrixisusedateachstep,describedinsection.8.2.5Unfortunately,theseoptimalcriteriaforinitialweightsoftendonotleadtooptimalperformance.Thismaybeforthreediﬀerentreasons.First,wemaybeusingthewrongcriteria—itmaynotactuallybebeneﬁcialtopreservethenormofasignalthroughouttheentirenetwork.Second,thepropertiesimposedatinitializationmaynotpersistafterlearninghasbeguntoproceed.Third,thecriteriamightsucceedatimprovingthespeedofoptimizationbutinadvertentlyincreasegeneralizationerror.Inpractice,weusuallyneedtotreatthescaleoftheweightsasahyperparameterwhoseoptimalvalueliessomewhereroughlynearbutnotexactlyequaltothetheoreticalpredictions.Onedrawbacktoscalingrulesthatsetalloftheinitialweightstohavethesamestandarddeviation,suchas1√m,isthateveryindividualweightbecomesextremelysmallwhenthelayersbecomelarge.()introducedanMartens2010alternativeinitializationschemecalledsparseinitializationinwhicheachunitisinitializedtohaveexactlyknon-zeroweights.Theideaistokeepthetotalamountofinputtotheunitindependentfromthenumberofinputsmwithoutmakingthemagnitudeofindividualweightelementsshrinkwithm.Sparseinitializationhelpstoachievemorediversityamongtheunitsatinitializationtime.However,italsoimposesaverystrongpriorontheweightsthatarechosentohavelargeGaussianvalues.Becauseittakesalongtimeforgradientdescenttoshrink“incorrect”largevalues,thisinitializationschemecancauseproblemsforunitssuchasmaxoutunitsthathaveseveralﬁltersthatmustbecarefullycoordinatedwitheachother.Whencomputationalresourcesallowit,itisusuallyagoodideatotreattheinitialscaleoftheweightsforeachlayerasahyperparameter,andtochoosethesescalesusingahyperparametersearchalgorithmdescribedinsection,such11.4.2asrandomsearch.Thechoiceofwhethertousedenseorsparseinitializationcanalsobemadeahyperparameter.Alternately,onecanmanuallysearchforthebestinitialscales.Agoodruleofthumbforchoosingtheinitialscalesistolookattherangeorstandarddeviationofactivationsorgradientsonasingleminibatchofdata.Iftheweightsaretoosmall,therangeofactivationsacrosstheminibatchwillshrinkastheactivationspropagateforwardthroughthenetwork.Byrepeatedlyidentifyingtheﬁrstlayerwithunacceptablysmallactivationsand304 CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSincreasingitsweights,itispossibletoeventuallyobtainanetworkwithreasonableinitialactivationsthroughout.Iflearningisstilltooslowatthispoint,itcanbeusefultolookattherangeorstandarddeviationofthegradientsaswellastheactivations. Thisprocedurecaninprinciplebeautomatedandisgenerallylesscomputationallycostlythanhyperparameteroptimizationbasedonvalidationseterrorbecauseitisbasedonfeedbackfromthebehavioroftheinitialmodelonasinglebatchofdata,ratherthanonfeedbackfromatrainedmodelonthevalidationset.Whilelongusedheuristically,thisprotocolhasrecentlybeenspeciﬁedmoreformallyandstudiedby().MishkinandMatas2015So far we have focused on the initialization ofthe weights.Fortunately,initializationofotherparametersistypicallyeasier.Theapproachforsettingthebiasesmustbecoordinatedwiththeapproachforsettingstheweights.Settingthebiasestozeroiscompatiblewithmostweightinitializationschemes.Thereareafewsituationswherewemaysetsomebiasestonon-zerovalues:•Ifabiasisforanoutputunit,thenitisoftenbeneﬁcialtoinitializethebiastoobtaintherightmarginalstatisticsoftheoutput.Todothis,weassumethattheinitialweightsaresmallenoughthattheoutputoftheunitisdeterminedonlybythebias.Thisjustiﬁessettingthebiastotheinverseoftheactivationfunctionappliedtothemarginalstatisticsoftheoutputinthetrainingset.Forexample,iftheoutputisadistributionoverclassesandthisdistributionisahighlyskeweddistributionwiththemarginalprobabilityofclassigivenbyelementciofsomevectorc,thenwecansetthebiasvectorbbysolvingtheequationsoftmax(b) =c.ThisappliesnotonlytoclassiﬁersbutalsotomodelswewillencounterinPart,suchasautoencodersandBoltzmannIIImachines.Thesemodelshavelayerswhoseoutputshouldresembletheinputdatax,anditcanbeveryhelpfultoinitializethebiasesofsuchlayerstomatchthemarginaldistributionover.x•Sometimeswemay wanttochoosethebiastoavoidcausingtoo muchsaturationatinitialization.Forexample,wemaysetthebiasofaReLUhiddenunitto0.1ratherthan0toavoidsaturatingtheReLUatinitialization.Thisapproachisnotcompatiblewithweightinitializationschemesthatdonotexpectstronginputfromthebiasesthough.Forexample, itisnotrecommendedforusewithrandomwalkinitialization(,).Sussillo2014•Sometimesaunitcontrolswhetherotherunitsareabletoparticipateinafunction.Insuchsituations,wehaveaunitwithoutputuandanotherunith∈[0,1],andtheyaremultipliedtogethertoproduceanoutputuh. We305 CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELScanviewhasagatethatdetermineswhetheruhu≈oruh≈0. Inthesesituations,wewanttosetthebiasforhsothath≈1mostofthetimeatinitialization.Otherwiseudoesnothaveachancetolearn.Forexample,Jozefowicz2015etal.()advocatesettingthebiastofortheforgetgateof1theLSTMmodel,describedinsection.10.10Anothercommontypeofparameterisavarianceorprecisionparameter.Forexample,wecanperformlinearregressionwithaconditionalvarianceestimateusingthemodelpyy(|Nx) = (|wTx+1)b,/β(8.24)whereβisaprecisionparameter.Wecanusuallyinitializevarianceorprecisionparametersto1safely.Anotherapproachistoassumetheinitialweightsarecloseenoughtozerothatthebiasesmaybesetwhileignoringtheeﬀectoftheweights,thensetthebiasestoproducethecorrectmarginalmeanoftheoutput,andsetthevarianceparameterstothemarginalvarianceoftheoutputinthetrainingset.Besidesthesesimpleconstantorrandommethodsofinitializingmodelparame-ters,itispossibletoinitializemodelparametersusingmachinelearning.AcommonstrategydiscussedinpartofthisbookistoinitializeasupervisedmodelwithIIItheparameterslearnedbyanunsupervisedmodeltrainedonthesameinputs.Onecanalsoperformsupervisedtrainingonarelatedtask.Evenperformingsupervisedtrainingonanunrelatedtaskcansometimesyieldaninitializationthatoﬀersfasterconvergencethanarandominitialization.Someoftheseinitializationstrategiesmayyieldfasterconvergenceandbettergeneralizationbecausetheyencodeinformationaboutthedistributionintheinitialparametersofthemodel.Othersapparentlyperformwellprimarilybecausetheysettheparameterstohavetherightscaleorsetdiﬀerentunitstocomputediﬀerentfunctionsfromeachother.8.5AlgorithmswithAdaptiveLearningRatesNeuralnetworkresearchershavelongrealizedthatthelearningratewasreliablyoneofthehyperparametersthatisthemostdiﬃculttosetbecauseithasasigniﬁcantimpactonmodelperformance.Aswehavediscussedinsectionsand,the4.38.2costisoftenhighlysensitivetosomedirectionsinparameterspaceandinsensitivetoothers.Themomentumalgorithmcanmitigatetheseissuessomewhat,butdoessoattheexpenseofintroducinganotherhyperparameter.Inthefaceofthis,itisnaturaltoaskifthereisanotherway.Ifwebelievethatthedirectionsofsensitivityaresomewhataxis-aligned,itcanmakesensetouseaseparatelearning306 CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSrateforeachparameter,andautomaticallyadapttheselearningratesthroughoutthecourseoflearning.Thealgorithm(,)isanearlyheuristicapproachdelta-bar-deltaJacobs1988toadaptingindividuallearningratesformodelparametersduringtraining.Theapproachisbasedonasimpleidea:ifthepartialderivativeoftheloss,withrespecttoagivenmodelparameter,remainsthesamesign,thenthelearningrateshouldincrease.Ifthepartialderivativewithrespecttothatparameterchangessign,thenthelearningrateshoulddecrease. Ofcourse,thiskindofrulecanonlybeappliedtofullbatchoptimization.Morerecently,anumberofincremental(ormini-batch-based)methodshavebeenintroducedthatadaptthelearningratesofmodelparameters.Thissectionwillbrieﬂyreviewafewofthesealgorithms.8.5.1AdaGradTheAdaGradalgorithm,showninalgorithm,individuallyadaptsthelearning8.4ratesofallmodelparametersbyscalingtheminverselyproportionaltothesquarerootofthesumofalloftheirhistoricalsquaredvalues(,).TheDuchietal.2011parameterswiththelargestpartialderivativeofthelosshaveacorrespondinglyrapiddecreaseintheirlearningrate,whileparameterswithsmallpartialderivativeshavearelativelysmalldecreaseintheirlearningrate.Theneteﬀectisgreaterprogressinthemoregentlyslopeddirectionsofparameterspace.Inthecontextofconvexoptimization,theAdaGradalgorithmenjoyssomedesirabletheoreticalproperties.However,empiricallyithasbeenfoundthat—fortrainingdeepneuralnetworkmodels—theaccumulationofsquaredgradientsfromthebeginningoftrainingcanresultinaprematureandexcessivedecreaseintheeﬀectivelearningrate.AdaGradperformswellforsomebutnotalldeeplearningmodels.8.5.2RMSPropTheRMSPropalgorithm(,)modiﬁesAdaGradtoperformbetterinHinton2012thenon-convexsettingbychangingthegradientaccumulationintoanexponentiallyweightedmovingaverage.AdaGradisdesignedtoconvergerapidlywhenappliedtoaconvexfunction. Whenappliedtoanon-convexfunctiontotrainaneuralnetwork,thelearningtrajectorymaypassthroughmanydiﬀerentstructuresandeventuallyarriveataregionthatisalocallyconvexbowl.AdaGradshrinksthelearningrateaccordingtotheentirehistoryofthesquaredgradientandmay307 CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSAlgorithm8.4TheAdaGradalgorithmRequire:GloballearningrateRequire:InitialparameterθRequire:Smallconstant,perhapsδ10−7,fornumericalstabilityInitializegradientaccumulationvariabler= 0whiledostoppingcriterionnotmetSampleaminibatchofmexamplesfromthetrainingset{x(1),...,x()m}withcorrespondingtargetsy()i.Computegradient:g←1m∇θiLf((x()i;)θ,y()i)Accumulatesquaredgradient:rrgg←+Computeupdate:∆θ←−δ+√rg.(Divisionandsquarerootappliedelement-wise)Applyupdate:θθθ←+∆endwhilehavemadethelearningratetoosmallbeforearrivingatsuchaconvexstructure.RMSPropusesanexponentiallydecayingaveragetodiscardhistoryfromtheextremepastsothatitcanconvergerapidlyafterﬁndingaconvexbowl,asifitwereaninstanceoftheAdaGradalgorithminitializedwithinthatbowl.RMSPropisshowninitsstandardforminalgorithmandcombinedwith8.5Nesterovmomentuminalgorithm.ComparedtoAdaGrad,theuseofthe8.6movingaverageintroducesanewhyperparameter,ρ,thatcontrolsthelengthscaleofthemovingaverage.Empirically,RMSProphasbeenshowntobeaneﬀectiveandpracticalop-timizationalgorithmfordeepneuralnetworks.Itiscurrentlyoneofthego-tooptimizationmethodsbeingemployedroutinelybydeeplearningpractitioners.8.5.3AdamAdam(,)isyetanotheradaptivelearningrateoptimizationKingmaandBa2014algorithmandispresentedinalgorithm.Thename“Adam” derivesfrom8.7thephrase“adaptivemoments.”Inthecontextoftheearlieralgorithms,itisperhapsbestseenasavariantonthecombinationofRMSPropandmomentumwithafewimportantdistinctions.First,inAdam,momentumisincorporateddirectlyasanestimateoftheﬁrstordermoment(withexponentialweighting)ofthegradient.ThemoststraightforwardwaytoaddmomentumtoRMSPropistoapplymomentumtotherescaledgradients.Theuseofmomentumincombinationwithrescalingdoesnothaveacleartheoreticalmotivation.Second,Adamincludes308 CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSAlgorithm8.5TheRMSPropalgorithmRequire:Globallearningrate,decayrate.ρRequire:InitialparameterθRequire:Smallconstantδ, usually10−6, usedtostabilizedivision bysmallnumbers.Initializeaccumulationvariablesr= 0whiledostoppingcriterionnotmetSampleaminibatchofmexamplesfromthetrainingset{x(1),...,x()m}withcorrespondingtargetsy()i.Computegradient:g←1m∇θiLf((x()i;)θ,y()i)Accumulatesquaredgradient:rrgg←ρ+(1)−ρComputeparameterupdate:∆θ=−√δ+rg.(1√δ+rappliedelement-wise)Applyupdate:θθθ←+∆endwhilebiascorrectionstotheestimatesofboththeﬁrst-ordermoments(themomentumterm)andthe(uncentered)second-ordermomentstoaccountfortheirinitializationattheorigin(seealgorithm).RMSPropalsoincorporatesanestimateofthe8.7(uncentered)second-ordermoment,howeveritlacksthecorrectionfactor.Thus,unlikeinAdam,theRMSPropsecond-ordermomentestimatemayhavehighbiasearlyintraining.Adamisgenerallyregardedasbeingfairlyrobusttothechoiceofhyperparameters,thoughthelearningratesometimesneedstobechangedfromthesuggesteddefault.8.5.4ChoosingtheRightOptimizationAlgorithmInthissection,wediscussedaseriesofrelatedalgorithmsthateachseektoaddressthechallengeofoptimizingdeepmodelsbyadaptingthelearningrateforeachmodelparameter.Atthispoint,anaturalquestionis:whichalgorithmshouldonechoose?Unfortunately,thereiscurrentlynoconsensusonthispoint.()Schauletal.2014presentedavaluablecomparisonofalargenumberofoptimizationalgorithmsacrossawiderangeoflearningtasks.Whiletheresultssuggestthatthefamilyofalgorithmswithadaptivelearningrates(representedbyRMSPropandAdaDelta)performedfairlyrobustly,nosinglebestalgorithmhasemerged.Currently,themostpopularoptimizationalgorithmsactivelyinuseincludeSGD,SGDwithmomentum,RMSProp,RMSPropwithmomentum,AdaDeltaandAdam.Thechoiceofwhichalgorithmtouse,atthispoint,seemstodepend309 CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSAlgorithm8.6RMSPropalgorithmwithNesterovmomentumRequire:Globallearningrate,decayrate,momentumcoeﬃcient.ραRequire:Initialparameter,initialvelocity.θvInitializeaccumulationvariabler= 0whiledostoppingcriterionnotmetSampleaminibatchofmexamplesfromthetrainingset{x(1),...,x()m}withcorrespondingtargetsy()i.Computeinterimupdate:˜θθv←+αComputegradient:g←1m∇˜θiLf((x()i;˜θy),()i)Accumulategradient:rrgg←ρ+(1)−ρComputevelocityupdate:vv←α−√rg.(1√rappliedelement-wise)Applyupdate:θθv←+endwhilelargelyontheuser’sfamiliaritywiththealgorithm(foreaseofhyperparametertuning).8.6ApproximateSecond-OrderMethodsInthissectionwediscusstheapplicationofsecond-ordermethodstothetrainingofdeepnetworks.See()foranearliertreatmentofthissubject.LeCunetal.1998aForsimplicityofexposition,theonlyobjectivefunctionweexamineistheempiricalrisk:J() = θEx,y∼ˆpdata()x,y[((;))] =Lfxθ,y1mmi=1Lf((x()i;)θ,y()i).(8.25)Howeverthemethodswediscusshereextendreadilytomoregeneralobjectivefunctionsthat,forinstance,includeparameterregularizationtermssuchasthosediscussedinchapter.78.6.1Newton’sMethodInsection,weintroducedsecond-ordergradientmethods.Incontrasttoﬁrst-4.3ordermethods,second-ordermethodsmakeuseofsecondderivativestoimproveoptimization.Themostwidelyusedsecond-ordermethodisNewton’smethod.WenowdescribeNewton’smethodinmoredetail,withemphasisonitsapplicationtoneuralnetworktraining.310 CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSAlgorithm8.7TheAdamalgorithmRequire:Stepsize(Suggesteddefault:)0001.Require:Exponentialdecayratesformomentestimates,ρ1andρ2in[0,1).(Suggesteddefaults:andrespectively)09.0999.Require:Smallconstantδusedfornumericalstabilization.(Suggesteddefault:10−8)Require:InitialparametersθInitialize1stand2ndmomentvariables,s= 0r= 0Initializetimestept= 0whiledostoppingcriterionnotmetSampleaminibatchofmexamplesfromthetrainingset{x(1),...,x()m}withcorrespondingtargetsy()i.Computegradient:g←1m∇θiLf((x()i;)θ,y()i)tt←+1Updatebiasedﬁrstmomentestimate:s←ρ1s+(1−ρ1)gUpdatebiasedsecondmomentestimate:r←ρ2r+(1−ρ2)ggCorrectbiasinﬁrstmoment:ˆs←s1−ρt1Correctbiasinsecondmoment:ˆr←r1−ρt2Computeupdate:∆= θ−ˆs√ˆr+δ(operationsappliedelement-wise)Applyupdate:θθθ←+∆endwhileNewton’smethodisanoptimizationschemebasedonusingasecond-orderTay-lorseriesexpansiontoapproximateJ(θ)nearsomepointθ0,ignoringderivativesofhigherorder:JJ() θ≈(θ0)+(θθ−0)∇θJ(θ0)+12(θθ−0)Hθθ(−0),(8.26)whereHistheHessianofJwithrespecttoθevaluatedatθ0.Ifwethensolveforthecriticalpointofthisfunction,weobtaintheNewtonparameterupdaterule:θ∗= θ0−H−1∇θJ(θ0)(8.27)Thusforalocallyquadraticfunction(withpositivedeﬁniteH),byrescalingthegradientbyH−1,Newton’smethodjumpsdirectlytotheminimum. Iftheobjectivefunctionisconvexbutnotquadratic(therearehigher-orderterms),thisupdatecanbeiterated,yieldingthetrainingalgorithmassociatedwithNewton’smethod,giveninalgorithm.8.8311 CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSAlgorithm8.8Newton’smethodwithobjectiveJ(θ)=1mmi=1Lf((x()i;)θ,y()i).Require:Initialparameterθ0Require:TrainingsetofexamplesmwhiledostoppingcriterionnotmetComputegradient:g←1m∇θiLf((x()i;)θ,y()i)ComputeHessian:H←1m∇2θiLf((x()i;)θ,y()i)ComputeHessianinverse:H−1Computeupdate:∆= θ−H−1gApplyupdate:θθθ= +∆endwhileForsurfacesthatarenotquadratic,aslongastheHessianremainspositivedeﬁnite,Newton’smethodcanbeappliediteratively.Thisimpliesatwo-stepiterativeprocedure.First,updateorcomputetheinverseHessian(i.e.byupdat-ingthequadraticapproximation). Second,updatetheparametersaccordingtoequation.8.27Insection,wediscussedhowNewton’smethodisappropriateonlywhen8.2.3theHessianispositivedeﬁnite.Indeeplearning,thesurfaceoftheobjectivefunctionistypicallynon-convexwithmanyfeatures,suchassaddlepoints,thatareproblematicforNewton’smethod. IftheeigenvaluesoftheHessianarenotallpositive,forexample,nearasaddlepoint,thenNewton’smethodcanactuallycauseupdatestomoveinthewrongdirection.ThissituationcanbeavoidedbyregularizingtheHessian.Commonregularizationstrategiesincludeaddingaconstant,,alongthediagonaloftheHessian.Theregularizedupdatebecomesαθ∗= θ0−[((Hfθ0))+]αI−1∇θf(θ0).(8.28)ThisregularizationstrategyisusedinapproximationstoNewton’smethod,suchastheLevenberg–Marquardtalgorithm(Levenberg1944Marquardt1963,;,),andworksfairlywellaslongasthenegativeeigenvaluesoftheHessianarestillrelativelyclosetozero.Incaseswheretherearemoreextremedirectionsofcurvature,thevalueofαwouldhavetobesuﬃcientlylargetooﬀsetthenegativeeigenvalues.However,asαincreasesinsize,theHessianbecomesdominatedbytheαIdiagonalandthedirectionchosenbyNewton’smethodconvergestothestandardgradientdividedbyα. Whenstrongnegativecurvatureispresent,αmayneedtobesolargethatNewton’smethodwouldmakesmallerstepsthangradientdescentwithaproperlychosenlearningrate.Beyondthechallengescreatedbycertainfeaturesoftheobjectivefunction,312 CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSsuchassaddlepoints,theapplicationofNewton’smethodfortraininglargeneuralnetworksislimitedbythesigniﬁcantcomputationalburdenitimposes.ThenumberofelementsintheHessianissquaredinthenumberofparameters,sowithkparameters(andforevenverysmallneuralnetworksthenumberofparameterskcanbeinthemillions),Newton’smethodwouldrequiretheinversionofakk×matrix—withcomputationalcomplexityofO(k3).Also,sincetheparameterswillchangewitheveryupdate,theinverseHessianhastobecomputedateverytrainingiteration.Asaconsequence,onlynetworkswithaverysmallnumberofparameterscanbepracticallytrainedviaNewton’smethod.Intheremainderofthissection,wewilldiscussalternativesthatattempttogainsomeoftheadvantagesofNewton’smethodwhileside-steppingthecomputationalhurdles.8.6.2ConjugateGradientsConjugategradientsisamethodtoeﬃcientlyavoidthecalculationoftheinverseHessianbyiterativelydescendingconjugatedirections.Theinspirationforthisapproachfollowsfromacarefulstudyoftheweaknessofthemethodofsteepestdescent(seesectionfordetails),wherelinesearchesareappliediterativelyin4.3thedirectionassociatedwiththegradient.Figureillustrateshowthemethodof8.6steepestdescent,whenappliedinaquadraticbowl,progressesinaratherineﬀectiveback-and-forth,zig-zagpattern.Thishappensbecauseeachlinesearchdirection,whengivenbythegradient,isguaranteedtobeorthogonaltothepreviouslinesearchdirection.Lettheprevioussearchdirectionbedt−1.Attheminimum,wherethelinesearchterminates,thedirectionalderivativeiszeroindirectiondt−1:∇θJ(θ)·dt−1=0.Sincethegradientatthispointdeﬁnesthecurrentsearchdirection,dt=∇θJ(θ) willhavenocontributioninthedirectiondt−1.Thusdtisorthogonaltodt−1.Thisrelationshipbetweendt−1anddtisillustratedinﬁgurefor8.6multipleiterationsofsteepestdescent.Asdemonstratedintheﬁgure,thechoiceoforthogonaldirectionsofdescentdonotpreservetheminimumalongtheprevioussearchdirections.Thisgivesrisetothezig-zagpatternofprogress,wherebydescendingtotheminimuminthecurrentgradientdirection,wemustre-minimizetheobjectiveinthepreviousgradientdirection.Thus,byfollowingthegradientattheendofeachlinesearchweare,inasense,undoingprogresswehavealreadymadeinthedirectionofthepreviouslinesearch.Themethodofconjugategradientsseekstoaddressthisproblem.Inthemethodofconjugategradients,weseektoﬁndasearchdirectionthatisconjugatetothepreviouslinesearchdirection,i.e.itwillnotundoprogressmadeinthatdirection.Attrainingiterationt,thenextsearchdirectiondttakes313 CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELS  Figure8.6:Themethodofsteepestdescentappliedtoaquadraticcostsurface.Themethodofsteepestdescentinvolvesjumpingtothepointoflowestcostalongthelinedeﬁnedbythegradientattheinitialpointoneachstep.Thisresolvessomeoftheproblemsseenwithusingaﬁxedlearningrateinﬁgure,butevenwiththeoptimalstepsize4.6thealgorithmstillmakesback-and-forthprogresstowardtheoptimum.Bydeﬁnition,attheminimumoftheobjectivealongagivendirection,thegradientattheﬁnalpointisorthogonaltothatdirection.theform:dt= ∇θJβ()+θtdt−1(8.29)whereβtisacoeﬃcientwhosemagnitudecontrolshowmuchofthedirection,dt−1,weshouldaddbacktothecurrentsearchdirection.Twodirections,dtanddt−1,aredeﬁnedasconjugateifdtHdt−1= 0,whereHistheHessianmatrix.ThestraightforwardwaytoimposeconjugacywouldinvolvecalculationoftheeigenvectorsofHtochooseβt,whichwouldnotsatisfyourgoalofdevelopingamethodthatismorecomputationallyviablethanNewton’smethodforlargeproblems. Canwecalculatetheconjugatedirectionswithoutresortingtothesecalculations?Fortunatelytheanswertothatisyes.Twopopularmethodsforcomputingtheβtare:1. Fletcher-Reeves:βt=∇θJ(θt)∇θJ(θt)∇θJ(θt−1)∇θJ(θt−1)(8.30)314 CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELS2. Polak-Ribière:βt=(∇θJ(θt)−∇θJ(θt−1))∇θJ(θt)∇θJ(θt−1)∇θJ(θt−1)(8.31)Foraquadraticsurface,theconjugatedirectionsensurethatthegradientalongthepreviousdirectiondoesnotincreaseinmagnitude.Wethereforestayattheminimumalongthepreviousdirections.Asaconsequence,inak-dimensionalparameterspace,theconjugategradientmethodrequiresatmostklinesearchestoachievetheminimum.Theconjugategradientalgorithmisgiveninalgorithm.8.9Algorithm8.9TheconjugategradientmethodRequire:Initialparametersθ0Require:TrainingsetofexamplesmInitializeρ0= 0Initializeg0= 0Initializet= 1whiledostoppingcriterionnotmetInitializethegradientgt= 0Computegradient:gt←1m∇θiLf((x()i;)θ,y()i)Computeβt=(gt−gt−1)gtgt−1gt−1(Polak-Ribière)(Nonlinearconjugategradient:optionallyresetβttozero,forexampleiftisamultipleofsomeconstant,suchas)kk= 5Computesearchdirection:ρt= −gt+βtρt−1Performlinesearchtoﬁnd:∗= argmin1mmi=1Lf((x()i;θt+ρt),y()i)(Onatrulyquadraticcostfunction,analyticallysolvefor∗ratherthanexplicitlysearchingforit)Applyupdate:θt+1= θt+∗ρttt←+1endwhileNonlinearConjugateGradients:Sofarwehavediscussedthemethodofconjugategradientsasitisappliedtoquadraticobjectivefunctions. Ofcourse,ourprimaryinterestinthischapteristoexploreoptimizationmethodsfortrainingneuralnetworksandotherrelateddeeplearningmodelswherethecorrespondingobjectivefunctionisfarfromquadratic.Perhapssurprisingly,themethodofconjugategradientsisstillapplicableinthissetting,thoughwithsomemodiﬁcation.Withoutanyassurancethattheobjectiveisquadratic,theconjugatedirections315 CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSarenolongerassuredtoremainattheminimumoftheobjectiveforpreviousdirections.Asaresult,thenonlinearconjugategradientsalgorithmincludesoccasionalresetswherethemethodofconjugategradientsisrestartedwithlinesearchalongtheunalteredgradient.Practitionersreportreasonableresultsinapplicationsofthenonlinearconjugategradientsalgorithmtotrainingneuralnetworks,thoughitisoftenbeneﬁcialtoinitializetheoptimizationwithafewiterationsofstochasticgradientdescentbeforecommencingnonlinearconjugategradients.Also,whilethe(nonlinear)conjugategradientsalgorithmhastraditionallybeencastasabatchmethod,minibatchversionshavebeenusedsuccessfullyforthetrainingofneuralnetworks(,Leetal.2011). Adaptationsofconjugategradientsspeciﬁcallyforneuralnetworkshavebeenproposedearlier,suchasthescaledconjugategradientsalgorithm(,Moller1993).8.6.3BFGSTheBroyden–Fletcher–Goldfarb–Shanno(BFGS)algorithmattemptstobringsomeoftheadvantagesofNewton’smethodwithoutthecomputationalburden.In thatrespect, BFGSissimilartotheconjugategradientmethod.However,BFGStakesamoredirectapproachtotheapproximationofNewton’supdate.RecallthatNewton’supdateisgivenbyθ∗= θ0−H−1∇θJ(θ0),(8.32)whereHistheHessianofJwithrespecttoθevaluatedatθ0.TheprimarycomputationaldiﬃcultyinapplyingNewton’supdateisthecalculationoftheinverseHessianH−1.Theapproachadoptedbyquasi-Newtonmethods(ofwhichtheBFGSalgorithmisthemostprominent)istoapproximatetheinversewithamatrixMtthatisiterativelyreﬁnedbylowrankupdatestobecomeabetterapproximationofH−1.ThespeciﬁcationandderivationoftheBFGSapproximationisgiveninmanytextbooksonoptimization,includingLuenberger1984().OncetheinverseHessianapproximationMtisupdated,thedirectionofdescentρtisdeterminedbyρt=Mtgt.Alinesearchisperformedinthisdirectiontodeterminethesizeofthestep,∗,takeninthisdirection.Theﬁnalupdatetotheparametersisgivenby:θt+1= θt+∗ρt.(8.33)Likethemethodofconjugategradients,theBFGSalgorithmiteratesaseriesoflinesearcheswiththedirectionincorporatingsecond-orderinformation.However316 CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSunlikeconjugategradients,thesuccessoftheapproachisnotheavilydependentonthelinesearchﬁndingapointveryclosetothetrueminimumalongtheline.Thus,relativetoconjugategradients,BFGShastheadvantagethatitcanspendlesstimereﬁningeachlinesearch.Ontheotherhand,theBFGSalgorithmmuststoretheinverseHessianmatrix,M,thatrequiresO(n2)memory,makingBFGSimpracticalformostmoderndeeplearningmodelsthattypicallyhavemillionsofparameters.Limited Memory BFGS (or L-BFGS)The memorycosts ofthe BFGSalgorithmcanbesigniﬁcantlydecreasedbyavoidingstoringthecompleteinverseHessianapproximationM.TheL-BFGSalgorithmcomputestheapproximationMusingthesamemethodastheBFGSalgorithm,butbeginningwiththeassumptionthatM(1)t−istheidentitymatrix,ratherthanstoringtheapproximationfromonesteptothenext.Ifusedwithexactlinesearches,thedirectionsdeﬁnedbyL-BFGSaremutuallyconjugate.However,unlikethemethodofconjugategradients,thisprocedureremainswellbehavedwhentheminimumofthelinesearchisreachedonlyapproximately.TheL-BFGSstrategywithnostoragedescribedherecanbegeneralizedtoincludemoreinformationabouttheHessianbystoringsomeofthevectorsusedtoupdateateachtimestep,whichcostsonlyperstep.MOn()8.7OptimizationStrategiesandMeta-AlgorithmsManyoptimizationtechniquesarenotexactlyalgorithms, butrathergeneraltemplatesthatcanbespecializedtoyieldalgorithms,orsubroutinesthatcanbeincorporatedintomanydiﬀerentalgorithms.8.7.1BatchNormalizationBatchnormalization(,)isoneofthemostexcitingrecentIoﬀeandSzegedy2015innovationsinoptimizingdeepneuralnetworksanditisactuallynotanoptimizationalgorithmatall.Instead,itisamethodofadaptivereparametrization,motivatedbythediﬃcultyoftrainingverydeepmodels.Verydeepmodelsinvolvethecompositionofseveralfunctionsorlayers.Thegradienttellshowtoupdateeachparameter,undertheassumptionthattheotherlayersdonotchange.Inpractice,weupdateallofthelayerssimultaneously.Whenwemaketheupdate,unexpectedresultscanhappenbecausemanyfunctionscomposedtogetherarechangedsimultaneously,usingupdatesthatwerecomputedundertheassumptionthattheotherfunctionsremainconstant.Asasimple317 CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSexample,supposewehaveadeepneuralnetworkthathasonlyoneunitperlayeranddoesnotuseanactivationfunctionateachhiddenlayer:ˆy=xw1w2w3...wl.Here,wiprovidestheweightusedbylayeri.Theoutputoflayeriishi=hi−1wi.Theoutputˆyisalinearfunctionoftheinputx,butanonlinearfunctionoftheweightswi.Supposeourcostfunctionhasputagradientofon1ˆy,sowewishtodecreaseˆyslightly.Theback-propagationalgorithmcanthencomputeagradientg=∇wˆy.Considerwhathappenswhenwemakeanupdatewwg←−.Theﬁrst-orderTaylorseriesapproximationofˆypredictsthatthevalueofˆywilldecreasebygg.Ifwewantedtodecreaseˆyby.1,thisﬁrst-orderinformationavailableinthegradientsuggestswecouldsetthelearningrateto.1gg.However,theactualupdatewillincludesecond-orderandthird-ordereﬀects,onuptoeﬀectsoforderl.Thenewvalueofˆyisgivenbyxw(1−g1)(w2−g2)(...wl−gl).(8.34)Anexampleofonesecond-ordertermarisingfromthisupdateis2g1g2li=3wi.Thistermmightbenegligibleifli=3wiissmall,ormightbeexponentiallylargeiftheweightsonlayersthrough3laregreaterthan.Thismakesitveryhard1tochooseanappropriatelearningrate,becausetheeﬀectsofanupdatetotheparametersforonelayerdependssostronglyonalloftheotherlayers.Second-orderoptimizationalgorithmsaddressthisissuebycomputinganupdatethattakesthesesecond-orderinteractionsintoaccount,butwecanseethatinverydeepnetworks,evenhigher-orderinteractionscanbesigniﬁcant.Evensecond-orderoptimizationalgorithmsareexpensiveandusuallyrequirenumerousapproximationsthatpreventthemfromtrulyaccountingforallsigniﬁcantsecond-orderinteractions.Buildingann-thorderoptimizationalgorithmforn>2thusseemshopeless.Whatcanwedoinstead?Batchnormalizationprovidesanelegantwayofreparametrizingalmostanydeepnetwork.Thereparametrizationsigniﬁcantlyreducestheproblemofcoordinatingupdatesacrossmanylayers.Batchnormalizationcanbeappliedtoanyinputorhiddenlayerinanetwork.LetHbeaminibatchofactivationsofthelayertonormalize,arrangedasadesignmatrix,withtheactivationsforeachexampleappearinginarowofthematrix.Tonormalize,wereplaceitwithHH=Hµ−σ,(8.35)whereµisavectorcontainingthemeanofeachunitandσisavectorcontainingthestandarddeviationofeachunit.ThearithmetichereisbasedonbroadcastingthevectorµandthevectorσtobeappliedtoeveryrowofthematrixH.Withineachrow,thearithmeticiselement-wise,soHi,jisnormalizedbysubtractingµj318 CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSanddividingbyσj.TherestofthenetworkthenoperatesonHinexactlythesamewaythattheoriginalnetworkoperatedon.HAttrainingtime,µ=1miHi,:(8.36)andσ=δ+1mi()Hµ−2i,(8.37)whereδisasmallpositivevaluesuchas10−8imposedtoavoidencounteringtheundeﬁnedgradientof√zatz=0.Crucially, weback-propagatethroughtheseoperationsforcomputingthemeanandthestandarddeviation,andforapplyingthemtonormalizeH.Thismeansthatthegradientwillneverproposeanoperation thatactssimplytoincreasethestandard deviationormeanofhi;thenormalizationoperationsremovetheeﬀectofsuchanactionandzerooutitscomponentinthegradient.Thiswasamajorinnovationofthebatchnormalizationapproach. Previousapproacheshadinvolvedaddingpenaltiestothecostfunctiontoencourageunitstohavenormalizedactivationstatisticsorinvolvedinterveningtorenormalizeunitstatisticsaftereachgradientdescentstep.Theformerapproachusuallyresultedinimperfectnormalizationandthelatterusuallyresultedinsigniﬁcantwastedtimeasthelearningalgorithmrepeatedlyproposedchangingthemeanandvarianceandthenormalizationsteprepeatedlyundidthischange.Batchnormalizationreparametrizesthemodeltomakesomeunitsalwaysbestandardizedbydeﬁnition,deftlysidesteppingbothproblems.Attesttime,µandσmaybereplacedbyrunningaveragesthatwerecollectedduringtrainingtime.Thisallowsthemodeltobeevaluatedonasingleexample,withoutneedingtousedeﬁnitionsofµandσthatdependonanentireminibatch.Revisitingtheˆy=xw1w2...wlexample,weseethatwecanmostlyresolvethediﬃcultiesinlearningthismodelbynormalizinghl−1.SupposethatxisdrawnfromaunitGaussian.Thenhl−1willalsocomefromaGaussian,becausethetransformationfromxtohlislinear.However,hl−1willnolongerhavezeromeanandunitvariance.Afterapplyingbatchnormalization,weobtainthenormalizedˆhl−1thatrestoresthezeromeanandunitvarianceproperties.Foralmostanyupdatetothelowerlayers,ˆhl−1willremainaunitGaussian.Theoutputˆymaythenbelearnedasasimplelinearfunctionˆy=wlˆhl−1.Learninginthismodelisnowverysimplebecausetheparametersatthelowerlayerssimplydonothaveaneﬀectinmostcases;theiroutputisalwaysrenormalizedtoaunitGaussian. Insomecornercases,thelowerlayerscanhaveaneﬀect.Changingoneofthelowerlayerweightstocanmaketheoutputbecomedegenerate,andchangingthesign0319 CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSofoneofthelowerweightscanﬂiptherelationshipbetweenˆhl−1andy. Thesesituationsareveryrare.Withoutnormalization,nearlyeveryupdatewouldhaveanextremeeﬀectonthestatisticsofhl−1.Batchnormalizationhasthusmadethismodelsigniﬁcantlyeasiertolearn. Inthisexample,theeaseoflearningofcoursecameatthecostofmakingthelowerlayersuseless.Inourlinearexample,thelowerlayersnolongerhaveanyharmfuleﬀect,buttheyalsonolongerhaveanybeneﬁcialeﬀect.Thisisbecausewehavenormalizedouttheﬁrstandsecondorderstatistics,whichisallthatalinearnetworkcaninﬂuence.Inadeepneuralnetworkwithnonlinearactivationfunctions,thelowerlayerscanperformnonlineartransformationsofthedata,sotheyremainuseful.Batchnormalizationactstostandardizeonlythemeanandvarianceofeachunitinordertostabilizelearning,butallowstherelationshipsbetweenunitsandthenonlinearstatisticsofasingleunittochange.Becausetheﬁnallayerofthenetworkisabletolearnalineartransformation,wemayactuallywishtoremovealllinearrelationshipsbetweenunitswithinalayer.Indeed,thisistheapproachtakenby(),whoprovidedDesjardinsetal.2015theinspirationforbatchnormalization.Unfortunately, eliminatingalllinearinteractionsismuchmoreexpensivethanstandardizingthemeanandstandarddeviationofeachindividualunit,andsofarbatchnormalizationremainsthemostpracticalapproach.Normalizingthemeanandstandarddeviationofaunitcanreducetheexpressivepowerofthe neuralnetworkcontainingthatunit.Inordertomaintaintheexpressivepowerofthenetwork,itiscommontoreplacethebatchofhiddenunitactivationsHwithγH+βratherthansimplythenormalizedH.Thevariablesγandβarelearnedparametersthatallowthenewvariabletohaveanymeanandstandarddeviation.Atﬁrstglance,thismayseemuseless—whydidwesetthemeanto0,andthenintroduceaparameterthatallowsittobesetbacktoanyarbitraryvalueβ?Theansweristhatthenewparametrizationcanrepresentthesamefamilyoffunctionsoftheinputastheoldparametrization,butthenewparametrizationhasdiﬀerentlearningdynamics.Intheoldparametrization,themeanofHwasdeterminedbyacomplicatedinteractionbetweentheparametersinthelayersbelowH.Inthenewparametrization,themeanofγH+βisdeterminedsolelybyβ.Thenewparametrizationismucheasiertolearnwithgradientdescent.Mostneuralnetworklayerstaketheformofφ(XW+b)whereφissomeﬁxednonlinearactivationfunctionsuchastherectiﬁedlineartransformation.ItisnaturaltowonderwhetherweshouldapplybatchnormalizationtotheinputX,ortothetransformedvalueXW+b.()recommendIoﬀeandSzegedy2015320 CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSthelatter.Morespeciﬁcally,XW+bshouldbereplacedbyanormalizedversionofXW.Thebiastermshouldbeomittedbecauseitbecomesredundantwiththeβparameterappliedbythebatchnormalizationreparametrization.Theinputtoalayerisusuallytheoutputofanonlinearactivationfunctionsuchastherectiﬁedlinearfunctioninapreviouslayer. Thestatisticsoftheinputarethusmorenon-Gaussianandlessamenabletostandardizationbylinearoperations.Inconvolutionalnetworks,describedinchapter,itisimportanttoapplythe9samenormalizingµandσateveryspatiallocationwithinafeaturemap,sothatthestatisticsofthefeaturemapremainthesameregardlessofspatiallocation.8.7.2CoordinateDescentInsomecases,itmaybepossibletosolveanoptimizationproblemquicklybybreakingitintoseparatepieces.Ifweminimizef(x)withrespecttoasinglevariablexi, then minimize itwith respect to anothervariablexjand soon,repeatedlycyclingthroughallvariables,weareguaranteedtoarriveata(local)minimum.Thispracticeisknownascoordinatedescent,becauseweoptimizeonecoordinateatatime. Moregenerally,blockcoordinatedescentreferstominimizingwithrespecttoasubsetofthevariablessimultaneously.Theterm“coordinatedescent”isoftenusedtorefertoblockcoordinatedescentaswellasthestrictlyindividualcoordinatedescent.Coordinatedescentmakesthemostsensewhenthediﬀerentvariablesintheoptimizationproblemcanbeclearlyseparatedintogroupsthatplayrelativelyisolatedroles,orwhenoptimizationwithrespecttoonegroupofvariablesissigniﬁcantlymoreeﬃcientthanoptimizationwithrespecttoallofthevariables.Forexample,considerthecostfunctionJ,(HW) =i,j|Hi,j|+i,jXW−H2i,j.(8.38)Thisfunctiondescribesalearningproblemcalledsparsecoding,wherethegoalistoﬁndaweightmatrixWthatcanlinearlydecodeamatrixofactivationvaluesHtoreconstructthetrainingsetX.MostapplicationsofsparsecodingalsoinvolveweightdecayoraconstraintonthenormsofthecolumnsofW,inordertopreventthepathologicalsolutionwithextremelysmallandlarge.HWThefunctionJisnotconvex.However, wecandividetheinputstothetrainingalgorithmintotwosets:thedictionaryparametersWandthecoderepresentationsH.Minimizingtheobjectivefunctionwithrespecttoeitheroneofthesesetsofvariablesisaconvexproblem.Blockcoordinatedescentthusgives321 CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSusanoptimizationstrategythatallowsustouseeﬃcientconvexoptimizationalgorithms,byalternatingbetweenoptimizingWwithHﬁxed,thenoptimizingHWwithﬁxed.Coordinatedescentisnotaverygoodstrategywhenthevalueofonevariablestronglyinﬂuencestheoptimalvalueofanothervariable,asinthefunctionf(x) =(x1−x2)2+αx21+x22whereαisapositiveconstant.Theﬁrsttermencouragesthetwovariablestohavesimilarvalue,whilethesecondtermencouragesthemtobenearzero.Thesolutionistosetbothtozero.Newton’smethodcansolvetheprobleminasinglestepbecauseitisapositivedeﬁnitequadraticproblem.However,forsmallα,coordinatedescentwillmakeveryslowprogressbecausetheﬁrsttermdoesnotallowasinglevariabletobechangedtoavaluethatdiﬀerssigniﬁcantlyfromthecurrentvalueoftheothervariable.8.7.3PolyakAveragingPolyakaveraging(PolyakandJuditsky1992,)consistsofaveragingtogetherseveralpoints inthe trajectory throughparameter spacevisited by anoptimizationalgorithm. Iftiterationsofgradientdescentvisitpointsθ(1),...,θ()t,thentheoutputofthePolyakaveragingalgorithmisˆθ()t=1tiθ()i. Onsomeproblemclasses,suchasgradientdescentappliedtoconvexproblems,thisapproachhasstrongconvergenceguarantees.Whenappliedtoneuralnetworks,itsjustiﬁcationismoreheuristic,butitperformswellinpractice.Thebasicideaisthattheoptimizationalgorithmmayleapbackandforthacrossavalleyseveraltimeswithoutevervisitingapointnearthebottomofthevalley.Theaverageofallofthelocationsoneithersideshouldbeclosetothebottomofthevalleythough.Innon-convexproblems,thepathtakenbytheoptimizationtrajectorycanbeverycomplicatedandvisitmanydiﬀerentregions.Includingpointsinparameterspacefromthedistantpastthatmaybeseparatedfromthecurrentpointbylargebarriersinthecostfunctiondoesnotseemlikeausefulbehavior.Asaresult,whenapplyingPolyakaveragingtonon-convexproblems,itistypicaltouseanexponentiallydecayingrunningaverage:ˆθ()t= αˆθ(1)t−+(1)−αθ()t.(8.39)Therunningaverageapproachisusedinnumerousapplications.SeeSzegedyetal.()forarecentexample.2015322 CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELS8.7.4SupervisedPretrainingSometimes,directlytrainingamodeltosolveaspeciﬁctaskcanbetooambitiousifthemodeliscomplexandhardtooptimizeorifthetaskisverydiﬃcult.Itissometimesmoreeﬀectivetotrainasimplermodeltosolvethetask,thenmakethemodelmorecomplex.Itcanalsobemoreeﬀectivetotrainthemodeltosolveasimplertask,thenmoveontoconfronttheﬁnaltask.Thesestrategiesthatinvolvetrainingsimplemodelsonsimpletasksbeforeconfrontingthechallengeoftrainingthedesiredmodeltoperformthedesiredtaskarecollectivelyknownaspretraining.Greedyalgorithmsbreakaproblemintomanycomponents,thensolvefortheoptimalversionofeachcomponentinisolation.Unfortunately,combiningtheindividuallyoptimalcomponentsisnotguaranteedtoyieldanoptimalcompletesolution.However,greedyalgorithmscanbecomputationallymuchcheaperthanalgorithmsthatsolveforthebestjointsolution,andthequalityofagreedysolutionisoftenacceptableifnotoptimal.Greedyalgorithmsmayalsobefollowedbyaﬁne-tuningstageinwhichajointoptimizationalgorithmsearchesforanoptimalsolutiontothefullproblem.Initializingthejointoptimizationalgorithmwithagreedysolutioncangreatlyspeeditupandimprovethequalityofthesolutionitﬁnds.Pretraining,andespeciallygreedypretraining,algorithmsareubiquitousindeeplearning.Inthissection,wedescribespeciﬁcallythosepretrainingalgorithmsthatbreaksupervisedlearningproblemsintoothersimplersupervisedlearningproblems.Thisapproachisknownas.greedysupervisedpretrainingIntheoriginal(,)versionofgreedysupervisedpretraining,Bengioetal.2007eachstageconsistsofasupervisedlearningtrainingtaskinvolvingonlyasubsetofthelayersintheﬁnalneuralnetwork.Anexampleofgreedysupervisedpretrainingisillustratedinﬁgure,inwhicheachaddedhiddenlayerispretrainedaspart8.7ofashallowsupervisedMLP,takingasinputtheoutputofthepreviouslytrainedhiddenlayer.Insteadofpretrainingonelayeratatime,SimonyanandZisserman()pretrainadeepconvolutionalnetwork(elevenweightlayers)andthenuse2015theﬁrstfourandlastthreelayersfromthisnetworktoinitializeevendeepernetworks(withuptonineteenlayersofweights).Themiddlelayersofthenew,verydeepnetworkareinitializedrandomly.Thenewnetworkisthenjointlytrained.Anotheroption,exploredbyYu2010etal.()istousetheofthepreviouslyoutputstrainedMLPs,aswellastherawinput,asinputsforeachaddedstage.Why would greedy supervised pretraininghelp?The hypothesis initiallydiscussedby()isthatithelpstoprovidebetterguidancetotheBengioetal.2007323 CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELS y yh(1)h(1)xx(a)U(1)U(1)W(1)W(1)y yh(1)h(1)x x(b)U(1)U(1)W(1)W(1) y yh(1)h(1)x x(c)U(1)U(1)W(1)W(1)h(2)h(2)y yU(2)U(2)W(2)W(2)y yh(1)h(1)x x(d)U(1)U(1)W(1)W(1)h(2)h(2)yU(2)U(2)W(2)W(2) Figure8.7:Illustrationofoneformofgreedysupervisedpretraining(,).Bengioetal.2007(a)Westartbytrainingasuﬃcientlyshallowarchitecture.Anotherdrawingofthe(b)samearchitecture.Wekeeponlytheinput-to-hiddenlayeroftheoriginalnetworkand(c)discardthehidden-to-outputlayer.WesendtheoutputoftheﬁrsthiddenlayerasinputtoanothersupervisedsinglehiddenlayerMLPthatistrainedwiththesameobjectiveastheﬁrstnetworkwas,thusaddingasecondhiddenlayer.Thiscanberepeatedforasmanylayersasdesired.Anotherdrawingoftheresult,viewedasafeedforwardnetwork.(d)Tofurtherimprovetheoptimization,wecanjointlyﬁne-tuneallthelayers,eitheronlyattheendorateachstageofthisprocess.324 CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSintermediatelevelsofadeephierarchy.Ingeneral,pretrainingmayhelpbothintermsofoptimizationandintermsofgeneralization.Anapproachrelatedtosupervisedpretrainingextendstheideatothecontextoftransferlearning:Yosinski2014etal.()pretrainadeepconvolutionalnetwith8layersofweightsonasetoftasks(asubsetofthe1000ImageNetobjectcategories)andtheninitializeasame-sizenetworkwiththeﬁrstklayersoftheﬁrstnet.Allthelayersofthesecondnetwork(withtheupperlayersinitializedrandomly)arethenjointlytrainedtoperformadiﬀerentsetoftasks(anothersubsetofthe1000ImageNetobjectcategories),withfewertrainingexamplesthanfortheﬁrstsetoftasks.Otherapproachestotransferlearningwithneuralnetworksarediscussedinsection.15.2AnotherrelatedlineofworkistheFitNets(,)approach.Romeroetal.2015Thisapproachbeginsbytraininganetworkthathaslowenoughdepthandgreatenoughwidth(numberofunitsperlayer)tobeeasytotrain.Thisnetworkthenbecomesateacherforasecondnetwork,designatedthestudent.Thestudentnetworkismuchdeeperandthinner(eleventonineteenlayers)andwouldbediﬃculttotrainwithSGDundernormalcircumstances.Thetrainingofthestudentnetworkismadeeasierbytrainingthestudentnetworknotonlytopredicttheoutputfortheoriginaltask,butalsotopredictthevalueofthemiddlelayeroftheteachernetwork.Thisextrataskprovidesasetofhintsabouthowthehiddenlayersshouldbeusedandcansimplifytheoptimizationproblem.Additionalparametersareintroducedtoregressthemiddlelayerofthe5-layerteachernetworkfromthemiddlelayerofthedeeperstudentnetwork.However,insteadofpredictingtheﬁnalclassiﬁcationtarget,theobjectiveistopredictthemiddlehiddenlayeroftheteachernetwork. Thelowerlayersofthestudentnetworksthushavetwoobjectives:tohelptheoutputsofthestudentnetworkaccomplishtheirtask,aswellastopredicttheintermediatelayeroftheteachernetwork.Althoughathinanddeepnetworkappearstobemorediﬃculttotrainthanawideandshallownetwork,thethinanddeepnetworkmaygeneralizebetterandcertainlyhaslowercomputationalcostifitisthinenoughtohavefarfewerparameters.Withoutthehintsonthehiddenlayer,thestudentnetworkperformsverypoorlyintheexperiments,bothonthetrainingandtestset.Hintsonmiddlelayersmaythusbeoneofthetoolstohelptrainneuralnetworksthatotherwiseseemdiﬃculttotrain,butotheroptimizationtechniquesorchangesinthearchitecturemayalsosolvetheproblem.325 CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELS8.7.5DesigningModelstoAidOptimizationToimproveoptimization,thebeststrategyisnotalwaystoimprovetheoptimizationalgorithm.Instead,manyimprovementsintheoptimizationofdeepmodelshavecomefromdesigningthemodelstobeeasiertooptimize.Inprinciple,wecoulduseactivationfunctionsthatincreaseanddecreaseinjaggednon-monotonicpatterns.However,thiswouldmakeoptimizationextremelydiﬃcult.Inpractice,itismoreimportanttochooseamodelfamilythatiseasytooptimizethantouseapowerfuloptimizationalgorithm.Mostoftheadvancesinneuralnetworklearningoverthepast30yearshavebeenobtainedbychangingthemodelfamilyratherthanchangingtheoptimizationprocedure.Stochasticgradientdescentwithmomentum,whichwasusedtotrainneuralnetworksinthe1980s,remainsinuseinmodernstateoftheartneuralnetworkapplications.Speciﬁcally,modernneuralnetworksreﬂectadesignchoicetouselineartrans-formationsbetweenlayersandactivationfunctionsthatarediﬀerentiablealmosteverywhereandhavesigniﬁcantslopeinlargeportionsoftheirdomain. Inpar-ticular,modelinnovationsliketheLSTM,rectiﬁedlinearunitsandmaxoutunitshaveallmovedtowardusingmorelinearfunctionsthanpreviousmodelslikedeepnetworksbasedonsigmoidalunits.Thesemodelshavenicepropertiesthatmakeoptimizationeasier.ThegradientﬂowsthroughmanylayersprovidedthattheJacobianofthelineartransformationhasreasonablesingularvalues. Moreover,linearfunctionsconsistentlyincreaseinasingledirection,soevenifthemodel’soutputisveryfarfromcorrect,itisclearsimplyfromcomputingthegradientwhichdirectionitsoutputshouldmovetoreducethelossfunction.Inotherwords,modernneuralnetshavebeendesignedsothattheirlocalgradientinformationcorrespondsreasonablywelltomovingtowardadistantsolution.Othermodeldesignstrategiescanhelptomakeoptimizationeasier.Forexample,linearpathsorskipconnectionsbetweenlayersreducethelengthoftheshortestpathfromthelower layer’sparameters totheoutput, andthusmitigatethevanishinggradientproblem(Srivastava2015etal.,).Arelatedideatoskipconnectionsisaddingextracopiesoftheoutputthatareattachedtotheintermediatehiddenlayersofthenetwork,asinGoogLeNet(,)Szegedyetal.2014aanddeeply-supervisednets(,).These“auxiliaryheads”aretrainedLeeetal.2014toperformthesametaskastheprimaryoutputatthetopofthenetworkinordertoensurethatthelowerlayersreceivealargegradient.Whentrainingiscompletetheauxiliaryheadsmaybediscarded. Thisisanalternativetothepretrainingstrategies,whichwereintroducedintheprevioussection.Inthisway,onecantrainjointlyallthelayersinasinglephasebutchangethearchitecture,sothatintermediatelayers(especiallythelowerones)cangetsomehintsaboutwhatthey326 CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSshoulddo,viaashorterpath.Thesehintsprovideanerrorsignaltolowerlayers.8.7.6ContinuationMethodsandCurriculumLearningAsarguedinsection,manyofthechallengesinoptimizationarisefromthe8.2.7globalstructureofthecostfunctionandcannotberesolvedmerelybymakingbetterestimatesoflocalupdatedirections.Thepredominantstrategyforovercomingthisproblemistoattempttoinitializetheparametersinaregionthatisconnectedtothesolutionbyashortpaththroughparameterspacethatlocaldescentcandiscover.Continuationmethodsareafamilyofstrategiesthatcanmakeoptimizationeasierbychoosinginitialpointstoensurethatlocaloptimizationspendsmostofitstimeinwell-behavedregionsofspace.Theideabehindcontinuationmethodsistoconstructaseriesofobjectivefunctionsoverthesameparameters.InordertominimizeacostfunctionJ(θ),wewillconstructnewcostfunctions{J(0),...,J()n}.Thesecostfunctionsaredesignedtobeincreasinglydiﬃcult,withJ(0)beingfairlyeasytominimize,andJ()n,themostdiﬃcult,beingJ(θ),thetruecostfunctionmotivatingtheentireprocess.WhenwesaythatJ()iiseasierthanJ(+1)i,wemeanthatitiswellbehavedovermoreofθspace.Arandominitializationismorelikelytolandintheregionwherelocaldescentcanminimizethecostfunctionsuccessfullybecausethisregionislarger.Theseriesofcostfunctionsaredesignedsothatasolutiontooneisagoodinitialpointofthenext.Wethusbeginbysolvinganeasyproblemthenreﬁnethesolutiontosolveincrementallyharderproblemsuntilwearriveatasolutiontothetrueunderlyingproblem.Traditionalcontinuationmethods(predatingtheuseofcontinuationmethodsforneuralnetworktraining)areusuallybasedonsmoothingtheobjectivefunction.SeeWu1997()foranexampleofsuchamethodandareviewofsomerelatedmethods.Continuationmethodsarealsocloselyrelatedtosimulatedannealing,whichaddsnoisetotheparameters(Kirkpatrick 1983etal.,).Continuationmethodshavebeenextremelysuccessfulinrecentyears.SeeMobahiandFisher()foranoverviewofrecentliterature,especiallyforAIapplications.2015Continuationmethodstraditionallyweremostlydesignedwiththegoalofovercomingthechallengeoflocalminima.Speciﬁcally,theyweredesignedtoreachaglobalminimumdespitethepresenceofmanylocalminima.Todoso,thesecontinuationmethodswouldconstructeasiercostfunctionsby“blurring”theoriginalcostfunction.ThisblurringoperationcanbedonebyapproximatingJ()i() = θEθ∼N(θ;θ,σ()2i)J(θ)(8.40)viasampling.Theintuitionforthisapproachisthatsomenon-convexfunctions327 CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSbecomeapproximatelyconvexwhenblurred.Inmanycases,thisblurringpreservesenoughinformationaboutthelocationofaglobalminimumthatwecanﬁndtheglobalminimumbysolvingprogressivelylessblurredversionsoftheproblem.Thisapproachcanbreakdowninthreediﬀerentways.First,itmightsuccessfullydeﬁneaseriesofcostfunctionswheretheﬁrstisconvexandtheoptimumtracksfromonefunctiontothenextarrivingattheglobalminimum,butitmightrequiresomanyincrementalcostfunctionsthatthecostoftheentireprocedureremainshigh.NP-hardoptimizationproblemsremainNP-hard,evenwhencontinuationmethodsareapplicable.Theothertwowaysthatcontinuationmethodsfailbothcorrespondtothemethodnotbeingapplicable.First,thefunctionmightnotbecomeconvex,nomatterhowmuchitisblurred.ConsiderforexamplethefunctionJ(θ) =−θθ.Second,thefunctionmaybecomeconvexasaresultofblurring,buttheminimumofthisblurredfunctionmaytracktoalocalratherthanaglobalminimumoftheoriginalcostfunction.Thoughcontinuationmethodsweremostlyoriginallydesignedtodealwiththeproblemoflocalminima,localminimaarenolongerbelievedtobetheprimaryproblemforneuralnetworkoptimization.Fortunately,continuationmethodscanstillhelp.Theeasierobjectivefunctionsintroducedbythecontinuationmethodcaneliminateﬂatregions,decreasevarianceingradientestimates,improveconditioningoftheHessianmatrix,ordoanythingelsethatwilleithermakelocalupdateseasiertocomputeorimprovethecorrespondencebetweenlocalupdatedirectionsandprogresstowardaglobalsolution.Bengio2009etal.()observedthatanapproachcalledcurriculumlearningorshapingcanbeinterpretedasacontinuationmethod.Curriculumlearningisbasedontheideaofplanningalearningprocesstobeginbylearningsimpleconceptsandprogresstolearningmorecomplexconceptsthatdependonthesesimplerconcepts.Thisbasicstrategywaspreviouslyknowntoaccelerateprogressinanimaltraining(,;,;Skinner1958Peterson2004KruegerandDayan2009,)andmachinelearning(,;,;,).()Solomonoﬀ1989Elman1993Sanger1994Bengioetal.2009justiﬁedthisstrategyasacontinuationmethod,whereearlierJ()iaremadeeasierbyincreasingtheinﬂuenceofsimplerexamples(eitherbyassigningtheircontributionstothecostfunctionlargercoeﬃcients,orbysamplingthemmorefrequently),andexperimentallydemonstratedthatbetterresultscouldbeobtainedbyfollowingacurriculumonalarge-scaleneurallanguagemodelingtask.Curriculumlearninghasbeensuccessfulonawiderangeofnaturallanguage(Spitkovsky2010etal.,;Collobert2011aMikolov2011bTuandHonavar2011etal.,;etal.,;,)andcomputervision(,;,;,)Kumaretal.2010LeeandGrauman2011SupancicandRamanan2013tasks.Curriculumlearningwasalsoveriﬁedasbeingconsistentwiththewayinwhichhumansteach(,):teachersstartbyshowingeasierandKhanetal.2011328 CHAPTER8.OPTIMIZATIONFORTRAININGDEEPMODELSmoreprototypicalexamplesandthenhelpthelearnerreﬁnethedecisionsurfacewiththelessobviouscases.Curriculum-basedstrategiesaremoreeﬀectiveforteachinghumansthanstrategiesbasedonuniformsamplingofexamples,andcanalsoincreasetheeﬀectivenessofotherteachingstrategies(,BasuandChristensen2013).Anotherimportantcontributiontoresearchoncurriculumlearningaroseinthecontextoftrainingrecurrentneuralnetworkstocapturelong-termdependencies:ZarembaandSutskever2014()foundthatmuchbetterresultswereobtainedwithastochasticcurriculum,inwhicharandommixofeasyanddiﬃcultexamplesisalwayspresentedtothelearner,butwheretheaverageproportionofthemorediﬃcultexamples(here,thosewithlonger-termdependencies)isgraduallyincreased.Withadeterministiccurriculum,noimprovementoverthebaseline(ordinarytrainingfromthefulltrainingset)wasobserved.Wehavenowdescribedthebasicfamilyofneuralnetworkmodelsandhowtoregularizeandoptimizethem.Inthechaptersahead,weturntospecializationsoftheneuralnetworkfamily,thatallowneuralnetworkstoscaletoverylargesizesandprocessinputdatathathasspecialstructure.Theoptimizationmethodsdiscussedinthischapterareoftendirectlyapplicabletothesespecializedarchitectureswithlittleornomodiﬁcation. 329 Chapter9ConvolutionalNetworksConvolutionalnetworks(,),alsoknownasLeCun1989convolutionalneuralnetworksorCNNs,areaspecializedkindofneuralnetworkforprocessingdatathathasaknown,grid-liketopology.Examplesincludetime-seriesdata,whichcanbethoughtofasa1Dgridtakingsamplesatregulartimeintervals,andimagedata,whichcanbethoughtofasa2Dgridofpixels.Convolutionalnetworkshavebeentremendouslysuccessfulinpracticalapplications.Thename“convolutionalneuralnetwork”indicatesthatthenetworkemploysamathematicaloperationcalledconvolution.Convolutionisaspecializedkindoflinearoperation.Convolutionalnetworksaresimplyneuralnetworksthatuseconvolutioninplaceofgeneralmatrixmultiplicationinatleastoneoftheirlayers.Inthis chapter, wewillﬁrst describewhatconvolutionis.Next, wewillexplainthemotivationbehindusingconvolutioninaneuralnetwork.Wewillthendescribeanoperationcalledpooling,whichalmostallconvolutionalnetworksemploy.Usually,theoperationusedinaconvolutionalneuralnetworkdoesnotcorrespondpreciselytothedeﬁnitionofconvolutionasusedinotherﬁeldssuchasengineeringorpuremathematics.Wewilldescribeseveralvariantsontheconvolutionfunctionthatarewidelyusedinpracticeforneuralnetworks.Wewillalso show how convolutionmaybeappliedtomanykindsofdata, withdiﬀerentnumbersofdimensions.Wethendiscussmeansofmakingconvolutionmoreeﬃcient.Convolutionalnetworksstandoutasanexampleofneuroscientiﬁcprinciplesinﬂuencingdeeplearning.Wewilldiscusstheseneuroscientiﬁcprinciples,thenconcludewithcommentsabouttheroleconvolutionalnetworkshaveplayedinthehistoryofdeeplearning.Onetopicthischapterdoesnotaddressishowtochoosethearchitectureofyourconvolutionalnetwork.Thegoalofthischapteristodescribethekindsoftoolsthatconvolutionalnetworksprovide,whilechapter11330 CHAPTER9.CONVOLUTIONALNETWORKSdescribesgeneralguidelinesforchoosingwhichtoolstouseinwhichcircumstances.Researchintoconvolutionalnetworkarchitecturesproceedssorapidlythatanewbestarchitectureforagivenbenchmarkisannouncedeveryfewweekstomonths,renderingitimpracticaltodescribethebestarchitectureinprint.However,thebestarchitectureshaveconsistentlybeencomposedofthebuildingblocksdescribedhere.9.1TheConvolutionOperationInitsmostgeneralform,convolutionisanoperationontwofunctionsofareal-valuedargument.Tomotivatethedeﬁnitionofconvolution,westartwithexamplesoftwofunctionswemightuse.Supposewearetrackingthelocationofaspaceshipwithalasersensor.Ourlasersensorprovidesasingleoutputx(t),thepositionofthespaceshipattimet.Bothxandtarereal-valued,i.e.,wecangetadiﬀerentreadingfromthelasersensoratanyinstantintime.Nowsupposethatourlasersensorissomewhatnoisy.Toobtainalessnoisyestimateofthespaceship’sposition,wewouldliketoaveragetogetherseveralmeasurements.Ofcourse,morerecentmeasurementsaremorerelevant,sowewillwantthistobeaweightedaveragethatgivesmoreweighttorecentmeasurements.Wecandothiswithaweightingfunctionw(a),whereaistheageofameasurement.Ifweapplysuchaweightedaverageoperationateverymoment,weobtainanewfunctionprovidingasmoothedestimateofthepositionofthespaceship:sst() =xawtada()(−)(9.1)Thisoperationiscalledconvolution.Theconvolutionoperationistypicallydenotedwithanasterisk:stxwt() = (∗)()(9.2)Inourexample,wneedstobeavalidprobabilitydensityfunction,ortheoutputisnotaweightedaverage.Also,wneedstobeforallnegativearguments,0oritwilllookintothefuture,whichispresumablybeyondourcapabilities.Theselimitationsareparticulartoourexamplethough.Ingeneral,convolutionisdeﬁnedforanyfunctionsforwhichtheaboveintegralisdeﬁned,andmaybeusedforotherpurposesbesidestakingweightedaverages.Inconvolutionalnetworkterminology,theﬁrstargument(inthisexample,thefunctionx)totheconvolutionisoftenreferredtoastheinputandthesecond331 CHAPTER9.CONVOLUTIONALNETWORKSargument(inthisexample,thefunctionw)asthekernel.Theoutputissometimesreferredtoasthe.featuremapInourexample,theideaofalasersensorthatcanprovidemeasurementsateveryinstantintimeisnotrealistic.Usually,whenweworkwithdataonacomputer,timewillbediscretized,andoursensorwillprovidedataatregularintervals.Inourexample,itmightbemorerealistictoassumethatourlaserprovidesameasurementoncepersecond.Thetimeindextcanthentakeononlyintegervalues.Ifwenowassumethatxandwaredeﬁnedonlyonintegert,wecandeﬁnethediscreteconvolution:stxwt() = (∗)() =∞a=−∞xawta()(−)(9.3)Inmachinelearningapplications,theinputisusuallyamultidimensionalarrayofdataandthekernelisusuallyamultidimensionalarrayofparametersthatareadaptedbythelearningalgorithm.Wewillrefertothesemultidimensionalarraysastensors.Becauseeachelementoftheinputandkernelmustbeexplicitlystoredseparately,weusuallyassumethatthesefunctionsarezeroeverywherebuttheﬁnitesetofpointsforwhichwestorethevalues.Thismeansthatinpracticewecanimplementtheinﬁnitesummationasasummationoveraﬁnitenumberofarrayelements.Finally,weoftenuseconvolutionsovermorethanoneaxisatatime.Forexample,ifweuseatwo-dimensionalimageIasourinput,weprobablyalsowanttouseatwo-dimensionalkernel:KSi,jIKi,j() = (∗)() =mnIm,nKim,jn.()(−−)(9.4)Convolutioniscommutative,meaningwecanequivalentlywrite:Si,jKIi,j() = (∗)() =mnIim,jnKm,n.(−−)()(9.5)Usuallythelatterformulaismorestraightforwardtoimplementinamachinelearninglibrary,becausethereislessvariationintherangeofvalidvaluesofmand.nThecommutativepropertyofconvolutionarisesbecausewehaveﬂippedthekernelrelativetotheinput,inthesensethatasmincreases,theindexintotheinputincreases,buttheindexintothekerneldecreases.Theonlyreasontoﬂipthekernelistoobtainthecommutativeproperty.Whilethecommutativeproperty332 CHAPTER9.CONVOLUTIONALNETWORKSisusefulforwritingproofs,itisnotusuallyanimportantpropertyofaneuralnetworkimplementation.Instead,manyneuralnetworklibrariesimplementarelatedfunctioncalledthecross-correlation,whichisthesameasconvolutionbutwithoutﬂippingthekernel:Si,jIKi,j() = (∗)() =mnIim,jnKm,n.(++)()(9.6)Manymachinelearninglibrariesimplementcross-correlationbutcallitconvolution.Inthistextwewillfollowthisconventionofcallingbothoperationsconvolution,andspecifywhetherwemeantoﬂipthekernelornotincontextswherekernelﬂippingisrelevant.Inthecontextofmachinelearning,thelearningalgorithmwilllearntheappropriatevaluesofthekernelintheappropriateplace,soanalgorithmbasedonconvolutionwithkernelﬂippingwilllearnakernelthatisﬂippedrelativetothekernellearnedbyanalgorithmwithouttheﬂipping.Itisalsorareforconvolutiontobeusedaloneinmachinelearning;insteadconvolutionisusedsimultaneouslywithotherfunctions,andthecombinationofthesefunctionsdoesnotcommuteregardlessofwhethertheconvolutionoperationﬂipsitskernelornot.Seeﬁgureforanexampleofconvolution(withoutkernelﬂipping)applied9.1toa2-Dtensor.Discreteconvolutioncanbeviewedasmultiplicationbyamatrix.However,thematrixhasseveralentriesconstrainedtobeequaltootherentries.Forexample,forunivariatediscreteconvolution,eachrowofthematrixisconstrainedtobeequaltotherowaboveshiftedbyoneelement.ThisisknownasaToeplitzmatrix.Intwodimensions,adoublyblockcirculantmatrixcorrespondstoconvolution.Inadditiontotheseconstraintsthatseveralelementsbeequaltoeachother,convolutionusuallycorrespondstoaverysparsematrix(amatrixwhoseentriesaremostlyequaltozero).Thisisbecausethekernelisusuallymuchsmallerthantheinputimage.Anyneuralnetworkalgorithmthatworkswithmatrixmultiplicationanddoesnotdependonspeciﬁcpropertiesofthematrixstructureshouldworkwithconvolution,withoutrequiringanyfurtherchangestotheneuralnetwork.Typicalconvolutionalneuralnetworksdomakeuseoffurtherspecializationsinordertodealwithlargeinputseﬃciently,butthesearenotstrictlynecessaryfromatheoreticalperspective.333 CHAPTER9.CONVOLUTIONALNETWORKS abcdefghijklwxyz aw+bx+ey+fzaw+bx+ey+fzbw+cx+fy+gzbw+cx+fy+gzcw+dx+gy+hzcw+dx+gy+hzew+fx+iy+jzew+fx+iy+jzfw+gx+jy+kzfw+gx+jy+kzgw+hx+ky+lzgw+hx+ky+lzInputKernel Output Figure9.1:Anexampleof2-Dconvolutionwithoutkernel-ﬂipping.Inthiscasewerestricttheoutputtoonlypositionswherethekernelliesentirelywithintheimage,called“valid”convolutioninsomecontexts.Wedrawboxeswitharrowstoindicatehowtheupper-leftelementoftheoutputtensorisformedbyapplyingthekerneltothecorrespondingupper-leftregionoftheinputtensor. 334 CHAPTER9.CONVOLUTIONALNETWORKS9.2MotivationConvolutionleveragesthreeimportantideasthatcanhelpimproveamachinelearningsystem:sparseinteractions,parametersharingandequivariantrepresentations.Moreover, convolutionprovidesameansforworkingwithinputsofvariablesize.Wenowdescribeeachoftheseideasinturn.Traditionalneuralnetworklayersusematrixmultiplicationbyamatrixofparameterswithaseparateparameterdescribingtheinteractionbetweeneachinputunitandeachoutputunit.Thismeanseveryoutputunitinteractswitheveryinputunit.Convolutionalnetworks,however,typicallyhavesparseinteractions(alsoreferredtoassparseconnectivityorsparseweights).Thisisaccomplishedbymakingthekernelsmallerthantheinput.Forexample,whenprocessinganimage,theinputimagemighthavethousandsormillionsofpixels,butwecandetectsmall,meaningfulfeaturessuchasedgeswithkernelsthatoccupyonlytensorhundredsofpixels.Thismeansthatweneedtostorefewerparameters,whichbothreducesthememoryrequirementsofthemodelandimprovesitsstatisticaleﬃciency.Italsomeansthatcomputingtheoutputrequiresfeweroperations.Theseimprovementsineﬃciencyareusuallyquitelarge.Ifthereareminputsandnoutputs,thenmatrixmultiplicationrequiresmn×parametersandthealgorithmsusedinpracticehaveO(mn×)runtime(perexample).Ifwelimitthenumberofconnectionseachoutputmayhavetok,thenthesparselyconnectedapproachrequiresonlykn×parametersandO(kn×)runtime.Formanypracticalapplications,itispossibletoobtaingoodperformanceonthemachinelearningtaskwhilekeepingkseveralordersofmagnitudesmallerthanm. Forgraphicaldemonstrationsofsparseconnectivity,seeﬁgureandﬁgure.Inadeepconvolutionalnetwork,9.29.3unitsinthedeeperlayersmayindirectlyinteractwithalargerportionoftheinput,asshowninﬁgure.Thisallowsthenetworktoeﬃcientlydescribecomplicated9.4interactionsbetweenmanyvariablesbyconstructingsuchinteractionsfromsimplebuildingblocksthateachdescribeonlysparseinteractions.Parametersharingreferstousingthesameparameterformorethanonefunctioninamodel.Inatraditionalneuralnet,eachelementoftheweightmatrixisusedexactlyoncewhencomputingtheoutputofalayer.Itismultipliedbyoneelementoftheinputandthenneverrevisited.Asasynonymforparametersharing,onecansaythatanetworkhastiedweights,becausethevalueoftheweightappliedtooneinputistiedtothevalueofaweightappliedelsewhere.Inaconvolutionalneuralnet,eachmemberofthekernelisusedateverypositionoftheinput(exceptperhapssomeoftheboundarypixels, dependingonthedesigndecisionsregardingtheboundary).Theparametersharingusedbytheconvolutionoperationmeansthatratherthanlearningaseparatesetofparameters335 CHAPTER9.CONVOLUTIONALNETWORKS x1x1x2x2x3x3s2s2s1s1s3s3x4x4s4s4x5x5s5s5 x1x1x2x2x3x3s2s2s1s1s3s3x4x4s4s4x5x5s5s5 Figure9.2:Sparseconnectivity,viewedfrombelow:Wehighlightoneinputunit,x3,andalsohighlighttheoutputunitsinsthatareaﬀectedbythisunit.(Top)Whensisformedbyconvolutionwithakernelofwidth,onlythreeoutputsareaﬀectedby3x.(Bottom)Whenisformedbymatrixmultiplication,connectivityisnolongersparse,sosalloftheoutputsareaﬀectedbyx3. 336 CHAPTER9.CONVOLUTIONALNETWORKS x1x1x2x2x3x3s2s2s1s1s3s3x4x4s4s4x5x5s5s5 x1x1x2x2x3x3s2s2s1s1s3s3x4x4s4s4x5x5s5s5 Figure9.3:Sparseconnectivity,viewedfromabove: Wehighlightoneoutputunit,s3,andalsohighlighttheinputunitsinxthataﬀectthisunit.Theseunitsareknownasthereceptiveﬁeldofs3.(Top)Whensisformedbyconvolutionwithakernelofwidth,onlythreeinputsaﬀect3s3.When(Bottom)sisformedbymatrixmultiplication,connectivityisnolongersparse,soalloftheinputsaﬀects3. x1x1x2x2x3x3h2h2h1h1h3h3x4x4h4h4x5x5h5h5g2g2g1g1g3g3g4g4g5g5 Figure9.4:Thereceptiveﬁeldoftheunitsinthedeeperlayersofaconvolutionalnetworkislargerthanthereceptiveﬁeldoftheunitsintheshallowlayers.Thiseﬀectincreasesifthenetworkincludesarchitecturalfeatureslikestridedconvolution(ﬁgure)orpooling9.12(section).Thismeansthateventhough9.3directconnectionsinaconvolutionalnetareverysparse,unitsinthedeeperlayerscanbeindirectlyconnectedtoallormostoftheinputimage.337 CHAPTER9.CONVOLUTIONALNETWORKS x1x1x2x2x3x3s2s2s1s1s3s3x4x4s4s4x5x5s5s5 x1x1x2x2x3x3x4x4x5x5s2s2s1s1s3s3s4s4s5s5Figure9.5:Parametersharing:Blackarrowsindicatetheconnectionsthatuseaparticularparameterintwodiﬀerentmodels. (Top)Theblackarrowsindicateusesofthecentralelementofa3-elementkernelinaconvolutionalmodel.Duetoparametersharing,thissingleparameterisusedatallinputlocations.Thesingleblackarrowindicates(Bottom)theuseofthecentralelementoftheweightmatrixinafullyconnectedmodel.Thismodelhasnoparametersharingsotheparameterisusedonlyonce.foreverylocation,welearnonlyoneset.Thisdoesnotaﬀecttheruntimeofforwardpropagation—itisstillO(kn×)—butitdoesfurtherreducethestoragerequirementsofthemodeltokparameters.Recallthatkisusuallyseveralordersofmagnitudelessthanm.Sincemandnareusuallyroughlythesamesize,kispracticallyinsigniﬁcantcomparedtomn×.Convolutionisthusdramaticallymoreeﬃcientthandensematrixmultiplicationintermsofthememoryrequirementsandstatisticaleﬃciency.Foragraphicaldepictionofhowparametersharingworks,seeﬁgure.9.5Asanexampleofbothoftheseﬁrsttwoprinciplesinaction,ﬁgureshows9.6howsparseconnectivityandparametersharingcandramaticallyimprovetheeﬃciencyofalinearfunctionfordetectingedgesinanimage.Inthecaseofconvolution,theparticularformofparametersharingcausesthelayertohaveapropertycalledequivariancetotranslation.Tosayafunctionisequivariantmeansthatiftheinputchanges,theoutputchangesinthesameway.Speciﬁcally,afunctionf(x)isequivarianttoafunctiongiff(g(x))=g(f(x)).Inthecaseofconvolution,ifweletgbeanyfunctionthattranslatestheinput,i.e.,shiftsit,thentheconvolutionfunctionisequivarianttog.Forexample,letIbeafunctiongivingimagebrightnessatintegercoordinates.Letgbeafunction338 CHAPTER9.CONVOLUTIONALNETWORKSmappingoneimagefunctiontoanotherimagefunction,suchthatI=g(I)istheimagefunctionwithI(x,y)=I(x−1,y).ThisshiftseverypixelofIoneunittotheright.IfweapplythistransformationtoI,thenapplyconvolution,theresultwillbethesameasifweappliedconvolutiontoI,thenappliedthetransformationgtotheoutput.Whenprocessingtimeseriesdata,thismeansthatconvolutionproducesasortoftimelinethatshowswhendiﬀerentfeaturesappearintheinput.Ifwemoveaneventlaterintimeintheinput,theexactsamerepresentationofitwillappearintheoutput,justlaterintime.Similarlywithimages,convolutioncreatesa2-Dmapofwherecertainfeaturesappearintheinput.Ifwemovetheobjectintheinput,itsrepresentationwillmovethesameamountintheoutput.Thisisusefulforwhenweknowthatsomefunctionofasmallnumberofneighboringpixelsisusefulwhenappliedtomultipleinputlocations.Forexample,whenprocessingimages,itisusefultodetectedgesintheﬁrstlayerofaconvolutionalnetwork.Thesameedgesappearmoreorlesseverywhereintheimage,soitispracticaltoshareparametersacrosstheentireimage.Insomecases,wemaynotwishtoshareparametersacrosstheentireimage.Forexample,ifweareprocessingimagesthatarecroppedtobecenteredonanindividual’sface,weprobablywanttoextractdiﬀerentfeaturesatdiﬀerentlocations—thepartofthenetworkprocessingthetopofthefaceneedstolookforeyebrows,whilethepartofthenetworkprocessingthebottomofthefaceneedstolookforachin.Convolutionisnotnaturallyequivarianttosomeothertransformations,suchaschangesinthescaleorrotationofanimage.Othermechanismsarenecessaryforhandlingthesekindsoftransformations.Finally,somekindsofdatacannotbeprocessedbyneuralnetworksdeﬁnedbymatrixmultiplicationwithaﬁxed-shapematrix.Convolutionenablesprocessingofsomeofthesekindsofdata.Wediscussthisfurtherinsection.9.79.3PoolingAtypicallayerofaconvolutionalnetworkconsistsofthreestages(seeﬁgure).9.7Intheﬁrststage,thelayerperformsseveralconvolutionsinparalleltoproduceasetoflinearactivations.Inthesecondstage,eachlinearactivationisrunthroughanonlinearactivationfunction,suchastherectiﬁedlinearactivationfunction.Thisstageissometimescalledthedetectorstage. Inthethirdstage,weuseapoolingfunctiontomodifytheoutputofthelayerfurther.Apoolingfunctionreplacestheoutputofthenetatacertainlocationwithasummarystatisticofthenearbyoutputs.Forexample,themaxpooling(Zhou339 CHAPTER9.CONVOLUTIONALNETWORKS Figure9.6:Eﬃciencyofedgedetection. Theimageontherightwasformedbytakingeachpixelintheoriginalimageandsubtractingthevalueofitsneighboringpixelontheleft. Thisshowsthestrengthofalloftheverticallyorientededgesintheinputimage,whichcanbeausefuloperationforobjectdetection.Bothimagesare280pixelstall.Theinputimageis320pixelswidewhiletheoutputimageis319pixelswide.Thistransformationcanbedescribedbyaconvolutionkernelcontainingtwoelements,andrequires319×280×3=267,960ﬂoatingpointoperations(twomultiplicationsandoneadditionperoutputpixel)tocomputeusingconvolution.Todescribethesametransformationwithamatrixmultiplicationwouldtake320×280×319×280,orovereightbillion,entriesinthematrix,makingconvolutionfourbilliontimesmoreeﬃcientforrepresentingthistransformation.Thestraightforwardmatrixmultiplicationalgorithmperformsoversixteenbillionﬂoatingpointoperations,makingconvolutionroughly60,000timesmoreeﬃcientcomputationally.Ofcourse,mostoftheentriesofthematrixwouldbezero.Ifwestoredonlythenonzeroentriesofthematrix,thenbothmatrixmultiplicationandconvolutionwouldrequirethesamenumberofﬂoatingpointoperationstocompute.Thematrixwouldstillneedtocontain2×319×280=178,640entries.Convolutionisanextremelyeﬃcientwayofdescribingtransformationsthatapplythesamelineartransformationofasmall,localregionacrosstheentireinput.(Photocredit:PaulaGoodfellow) 340 CHAPTER9.CONVOLUTIONALNETWORKS Convolutional Layer Input to layerConvolution stage:Ane transformﬃDetector stage:Nonlinearitye.g., rectiﬁed linearPooling stageNext layer Input to layersConvolution layer:Ane transform ﬃDetector layer: Nonlinearitye.g., rectiﬁed linearPooling layerNext layerComplex layer terminologySimple layer terminology Figure9.7:Thecomponentsofatypicalconvolutionalneuralnetworklayer.Therearetwocommonlyusedsetsofterminologyfordescribingtheselayers.(Left)Inthisterminology,theconvolutionalnetisviewedasasmallnumberofrelativelycomplexlayers,witheachlayerhavingmany“stages.”Inthisterminology,thereisaone-to-onemappingbetweenkerneltensorsandnetworklayers.Inthisbookwegenerallyusethisterminology.(Right)Inthisterminology,theconvolutionalnetisviewedasalargernumberofsimplelayers;everystepofprocessingisregardedasalayerinitsownright.Thismeansthatnotevery“layer”hasparameters. 341 CHAPTER9.CONVOLUTIONALNETWORKSandChellappa1988,)operationreportsthemaximumoutputwithinarectangularneighborhood.Otherpopularpoolingfunctionsincludetheaverageofarectangularneighborhood,theL2normofarectangularneighborhood,oraweightedaveragebasedonthedistancefromthecentralpixel.Inallcases,poolinghelpstomaketherepresentationbecomeapproximatelyinvarianttosmalltranslationsoftheinput.Invariancetotranslationmeansthatifwetranslatetheinputbyasmallamount,thevaluesofmostofthepooledoutputsdonotchange.Seeﬁgureforanexampleofhowthisworks.9.8Invariancetolocaltranslationcanbeaveryusefulpropertyifwecaremoreaboutwhethersomefeatureispresentthanexactlywhereitis.Forexample,whendeterminingwhetheranimagecontainsaface,weneednotknowthelocationoftheeyeswithpixel-perfectaccuracy,wejustneedtoknowthatthereisaneyeontheleftsideofthefaceandaneyeontherightsideoftheface.Inothercontexts,itismoreimportanttopreservethelocationofafeature.Forexample,ifwewanttoﬁndacornerdeﬁnedbytwoedgesmeetingataspeciﬁcorientation,weneedtopreservethelocationoftheedgeswellenoughtotestwhethertheymeet.Theuseofpoolingcanbeviewedasaddinganinﬁnitelystrongpriorthatthefunctionthelayerlearnsmustbeinvarianttosmalltranslations.Whenthisassumptioniscorrect,itcangreatlyimprovethestatisticaleﬃciencyofthenetwork.Poolingoverspatialregionsproducesinvariancetotranslation,butifwepoolovertheoutputsofseparatelyparametrizedconvolutions,thefeaturescanlearnwhichtransformationstobecomeinvariantto(seeﬁgure).9.9Becausepoolingsummarizestheresponsesoverawholeneighborhood,itispossibletousefewerpoolingunitsthandetectorunits,byreportingsummarystatisticsforpoolingregionsspacedkpixelsapartratherthan1pixelapart.Seeﬁgureforanexample.Thisimprovesthecomputationaleﬃciencyofthe9.10networkbecausethenextlayerhasroughlyktimesfewerinputstoprocess.Whenthenumberofparametersinthenextlayerisafunctionofitsinputsize(suchaswhenthenextlayerisfullyconnectedandbasedonmatrixmultiplication)thisreductionintheinputsizecanalsoresultinimprovedstatisticaleﬃciencyandreducedmemoryrequirementsforstoringtheparameters.Formanytasks,poolingisessentialforhandlinginputsofvaryingsize. Forexample,ifwewanttoclassifyimagesofvariablesize,theinputtotheclassiﬁcationlayermusthaveaﬁxedsize.Thisisusuallyaccomplishedbyvaryingthesizeofanoﬀsetbetweenpoolingregionssothattheclassiﬁcationlayeralwaysreceivesthesamenumberofsummarystatisticsregardlessoftheinputsize.Forexample,theﬁnalpoolinglayerofthenetworkmaybedeﬁnedtooutputfoursetsofsummarystatistics,oneforeachquadrantofanimage,regardlessoftheimagesize.342 CHAPTER9.CONVOLUTIONALNETWORKS 0.11.0.21.1.1.0.10.2............ 0.30.11.1.0.31.0.21.............DETECTOR STAGEPOOLING STAGE POOLING STAGE DETECTOR STAGEFigure9.8:Maxpoolingintroducesinvariance.(Top)Aviewofthemiddleoftheoutputofaconvolutionallayer.Thebottomrowshowsoutputsofthenonlinearity.Thetoprowshowstheoutputsofmaxpooling,withastrideofonepixelbetweenpoolingregionsandapoolingregionwidthofthreepixels.Aviewofthesamenetwork,after(Bottom)theinputhasbeenshiftedtotherightbyonepixel.Everyvalueinthebottomrowhaschanged,butonlyhalfofthevaluesinthetoprowhavechanged,becausethemaxpoolingunitsareonlysensitivetothemaximumvalueintheneighborhood,notitsexactlocation. 343 CHAPTER9.CONVOLUTIONALNETWORKSLarge responsein pooling unitLarge responsein pooling unitLargeresponsein detectorunit 1Largeresponsein detectorunit 3 Figure9.9:Exampleoflearnedinvariances:Apoolingunitthatpoolsovermultiplefeaturesthatarelearnedwithseparateparameterscanlearntobeinvarianttotransformationsoftheinput.Hereweshowhowasetofthreelearnedﬁltersandamaxpoolingunitcanlearntobecomeinvarianttorotation.Allthreeﬁltersareintendedtodetectahand-written5.Eachﬁlterattemptstomatchaslightlydiﬀerentorientationofthe5.Whena5appearsintheinput,thecorrespondingﬁlterwillmatchitandcausealargeactivationinadetectorunit.Themaxpoolingunitthenhasalargeactivationregardlessofwhichdetectorunitwasactivated.Weshowherehowthenetworkprocessestwodiﬀerentinputs,resultingintwodiﬀerentdetectorunitsbeingactivated.Theeﬀectonthepoolingunitisroughlythesameeitherway.Thisprincipleisleveragedbymaxoutnetworks(Goodfellowetal.,2013a)andotherconvolutionalnetworks.Maxpoolingoverspatialpositionsisnaturallyinvarianttotranslation;thismulti-channelapproachisonlynecessaryforlearningothertransformations. 0.11.0.21.0.20.10.10.00.1Figure9.10:Poolingwithdownsampling.Hereweusemax-poolingwithapoolwidthofthreeandastridebetweenpoolsoftwo.Thisreducestherepresentationsizebyafactoroftwo,whichreducesthecomputationalandstatisticalburdenonthenextlayer.Notethattherightmostpoolingregionhasasmallersize,butmustbeincludedifwedonotwanttoignoresomeofthedetectorunits.344 CHAPTER9.CONVOLUTIONALNETWORKSSometheoreticalworkgivesguidanceastowhichkindsofpoolingoneshoulduseinvarioussituations(,).ItisalsopossibletodynamicallyBoureauetal.2010poolfeaturestogether,forexample,byrunningaclusteringalgorithmonthelocationsofinterestingfeatures(,).ThisapproachyieldsaBoureauetal.2011diﬀerentsetofpoolingregionsforeachimage.Anotherapproachistolearnasinglepoolingstructurethatisthenappliedtoallimages(,).Jiaetal.2012Poolingcancomplicatesomekindsofneuralnetworkarchitecturesthatusetop-downinformation,suchasBoltzmannmachinesandautoencoders.Theseissueswillbediscussedfurtherwhenwepresentthesetypesofnetworksinpart.IIIPoolinginconvolutionalBoltzmannmachinesispresentedinsection. The20.6inverse-likeoperationsonpoolingunitsneededinsomediﬀerentiablenetworkswillbecoveredinsection.20.10.6Someexamplesofcompleteconvolutionalnetworkarchitecturesforclassiﬁcationusingconvolutionandpoolingareshowninﬁgure.9.119.4Convolutionand Pooling asan InﬁnitelyStrongPriorRecalltheconceptofapriorprobabilitydistributionfromsection.Thisis5.2aprobabilitydistributionovertheparametersofamodelthatencodesourbeliefsaboutwhatmodelsarereasonable,beforewehaveseenanydata.Priorscanbeconsideredweakorstrongdependingonhowconcentratedtheprobabilitydensityintheprioris.Aweakpriorisapriordistributionwithhighentropy,suchasaGaussiandistributionwithhighvariance.Suchapriorallowsthedatatomovetheparametersmoreorlessfreely.Astrongpriorhasverylowentropy,suchasaGaussiandistributionwithlowvariance.Suchapriorplaysamoreactiveroleindeterminingwheretheparametersendup.Aninﬁnitelystrongpriorplaceszeroprobabilityonsomeparametersandsaysthattheseparametervaluesarecompletelyforbidden,regardlessofhowmuchsupportthedatagivestothosevalues.Wecanimagineaconvolutionalnetasbeingsimilartoafullyconnectednet,butwithaninﬁnitelystrongprioroveritsweights.Thisinﬁnitelystrongpriorsaysthattheweightsforonehiddenunitmustbeidenticaltotheweightsofitsneighbor,butshiftedinspace.Theprioralsosaysthattheweightsmustbezero,exceptforinthesmall,spatiallycontiguousreceptiveﬁeldassignedtothathiddenunit.Overall,wecanthinkoftheuseofconvolutionasintroducinganinﬁnitelystrongpriorprobabilitydistributionovertheparametersofalayer.Thisprior345 CHAPTER9.CONVOLUTIONALNETWORKS Input image: 256x256x3Output of convolution + ReLU: 256x256x64Output of pooling with stride 4: 64x64x64Output of convolution + ReLU: 64x64x64Output of pooling with stride 4: 16x16x64Output of reshape to vector:16,384 unitsOutput of matrix multiply: 1,000 unitsOutput of softmax: 1,000 class probabilities Input image: 256x256x3Output of convolution + ReLU: 256x256x64Output of pooling with stride 4: 64x64x64Output of convolution + ReLU: 64x64x64Output of pooling to 3x3 grid: 3x3x64Output of reshape to vector:576 unitsOutput of matrix multiply: 1,000 unitsOutput of softmax: 1,000 class probabilities Input image: 256x256x3Output of convolution + ReLU: 256x256x64Output of pooling with stride 4: 64x64x64Output of convolution + ReLU: 64x64x64Output of convolution:16x16x1,000Output of average pooling: 1x1x1,000Output of softmax: 1,000 class probabilities Output of pooling with stride 4: 16x16x64 Figure9.11:Examplesofarchitecturesforclassiﬁcationwithconvolutionalnetworks.Thespeciﬁcstridesanddepthsusedinthisﬁgurearenotadvisableforrealuse;theyaredesignedtobeveryshallowinordertoﬁtontothepage. Realconvolutionalnetworksalsoofteninvolvesigniﬁcantamountsofbranching,unlikethechainstructuresusedhereforsimplicity.(Left)Aconvolutionalnetworkthatprocessesaﬁxedimagesize.Afteralternatingbetweenconvolutionandpoolingforafewlayers,thetensorfortheconvolutionalfeaturemapisreshapedtoﬂattenoutthespatialdimensions.Therestofthenetworkisanordinaryfeedforwardnetworkclassiﬁer,asdescribedinchapter.6(Center)Aconvolutionalnetworkthatprocessesavariable-sizedimage,butstillmaintainsafullyconnectedsection.Thisnetworkusesapoolingoperationwithvariably-sizedpoolsbutaﬁxednumberofpools,inordertoprovideaﬁxed-sizevectorof576unitstothefullyconnectedportionofthenetwork. Aconvolutionalnetworkthatdoesnot(Right)haveanyfullyconnectedweightlayer.Instead,thelastconvolutionallayeroutputsonefeaturemapperclass.Themodelpresumablylearnsamapofhowlikelyeachclassistooccurateachspatiallocation.Averagingafeaturemapdowntoasinglevalueprovidestheargumenttothesoftmaxclassiﬁeratthetop.346 CHAPTER9.CONVOLUTIONALNETWORKSsaysthatthefunctionthelayershouldlearncontainsonlylocalinteractionsandisequivarianttotranslation.Likewise,theuseofpoolingisaninﬁnitelystrongpriorthateachunitshouldbeinvarianttosmalltranslations.Ofcourse,implementingaconvolutionalnetasafullyconnectednetwithaninﬁnitelystrongpriorwouldbeextremelycomputationallywasteful.Butthinkingofaconvolutionalnetasafullyconnectednetwithaninﬁnitelystrongpriorcangiveussomeinsightsintohowconvolutionalnetswork.Onekeyinsightisthatconvolutionandpoolingcancauseunderﬁtting.Likeanyprior,convolutionandpoolingareonlyusefulwhentheassumptionsmadebythepriorarereasonablyaccurate.Ifataskreliesonpreservingprecisespatialinformation,thenusingpoolingonallfeaturescanincreasethetrainingerror.Someconvolutionalnetworkarchitectures(,)aredesignedtoSzegedyetal.2014ausepoolingonsomechannelsbutnotonotherchannels,inordertogetbothhighlyinvariantfeaturesandfeaturesthatwillnotunderﬁtwhenthetranslationinvariancepriorisincorrect.Whenataskinvolvesincorporatinginformationfromverydistantlocationsintheinput,thenthepriorimposedbyconvolutionmaybeinappropriate.Anotherkeyinsightfromthisviewisthatweshouldonlycompareconvolu-tionalmodelstootherconvolutionalmodelsinbenchmarksofstatisticallearningperformance.Modelsthatdonotuseconvolutionwouldbeabletolearnevenifwepermutedallofthepixelsintheimage.Formanyimagedatasets,thereareseparatebenchmarksformodelsthatarepermutationinvariantandmustdiscovertheconceptoftopologyvialearning,andmodelsthathavetheknowledgeofspatialrelationshipshard-codedintothembytheirdesigner.9.5VariantsoftheBasicConvolutionFunctionWhendiscussingconvolutioninthecontextofneuralnetworks,weusuallydonotreferexactlytothestandarddiscreteconvolutionoperationasitisusuallyunderstoodinthemathematicalliterature.Thefunctionsusedinpracticediﬀerslightly.Herewedescribethesediﬀerencesindetail,andhighlightsomeusefulpropertiesofthefunctionsusedinneuralnetworks.First,whenwerefertoconvolutioninthecontextofneuralnetworks,weusuallyactuallymeananoperationthatconsistsofmanyapplicationsofconvolutioninparallel.Thisisbecauseconvolutionwithasinglekernelcanonlyextractonekindoffeature,albeitatmanyspatiallocations.Usuallywewanteachlayerofournetworktoextractmanykindsoffeatures,atmanylocations.347 CHAPTER9.CONVOLUTIONALNETWORKSAdditionally,theinputisusuallynotjustagridofrealvalues.Rather,itisagridofvector-valuedobservations. Forexample,acolorimagehasared,greenandblueintensityateachpixel.Inamultilayerconvolutionalnetwork,theinputtothesecondlayeristheoutputoftheﬁrstlayer,whichusuallyhastheoutputofmanydiﬀerentconvolutionsateachposition.Whenworkingwithimages,weusuallythinkoftheinputandoutputoftheconvolutionasbeing3-Dtensors,withoneindexintothediﬀerentchannelsandtwoindicesintothespatialcoordinatesofeachchannel.Softwareimplementationsusuallyworkinbatchmode,sotheywillactuallyuse4-Dtensors,withthefourthaxisindexingdiﬀerentexamplesinthebatch,butwewillomitthebatchaxisinourdescriptionhereforsimplicity.Becauseconvolutionalnetworksusuallyusemulti-channelconvolution,thelinearoperationstheyarebasedonarenotguaranteedtobecommutative,evenifkernel-ﬂippingisused.Thesemulti-channeloperationsareonlycommutativeifeachoperationhasthesamenumberofoutputchannelsasinputchannels.Assumewehavea4-DkerneltensorKwithelementKi,j,k,lgivingtheconnectionstrengthbetweenaunitinchannelioftheoutputandaunitinchanneljoftheinput,withanoﬀsetofkrowsandlcolumnsbetweentheoutputunitandtheinputunit.AssumeourinputconsistsofobserveddataVwithelementVi,j,kgivingthevalueoftheinputunitwithinchanneliatrowjandcolumnk.AssumeouroutputconsistsofZwiththesameformatasV.IfZisproducedbyconvolvingKacrosswithoutﬂipping,thenVKZi,j,k=l,m,nVl,jm,kn+−1+−1Ki,l,m,n(9.7)wherethesummationoverl,mandnisoverallvaluesforwhichthetensorindexingoperationsinsidethesummationisvalid.Inlinearalgebranotation,weindexintoarraysusingafortheﬁrstentry.Thisnecessitatesthe1−1intheaboveformula.ProgramminglanguagessuchasCandPythonindexstartingfrom,rendering0theaboveexpressionevensimpler.Wemaywanttoskipoversomepositionsofthekernelinordertoreducethecomputationalcost(attheexpenseofnotextractingourfeaturesasﬁnely).Wecanthinkofthisasdownsamplingtheoutputofthefullconvolutionfunction.Ifwewanttosampleonlyeveryspixelsineachdirectionintheoutput,thenwecandeﬁneadownsampledconvolutionfunctionsuchthatcZi,j,k= ()cKV,,si,j,k=l,m,nVl,jsm,ksn(−×1)+(−×1)+Ki,l,m,n.(9.8)Werefertosasthestrideofthisdownsampledconvolution.Itisalsopossible348 CHAPTER9.CONVOLUTIONALNETWORKStodeﬁneaseparatestrideforeachdirectionofmotion.Seeﬁgureforan9.12illustration.Oneessentialfeatureofanyconvolutionalnetworkimplementationistheabilitytoimplicitlyzero-padtheinputVinordertomakeitwider.Withoutthisfeature,thewidthoftherepresentationshrinksbyonepixellessthanthekernelwidthateachlayer. Zeropaddingtheinputallowsustocontrolthekernelwidthandthesizeoftheoutputindependently.Withoutzeropadding,weareforcedtochoosebetweenshrinkingthespatialextentofthenetworkrapidlyandusingsmallkernels—bothscenariosthatsigniﬁcantlylimittheexpressivepowerofthenetwork.Seeﬁgureforanexample.9.13Threespecialcasesofthezero-paddingsettingareworthmentioning.Oneistheextremecaseinwhichnozero-paddingisusedwhatsoever,andtheconvolutionkernelisonlyallowedtovisitpositionswheretheentirekerneliscontainedentirelywithintheimage.InMATLABterminology,thisiscalledvalidconvolution.Inthiscase,allpixelsintheoutputareafunctionofthesamenumberofpixelsintheinput,sothebehaviorofanoutputpixelissomewhatmoreregular.However,thesizeoftheoutputshrinksateachlayer.Iftheinputimagehaswidthmandthekernelhaswidthk,theoutputwillbeofwidthmk−+1. Therateofthisshrinkagecanbedramaticifthekernelsusedarelarge.Sincetheshrinkageisgreaterthan0,itlimitsthenumberofconvolutionallayersthatcanbeincludedinthenetwork.Aslayersareadded,thespatialdimensionofthenetworkwilleventuallydropto1×1,atwhichpointadditionallayerscannotmeaningfullybeconsideredconvolutional.Anotherspecialcaseofthezero-paddingsettingiswhenjustenoughzero-paddingisaddedtokeepthesizeoftheoutputequaltothesizeoftheinput.MATLABcallsthissameconvolution.Inthiscase,thenetworkcancontainasmanyconvolutionallayersastheavailablehardwarecansupport,sincetheoperationofconvolutiondoesnotmodifythearchitecturalpossibilitiesavailabletothenextlayer.However,theinputpixelsneartheborderinﬂuencefeweroutputpixelsthantheinputpixelsnearthecenter.Thiscanmaketheborderpixelssomewhatunderrepresentedinthemodel.Thismotivatestheotherextremecase,whichMATLABreferstoasfullconvolution,inwhichenoughzeroesareaddedforeverypixeltobevisitedktimesineachdirection,resultinginanoutputimageofwidthm+k−1.Inthiscase,theoutputpixelsneartheborderareafunctionoffewerpixelsthantheoutputpixelsnearthecenter.Thiscanmakeitdiﬃculttolearnasinglekernelthatperformswellatallpositionsintheconvolutionalfeaturemap.Usuallytheoptimalamountofzeropadding(intermsoftestsetclassiﬁcationaccuracy)liessomewherebetween“valid”and“same”convolution.349 CHAPTER9.CONVOLUTIONALNETWORKS x1x1x2x2x3x3s1s1s2s2x4x4x5x5s3s3 x1x1x2x2x3x3z2z2z1z1z3z3x4x4z4z4x5x5z5z5s1s1s2s2s3s3Stridedconvolution Downsampling ConvolutionFigure 9.12:Convolution witha stride.Inthisexample,we use astride oftwo.(Top)Convolutionwithastridelengthoftwoimplementedinasingleoperation. (Bot-tom)Convolutionwithastridegreaterthanonepixelismathematicallyequivalenttoconvolutionwithunitstridefollowedbydownsampling.Obviously,thetwo-stepapproachinvolvingdownsamplingiscomputationallywasteful,becauseitcomputesmanyvaluesthatarethendiscarded. 350 CHAPTER9.CONVOLUTIONALNETWORKS ......... .................. Figure9.13:Theeﬀectofzeropaddingonnetworksize:Consideraconvolutionalnetworkwithakernelofwidthsixateverylayer.Inthisexample,wedonotuseanypooling,soonlytheconvolutionoperationitselfshrinksthenetworksize.(Top)Inthisconvolutionalnetwork,wedonotuseanyimplicitzeropadding.Thiscausestherepresentationtoshrinkbyﬁvepixelsateachlayer.Startingfromaninputofsixteenpixels,weareonlyabletohavethreeconvolutionallayers,andthelastlayerdoesnotevermovethekernel,soarguablyonlytwoofthelayersaretrulyconvolutional.Therateofshrinkingcanbemitigatedbyusingsmallerkernels,butsmallerkernelsarelessexpressiveandsomeshrinkingisinevitableinthiskindofarchitecture.Byaddingﬁveimplicitzeroes(Bottom)toeachlayer,wepreventtherepresentationfromshrinkingwithdepth.Thisallowsustomakeanarbitrarilydeepconvolutionalnetwork.351 CHAPTER9.CONVOLUTIONALNETWORKSInsomecases,wedonotactuallywanttouseconvolution,butratherlocallyconnectedlayers(,,).Inthiscase,theadjacencymatrixintheLeCun19861989graphofourMLPisthesame,buteveryconnectionhasitsownweight,speciﬁedbya6-DtensorW. TheindicesintoWarerespectively:i,theoutputchannel,j,theoutputrow,k,theoutputcolumn,l,theinputchannel,m,therowoﬀsetwithintheinput,andn,thecolumnoﬀsetwithintheinput.ThelinearpartofalocallyconnectedlayeristhengivenbyZi,j,k=l,m,n[Vl,jm,kn+−1+−1wi,j,k,l,m,n].(9.9)Thisissometimesalsocalledunsharedconvolution,becauseitisasimilaroper-ationtodiscreteconvolutionwithasmallkernel,butwithoutsharingparametersacrosslocations.Figurecompareslocalconnections,convolution,andfull9.14connections.Locallyconnectedlayersareusefulwhenweknowthateachfeatureshouldbeafunctionofasmallpartofspace,butthereisnoreasontothinkthatthesamefeatureshouldoccuracrossallofspace.Forexample,ifwewanttotellifanimageisapictureofaface,weonlyneedtolookforthemouthinthebottomhalfoftheimage.Itcanalsobeusefultomakeversionsofconvolutionorlocallyconnectedlayersinwhichtheconnectivityisfurtherrestricted,forexampletoconstraineachoutputchannelitobeafunctionofonlyasubsetoftheinputchannelsl.Acommonwaytodothisistomaketheﬁrstmoutputchannelsconnecttoonlytheﬁrstninputchannels,thesecondmoutputchannelsconnecttoonlythesecondninputchannels,andsoon.Seeﬁgureforanexample.Modelinginteractions9.15betweenfewchannelsallowsthenetworktohavefewerparametersinordertoreducememoryconsumptionandincreasestatisticaleﬃciency,andalsoreducestheamountofcomputationneededtoperformforwardandback-propagation.Itaccomplishesthesegoalswithoutreducingthenumberofhiddenunits.Tiledconvolution(,;,)oﬀersacom-GregorandLeCun2010aLeetal.2010promisebetweenaconvolutionallayerandalocallyconnectedlayer.Ratherthanlearningaseparatesetofweightsatspatiallocation,welearnasetofkernelseverythatwerotatethroughaswemovethroughspace.Thismeansthatimmediatelyneighboringlocationswillhavediﬀerentﬁlters,likeinalocallyconnectedlayer,butthememoryrequirementsforstoringtheparameterswillincreaseonlybyafactorofthesizeofthissetofkernels,ratherthanthesizeoftheentireoutputfeaturemap.Seeﬁgureforacomparisonoflocallyconnectedlayers,tiled9.16convolution,andstandardconvolution.352 CHAPTER9.CONVOLUTIONALNETWORKS x1x1x2x2x3x3s2s2s1s1s3s3x4x4s4s4x5x5s5s5 x1x1x2x2s1s1s3s3x5x5s5s5x1x1x2x2x3x3s2s2s1s1s3s3x4x4s4s4x5x5s5s5 a ba ba ba ba a bc de fg h i x4x4x3x3s4s4s2s2Figure9.14:Comparisonoflocalconnections,convolution,andfullconnections.(Top)Alocallyconnectedlayerwithapatchsizeoftwopixels.Eachedgeislabeledwithauniquelettertoshowthateachedgeisassociatedwithitsownweightparameter.(Center)Aconvolutionallayerwithakernelwidthoftwopixels.Thismodelhasexactlythesameconnectivityasthelocallyconnectedlayer.Thediﬀerenceliesnotinwhichunitsinteractwitheachother,butinhowtheparametersareshared.Thelocallyconnectedlayerhasnoparametersharing.Theconvolutionallayerusesthesametwoweightsrepeatedlyacrosstheentireinput,asindicatedbytherepetitionoftheletterslabelingeachedge.(Bottom)Afullyconnectedlayerresemblesalocallyconnectedlayerinthesensethateachedgehasitsownparameter(therearetoomanytolabelexplicitlywithlettersinthisdiagram).However,itdoesnothavetherestrictedconnectivityofthelocallyconnectedlayer.353 CHAPTER9.CONVOLUTIONALNETWORKS Input TensorOutput Tensor Spatial coordinatesChannel coordinates Figure9.15: Aconvolutionalnetworkwiththeﬁrsttwooutputchannelsconnectedtoonlytheﬁrsttwoinputchannels,andthesecondtwooutputchannelsconnectedtoonlythesecondtwoinputchannels.354 CHAPTER9.CONVOLUTIONALNETWORKS x1x1x2x2x3x3s2s2s1s1s3s3x4x4s4s4x5x5s5s5x1x1x2x2x3x3s2s2s1s1s3s3x4x4s4s4x5x5s5s5 a ba ba ba ba a bc de fg h i x1x1x2x2x3x3s2s2s1s1s3s3x4x4s4s4x5x5s5s5a bc da bc da Figure9.16:Acomparisonoflocallyconnectedlayers,tiledconvolution,andstandardconvolution.Allthreehavethesamesetsofconnectionsbetweenunits,whenthesamesizeofkernelisused.Thisdiagramillustratestheuseofakernelthatistwopixelswide.Thediﬀerencesbetweenthemethodsliesinhowtheyshareparameters.(Top)Alocallyconnectedlayerhasnosharingatall.Weindicatethateachconnectionhasitsownweightbylabelingeachconnectionwithauniqueletter.Tiledconvolutionhasasetof(Center)tdiﬀerentkernels.Hereweillustratethecaseoft= 2. Oneofthesekernelshasedgeslabeled“a”and“b,”whiletheotherhasedgeslabeled“c”and“d.” Eachtimewemoveonepixeltotherightintheoutput,wemoveontousingadiﬀerentkernel.Thismeansthat,likethelocallyconnectedlayer,neighboringunitsintheoutputhavediﬀerentparameters.Unlikethelocallyconnectedlayer,afterwehavegonethroughalltavailablekernels,wecyclebacktotheﬁrstkernel.Iftwooutputunitsareseparatedbyamultipleoftsteps,thentheyshareparameters.Traditionalconvolutionisequivalenttotiled(Bottom)convolutionwitht= 1.Thereisonlyonekernelanditisappliedeverywhere,asindicatedinthediagrambyusingthekernelwithweightslabeled“a”and“b”everywhere.355 CHAPTER9.CONVOLUTIONALNETWORKSTodeﬁnetiledconvolutionalgebraically,letkbea6-Dtensor,wheretwoofthedimensionscorrespondtodiﬀerentlocationsintheoutputmap.Ratherthanhavingaseparateindexforeachlocationintheoutputmap,outputlocationscyclethroughasetoftdiﬀerentchoicesofkernelstackineachdirection.Iftisequaltotheoutputwidth,thisisthesameasalocallyconnectedlayer.Zi,j,k=l,m,nVl,jm,kn+−1+−1Ki,l,m,n,jt,kt%+1%+1,(9.10)whereis themodulooperation,with%t%t=0(,t+1)%t=1,etc.It isstraightforwardtogeneralizethisequationtouseadiﬀerenttilingrangeforeachdimension.Bothlocallyconnectedlayersandtiledconvolutionallayershaveaninterestinginteractionwithmax-pooling:thedetectorunitsoftheselayersaredrivenbydiﬀerentﬁlters.Iftheseﬁlterslearntodetectdiﬀerenttransformedversionsofthesameunderlyingfeatures,thenthemax-pooledunitsbecomeinvarianttothelearnedtransformation(seeﬁgure).Convolutionallayersarehard-codedtobe9.9invariantspeciﬁcallytotranslation.Otheroperationsbesidesconvolutionareusuallynecessarytoimplementaconvolutionalnetwork.Toperformlearning,onemustbeabletocomputethegradientwithrespecttothekernel,giventhegradientwithrespecttotheoutputs.Insomesimplecases, thisoperationcanbeperformedusingtheconvolutionoperation,butmanycasesofinterest,includingthecaseofstridegreaterthan1,donothavethisproperty.Recallthatconvolutionisalinearoperationandcanthusbedescribedasamatrixmultiplication(ifweﬁrstreshapetheinputtensorintoaﬂatvector).Thematrixinvolvedisafunctionoftheconvolutionkernel.Thematrixissparseandeachelementofthekerneliscopiedtoseveralelementsofthematrix.Thisviewhelpsustoderivesomeoftheotheroperationsneededtoimplementaconvolutionalnetwork.Multiplicationbythetransposeofthematrixdeﬁnedbyconvolutionisonesuchoperation.Thisistheoperationneededtoback-propagateerrorderivativesthroughaconvolutionallayer,soitisneededtotrainconvolutionalnetworksthathavemorethanonehiddenlayer.Thissameoperationisalsoneededifwewishtoreconstructthevisibleunitsfromthehiddenunits(,).Simardetal.1992Reconstructingthevisibleunitsisanoperationcommonlyusedinthemodelsdescribedinpartofthisbook,suchasautoencoders,RBMs,andsparsecoding.IIITransposeconvolutionisnecessarytoconstructconvolutionalversionsofthosemodels.Likethekernelgradientoperation,thisinputgradientoperationcanbe356 CHAPTER9.CONVOLUTIONALNETWORKSimplementedusingaconvolutioninsomecases,butinthegeneralcaserequiresathirdoperationtobeimplemented.Caremustbetakentocoordinatethistransposeoperationwiththeforwardpropagation.Thesizeoftheoutputthatthetransposeoperationshouldreturndependsonthezeropaddingpolicyandstrideoftheforwardpropagationoperation,aswellasthesizeoftheforwardpropagation’soutputmap.Insomecases,multiplesizesofinputtoforwardpropagationcanresultinthesamesizeofoutputmap,sothetransposeoperationmustbeexplicitlytoldwhatthesizeoftheoriginalinputwas.Thesethreeoperations—convolution,backpropfromoutputtoweights,andbackpropfromoutputtoinputs—aresuﬃcienttocomputeallofthegradientsneededtotrainanydepthoffeedforwardconvolutionalnetwork,aswellastotrainconvolutionalnetworkswithreconstructionfunctionsbasedonthetransposeofconvolution. See()forafullderivationoftheequationsintheGoodfellow2010fullygeneralmulti-dimensional,multi-examplecase.Togiveasenseofhowtheseequationswork,wepresentthetwodimensional,singleexampleversionhere.SupposewewanttotrainaconvolutionalnetworkthatincorporatesstridedconvolutionofkernelstackKappliedtomulti-channelimageVwithstridesasdeﬁnedbyc(KV,,s)asinequation.Supposewewanttominimizesomeloss9.8functionJ(VK,).Duringforwardpropagation,wewillneedtousecitselftooutputZ,whichisthenpropagatedthroughtherestofthenetworkandusedtocomputethecostfunctionJ.Duringback-propagation,wewillreceiveatensorGsuchthatGi,j,k=∂∂Zi,j,kJ,.(VK)Totrainthenetwork,weneedtocomputethederivativeswithrespecttotheweightsinthekernel.Todoso,wecanuseafunctiong,,s(GV)i,j,k,l=∂∂Ki,j,k,lJ,(VK) =m,nGi,m,nVj,msk,nsl(−×1)+(−×1)+.(9.11)Ifthislayerisnotthebottomlayerofthenetwork,wewillneedtocomputethegradientwithrespecttoVinordertoback-propagatetheerrorfartherdown.Todoso,wecanuseafunctionh,,s(KG)i,j,k=∂∂Vi,j,kJ,(VK)(9.12)=l,ms.t.(1)+=l−×smjn,ps.t.(1)+=n−×spkqKq,i,m,pGq,l,n.(9.13)Autoencodernetworks, describedinchapter, arefeedforwardnetworks14trainedtocopytheirinputtotheiroutput.AsimpleexampleisthePCAalgorithm,357 CHAPTER9.CONVOLUTIONALNETWORKSthatcopiesitsinputxtoanapproximatereconstructionrusingthefunctionWWx.Itiscommonformore generalautoencoders tousemultiplicationbythetransposeoftheweightmatrixjustasPCAdoes. Tomakesuchmodelsconvolutional,wecanusethefunctionhtoperformthetransposeoftheconvolutionoperation.SupposewehavehiddenunitsHinthesameformatasZandwedeﬁneareconstructionRKH= (h,,s.)(9.14)Inordertotraintheautoencoder,wewillreceivethegradientwithrespecttoRasatensorE.Totrainthedecoder,weneedtoobtainthegradientwithrespecttoK.Thisisgivenbyg(HE,,s).Totraintheencoder,weneedtoobtainthegradientwithrespecttoH.Thisisgivenbyc(KE,,s).Itisalsopossibletodiﬀerentiatethroughgusingcandh,buttheseoperationsarenotneededfortheback-propagationalgorithmonanystandardnetworkarchitectures.Generally,wedonotuseonlyalinearoperationinordertotransformfromtheinputstotheoutputsinaconvolutionallayer.Wegenerallyalsoaddsomebiastermtoeachoutputbeforeapplyingthenonlinearity.Thisraisesthequestionofhowtoshareparametersamongthebiases. Forlocallyconnectedlayersitisnaturaltogiveeachunititsownbias,andfortiledconvolution,itisnaturaltosharethebiaseswiththesametilingpatternasthekernels.Forconvolutionallayers,itistypicaltohaveonebiasperchanneloftheoutputandshareitacrossalllocationswithineachconvolutionmap.However,iftheinputisofknown,ﬁxedsize,itisalsopossibletolearnaseparatebiasateachlocationoftheoutputmap.Separatingthebiasesmayslightlyreducethestatisticaleﬃciencyofthemodel,butalsoallowsthemodeltocorrectfordiﬀerencesintheimagestatisticsatdiﬀerentlocations.Forexample,whenusingimplicitzeropadding,detectorunitsattheedgeoftheimagereceivelesstotalinputandmayneedlargerbiases.9.6StructuredOutputsConvolutionalnetworkscanbeusedtooutputahigh-dimensional,structuredobject,ratherthanjustpredictingaclasslabelforaclassiﬁcationtaskorarealvalueforaregressiontask.Typicallythisobjectisjustatensor,emittedbyastandardconvolutionallayer.Forexample,themodelmightemitatensorS,whereSi,j,kistheprobabilitythatpixel(j,k)oftheinputtothenetworkbelongstoclassi.Thisallowsthemodeltolabeleverypixelinanimageanddrawprecisemasksthatfollowtheoutlinesofindividualobjects.Oneissuethatoftencomesupisthattheoutputplanecanbesmallerthanthe358 CHAPTER9.CONVOLUTIONALNETWORKSˆY(1)ˆY(1)ˆY(2)ˆY(2)ˆY(3)ˆY(3)H(1)H(1)H(2)H(2)H(3)H(3)X XUUUVVVWW Figure9.17:Anexampleofarecurrentconvolutionalnetworkforpixellabeling.Theinputisanimagetensor,withaxescorrespondingtoimagerows,imagecolumns,andXchannels(red,green,blue).ThegoalistooutputatensoroflabelsˆY,withaprobabilitydistributionoverlabelsforeachpixel.Thistensorhasaxescorrespondingtoimagerows,imagecolumns,andthediﬀerentclasses.RatherthanoutputtingˆYinasingleshot,therecurrentnetworkiterativelyreﬁnesitsestimateˆYbyusingapreviousestimateofˆYasinputforcreatinganewestimate. Thesameparametersareusedforeachupdatedestimate,andtheestimatecanbereﬁnedasmanytimesaswewish.ThetensorofconvolutionkernelsUisusedoneachsteptocomputethehiddenrepresentationgiventheinputimage.ThekerneltensorVisusedtoproduceanestimateofthelabelsgiventhehiddenvalues.Onallbuttheﬁrststep,thekernelsWareconvolvedoverˆYtoprovideinputtothehiddenlayer.Ontheﬁrsttimestep,thistermisreplacedbyzero.Becausethesameparametersareusedoneachstep,thisisanexampleofarecurrentnetwork,asdescribedinchapter.10inputplane,asshowninﬁgure.Inthekindsofarchitecturestypicallyusedfor9.13classiﬁcationofasingleobjectinanimage,thegreatestreductioninthespatialdimensionsofthenetworkcomesfromusingpoolinglayerswithlargestride.Inordertoproduceanoutputmapofsimilarsizeastheinput,onecanavoidpoolingaltogether(,).Anotherstrategyistosimplyemitalower-resolutionJainetal.2007gridoflabels(,,).Finally,inprinciple,onecouldPinheiroandCollobert20142015useapoolingoperatorwithunitstride.Onestrategyforpixel-wiselabelingofimagesistoproduceaninitialguessoftheimagelabels,thenreﬁnethisinitialguessusingtheinteractionsbetweenneighboringpixels.Repeatingthisreﬁnementstepseveraltimescorrespondstousingthesameconvolutionsateachstage,sharingweightsbetweenthelastlayersofthedeepnet(,).ThismakesthesequenceofcomputationsperformedJainetal.2007bythesuccessiveconvolutionallayerswithweightssharedacrosslayersaparticularkindofrecurrentnetwork(,,).FigureshowsPinheiroandCollobert201420159.17thearchitectureofsucharecurrentconvolutionalnetwork.359 CHAPTER9.CONVOLUTIONALNETWORKSOnceapredictionforeachpixelismade,variousmethodscanbeusedtofurtherprocessthesepredictionsinordertoobtainasegmentationoftheimageintoregions(,;Briggmanetal.2009Turaga2010Farabet2013etal.,;etal.,).Thegeneralideaistoassumethatlargegroupsofcontiguouspixelstendtobeassociatedwiththesamelabel.Graphicalmodelscandescribetheprobabilisticrelationshipsbetweenneighboringpixels.Alternatively,theconvolutionalnetworkcanbetrainedtomaximizeanapproximationofthegraphicalmodeltrainingobjective(,;,).Ningetal.2005Thompsonetal.20149.7DataTypesThedatausedwithaconvolutionalnetworkusuallyconsistsofseveralchannels,eachchannelbeingtheobservationofadiﬀerentquantityatsomepointinspaceortime.Seetableforexamplesofdatatypeswithdiﬀerentdimensionalities9.1andnumberofchannels.Foranexampleofconvolutionalnetworksappliedtovideo,seeChenetal.().2010Sofarwehavediscussedonlythecasewhereeveryexampleinthetrainandtestdatahasthesamespatialdimensions.Oneadvantagetoconvolutionalnetworksisthattheycanalsoprocessinputswithvaryingspatialextents.Thesekindsofinputsimplycannotberepresentedbytraditional,matrixmultiplication-basedneuralnetworks.Thisprovidesacompellingreasontouseconvolutionalnetworksevenwhencomputationalcostandoverﬁttingarenotsigniﬁcantissues.Forexample,consideracollectionofimages,whereeachimagehasadiﬀerentwidthandheight.Itisunclearhowtomodelsuchinputswithaweightmatrixofﬁxedsize.Convolutionisstraightforwardtoapply;thekernelissimplyappliedadiﬀerentnumberoftimesdependingonthesizeoftheinput,andtheoutputoftheconvolutionoperationscalesaccordingly.Convolutionmaybeviewedasmatrixmultiplication;thesameconvolutionkernelinducesadiﬀerentsizeofdoublyblockcirculantmatrixforeachsizeofinput. Sometimestheoutputofthenetworkisallowedtohavevariablesizeaswellastheinput,forexampleifwewanttoassignaclasslabeltoeachpixeloftheinput.Inthiscase,nofurtherdesignworkisnecessary.Inothercases,thenetworkmustproducesomeﬁxed-sizeoutput,forexampleifwewanttoassignasingleclasslabeltotheentireimage.Inthiscasewemustmakesomeadditionaldesignsteps,likeinsertingapoolinglayerwhosepoolingregionsscaleinsizeproportionaltothesizeoftheinput,inordertomaintainaﬁxednumberofpooledoutputs.Someexamplesofthiskindofstrategyareshowninﬁgure.9.11360 CHAPTER9.CONVOLUTIONALNETWORKSSinglechannelMulti-channel1-DAudio waveform:The axis weconvolveovercorrespondstotime.Wediscretizetimeandmeasuretheamplitudeofthewaveformoncepertimestep.Skeletonanimationdata:Anima-tionsof3-Dcomputer-renderedcharactersaregeneratedbyalter-ingtheposeofa“skeleton”overtime.Ateachpointintime,theposeofthecharacterisdescribedbyaspeciﬁcationoftheanglesofeachofthejointsinthecharac-ter’sskeleton.Eachchannelinthedatawefeedtotheconvolu-tionalmodelrepresentstheangleaboutoneaxisofonejoint.2-DAudiodatathathasbeenprepro-cessedwithaFouriertransform:Wecantransformtheaudiowave-formintoa2Dtensorwithdif-ferentrowscorrespondingtodif-ferentfrequencies anddiﬀerentcolumnscorrespondingtodiﬀer-entpointsintime.Usingconvolu-tioninthetimemakesthemodelequivarianttoshiftsintime.Us-ingconvolutionacrossthefre-quencyaxismakesthemodelequivarianttofrequency,sothatthesamemelodyplayedinadif-ferentoctaveproducesthesamerepresentationbutatadiﬀerentheightinthenetwork’soutput.Colorimagedata:Onechannelcontainstheredpixels,onethegreen pixels, and one thebluepixels.Theconvolutionkernelmovesoverboththehorizontalandverticalaxesofthe image,conferringtranslationequivari-anceinbothdirections. 3-DVolumetricdata:Acommonsourceofthiskindofdataismed-icalimagingtechnology,suchasCTscans.Colorvideodata:Oneaxiscorre-spondstotime,onetotheheightofthevideoframe,andonetothewidthofthevideoframe.Table9.1:Examplesofdiﬀerentformatsofdatathatcanbeusedwithconvolutionalnetworks.361 CHAPTER9.CONVOLUTIONALNETWORKSNotethattheuseofconvolutionforprocessingvariablesizedinputsonlymakessenseforinputsthathavevariablesizebecausetheycontainvaryingamountsofobservationofthesamekindofthing—diﬀerentlengthsofrecordingsovertime,diﬀerentwidthsofobservationsoverspace,etc.Convolutiondoesnotmakesenseiftheinputhasvariablesizebecauseitcanoptionallyincludediﬀerentkindsofobservations.Forexample,ifweareprocessingcollegeapplications,andourfeaturesconsistofbothgradesandstandardizedtestscores,butnoteveryapplicanttookthestandardizedtest,thenitdoesnotmakesensetoconvolvethesameweightsoverboththefeaturescorrespondingtothegradesandthefeaturescorrespondingtothetestscores.9.8EﬃcientConvolutionAlgorithmsModernconvolutionalnetworkapplicationsofteninvolvenetworkscontainingmorethanonemillionunits.Powerfulimplementationsexploitingparallelcomputationresources,asdiscussedinsection,areessential. However,inmanycasesit12.1isalsopossibletospeedupconvolutionbyselectinganappropriateconvolutionalgorithm.ConvolutionisequivalenttoconvertingboththeinputandthekerneltothefrequencydomainusingaFouriertransform,performingpoint-wisemultiplicationofthetwosignals, andconvertingbacktothetimedomainusinganinverseFouriertransform.Forsomeproblemsizes,thiscanbefasterthanthenaiveimplementationofdiscreteconvolution.Whenad-dimensionalkernelcanbeexpressedas theouterproductofdvectors,onevectorperdimension,thekerneliscalledseparable.Whenthekernelisseparable,naiveconvolutionisineﬃcient.Itisequivalenttocomposedone-dimensionalconvolutionswitheachofthesevectors.Thecomposedapproachissigniﬁcantlyfasterthanperformingoned-dimensionalconvolutionwiththeirouterproduct.Thekernelalsotakesfewerparameterstorepresentasvectors.Ifthekerneliswelementswideineachdimension,thennaivemultidimensionalconvolutionrequiresO(wd)runtimeandparameterstoragespace,whileseparableconvolutionrequiresO(wd×)runtimeandparameterstoragespace.Ofcourse,noteveryconvolutioncanberepresentedinthisway.Devisingfasterwaysofperformingconvolutionorapproximateconvolutionwithoutharmingtheaccuracyofthemodelisanactiveareaofresearch.Eventech-niquesthatimprovetheeﬃciencyofonlyforwardpropagationareusefulbecauseinthecommercialsetting,itistypicaltodevotemoreresourcestodeploymentofanetworkthantoitstraining.362 CHAPTER9.CONVOLUTIONALNETWORKS9.9RandomorUnsupervisedFeaturesTypically,themostexpensivepartofconvolutionalnetworktrainingislearningthefeatures.Theoutputlayerisusuallyrelativelyinexpensiveduetothesmallnumberoffeaturesprovidedasinputtothislayerafterpassingthroughseverallayersofpooling.Whenperformingsupervisedtrainingwithgradientdescent,everygradientsteprequiresacompleterunofforwardpropagationandbackwardpropagationthroughtheentirenetwork.Onewaytoreducethecostofconvolutionalnetworktrainingistousefeaturesthatarenottrainedinasupervisedfashion.Therearethreebasicstrategiesforobtaining convolutionkernelswithoutsupervisedtraining.Oneistosimplyinitializethemrandomly.Anotheristodesignthembyhand,forexamplebysettingeachkerneltodetectedgesatacertainorientationorscale.Finally,onecanlearnthekernelswithanunsupervisedcriterion.Forexample,()applyCoatesetal.2011k-meansclusteringtosmallimagepatches,thenuseeachlearnedcentroidasaconvolutionkernel. PartIIIdescribesmanymoreunsupervisedlearningapproaches.Learningthefeatureswithanunsupervisedcriterionallowsthemtobedeterminedseparatelyfromtheclassiﬁerlayeratthetopofthearchitecture.Onecanthenextractthefeaturesfortheentiretrainingsetjustonce,essentiallyconstructinganewtrainingsetforthelastlayer.Learningthelastlayeristhentypicallyaconvexoptimizationproblem,assumingthelastlayerissomethinglikelogisticregressionoranSVM.Randomﬁltersoftenworksurprisinglywellinconvolutionalnetworks(Jarrettetal.etal.etal.,;2009Saxe,;2011Pinto,;2011CoxandPinto2011Saxe,).etal.()showedthatlayersconsistingofconvolutionfollowingbypoolingnaturally2011becomefrequencyselectiveandtranslationinvariantwhenassignedrandomweights.Theyarguethatthisprovidesaninexpensivewaytochoosethearchitectureofaconvolutionalnetwork:ﬁrstevaluatetheperformanceofseveralconvolutionalnetworkarchitecturesbytrainingonlythelastlayer,thentakethebestofthesearchitecturesandtraintheentirearchitectureusingamoreexpensiveapproach.Anintermediateapproachistolearnthefeatures,butusingmethodsthatdonotrequirefullforwardandback-propagationateverygradientstep.Aswithmultilayerperceptrons,weusegreedylayer-wisepretraining,totraintheﬁrstlayerinisolation,thenextractallfeaturesfromtheﬁrstlayeronlyonce,thentrainthesecondlayerinisolationgiventhosefeatures,andsoon.Chapterhasdescribed8howtoperformsupervisedgreedylayer-wisepretraining,andpartextendsthisIIItogreedylayer-wisepretrainingusinganunsupervisedcriterionateachlayer.Thecanonicalexampleofgreedylayer-wisepretrainingofaconvolutionalmodelistheconvolutionaldeepbeliefnetwork(,).ConvolutionalnetworksoﬀerLeeetal.2009363 CHAPTER9.CONVOLUTIONALNETWORKSustheopportunitytotakethepretrainingstrategyonestepfurtherthanispossiblewithmultilayerperceptrons.Insteadoftraininganentireconvolutionallayeratatime,wecantrainamodelofasmallpatch,as()dowithCoatesetal.2011k-means.Wecanthenusetheparametersfromthispatch-basedmodeltodeﬁnethekernelsofaconvolutionallayer.Thismeansthatitispossibletouseunsupervisedlearningtotrainaconvolutionalnetworkwithouteverusingconvolutionduringthetrainingprocess.Usingthisapproach,wecantrainverylargemodelsandincurahighcomputationalcostonlyatinferencetime(,;,Ranzatoetal.2007bJarrettetal.2009Kavukcuoglu2010Coates2013;etal.,;etal.,).Thisapproachwaspopularfromroughly2007–2013,whenlabeleddatasetsweresmallandcomputationalpowerwasmorelimited.Today,mostconvolutionalnetworksaretrainedinapurelysupervisedfashion,usingfullforwardandback-propagationthroughtheentirenetworkoneachtrainingiteration.Aswithotherapproachestounsupervisedpretraining,itremainsdiﬃculttoteaseapartthecauseofsomeofthebeneﬁtsseenwiththisapproach.Unsupervisedpretrainingmayoﬀersomeregularizationrelativetosupervisedtraining,oritmaysimplyallowustotrainmuchlargerarchitecturesduetothereducedcomputationalcostofthelearningrule.9.10TheNeuroscientiﬁcBasisforConvolutionalNet-worksConvolutional networksare perhaps thegreatest successstory ofbiologicallyinspiredartiﬁcialintelligence.Thoughconvolutionalnetworkshavebeenguidedbymanyotherﬁelds,someofthekeydesignprinciplesofneuralnetworksweredrawnfromneuroscience.Thehistoryofconvolutionalnetworksbeginswithneuroscientiﬁcexperimentslongbeforetherelevantcomputationalmodelsweredeveloped.NeurophysiologistsDavidHubelandTorstenWieselcollaboratedforseveralyearstodeterminemanyofthemostbasicfactsabouthowthemammalianvisionsystemworks(HubelandWiesel195919621968,,,).TheiraccomplishmentswereeventuallyrecognizedwithaNobelprize.Theirﬁndingsthathavehadthegreatestinﬂuenceoncontemporarydeeplearningmodelswerebasedonrecordingtheactivityofindividualneuronsincats.Theyobservedhowneuronsinthecat’sbrainrespondedtoimagesprojectedinpreciselocationsonascreeninfrontofthecat.Theirgreatdiscoverywasthatneuronsintheearlyvisualsystemrespondedmoststronglytoveryspeciﬁcpatternsoflight,suchaspreciselyorientedbars,butrespondedhardlyatalltootherpatterns.364 CHAPTER9.CONVOLUTIONALNETWORKSTheirworkhelpedtocharacterizemanyaspectsofbrainfunctionthatarebeyondthescopeofthisbook.Fromthepointofviewofdeeplearning,wecanfocusonasimpliﬁed,cartoonviewofbrainfunction.Inthissimpliﬁedview,wefocusonapartofthebraincalledV1,alsoknownastheprimaryvisualcortex. V1istheﬁrstareaofthebrainthatbeginstoperformsigniﬁcantlyadvancedprocessingofvisualinput. Inthiscartoonview,imagesareformedbylightarrivingintheeyeandstimulatingtheretina,thelight-sensitivetissueinthebackoftheeye.Theneuronsintheretinaperformsomesimplepreprocessingoftheimagebutdonotsubstantiallyalterthewayitisrepresented.Theimagethenpassesthroughtheopticnerveandabrainregioncalledthelateralgeniculatenucleus. Themainrole,asfarasweareconcernedhere,ofbothoftheseanatomicalregionsisprimarilyjusttocarrythesignalfromtheeyetoV1,whichislocatedatthebackofthehead.AconvolutionalnetworklayerisdesignedtocapturethreepropertiesofV1:1.V1isarrangedinaspatialmap.Itactuallyhasatwo-dimensionalstructuremirroring thestructure of theimage in theretina.For example, lightarrivingatthelowerhalfoftheretinaaﬀectsonlythecorrespondinghalfofV1.Convolutionalnetworkscapturethispropertybyhavingtheirfeaturesdeﬁnedintermsoftwodimensionalmaps.2.V1containsmanysimplecells.Asimplecell’sactivitycantosomeextentbecharacterizedbyalinear functionoftheimagein asmall, spatiallylocalizedreceptiveﬁeld.Thedetectorunitsofaconvolutionalnetworkaredesignedtoemulatethesepropertiesofsimplecells.3.V1alsocontainsmanycomplexcells.Thesecellsrespondtofeaturesthataresimilartothosedetectedbysimplecells,butcomplexcellsareinvarianttosmallshiftsinthepositionofthefeature.Thisinspiresthepoolingunitsofconvolutionalnetworks.Complexcellsarealsoinvarianttosomechangesinlightingthatcannotbecapturedsimplybypoolingoverspatiallocations.Theseinvarianceshaveinspiredsomeofthecross-channelpoolingstrategiesinconvolutionalnetworks,suchasmaxoutunits(,).Goodfellowetal.2013aThoughweknowthemostaboutV1,itisgenerallybelievedthatthesamebasicprinciplesapplytootherareasofthevisualsystem.Inourcartoonviewofthevisualsystem,thebasicstrategyofdetectionfollowedbypoolingisrepeatedlyappliedaswemovedeeperintothebrain.Aswepassthroughmultipleanatomicallayersofthebrain,weeventuallyﬁndcellsthatrespondtosomespeciﬁcconceptandareinvarianttomanytransformationsoftheinput.Thesecellshavebeen365 CHAPTER9.CONVOLUTIONALNETWORKSnicknamed“grandmothercells”—theideaisthatapersoncouldhaveaneuronthatactivateswhenseeinganimageoftheirgrandmother,regardlessofwhethersheappearsintheleftorrightsideoftheimage,whethertheimageisaclose-upofherfaceorzoomedoutshotofherentirebody,whethersheisbrightlylit,orinshadow,etc.Thesegrandmothercellshavebeenshowntoactuallyexistinthehumanbrain,inaregioncalledthemedialtemporallobe(,).ResearchersQuirogaetal.2005testedwhetherindividualneuronswouldrespondtophotosoffamousindividuals.Theyfoundwhathascometobecalledthe“HalleBerryneuron”:anindividualneuronthatisactivatedbytheconceptofHalleBerry.ThisneuronﬁreswhenapersonseesaphotoofHalleBerry,adrawingofHalleBerry,oreventextcontainingthewords“HalleBerry.”Ofcourse,thishasnothingtodowithHalleBerryherself;otherneuronsrespondedtothepresenceofBillClinton,JenniferAniston,etc.Thesemedialtemporallobeneuronsaresomewhatmoregeneralthanmodernconvolutionalnetworks,whichwouldnotautomaticallygeneralizetoidentifyingapersonorobjectwhenreadingitsname.Theclosestanalogtoaconvolutionalnetwork’slastlayeroffeaturesisabrainareacalledtheinferotemporalcortex(IT).Whenviewinganobject,informationﬂowsfromtheretina,throughtheLGN,toV1,thenonwardtoV2,thenV4,thenIT.Thishappenswithintheﬁrst100msofglimpsinganobject. Ifapersonisallowedtocontinuelookingattheobjectformoretime,theninformationwillbegintoﬂowbackwardsasthebrainusestop-downfeedbacktoupdatetheactivationsinthelowerlevelbrainareas.However,ifweinterrupttheperson’sgaze,andobserveonlytheﬁringratesthatresultfromtheﬁrst100msofmostlyfeedforwardactivation,thenITprovestobeverysimilartoaconvolutionalnetwork.ConvolutionalnetworkscanpredictITﬁringrates,andalsoperformverysimilarlyto(timelimited)humansonobjectrecognitiontasks(,).DiCarlo2013Thatbeingsaid,therearemanydiﬀerencesbetweenconvolutionalnetworksandthemammalianvisionsystem.Someofthesediﬀerencesarewellknowntocomputationalneuroscientists,butoutsidethescopeofthisbook.Someofthesediﬀerencesarenotyetknown,becausemanybasicquestionsabouthowthemammalianvisionsystemworksremainunanswered.Asabrieflist:•Thehumaneyeismostlyverylowresolution,exceptforatinypatchcalledthefovea.Thefoveaonlyobservesanareaaboutthesizeofathumbnailheldatarmslength.Thoughwefeelasifwecanseeanentiresceneinhighresolution,thisisanillusioncreatedbythesubconsciouspartofourbrain,asitstitchestogetherseveralglimpsesofsmallareas.Mostconvolutionalnetworksactuallyreceivelargefullresolutionphotographsasinput.Thehumanbrainmakes366 CHAPTER9.CONVOLUTIONALNETWORKSseveraleyemovementscalledsaccadestoglimpsethemostvisuallysalientortask-relevantpartsofascene.Incorporatingsimilarattentionmechanismsintodeeplearningmodelsisanactiveresearchdirection.Inthecontextofdeeplearning,attentionmechanismshavebeenmostsuccessfulfornaturallanguageprocessing,asdescribedinsection.Severalvisualmodels12.4.5.1withfoveationmechanismshavebeendevelopedbutsofarhavenotbecomethedominantapproach(LarochelleandHinton2010Denil2012,;etal.,).•Thehumanvisualsystemisintegratedwithmanyothersenses,suchashearing,andfactorslikeourmoodsandthoughts.Convolutionalnetworkssofararepurelyvisual.•Thehumanvisualsystemdoesmuchmorethanjustrecognizeobjects.Itisabletounderstandentirescenesincludingmanyobjectsandrelationshipsbetweenobjects,andprocessesrich3-Dgeometricinformationneededforourbodiestointerfacewiththeworld.Convolutionalnetworkshavebeenappliedtosomeoftheseproblemsbuttheseapplicationsareintheirinfancy.•EvensimplebrainareaslikeV1areheavilyimpactedbyfeedbackfromhigherlevels.Feedbackhasbeenexploredextensivelyinneuralnetworkmodelsbuthasnotyetbeenshowntooﬀeracompellingimprovement.•WhilefeedforwardITﬁringratescapturemuchofthesameinformationasconvolutionalnetworkfeatures,itisnotclearhowsimilartheintermediatecomputationsare.Thebrainprobablyusesverydiﬀerentactivationandpoolingfunctions.Anindividualneuron’sactivationprobablyisnotwell-characterizedbyasinglelinearﬁlterresponse.ArecentmodelofV1involvesmultiplequadraticﬁltersforeachneuron(,).IndeedourRustetal.2005cartoonpictureof“simplecells” and“complexcells” mightcreateanon-existentdistinction;simplecellsandcomplexcellsmightbothbethesamekindofcellbutwiththeir“parameters”enablingacontinuumofbehaviorsrangingfromwhatwecall“simple”towhatwecall“complex.”Itis alsoworthmentioningthatneuroscience hastold usrelativelylittleabouthowtotrainconvolutionalnetworks.Modelstructureswithparametersharingacrossmultiplespatiallocationsdatebacktoearlyconnectionistmodelsofvision(,),butthesemodelsdidnotusethemodernMarrandPoggio1976back-propagationalgorithmandgradientdescent.Forexample,theNeocognitron(Fukushima1980,)incorporatedmostofthemodelarchitecturedesignelementsofthemodernconvolutionalnetworkbutreliedonalayer-wiseunsupervisedclusteringalgorithm.367 CHAPTER9.CONVOLUTIONALNETWORKSLang andHinton 1988()introducedthe use ofback-propagation totraintime-delayneuralnetworks(TDNNs).Tousecontemporaryterminology,TDNNsareone-dimensionalconvolutionalnetworksappliedtotimeseries.Back-propagationappliedtothesemodelswasnotinspiredbyanyneuroscientiﬁcobserva-tionandisconsideredbysometobebiologicallyimplausible.Followingthesuccessofback-propagation-basedtrainingofTDNNs,(,)developedLeCunetal.1989themodernconvolutionalnetworkbyapplyingthesametrainingalgorithmto2-Dconvolutionappliedtoimages.Sofarwehavedescribedhowsimplecellsareroughlylinearandselectiveforcertainfeatures,complexcellsaremorenonlinearandbecomeinvarianttosometransformationsofthesesimplecellfeatures,andstacksoflayersthatalternatebetweenselectivityandinvariancecanyieldgrandmothercellsforveryspeciﬁcphenomena.Wehavenotyetdescribedpreciselywhattheseindividualcellsdetect.Inadeep,nonlinearnetwork,itcanbediﬃculttounderstandthefunctionofindividualcells.Simplecellsintheﬁrstlayerareeasiertoanalyze,becausetheirresponsesaredrivenbyalinearfunction.Inanartiﬁcialneuralnetwork,wecanjustdisplayanimageoftheconvolutionkerneltoseewhatthecorrespondingchannelofaconvolutionallayerrespondsto.Inabiologicalneuralnetwork,wedonothaveaccesstotheweightsthemselves.Instead,weputanelectrodeintheneuronitself,displayseveralsamplesofwhitenoiseimagesinfrontoftheanimal’sretina,andrecordhoweachofthesesamplescausestheneurontoactivate.Wecanthenﬁtalinearmodeltotheseresponsesinordertoobtainanapproximationoftheneuron’sweights.Thisapproachisknownasreversecorrelation(RingachandShapley2004,).ReversecorrelationshowsusthatmostV1cellshaveweightsthataredescribedbyGaborfunctions. TheGaborfunctiondescribestheweightata2-Dpointintheimage.Wecanthinkofanimageasbeingafunctionof2-Dcoordinates,I(x,y).Likewise,wecanthinkofasimplecellassamplingtheimageatasetoflocations,deﬁnedbyasetofxcoordinatesXandasetofycoordinates,Y,andapplyingweightsthatarealsoafunctionofthelocation,w(x,y).Fromthispointofview,theresponseofasimplecelltoanimageisgivenbysI() =x∈Xy∈Ywx,yIx,y.()()(9.15)Speciﬁcally,takestheformofaGaborfunction:wx,y()wx,yα,β(;x,βy,f,φ,x0,y0,τα) = exp−βxx2−βyy2cos(fx+)φ,(9.16)wherex= (xx−0)cos()+(τyy−0)sin()τ(9.17)368 CHAPTER9.CONVOLUTIONALNETWORKSandy= (−xx−0)sin()+(τyy−0)cos()τ.(9.18)Here,α,βx,βy,f,φ,x0,y0,andτareparametersthatcontrolthepropertiesoftheGaborfunction.FigureshowssomeexamplesofGaborfunctionswith9.18diﬀerentsettingsoftheseparameters.Theparametersx0,y0,andτdeﬁneacoordinatesystem. Wetranslateandrotatexandytoformxandy.Speciﬁcally,thesimplecellwillrespondtoimagefeaturescenteredatthepoint(x0,y0),anditwillrespondtochangesinbrightnessaswemovealongalinerotatedradiansfromthehorizontal.τViewedasafunctionofxandy,thefunctionwthenrespondstochangesinbrightnessaswemovealongthexaxis. Ithastwoimportantfactors:oneisaGaussianfunctionandtheotherisacosinefunction.TheGaussianfactorαexp−βxx2−βyy2canbeseenasagatingtermthatensuresthesimplecellwillonlyrespondtovaluesnearwherexandyarebothzero,inotherwords,nearthecenterofthecell’sreceptiveﬁeld.Thescalingfactorαadjuststhetotalmagnitudeofthesimplecell’sresponse,whileβxandβycontrolhowquicklyitsreceptiveﬁeldfallsoﬀ.Thecosinefactorcos(fx+φ) controlshowthesimplecellrespondstochangingbrightnessalongthexaxis.Theparameterfcontrolsthefrequencyofthecosineandcontrolsitsphaseoﬀset.φAltogether,thiscartoonviewofsimplecellsmeansthatasimplecellrespondstoaspeciﬁcspatialfrequencyofbrightnessinaspeciﬁcdirectionataspeciﬁclocation.Simplecellsaremostexcitedwhenthewaveofbrightnessintheimagehasthesamephaseastheweights.Thisoccurswhentheimageisbrightwheretheweightsarepositiveanddarkwheretheweightsarenegative.Simplecellsaremostinhibitedwhenthewaveofbrightnessisfullyoutofphasewiththeweights—whentheimageisdarkwheretheweightsarepositiveandbrightwheretheweightsarenegative.ThecartoonviewofacomplexcellisthatitcomputestheL2normofthe2-Dvectorcontainingtwosimplecells’responses:c(I)=s0()I2+s1()I2. Animportantspecialcaseoccurswhens1hasallofthesameparametersass0exceptforφ,andφissetsuchthats1isonequartercycleoutofphasewiths0.Inthiscase,s0ands1formaquadraturepair.AcomplexcelldeﬁnedinthiswayrespondswhentheGaussianreweightedimageI(x,y)exp(−βxx2−βyy2) containsahighamplitudesinusoidalwavewithfrequencyfindirectionτnear(x0,y0),regardlessofthephaseoﬀsetofthiswave.Inotherwords,thecomplexcellisinvarianttosmalltranslationsoftheimageindirectionτ,ortonegatingtheimage369 CHAPTER9.CONVOLUTIONALNETWORKS Figure9.18:Gaborfunctionswithavarietyofparametersettings.Whiteindicateslargepositiveweight,blackindicateslargenegativeweight,andthebackgroundgraycorrespondstozeroweight.(Left)Gaborfunctionswithdiﬀerentvaluesoftheparametersthatcontrolthecoordinatesystem:x0,y0,andτ. EachGaborfunctioninthisgridisassignedavalueofx0andy0proportionaltoitspositioninitsgrid,andτischosensothateachGaborﬁlterissensitivetothedirectionradiatingoutfromthecenterofthegrid.Fortheothertwoplots,x0,y0,andτareﬁxedtozero. Gaborfunctionswith(Center)diﬀerentGaussianscaleparametersβxandβy.Gaborfunctionsarearrangedinincreasingwidth(decreasingβx)aswemovelefttorightthroughthegrid,andincreasingheight(decreasingβy)aswemovetoptobottom.Fortheothertwoplots,theβvaluesareﬁxedto1.5×theimagewidth.Gaborfunctionswithdiﬀerentsinusoidparameters(Right)fandφ.Aswemovetoptobottom,fincreases,andaswemovelefttoright,φincreases.Fortheothertwoplots,isﬁxedto0andisﬁxedto5theimagewidth.φf×(replacingblackwithwhiteandviceversa).SomeofthemoststrikingcorrespondencesbetweenneuroscienceandmachinelearningcomefromvisuallycomparingthefeatureslearnedbymachinelearningmodelswiththoseemployedbyV1.()showedthatOlshausenandField1996asimpleunsupervisedlearningalgorithm, sparsecoding,learnsfeatureswithreceptiveﬁeldssimilartothoseofsimplecells.Sincethen,wehavefoundthatanextremelywidevarietyofstatisticallearningalgorithmslearnfeatureswithGabor-likefunctionswhenappliedtonaturalimages.Thisincludesmostdeeplearningalgorithms,whichlearnthesefeaturesintheirﬁrstlayer.Figure9.19showssomeexamples.Becausesomanydiﬀerentlearningalgorithmslearnedgedetectors,itisdiﬃculttoconcludethatanyspeciﬁclearningalgorithmisthe“right”modelofthebrainjustbasedonthefeaturesthatitlearns(thoughitcancertainlybeabadsignifanalgorithmdoeslearnsomesortofedgedetectornotwhenappliedtonaturalimages).Thesefeaturesareanimportantpartofthestatisticalstructureofnaturalimagesandcanberecoveredbymanydiﬀerentapproachestostatisticalmodeling.SeeHyvärinen2009etal.()forareviewoftheﬁeldofnaturalimagestatistics.370 CHAPTER9.CONVOLUTIONALNETWORKS Figure9.19:Manymachinelearningalgorithmslearnfeaturesthatdetectedgesorspeciﬁccolorsofedgeswhenappliedtonaturalimages.ThesefeaturedetectorsarereminiscentoftheGaborfunctionsknowntobepresentinprimaryvisualcortex.(Left)Weightslearnedbyanunsupervisedlearningalgorithm(spikeandslabsparsecoding)appliedtosmallimagepatches.(Right)Convolutionkernelslearnedbytheﬁrstlayerofafullysupervisedconvolutionalmaxoutnetwork.Neighboringpairsofﬁltersdrivethesamemaxoutunit.9.11ConvolutionalNetworksandtheHistoryofDeepLearningConvolutionalnetworkshaveplayedanimportantroleinthehistoryofdeeplearning.Theyareakeyexampleofasuccessfulapplicationofinsightsobtainedbystudyingthebraintomachinelearningapplications.Theywerealsosomeoftheﬁrstdeepmodelstoperformwell,longbeforearbitrarydeepmodelswereconsideredviable.Convolutionalnetworkswerealsosomeoftheﬁrstneuralnetworkstosolveimportantcommercialapplicationsandremainattheforefrontofcommercialapplicationsofdeeplearningtoday.Forexample,inthe1990s,theneuralnetworkresearchgroupatAT&Tdevelopedaconvolutionalnetworkforreadingchecks(,).Bytheendofthe1990s,thissystemdeployedLeCunetal.1998bbyNECwasreadingover10%ofallthechecksintheUS.Later,severalOCRandhandwritingrecognitionsystemsbasedonconvolutionalnetsweredeployedbyMicrosoft(,).SeechapterformoredetailsonsuchapplicationsSimardetal.200312andmoremodernapplicationsofconvolutionalnetworks.See()LeCunetal.2010foramorein-depthhistoryofconvolutionalnetworksupto2010.Convolutionalnetworkswerealsousedtowinmanycontests.ThecurrentintensityofcommercialinterestindeeplearningbeganwhenKrizhevskyetal.()wontheImageNetobjectrecognitionchallenge,butconvolutionalnetworks2012371 CHAPTER9.CONVOLUTIONALNETWORKShadbeenusedtowinothermachinelearningandcomputervisioncontestswithlessimpactforyearsearlier.Convolutionalnetsweresomeoftheﬁrstworkingdeepnetworkstrainedwithback-propagation.Itisnotentirelyclearwhyconvolutionalnetworkssucceededwhengeneralback-propagationnetworkswereconsideredtohavefailed.Itmaysimplybethatconvolutionalnetworksweremorecomputationallyeﬃcientthanfullyconnectednetworks,soitwaseasiertorunmultipleexperimentswiththemandtunetheirimplementationandhyperparameters.Largernetworksalsoseemtobeeasiertotrain.Withmodernhardware,largefullyconnectednetworksappeartoperformreasonablyonmanytasks,evenwhenusingdatasetsthatwereavailableandactivationfunctionsthatwerepopularduringthetimeswhenfullyconnectednetworkswerebelievednottoworkwell.Itmaybethattheprimarybarrierstothesuccessofneuralnetworkswerepsychological(practitionersdidnotexpectneuralnetworkstowork,sotheydidnotmakeaseriouseﬀorttouseneuralnetworks).Whateverthecase,itisfortunatethatconvolutionalnetworksperformedwelldecadesago.Inmanyways,theycarriedthetorchfortherestofdeeplearningandpavedthewaytotheacceptanceofneuralnetworksingeneral.Convolutionalnetworksprovideawaytospecializeneuralnetworkstoworkwithdatathathasacleargrid-structuredtopologyandtoscalesuchmodelstoverylargesize.Thisapproachhasbeenthemostsuccessfulonatwo-dimensional,imagetopology.Toprocessone-dimensional,sequentialdata,weturnnexttoanotherpowerfulspecializationoftheneuralnetworksframework:recurrentneuralnetworks. 372 Chapter10SequenceModeling:RecurrentandRecursiveNetsRecurrentneuralnetworksorRNNs(,)areafamilyofRumelhartetal.1986aneuralnetworksforprocessingsequentialdata.MuchasaconvolutionalnetworkisaneuralnetworkthatisspecializedforprocessingagridofvaluesXsuchasanimage,arecurrentneuralnetworkisaneuralnetworkthatisspecializedforprocessingasequenceofvaluesx(1),...,x()τ.Justasconvolutionalnetworkscanreadilyscaletoimageswithlargewidthandheight,andsomeconvolutionalnetworkscanprocessimagesofvariablesize,recurrentnetworkscanscaletomuchlongersequencesthanwouldbepracticalfornetworkswithoutsequence-basedspecialization.Mostrecurrentnetworkscanalsoprocesssequencesofvariablelength.Togofrommulti-layernetworkstorecurrentnetworks,weneedtotakeadvan-tageofoneoftheearlyideasfoundinmachinelearningandstatisticalmodelsofthe1980s:sharingparametersacrossdiﬀerentpartsofamodel.Parametersharingmakesitpossibletoextendandapplythemodeltoexamplesofdiﬀerentforms(diﬀerentlengths,here)andgeneralizeacrossthem.Ifwehadseparateparametersforeachvalueofthetimeindex,wecouldnotgeneralizetosequencelengthsnotseenduringtraining,norsharestatisticalstrengthacrossdiﬀerentsequencelengthsandacrossdiﬀerentpositionsintime.Suchsharingisparticularlyimportantwhenaspeciﬁcpieceofinformationcanoccuratmultiplepositionswithinthesequence.Forexample,considerthetwosentences“IwenttoNepalin2009”and“In2009,IwenttoNepal.”IfweaskamachinelearningmodeltoreadeachsentenceandextracttheyearinwhichthenarratorwenttoNepal,wewouldlikeittorecognizetheyear2009astherelevantpieceofinformation,whetheritappearsinthesixth373 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETSwordorthesecondwordofthesentence.Supposethatwetrainedafeedforwardnetworkthatprocessessentencesofﬁxedlength.Atraditionalfullyconnectedfeedforwardnetworkwouldhaveseparateparametersforeachinputfeature,soitwouldneedtolearnalloftherulesofthelanguageseparatelyateachpositioninthesentence.Bycomparison,arecurrentneuralnetworksharesthesameweightsacrossseveraltimesteps.Arelatedideaistheuseofconvolutionacrossa1-Dtemporalsequence.Thisconvolutionalapproachisthebasisfortime-delayneuralnetworks(LangandHinton1988Waibel1989Lang1990,;etal.,;etal.,).Theconvolutionoperationallowsanetworktoshareparametersacrosstime,butisshallow.Theoutputofconvolutionisasequencewhereeachmemberoftheoutputisafunctionofasmallnumberofneighboringmembersoftheinput.Theideaofparametersharingmanifestsintheapplicationofthesameconvolutionkernelateachtimestep.Recurrentnetworksshareparametersinadiﬀerentway.Eachmemberoftheoutputisafunctionofthepreviousmembersoftheoutput.Eachmemberoftheoutputisproducedusingthesameupdateruleappliedtothepreviousoutputs.Thisrecurrentformulationresultsinthesharingofparametersthroughaverydeepcomputationalgraph.Forthesimplicityofexposition,werefertoRNNsasoperatingonasequencethatcontainsvectorsx()twiththetimestepindextrangingfromto1τ.Inpractice,recurrentnetworksusuallyoperateonminibatchesofsuchsequences,withadiﬀerentsequencelengthτforeachmemberoftheminibatch.Wehaveomittedtheminibatchindicestosimplifynotation.Moreover,thetimestepindexneednotliterallyrefertothepassageoftimeintherealworld.Sometimesitrefersonlytothepositioninthesequence.RNNsmayalsobeappliedintwodimensionsacrossspatialdatasuchasimages,andevenwhenappliedtodatainvolvingtime,thenetworkmayhaveconnectionsthatgobackwardsintime,providedthattheentiresequenceisobservedbeforeitisprovidedtothenetwork.Thischapterextendstheideaofacomputationalgraphtoincludecycles.Thesecyclesrepresenttheinﬂuenceofthepresentvalueofavariableonitsownvalueatafuturetimestep.Suchcomputationalgraphsallowustodeﬁnerecurrentneuralnetworks.Wethendescribemanydiﬀerentwaystoconstruct,train,anduserecurrentneuralnetworks.Formoreinformationonrecurrentneuralnetworksthanisavailableinthischapter,wereferthereadertothetextbookofGraves2012().374 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS10.1UnfoldingComputationalGraphsAcomputationalgraphisawaytoformalizethestructureofasetofcomputations,suchasthoseinvolvedinmappinginputsandparameterstooutputsandloss.Pleaserefertosectionforageneralintroduction.Inthissectionweexplain6.5.1theideaofunfoldingarecursiveorrecurrentcomputationintoacomputationalgraphthathasarepetitivestructure,typicallycorrespondingtoachainofevents.Unfoldingthisgraphresultsinthesharingofparametersacrossadeepnetworkstructure.Forexample,considertheclassicalformofadynamicalsystem:s()t= (fs(1)t−;)θ,(10.1)wheres()tiscalledthestateofthesystem.Equationisrecurrentbecausethedeﬁnitionof10.1sattimetrefersbacktothesamedeﬁnitionattime.t−1Foraﬁnitenumberoftimestepsτ,thegraphcanbeunfoldedbyapplyingthedeﬁnitionτ−1times.Forexample,ifweunfoldequationfor10.1τ= 3timesteps,weobtains(3)=(fs(2);)θ(10.2)=((ffs(1););)θθ(10.3)Unfoldingtheequationbyrepeatedlyapplyingthedeﬁnitioninthiswayhasyieldedanexpressionthatdoesnotinvolverecurrence.Suchanexpressioncannowberepresentedbyatraditionaldirectedacycliccomputationalgraph. Theunfoldedcomputationalgraphofequationandequationisillustratedin10.110.3ﬁgure.10.1s(t−1)s(t−1)s()ts()ts(+1)ts(+1)tf fs()...s()...s()...s()...f ff ff fFigure10.1:Theclassicaldynamicalsystemdescribedbyequation,illustratedasan10.1unfoldedcomputationalgraph. Eachnoderepresentsthestateatsometimetandthefunctionfmapsthestateatttothestateatt+1.Thesameparameters(thesamevalueofusedtoparametrize)areusedforalltimesteps.θfAsanotherexample,letusconsideradynamicalsystemdrivenbyanexternalsignalx()t,s()t= (fs(1)t−,x()t;)θ,(10.4)375 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETSwhereweseethatthestatenowcontainsinformationaboutthewholepastsequence.Recurrentneuralnetworkscanbebuiltinmanydiﬀerentways.Muchasalmostanyfunctioncanbeconsideredafeedforwardneuralnetwork,essentiallyanyfunctioninvolvingrecurrencecanbeconsideredarecurrentneuralnetwork.Manyrecurrentneuralnetworksuseequationorasimilarequationto10.5deﬁnethevaluesoftheirhiddenunits. Toindicatethatthestateisthehiddenunitsofthenetwork,wenowrewriteequationusingthevariable10.4htorepresentthestate:h()t= (fh(1)t−,x()t;)θ,(10.5)illustratedinﬁgure,typicalRNNswilladdextraarchitecturalfeaturessuch10.2asoutputlayersthatreadinformationoutofthestatetomakepredictions.hWhentherecurrentnetworkistrainedtoperformataskthatrequirespredictingthefuturefromthepast,thenetworktypicallylearnstouseh()tasakindoflossysummaryofthetask-relevantaspectsofthepastsequenceofinputsuptot.Thissummaryisingeneralnecessarilylossy,sinceitmapsanarbitrarylengthsequence(x()t,x(1)t−,x(2)t−,...,x(2),x(1))toaﬁxedlengthvectorh()t.Dependingonthetrainingcriterion,thissummarymightselectivelykeepsomeaspectsofthepastsequencewithmoreprecisionthanotheraspects.Forexample,iftheRNNisusedinstatisticallanguagemodeling,typicallytopredictthenextwordgivenpreviouswords,itmaynotbenecessarytostorealloftheinformationintheinputsequenceuptotimet,butratheronlyenoughinformationtopredicttherestofthesentence.Themostdemandingsituationiswhenweaskh()ttoberichenoughtoallowonetoapproximatelyrecovertheinputsequence,asinautoencoderframeworks(chapter).14f fh hx xh(t−1)h(t−1)h()th()th(+1)th(+1)tx(t−1)x(t−1)x()tx()tx(+1)tx(+1)th()...h()...h()...h()...f fUnfoldf ff ffFigure10.2:Arecurrentnetworkwithnooutputs.Thisrecurrentnetworkjustprocessesinformationfromtheinputxbyincorporatingitintothestatehthatispassedforwardthroughtime.(Left)Circuitdiagram.Theblacksquareindicatesadelayofasingletimestep.Thesamenetworkseenasanunfoldedcomputationalgraph,whereeach(Right)nodeisnowassociatedwithoneparticulartimeinstance.Equationcanbedrawnintwodiﬀerentways.OnewaytodrawtheRNN10.5iswithadiagramcontainingonenodeforeverycomponentthatmightexistina376 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETSphysicalimplementationofthemodel,suchasabiologicalneuralnetwork.Inthisview,thenetworkdeﬁnesacircuitthatoperatesinrealtime,withphysicalpartswhosecurrentstatecaninﬂuencetheirfuturestate,asintheleftofﬁgure.10.2Throughoutthischapter,weuseablacksquareinacircuitdiagramtoindicatethataninteractiontakesplacewithadelayofasingletimestep,fromthestateattimettothestateattimet+1.TheotherwaytodrawtheRNNisasanunfoldedcomputationalgraph,inwhicheachcomponentisrepresentedbymanydiﬀerentvariables,withonevariablepertimestep,representingthestateofthecomponentatthatpointintime.Eachvariableforeachtimestepisdrawnasaseparatenodeofthecomputationalgraph,asintherightofﬁgure.Whatwe10.2callunfoldingistheoperationthatmapsacircuitasintheleftsideoftheﬁguretoacomputationalgraphwithrepeatedpiecesasintherightside.Theunfoldedgraphnowhasasizethatdependsonthesequencelength.Wecanrepresenttheunfoldedrecurrenceafterstepswithafunctiontg()t:h()t=g()t(x()t,x(1)t−,x(2)t−,...,x(2),x(1))(10.6)=(fh(1)t−,x()t;)θ(10.7)Thefunctiong()ttakesthewholepastsequence(x()t,x(1)t−,x(2)t−,...,x(2),x(1))asinputandproducesthecurrentstate,buttheunfoldedrecurrentstructureallowsustofactorizeg()tintorepeatedapplicationofafunctionf.Theunfoldingprocessthusintroducestwomajoradvantages:1.Regardlessofthesequencelength,thelearnedmodelalwayshasthesameinputsize,becauseitisspeciﬁedintermsoftransitionfromonestatetoanotherstate,ratherthanspeciﬁedintermsofavariable-lengthhistoryofstates.2.Itispossibletousethetransitionfunctionsamefwiththesameparametersateverytimestep.Thesetwofactorsmakeitpossibletolearnasinglemodelfthatoperatesonalltimestepsandallsequencelengths,ratherthanneedingtolearnaseparatemodelg()tforallpossibletimesteps.Learningasingle,sharedmodelallowsgeneralizationtosequencelengthsthatdidnotappearinthetrainingset,andallowsthemodeltobeestimatedwithfarfewertrainingexamplesthanwouldberequiredwithoutparametersharing.Boththerecurrentgraphandtheunrolledgraphhavetheiruses.Therecurrentgraphissuccinct.Theunfoldedgraphprovidesanexplicitdescriptionofwhichcomputationstoperform.Theunfoldedgraphalsohelpstoillustratetheideaof377 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETSinformationﬂowforwardintime(computingoutputsandlosses)andbackwardintime(computinggradients)byexplicitlyshowingthepathalongwhichthisinformationﬂows.10.2RecurrentNeuralNetworksArmedwiththegraphunrollingandparametersharingideasofsection,we10.1candesignawidevarietyofrecurrentneuralnetworks. U UV VW Wo(t−1)o(t−1)h ho oy yL L x xo()to()to(+1)to(+1)tL(t−1)L(t−1)L()tL()tL(+1)tL(+1)ty(t−1)y(t−1)y()ty()ty(+1)ty(+1)t h(t−1)h(t−1)h()th()th(+1)th(+1)tx(t−1)x(t−1)x()tx()tx(+1)tx(+1)tW WW WW WW Wh()...h()...h()...h()...V VV VV VU UU UU UUnfold Figure10.3:Thecomputationalgraphtocomputethetraininglossofarecurrentnetworkthatmapsaninputsequenceofxvaluestoacorrespondingsequenceofoutputovalues.AlossLmeasureshowfareachoisfromthecorrespondingtrainingtargety.Whenusingsoftmaxoutputs,weassumeoistheunnormalizedlogprobabilities.ThelossLinternallycomputesˆy=softmax(o) andcomparesthistothetargety.TheRNNhasinputtohiddenconnectionsparametrizedbyaweightmatrixU,hidden-to-hiddenrecurrentconnectionsparametrizedbyaweightmatrixW,andhidden-to-outputconnectionsparametrizedbyaweightmatrixV.Equationdeﬁnesforwardpropagationinthismodel.10.8(Left)TheRNNanditslossdrawnwithrecurrentconnections.(Right)Thesameseenasantime-unfoldedcomputationalgraph,whereeachnodeisnowassociatedwithoneparticulartimeinstance.Someexamplesofimportantdesignpatternsforrecurrentneuralnetworksincludethefollowing:378 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS•Recurrentnetworksthatproduceanoutputateachtimestepandhaverecurrentconnectionsbetweenhiddenunits,illustratedinﬁgure.10.3•Recurrentnetworksthatproduceanoutputateachtimestepandhaverecurrentconnectionsonlyfromtheoutputatonetimesteptothehiddenunitsatthenexttimestep,illustratedinﬁgure10.4•Recurrentnetworkswithrecurrentconnectionsbetweenhiddenunits,thatreadanentiresequenceandthenproduceasingleoutput,illustratedinﬁgure.10.5ﬁgureisareasonablyrepresentativeexamplethatwereturntothroughout10.3mostofthechapter.Therecurrentneuralnetworkofﬁgureandequationisuniversalinthe10.310.8sensethatanyfunctioncomputablebyaTuringmachinecanbecomputedbysucharecurrentnetworkofaﬁnitesize.TheoutputcanbereadfromtheRNNafteranumberoftimestepsthatisasymptoticallylinearinthenumberoftimestepsusedbytheTuringmachineandasymptoticallylinearinthelengthoftheinput(SiegelmannandSontag1991Siegelmann1995SiegelmannandSontag1995,;,;,;Hyotyniemi1996,).ThefunctionscomputablebyaTuringmachinearediscrete,sotheseresultsregardexactimplementationofthefunction,notapproximations.TheRNN,whenusedasaTuringmachine,takesabinarysequenceasinputanditsoutputsmustbediscretizedtoprovideabinaryoutput.ItispossibletocomputeallfunctionsinthissettingusingasinglespeciﬁcRNNofﬁnitesize(SiegelmannandSontag1995()use886units).The“input”oftheTuringmachineisaspeciﬁcationofthefunctiontobecomputed,sothesamenetworkthatsimulatesthisTuringmachineissuﬃcientforallproblems.ThetheoreticalRNNusedfortheproofcansimulateanunboundedstackbyrepresentingitsactivationsandweightswithrationalnumbersofunboundedprecision.WenowdeveloptheforwardpropagationequationsfortheRNNdepictedinﬁgure.Theﬁguredoesnotspecifythechoiceofactivationfunctionforthe10.3hiddenunits.Hereweassumethehyperbolictangentactivationfunction.Also,theﬁguredoesnotspecifyexactlywhatformtheoutputandlossfunctiontake.Hereweassumethattheoutputisdiscrete,asiftheRNNisusedtopredictwordsorcharacters.Anaturalwaytorepresentdiscretevariablesistoregardtheoutputoasgivingtheunnormalizedlogprobabilitiesofeachpossiblevalueofthediscretevariable.Wecanthenapplythesoftmaxoperationasapost-processingsteptoobtainavectorˆyofnormalizedprobabilitiesovertheoutput.Forwardpropagationbeginswithaspeciﬁcationoftheinitialstateh(0).Then,foreachtimestepfrom379 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS UVWo(t−1)o(t−1)h ho oy yL L x xo()to()to(+1)to(+1)tL(t−1)L(t−1)L()tL()tL(+1)tL(+1)ty(t−1)y(t−1)y()ty()ty(+1)ty(+1)t h(t−1)h(t−1)h()th()th(+1)th(+1)tx(t−1)x(t−1)x()tx()tx(+1)tx(+1)tWWWWo()...o()...h()...h()...VVVUUUUnfoldFigure10.4:AnRNNwhoseonlyrecurrenceisthefeedbackconnectionfromtheoutputtothehiddenlayer.Ateachtimestept,theinputisxt,thehiddenlayeractivationsareh()t,theoutputsareo()t,thetargetsarey()tandthelossisL()t.(Left)Circuitdiagram.(Right)Unfoldedcomputationalgraph.SuchanRNNislesspowerful(canexpressasmallersetoffunctions)thanthoseinthefamilyrepresentedbyﬁgure.TheRNN10.3inﬁgurecanchoosetoputanyinformationitwantsaboutthepastintoitshidden10.3representationhandtransmithtothefuture.TheRNNinthisﬁgureistrainedtoputaspeciﬁcoutputvalueintoo,andoistheonlyinformationitisallowedtosendtothefuture.Therearenodirectconnectionsfromhgoingforward.Theprevioushisconnectedtothepresentonlyindirectly,viathepredictionsitwasusedtoproduce.Unlessoisveryhigh-dimensionalandrich,itwillusuallylackimportantinformationfromthepast.ThismakestheRNNinthisﬁgurelesspowerful,butitmaybeeasiertotrainbecauseeachtimestepcanbetrainedinisolationfromtheothers,allowinggreaterparallelizationduringtraining,asdescribedinsection.10.2.1 380 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETSttτ= 1to= ,weapplythefollowingupdateequations:a()t=+bWh(1)t−+Ux()t(10.8)h()t=tanh(a()t)(10.9)o()t=+cVh()t(10.10)ˆy()t=softmax(o()t)(10.11)wheretheparametersarethebiasvectorsbandcalongwiththeweightmatricesU,VandW,respectivelyforinput-to-hidden,hidden-to-outputandhidden-to-hiddenconnections.Thisisanexampleofarecurrentnetworkthatmapsaninputsequencetoanoutputsequenceofthesamelength.Thetotallossforagivensequenceofvaluespairedwithasequenceofvalueswouldthenbejustxythesumofthelossesoverallthetimesteps.Forexample,ifL()tisthenegativelog-likelihoodofy()tgivenx(1),...,x()t,thenL{x(1),...,x()τ}{,y(1),...,y()τ}(10.12)=tL()t(10.13)=−tlogpmodely()t|{x(1),...,x()t},(10.14)wherepmodely()t|{x(1),...,x()t}isgivenbyreadingtheentryfory()tfromthemodel’soutputvectorˆy()t.Computingthegradientofthislossfunctionwithrespecttotheparametersisanexpensiveoperation.Thegradientcomputationinvolvesperformingaforwardpropagationpassmovinglefttorightthroughourillustrationoftheunrolledgraphinﬁgure,followedbyabackwardpropagationpass10.3movingrighttoleftthroughthegraph.TheruntimeisO(τ) andcannotbereducedbyparallelizationbecausetheforwardpropagationgraphisinherentlysequential;eachtimestepmayonlybecomputedafterthepreviousone. Statescomputedintheforwardpassmustbestoreduntiltheyarereusedduringthebackwardpass,sothememorycostisalsoO(τ).Theback-propagationalgorithmappliedtotheunrolledgraphwithO(τ)costiscalledback-propagationthroughtimeorBPTTandisdiscussedfurtherinsection.Thenetworkwithrecurrence10.2.2betweenhiddenunitsisthusverypowerfulbutalsoexpensivetotrain.Isthereanalternative?10.2.1TeacherForcingandNetworkswithOutputRecurrenceThenetworkwithrecurrentconnectionsonlyfromtheoutputatonetimesteptothehiddenunitsatthenexttimestep(showninﬁgure)isstrictlylesspowerful10.4381 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETSbecauseitlackshidden-to-hiddenrecurrentconnections.Forexample,itcannotsimulateauniversalTuringmachine.Becausethisnetworklackshidden-to-hiddenrecurrence,itrequiresthattheoutputunitscapturealloftheinformationaboutthepastthatthenetworkwillusetopredictthefuture.Becausetheoutputunitsareexplicitlytrainedtomatchthetrainingsettargets,theyareunlikelytocapturethenecessaryinformationaboutthepasthistoryoftheinput,unlesstheuserknowshowtodescribethefullstateofthesystemandprovidesitaspartofthetrainingsettargets.Theadvantageofeliminatinghidden-to-hiddenrecurrenceisthat,foranylossfunctionbasedoncomparingthepredictionattimettothetrainingtargetattimet,allthetimestepsaredecoupled.Trainingcanthusbeparallelized,withthegradientforeachsteptcomputedinisolation.Thereisnoneedtocomputetheoutputfortheprevioustimestepﬁrst,becausethetrainingsetprovidestheidealvalueofthatoutput. h(t−1)h(t−1)Wh()th()t... ...x(t−1)x(t−1)x()tx()tx()...x()...WWUUUh()τh()τx()τx()τWUo()τo()τy()τy()τL()τL()τV... ...Figure10.5:Time-unfoldedrecurrentneuralnetworkwithasingleoutputattheendofthesequence.Suchanetworkcanbeusedtosummarizeasequenceandproduceaﬁxed-sizerepresentationusedasinputforfurtherprocessing. Theremightbeatargetrightattheend(asdepictedhere)orthegradientontheoutputo()tcanbeobtainedbyback-propagatingfromfurtherdownstreammodules.Modelsthathaverecurrentconnectionsfromtheiroutputsleadingbackintothemodelmaybetrainedwithteacherforcing.Teacherforcingisaprocedurethatemergesfromthemaximumlikelihoodcriterion,inwhichduringtrainingthemodelreceivesthegroundtruthoutputy()tasinputattimet+1. Wecanseethisbyexaminingasequencewithtwotimesteps.Theconditionalmaximum382 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS o(t−1)o(t−1)o()to()th(t−1)h(t−1)h()th()tx(t−1)x(t−1)x()tx()tWVVUUo(t−1)o(t−1)o()to()tL(t−1)L(t−1)L()tL()ty(t−1)y(t−1)y()ty()t h(t−1)h(t−1)h()th()tx(t−1)x(t−1)x()tx()tWVVUUTrain timeTest timeFigure10.6:Illustrationofteacherforcing.TeacherforcingisatrainingtechniquethatisapplicabletoRNNsthathaveconnectionsfromtheiroutputtotheirhiddenstatesatthenexttimestep.(Left)Attraintime,wefeedthecorrectoutputy()tdrawnfromthetrainsetasinputtoh(+1)t.Whenthemodelisdeployed,thetrueoutputisgenerally(Right)notknown.Inthiscase,weapproximatethecorrectoutputy()twiththemodel’soutputo()t,andfeedtheoutputbackintothemodel. 383 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETSlikelihoodcriterionislogpy(1),y(2)|x(1),x(2)(10.15)=logpy(2)|y(1),x(1),x(2)+logpy(1)|x(1),x(2)(10.16)Inthisexample,weseethatattimet= 2,themodelistrainedtomaximizetheconditionalprobabilityofy(2)givenboththexsequencesofarandthepreviousyvaluefromthetrainingset.Maximumlikelihoodthusspeciﬁesthatduringtraining,ratherthanfeedingthemodel’sownoutputbackintoitself,theseconnectionsshouldbefedwiththetargetvaluesspecifyingwhatthecorrectoutputshouldbe.Thisisillustratedinﬁgure.10.6Weoriginallymotivatedteacherforcingasallowingustoavoidback-propagationthroughtimeinmodelsthatlackhidden-to-hiddenconnections.Teacherforcingmaystillbeappliedtomodelsthathavehidden-to-hiddenconnectionssolongastheyhaveconnectionsfromtheoutputatonetimesteptovaluescomputedinthenexttimestep.However,assoonasthehiddenunitsbecomeafunctionofearliertimesteps,theBPTTalgorithmisnecessary.SomemodelsmaythusbetrainedwithbothteacherforcingandBPTT.Thedisadvantageofstrictteacherforcingarisesifthenetworkisgoingtobelaterusedinanopen-loopmode,withthenetworkoutputs(orsamplesfromtheoutputdistribution)fedbackasinput. Inthiscase,thekindofinputsthatthenetworkseesduringtrainingcouldbequitediﬀerentfromthekindofinputsthatitwillseeattesttime. Onewaytomitigatethisproblemistotrainwithbothteacher-forcedinputsandwithfree-runninginputs,forexamplebypredictingthecorrecttargetanumberofstepsinthefuturethroughtheunfoldedrecurrentoutput-to-inputpaths.Inthisway,thenetworkcanlearntotakeintoaccountinputconditions(suchasthoseitgeneratesitselfinthefree-runningmode)notseenduringtrainingandhowtomapthestatebacktowardsonethatwillmakethenetworkgenerateproperoutputsafterafewsteps.Anotherapproach(Bengioetal.,)tomitigatethegapbetweentheinputsseenattraintimeandthe2015binputsseenattesttimerandomlychoosestousegeneratedvaluesoractualdatavaluesasinput.Thisapproachexploitsacurriculumlearningstrategytograduallyusemoreofthegeneratedvaluesasinput.10.2.2ComputingtheGradientinaRecurrentNeuralNetworkComputingthegradientthrougharecurrentneuralnetworkisstraightforward.Onesimplyappliesthegeneralizedback-propagationalgorithmofsection6.5.6384 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETStotheunrolledcomputationalgraph.Nospecializedalgorithmsarenecessary.Gradientsobtainedbyback-propagationmaythenbeusedwithanygeneral-purposegradient-basedtechniquestotrainanRNN.TogainsomeintuitionforhowtheBPTTalgorithmbehaves,weprovideanexampleofhowtocomputegradientsbyBPTTfortheRNNequationsabove(equationandequation).Thenodesofourcomputationalgraphinclude10.810.12theparametersU,V,W,bandcaswellasthesequenceofnodesindexedbytforx()t,h()t,o()tandL()t. ForeachnodeNweneedtocomputethegradient∇NLrecursively,basedonthegradientcomputedatnodesthatfollowitinthegraph.Westarttherecursionwiththenodesimmediatelyprecedingtheﬁnalloss∂L∂L()t= 1.(10.17)Inthisderivationweassumethattheoutputso()tareusedastheargumenttothesoftmaxfunctiontoobtainthevectorˆyofprobabilitiesovertheoutput.Wealsoassumethatthelossisthenegativelog-likelihoodofthetruetargety()tgiventheinputsofar.Thegradient∇o()tLontheoutputsattimestept,foralli,t,isasfollows:(∇o()tL)i=∂L∂o()ti=∂L∂L()t∂L()t∂o()ti=ˆy()ti−1i,y()t.(10.18)Weworkourwaybackwards,startingfromtheendofthesequence.Attheﬁnaltimestep,τh()τonlyhaso()τasadescendent,soitsgradientissimple:∇h()τL= V∇o()τL.(10.19)Wecantheniteratebackwardsintimetoback-propagategradientsthroughtime,fromt=τ−1downtot= 1,notingthath()t(fort<τ)hasasdescendentsbotho()tandh(+1)t.Itsgradientisthusgivenby∇h()tL=∂h(+1)t∂h()t(∇h(+1)tL)+∂o()t∂h()t(∇o()tL)(10.20)= W(∇h(+1)tL)diag1−h(+1)t2+V(∇o()tL)(10.21)wherediag1−h(+1)t2indicatesthediagonalmatrixcontainingtheelements1−(h(+1)ti)2.ThisistheJacobianofthehyperbolictangentassociatedwiththehiddenunitattime.it+1385 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETSOncethegradientsonthe internalnodesofthe computationalgraphareobtained, wecanobtainthegradientsontheparameternodes.Becausetheparametersaresharedacrossmanytimesteps,wemusttakesomecarewhendenotingcalculusoperationsinvolvingthesevariables.Theequationswewishtoimplementusethebpropmethodofsection,thatcomputesthecontribution6.5.6ofasingleedgeinthecomputationalgraphtothegradient.However,the∇WfoperatorusedincalculustakesintoaccountthecontributionofWtothevalueoffduetoedgesinthecomputationalgraph.Toresolvethisambiguity,weallintroducedummyvariablesW()tthataredeﬁnedtobecopiesofWbutwitheachW()tusedonlyattimestept.Wemaythenuse∇W()ttodenotethecontributionoftheweightsattimesteptothegradient.tUsingthisnotation,thegradientontheremainingparametersisgivenby:∇cL=t∂o()t∂c∇o()tL=t∇o()tL(10.22)∇bL=t∂h()t∂b()t∇h()tL=tdiag1−h()t2∇h()tL(10.23)∇VL=ti∂L∂o()ti∇Vo()ti=t(∇o()tL)h()t(10.24)∇WL=ti∂L∂h()ti∇W()th()ti(10.25)=tdiag1−h()t2(∇h()tL)h(1)t−(10.26)∇UL=ti∂L∂h()ti∇U()th()ti(10.27)=tdiag1−h()t2(∇h()tL)x()t(10.28)Wedonotneedtocomputethegradientwithrespecttox()tfortrainingbecauseitdoesnothaveanyparametersasancestorsinthecomputationalgraphdeﬁningtheloss.386 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS10.2.3RecurrentNetworksasDirectedGraphicalModelsIntheexamplerecurrentnetworkwehavedevelopedsofar,thelossesL()twerecross-entropiesbetweentrainingtargetsy()tandoutputso()t.Aswithafeedforwardnetwork,itisinprinciplepossibletousealmostanylosswitharecurrentnetwork.Thelossshouldbechosenbasedonthetask.Aswithafeedforwardnetwork,weusuallywishtointerprettheoutputoftheRNNasaprobabilitydistribution,andweusuallyusethecross-entropyassociatedwiththatdistributiontodeﬁnetheloss.Meansquarederroristhecross-entropylossassociatedwithanoutputdistributionthatisaunitGaussian,forexample,justaswithafeedforwardnetwork.When we use apredictivelog-likelihood trainingobjective,such asequa-tion,wetraintheRNNtoestimatetheconditionaldistributionofthenext10.12sequenceelementy()tgiventhepastinputs.Thismaymeanthatwemaximizethelog-likelihoodlog(py()t|x(1),...,x()t),(10.29)or,ifthemodelincludesconnectionsfromtheoutputatonetimesteptothenexttimestep,log(py()t|x(1),...,x()t,y(1),...,y(1)t−).(10.30)Decomposingthejointprobabilityoverthesequenceofyvaluesasaseriesofone-stepprobabilisticpredictionsisonewaytocapturethefulljointdistributionacrossthewholesequence.Whenwedonotfeedpastyvaluesasinputsthatconditionthenextstepprediction,thedirectedgraphicalmodelcontainsnoedgesfromanyy()iinthepasttothecurrenty()t.Inthiscase,theoutputsyareconditionallyindependentgiventhesequenceofxvalues.Whenwedofeedtheactualyvalues(nottheirprediction,buttheactualobservedorgeneratedvalues)backintothenetwork,thedirectedgraphicalmodelcontainsedgesfromally()ivaluesinthepasttothecurrenty()tvalue.Asasimpleexample,letusconsiderthecasewheretheRNNmodelsonlyasequenceofscalarrandomvariablesY={y(1),...,y()τ},withnoadditionalinputsx.Theinputattimesteptissimplytheoutputattimestept−1.TheRNNthendeﬁnesadirectedgraphicalmodelovertheyvariables.Weparametrizethejointdistributionoftheseobservationsusingthechainrule(equation)forconditional3.6probabilities:PP() = Y(y(1),...,y()τ) =τt=1P(y()t|y(1)t−,y(2)t−,...,y(1))(10.31)wheretheright-handsideofthebarisemptyfort=1,ofcourse.Hencethenegativelog-likelihoodofasetofvalues{y(1),...,y()τ}accordingtosuchamodel387 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS y(1)y(1)y(2)y(2)y(3)y(3)y(4)y(4)y(5)y(5)y()...y()...Figure10.7:Fullyconnectedgraphicalmodelforasequencey(1),y(2),...,y()t,...:everypastobservationy()imayinﬂuencetheconditionaldistributionofsomey()t(fort>i),giventhepreviousvalues.Parametrizingthegraphicalmodeldirectlyaccordingtothisgraph(asinequation)mightbeveryineﬃcient,withanevergrowingnumberof10.6inputsandparametersforeachelementofthesequence.RNNsobtainthesamefullconnectivitybuteﬃcientparametrization,asillustratedinﬁgure.10.8isL=tL()t(10.32)whereL()t= log(−Py()t= y()t|y(1)t−,y(2)t−,...,y(1)).(10.33) y(1)y(1)y(2)y(2)y(3)y(3)y(4)y(4)y(5)y(5)y()...y()...h(1)h(1)h(2)h(2)h(3)h(3)h(4)h(4)h(5)h(5)h()...h()...Figure10.8:IntroducingthestatevariableinthegraphicalmodeloftheRNN,eventhoughitisadeterministicfunctionofitsinputs,helpstoseehowwecanobtainaveryeﬃcientparametrization,basedonequation.Everystageinthesequence(for10.5h()tandy()t)involvesthesamestructure(thesamenumberofinputsforeachnode)andcansharethesameparameterswiththeotherstages.Theedgesinagraphicalmodelindicatewhichvariablesdependdirectlyonothervariables.Manygraphicalmodelsaimtoachievestatisticalandcomputationaleﬃciencybyomittingedgesthatdonotcorrespondtostronginteractions.For388 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETSexample,itiscommontomaketheMarkovassumptionthatthegraphicalmodelshouldonlycontainedgesfrom{y()tk−,...,y(1)t−}toy()t,ratherthancontainingedgesfromtheentirepasthistory.However,insomecases,webelievethatallpastinputsshouldhaveaninﬂuenceonthenextelementofthesequence.RNNsareusefulwhenwebelievethatthedistributionovery()tmaydependonavalueofy()ifromthedistantpastinawaythatisnotcapturedbytheeﬀectofy()iony(1)t−.OnewaytointerpretanRNNasagraphicalmodelistoviewtheRNNasdeﬁningagraphicalmodelwhosestructureisthecompletegraph,abletorepresentdirectdependenciesbetweenanypairofyvalues.Thegraphicalmodelovertheyvalueswiththecompletegraphstructureisshowninﬁgure.Thecomplete10.7graphinterpretationoftheRNNisbasedonignoringthehiddenunitsh()tbymarginalizingthemoutofthemodel.ItismoreinterestingtoconsiderthegraphicalmodelstructureofRNNsthatresultsfromregardingthehiddenunitsh()tasrandomvariables.1IncludingthehiddenunitsinthegraphicalmodelrevealsthattheRNNprovidesaveryeﬃcientparametrizationofthejointdistributionovertheobservations.Supposethatwerepresentedanarbitraryjointdistributionoverdiscretevalueswithatabularrepresentation—anarraycontainingaseparateentryforeachpossibleassignmentofvalues,withthevalueofthatentrygivingtheprobabilityofthatassignmentoccurring. Ifycantakeonkdiﬀerentvalues,thetabularrepresentationwouldhaveO(kτ)parameters.Bycomparison,duetoparametersharing,thenumberofparametersintheRNNisO(1)asafunctionofsequencelength.ThenumberofparametersintheRNNmaybeadjustedtocontrolmodelcapacitybutisnotforcedtoscalewithsequencelength.EquationshowsthattheRNNparametrizes10.5long-termrelationshipsbetweenvariableseﬃciently,usingrecurrentapplicationsofthesamefunctionfandsameparametersθateachtimestep.Figure10.8illustratesthegraphicalmodelinterpretation.Incorporatingtheh()tnodesinthegraphicalmodeldecouplesthepastandthefuture,actingasanintermediatequantitybetweenthem.Avariabley()iinthedistantpastmayinﬂuenceavariabley()tviaitseﬀectonh.Thestructureofthisgraphshowsthatthemodelcanbeeﬃcientlyparametrizedbyusingthesameconditionalprobabilitydistributionsateachtimestep,andthatwhenthevariablesareallobserved,theprobabilityofthejointassignmentofallvariablescanbeevaluatedeﬃciently.Evenwiththeeﬃcientparametrizationofthegraphicalmodel,someoperationsremaincomputationallychallenging.Forexample,itisdiﬃculttopredictmissing1Theconditionaldistributionoverthesevariablesgiventheirparentsisdeterministic.Thisisperfectlylegitimate,thoughitissomewhatraretodesignagraphicalmodelwithsuchdeterministichiddenunits.389 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETSvaluesinthemiddleofthesequence.Thepricerecurrentnetworkspayfortheirreducednumberofparametersisthattheparametersmaybediﬃcult.optimizingTheparametersharingusedinrecurrentnetworksreliesontheassumptionthatthesameparameterscanbeusedfordiﬀerenttimesteps.Equivalently,theassumptionisthattheconditionalprobabilitydistributionoverthevariablesattimet+1 giventhevariablesattimetisstationary,meaningthattherelationshipbetweentheprevioustimestepandthenexttimestepdoesnotdependont.Inprinciple,itwouldbepossibletousetasanextrainputateachtimestepandletthelearnerdiscoveranytime-dependencewhilesharingasmuchasitcanbetweendiﬀerenttimesteps.Thiswouldalreadybemuchbetterthanusingadiﬀerentconditionalprobabilitydistributionforeacht,butthenetworkwouldthenhavetoextrapolatewhenfacedwithnewvaluesof.tTocompleteourviewofanRNNasagraphicalmodel,wemustdescribehowtodrawsamplesfromthemodel.Themainoperationthatweneedtoperformissimplytosamplefromtheconditionaldistributionateachtimestep. However,thereisoneadditionalcomplication. TheRNNmusthavesomemechanismfordeterminingthelengthofthesequence.Thiscanbeachievedinvariousways.Inthecasewhentheoutputisasymboltakenfromavocabulary,onecanaddaspecialsymbolcorrespondingtotheendofasequence(Schmidhuber2012,).Whenthatsymbolisgenerated,thesamplingprocessstops.Inthetrainingset,weinsertthissymbolasanextramemberofthesequence,immediatelyafterx()τineachtrainingexample.AnotheroptionistointroduceanextraBernoullioutputtothemodelthatrepresentsthedecisiontoeithercontinuegenerationorhaltgenerationateachtimestep.Thisapproachismoregeneralthantheapproachofaddinganextrasymboltothevocabulary,becauseitmaybeappliedtoanyRNN,ratherthanonlyRNNsthatoutputasequenceofsymbols.Forexample,itmaybeappliedtoanRNNthatemitsasequenceofrealnumbers.Thenewoutputunitisusuallyasigmoidunittrainedwiththecross-entropyloss.Inthisapproachthesigmoidistrainedtomaximizethelog-probabilityofthecorrectpredictionastowhetherthesequenceendsorcontinuesateachtimestep.Anotherwaytodeterminethesequencelengthτistoaddanextraoutputtothemodelthatpredictstheintegerτitself.Themodelcansampleavalueofτandthensampleτstepsworthofdata.Thisapproachrequiresaddinganextrainputtotherecurrentupdateateachtimestepsothattherecurrentupdateisawareofwhetheritisneartheendofthegeneratedsequence.Thisextrainputcaneitherconsistofthevalueofτorcanconsistofτt−,thenumberofremaining390 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETStimesteps.Withoutthisextrainput,theRNNmightgeneratesequencesthatendabruptly,suchasasentencethatendsbeforeitiscomplete.ThisapproachisbasedonthedecompositionP(x(1),...,x()τ) = ()(PτPx(1),...,x()τ|τ.)(10.34)ThestrategyofpredictingτdirectlyisusedforexamplebyGoodfellowetal.().2014d10.2.4ModelingSequencesConditionedonContextwithRNNsIntheprevioussectionwedescribedhowanRNNcouldcorrespondtoadirectedgraphicalmodeloverasequenceofrandomvariablesy()twithnoinputsx.Ofcourse,ourdevelopmentofRNNsasinequationincludedasequenceof10.8inputsx(1),x(2),...,x()τ.Ingeneral,RNNsallowtheextensionofthegraphicalmodelviewtorepresentnotonlyajointdistributionovertheyvariablesbutalsoaconditionaldistributionoverygivenx.Asdiscussedinthecontextoffeedforwardnetworksinsection,anymodelrepresentingavariable6.2.1.1P(y;θ)canbereinterpretedasamodelrepresentingaconditionaldistributionP(yω|)withω=θ.WecanextendsuchamodeltorepresentadistributionP(yx|)byusingthesameP(yω|)asbefore,butmakingωafunctionofx.InthecaseofanRNN,thiscanbeachievedindiﬀerentways.Wereviewherethemostcommonandobviouschoices.Previously,wehavediscussedRNNsthattakeasequenceofvectorsx()tfort=1,...,τasinput. Anotheroptionistotakeonlyasinglevectorxasinput.Whenxisaﬁxed-sizevector,wecansimplymakeitanextrainputoftheRNNthatgeneratestheysequence.SomecommonwaysofprovidinganextrainputtoanRNNare:1. asanextrainputateachtimestep,or2. astheinitialstateh(0),or3. both.Theﬁrstandmostcommonapproachisillustratedinﬁgure.Theinteraction10.9betweentheinputxandeachhiddenunitvectorh()tisparametrizedbyanewlyintroducedweightmatrixRthatwasabsentfromthemodelofonlythesequenceofyvalues. ThesameproductxRisaddedasadditionalinputtothehiddenunitsateverytimestep.Wecanthinkofthechoiceofxasdeterminingthevalue391 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETSofxRthatiseﬀectivelyanewbiasparameterusedforeachofthehiddenunits.Theweightsremainindependentoftheinput.Wecanthinkofthismodelastakingtheparametersθofthenon-conditionalmodelandturningthemintoω,wherethebiasparameterswithinarenowafunctionoftheinput.ω o(t−1)o(t−1)o()to()to(+1)to(+1)tL(t−1)L(t−1)L()tL()tL(+1)tL(+1)ty(t−1)y(t−1)y()ty()ty(+1)ty(+1)t h(t−1)h(t−1)h()th()th(+1)th(+1)tWWWWs()...s()...h()...h()...VVVUUU x xy()...y()... RRRRRFigure10.9:AnRNNthatmapsaﬁxed-lengthvectorxintoadistributionoversequencesY.ThisRNNisappropriatefortaskssuchasimagecaptioning,whereasingleimageisusedasinputtoamodelthatthenproducesasequenceofwordsdescribingtheimage.Eachelementy()toftheobservedoutputsequenceservesbothasinput(forthecurrenttimestep)and,duringtraining,astarget(fortheprevioustimestep).Ratherthanreceivingonlyasinglevectorxasinput,theRNNmayreceiveasequenceofvectorsx()tasinput.TheRNNdescribedinequationcorre-10.8spondstoaconditionaldistributionP(y(1),...,y()τ|x(1),...,x()τ)thatmakesaconditionalindependenceassumptionthatthisdistributionfactorizesastP(y()t|x(1),...,x()t).(10.35)Toremovetheconditionalindependenceassumption,wecanaddconnectionsfromtheoutputattimettothehiddenunitattimet+1,asshowninﬁgure.The10.10modelcanthenrepresentarbitraryprobabilitydistributionsovertheysequence.Thiskindofmodelrepresentingadistributionoverasequencegivenanother392 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS o(t−1)o(t−1)o()to()to(+1)to(+1)tL(t−1)L(t−1)L()tL()tL(+1)tL(+1)ty(t−1)y(t−1)y()ty()ty(+1)ty(+1)t h(t−1)h(t−1)h()th()th(+1)th(+1)tWWWWh()...h()...h()...h()...VVVUUUx(t−1)x(t−1)R x()tx()tx(+1)tx(+1)tRR Figure10.10: Aconditionalrecurrentneuralnetworkmappingavariable-lengthsequenceofxvaluesintoadistributionoversequencesofyvaluesofthesamelength.Comparedtoﬁgure,thisRNNcontainsconnectionsfromthepreviousoutputtothecurrentstate.10.3TheseconnectionsallowthisRNNtomodelanarbitrarydistributionoversequencesofygivensequencesofxofthesamelength.TheRNNofﬁgureisonlyabletorepresent10.3distributionsinwhichtheyvaluesareconditionallyindependentfromeachothergiventhevalues.x 393 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETSsequencestillhasonerestriction,whichisthatthelengthofbothsequencesmustbethesame.Wedescribehowtoremovethisrestrictioninsection.10.4 o(t−1)o(t−1)o()to()to(+1)to(+1)tL(t−1)L(t−1)L()tL()tL(+1)tL(+1)ty(t−1)y(t−1)y()ty()ty(+1)ty(+1)t h(t−1)h(t−1)h()th()th(+1)th(+1)tx(t−1)x(t−1)x()tx()tx(+1)tx(+1)tg(t−1)g(t−1)g()tg()tg(+1)tg(+1)t Figure10.11: Computationofatypicalbidirectionalrecurrentneuralnetwork,meanttolearntomapinputsequencesxtotargetsequencesy,withlossL()tateachstept.Thehrecurrencepropagatesinformationforwardintime(towardstheright)whilethegrecurrencepropagatesinformationbackwardintime(towardstheleft).Thusateachpointt,theoutputunitso()tcanbeneﬁtfromarelevantsummaryofthepastinitsh()tinputandfromarelevantsummaryofthefutureinitsg()tinput.10.3BidirectionalRNNsAlloftherecurrentnetworkswehaveconsidereduptonowhavea“causal”struc-ture,meaningthatthestateattimetonlycapturesinformationfromthepast,x(1),...,x(1)t−,andthepresentinputx()t.Someofthemodelswehavediscussedalsoallowinformationfrompastyvaluestoaﬀectthecurrentstatewhentheyvaluesareavailable.However,inmanyapplicationswewanttooutputapredictionofy()twhichmay394 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETSdependonthewholeinputsequence.Forexample,inspeechrecognition,thecorrectinterpretationofthecurrentsoundasaphonememaydependonthenextfewphonemesbecauseofco-articulationandpotentiallymayevendependonthenextfewwordsbecauseofthelinguisticdependenciesbetweennearbywords:iftherearetwointerpretationsofthecurrentwordthatarebothacousticallyplausible,wemayhavetolookfarintothefuture(andthepast)todisambiguatethem.Thisisalsotrueofhandwritingrecognitionandmanyothersequence-to-sequencelearningtasks,describedinthenextsection.Bidirectionalrecurrentneuralnetworks(orbidirectionalRNNs)wereinventedtoaddressthatneed(SchusterandPaliwal1997,).Theyhavebeenextremelysuc-cessful(Graves2012,)inapplicationswherethatneedarises,suchashandwritingrecognition(Graves2008GravesandSchmidhuber2009etal.,;,),speechrecogni-tion(GravesandSchmidhuber2005Graves2013Baldi,;etal.,)andbioinformatics(etal.,).1999Asthenamesuggests,bidirectionalRNNscombineanRNNthatmovesforwardthroughtimebeginningfromthestartofthesequencewithanotherRNNthatmovesbackwardthroughtimebeginningfromtheendofthesequence.Figure10.11illustratesthetypicalbidirectionalRNN,withh()tstandingforthestateofthesub-RNNthatmovesforwardthroughtimeandg()tstandingforthestateofthesub-RNNthatmovesbackwardthroughtime. Thisallowstheoutputunitso()ttocomputearepresentationthatdependsonboththepastandthefuturebutismostsensitivetotheinputvaluesaroundtimet,withouthavingtospecifyaﬁxed-sizewindowaroundt(asonewouldhavetodowithafeedforwardnetwork,aconvolutionalnetwork,oraregularRNNwithaﬁxed-sizelook-aheadbuﬀer).Thisideacanbenaturallyextendedto2-dimensionalinput,suchasimages,byhavingRNNs,eachonegoinginoneofthefourdirections: up,down,left,fourright.Ateachpoint(i,j)ofa2-Dgrid,anoutputOi,jcouldthencomputearepresentationthatwouldcapturemostlylocalinformationbutcouldalsodependon long-rangeinputs,ifthe RNN isable tolearn tocarry that information.Comparedtoaconvolutionalnetwork,RNNsappliedtoimagesaretypicallymoreexpensivebutallowforlong-rangelateralinteractionsbetweenfeaturesinthesamefeaturemap(,;Visinetal.2015Kalchbrenner2015etal.,).Indeed,theforwardpropagationequationsforsuchRNNsmaybewritteninaformthatshowstheyuseaconvolutionthatcomputesthebottom-upinputtoeachlayer,priortotherecurrentpropagationacrossthefeaturemapthatincorporatesthelateralinteractions.395 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS10.4Encoder-DecoderSequence-to-SequenceArchitec-turesWehaveseeninﬁgurehowanRNNcanmapaninputsequencetoaﬁxed-size10.5vector.WehaveseeninﬁgurehowanRNNcanmapaﬁxed-sizevectortoa10.9sequence. Wehaveseeninﬁgures,,andhowanRNNcan10.310.410.1010.11mapaninputsequencetoanoutputsequenceofthesamelength.Encoder…x(1)x(1)x(2)x(2)x()...x()...x(nx)x(nx) Decoder…y(1)y(1)y(2)y(2)y()...y()...y(ny)y(ny)C C Figure10.12: Exampleofanencoder-decoderorsequence-to-sequenceRNNarchitecture,forlearningtogenerateanoutputsequence(y(1),...,y(ny))givenaninputsequence(x(1),x(2),...,x(nx)).ItiscomposedofanencoderRNNthatreadstheinputsequenceandadecoderRNNthatgeneratestheoutputsequence(orcomputestheprobabilityofagivenoutputsequence).TheﬁnalhiddenstateoftheencoderRNNisusedtocomputeagenerallyﬁxed-sizecontextvariableCwhichrepresentsasemanticsummaryoftheinputsequenceandisgivenasinputtothedecoderRNN.HerewediscusshowanRNNcanbetrainedtomapaninputsequencetoanoutputsequencewhichisnotnecessarilyofthesamelength. Thiscomesupinmanyapplications,suchasspeechrecognition,machinetranslationorquestion396 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETSanswering,wheretheinputandoutputsequencesinthetrainingsetaregenerallynotofthesamelength(althoughtheirlengthsmightberelated).WeoftencalltheinputtotheRNNthe“context.”Wewanttoproducearepresentationofthiscontext,C.ThecontextCmightbeavectororsequenceofvectorsthatsummarizetheinputsequenceXx= ((1),...,x(nx)).ThesimplestRNNarchitectureformappingavariable-lengthsequencetoanothervariable-lengthsequencewasﬁrstproposedby()andChoetal.2014ashortlyafterbySutskever2014etal.(),whoindependentlydevelopedthatarchi-tectureandweretheﬁrsttoobtainstate-of-the-arttranslationusingthisapproach.Theformersystemisbasedonscoringproposalsgeneratedbyanothermachinetranslationsystem,whilethelatterusesastandalonerecurrentnetworktogeneratethetranslations. Theseauthorsrespectivelycalledthisarchitecture,illustratedinﬁgure,theencoder-decoderorsequence-to-sequencearchitecture.The10.12ideaisverysimple:(1)anencoderorreaderorinputRNNprocessestheinputsequence.TheencoderemitsthecontextC,usuallyasasimplefunctionofitsﬁnalhiddenstate. (2)adecoderorwriteroroutputRNNisconditionedonthatﬁxed-lengthvector(justlikeinﬁgure)togeneratetheoutputsequence10.9Y=(y(1),...,y(ny)).Theinnovationofthiskindofarchitectureoverthosepresentedinearliersectionsofthischapteristhatthelengthsnxandnycanvaryfromeachother,whilepreviousarchitecturesconstrainednx=ny=τ.Inasequence-to-sequencearchitecture,thetwoRNNsaretrainedjointlytomaximizetheaverageoflogP(y(1),...,y(ny)|x(1),...,x(nx))overallthepairsofxandysequencesinthetrainingset.ThelaststatehnxoftheencoderRNNistypicallyusedasarepresentationCoftheinputsequencethatisprovidedasinputtothedecoderRNN.IfthecontextCisavector,thenthedecoderRNNissimplyavector-to-sequenceRNNasdescribedinsection.Aswehaveseen,thereareatleast10.2.4twowaysforavector-to-sequenceRNNtoreceiveinput.TheinputcanbeprovidedastheinitialstateoftheRNN,ortheinputcanbeconnectedtothehiddenunitsateachtimestep.Thesetwowayscanalsobecombined.Thereisnoconstraintthattheencodermusthavethesamesizeofhiddenlayerasthedecoder.OneclearlimitationofthisarchitectureiswhenthecontextCoutputbytheencoderRNNhasadimensionthatistoosmalltoproperlysummarizealongsequence.Thisphenomenonwasobservedby()inthecontextBahdanauetal.2015ofmachinetranslation.TheyproposedtomakeCavariable-lengthsequenceratherthanaﬁxed-sizevector.Additionally,theyintroducedanattentionmechanismthatlearnstoassociateelementsofthesequenceCtoelementsoftheoutput397 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETSsequence.Seesectionformoredetails.12.4.5.110.5DeepRecurrentNetworksThecomputationinmostRNNscanbedecomposedintothreeblocksofparametersandassociatedtransformations:1. fromtheinputtothehiddenstate,2. fromtheprevioushiddenstatetothenexthiddenstate,and3. fromthehiddenstatetotheoutput.WiththeRNNarchitectureofﬁgure,eachofthesethreeblocksisassociated10.3withasingleweightmatrix.Inotherwords,whenthenetworkisunfolded,eachofthesecorrespondstoashallowtransformation. Byashallowtransformation,wemeanatransformationthatwouldberepresentedbyasinglelayerwithinadeepMLP.Typicallythisisatransformationrepresentedbyalearnedaﬃnetransformationfollowedbyaﬁxednonlinearity.Woulditbeadvantageoustointroducedepthineachoftheseoperations?Experimentalevidence(Graves2013Pascanu2014aetal.,;etal.,)stronglysuggestsso.Theexperimentalevidenceisinagreementwiththeideathatweneedenoughdepthinordertoperformtherequiredmappings.SeealsoSchmidhuber1992(),ElHihiandBengio1996Jaeger2007a(),or()forearlierworkondeepRNNs.Graves2013etal.()weretheﬁrsttoshowasigniﬁcantbeneﬁtofdecomposingthestateofanRNNintomultiplelayersasinﬁgure(left).Wecanthink10.13ofthelowerlayersinthehierarchydepictedinﬁgureaasplayingarole10.13intransformingtherawinputintoarepresentationthatismoreappropriate,atthehigherlevelsofthehiddenstate.Pascanu2014aetal.()goastepfurtherandproposetohaveaseparateMLP(possiblydeep)foreachofthethreeblocksenumeratedabove,asillustratedinﬁgureb.Considerationsofrepresentational10.13capacitysuggesttoallocateenoughcapacityineachofthesethreesteps,butdoingsobyaddingdepthmayhurtlearningbymakingoptimizationdiﬃcult.Ingeneral,itiseasiertooptimizeshallowerarchitectures,andaddingtheextradepthofﬁgurebmakestheshortestpathfromavariableintimestep10.13ttoavariableintimestept+1becomelonger.Forexample,ifanMLPwithasinglehiddenlayerisusedforthestate-to-statetransition,wehavedoubledthelengthoftheshortestpathbetweenvariablesinanytwodiﬀerenttimesteps,comparedwiththeordinaryRNNofﬁgure.However,asarguedby10.3Pascanu2014aetal.(),this398 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS hy xz (a)(b)(c)xhy xhy Figure10.13:Arecurrentneuralnetworkcanbemadedeepinmanyways(Pascanuetal.,).Thehiddenrecurrentstatecanbebrokendownintogroupsorganized2014a(a)hierarchically.Deepercomputation(e.g.,anMLP)canbeintroducedintheinput-to-(b)hidden,hidden-to-hiddenandhidden-to-outputparts. Thismaylengthentheshortestpathlinkingdiﬀerenttimesteps.Thepath-lengtheningeﬀectcanbemitigatedby(c)introducingskipconnections. 399 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETScanbemitigatedbyintroducingskipconnectionsinthehidden-to-hiddenpath,asillustratedinﬁgurec.10.1310.6RecursiveNeuralNetworks x(1)x(1)x(2)x(2)x(3)x(3)VVVy yL L x(4)x(4)Vo o UWUWUW Figure10.14:Arecursivenetworkhasacomputationalgraphthatgeneralizesthatoftherecurrentnetworkfromachaintoatree.Avariable-sizesequencex(1),x(2),...,x()tcanbemappedtoaﬁxed-sizerepresentation(theoutputo),withaﬁxedsetofparameters(theweightmatricesU,V,W).Theﬁgureillustratesasupervisedlearningcaseinwhichsometargetisprovidedwhichisassociatedwiththewholesequence.yRecursiveneuralnetworks2representyetanothergeneralizationofrecurrentnetworks,withadiﬀerentkindofcomputationalgraph,whichisstructuredasadeeptree,ratherthanthechain-likestructureofRNNs.Thetypicalcomputationalgraphforarecursivenetworkisillustratedinﬁgure.Recursiveneural10.142Wesuggesttonotabbreviate“recursiveneuralnetwork”as“RNN”toavoidconfusionwith“recurrentneuralnetwork.”400 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETSnetworkswereintroducedbyPollack1990()andtheirpotentialuseforlearningtoreasonwasdescribedby().RecursivenetworkshavebeensuccessfullyBottou2011appliedtoprocessingdatastructuresasinputtoneuralnets(Frasconi1997etal.,,1998Socher2011ac2013a),innaturallanguageprocessing(etal.,,,)aswellasincomputervision(,).Socheretal.2011bOneclearadvantageofrecursivenetsoverrecurrentnetsisthatforasequenceofthesamelengthτ,thedepth(measuredasthenumberofcompositionsofnonlinearoperations)canbedrasticallyreducedfromτtoO(logτ),whichmighthelpdealwithlong-termdependencies.Anopenquestionishowtobeststructurethetree.Oneoptionistohaveatreestructurewhichdoesnotdependonthedata,suchasabalancedbinarytree.Insomeapplicationdomains,externalmethodscansuggesttheappropriatetreestructure.Forexample,whenprocessingnaturallanguagesentences,thetreestructurefortherecursivenetworkcanbeﬁxedtothestructureoftheparsetreeofthesentenceprovidedbyanaturallanguageparser(,,). Ideally,onewouldlikethelearneritselftoSocheretal.2011a2013adiscoverandinferthetreestructurethatisappropriateforanygiveninput,assuggestedby().Bottou2011Manyvariantsoftherecursivenetideaarepossible.Forexample,Frasconietal.()and1997Frasconi1998etal.()associatethedatawithatreestructure,andassociatethe inputsandtargetswith individualnodesofthe tree.Thecomputationperformedbyeachnodedoesnothavetobethetraditionalartiﬁcialneuroncomputation(aﬃnetransformationofallinputsfollowedbyamonotonenonlinearity).Forexample,()proposeusingtensoroperationsSocheretal.2013aandbilinearforms,whichhavepreviouslybeenfoundusefultomodelrelationshipsbetweenconcepts(Weston2010Bordes2012etal.,;etal.,)whentheconceptsarerepresentedbycontinuousvectors(embeddings).10.7TheChallengeofLong-TermDependenciesThemathematicalchallengeoflearninglong-termdependenciesinrecurrentnet-workswasintroducedinsection.Thebasicproblemisthatgradientsprop-8.2.5agatedovermanystagestendtoeithervanish(mostofthetime)orexplode(rarely,butwithmuchdamagetotheoptimization).Evenifweassumethattheparametersaresuchthattherecurrentnetworkisstable(canstorememories,withgradientsnotexploding),thediﬃcultywithlong-termdependenciesarisesfromtheexponentiallysmallerweightsgiventolong-terminteractions(involvingthemultiplicationofmanyJacobians)comparedtoshort-termones.Manyothersourcesprovideadeepertreatment(,;Hochreiter1991Doya1993Bengio,;etal.,401 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS −−−6040200204060Inputcoordinate−4−3−2−101234Projectionofoutput012345Figure10.15:Whencomposingmanynonlinearfunctions(likethelinear-tanhlayershownhere),theresultishighlynonlinear,typicallywithmostofthevaluesassociatedwithatinyderivative,somevalueswithalargederivative,andmanyalternationsbetweenincreasinganddecreasing.Inthisplot,weplotalinearprojectionofa100-dimensionalhiddenstatedowntoasingledimension,plottedonthey-axis. Thex-axisisthecoordinateoftheinitialstatealongarandomdirectioninthe100-dimensionalspace.Wecanthusviewthisplotasalinearcross-sectionofahigh-dimensionalfunction.Theplotsshowthefunctionaftereachtimestep,orequivalently,aftereachnumberoftimesthetransitionfunctionhasbeencomposed.1994Pascanu2013;etal.,).Inthissection,wedescribetheprobleminmoredetail.Theremainingsectionsdescribeapproachestoovercomingtheproblem.Recurrentnetworksinvolvethecompositionofthesamefunctionmultipletimes,oncepertimestep.Thesecompositionscanresultinextremelynonlinearbehavior,asillustratedinﬁgure.10.15Inparticular,thefunctioncompositionemployedbyrecurrentneuralnetworkssomewhatresemblesmatrixmultiplication.Wecanthinkoftherecurrencerelationh()t= Wh(1)t−(10.36)asaverysimplerecurrentneuralnetworklackinganonlinearactivationfunction,andlackinginputsx.As described insection , thisrecurrencerelation8.2.5essentiallydescribesthepowermethod.Itmaybesimpliﬁedtoh()t=Wth(0),(10.37)andifadmitsaneigendecompositionoftheformWWQQ= Λ,(10.38)402 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETSwithorthogonal,therecurrencemaybesimpliﬁedfurthertoQh()t= QΛtQh(0).(10.39)Theeigenvaluesareraisedtothepoweroftcausingeigenvalueswithmagnitudelessthanonetodecaytozeroandeigenvalueswithmagnitudegreaterthanonetoexplode.Anycomponentofh(0)thatisnotalignedwiththelargesteigenvectorwilleventuallybediscarded.Thisproblemisparticulartorecurrentnetworks.Inthescalarcase,imaginemultiplyingaweightwbyitselfmanytimes.Theproductwtwilleithervanishorexplodedependingonthemagnitudeofw.However,ifwemakeanon-recurrentnetworkthathasadiﬀerentweightw()tateachtimestep,thesituationisdiﬀerent.Iftheinitialstateisgivenby,thenthestateattime1tisgivenbytw()t.Supposethatthew()tvaluesaregeneratedrandomly,independentlyfromoneanother,withzeromeanandvariancev.ThevarianceoftheproductisO(vn).Toobtainsomedesiredvariancev∗wemaychoosetheindividualweightswithvariancev=n√v∗.Verydeepfeedforwardnetworkswithcarefullychosenscalingcanthusavoidthevanishingandexplodinggradientproblem,asarguedby().Sussillo2014ThevanishingandexplodinggradientproblemforRNNswasindependentlydiscoveredbyseparateresearchers(,;,,).Hochreiter1991Bengioetal.19931994Onemayhopethattheproblemcanbeavoidedsimplybystayinginaregionofparameterspacewherethegradientsdonotvanishorexplode.Unfortunately,inordertostorememoriesinawaythatisrobusttosmallperturbations,theRNNmustenteraregionofparameterspacewheregradientsvanish(,,Bengioetal.19931994).Speciﬁcally,wheneverthemodelisabletorepresentlongtermdependencies,thegradientofalongterminteractionhasexponentiallysmallermagnitudethanthegradientofashortterminteraction. Itdoesnotmeanthatitisimpossibletolearn,butthatitmighttakeaverylongtimetolearnlong-termdependencies,becausethesignalaboutthesedependencieswilltendtobehiddenbythesmallestﬂuctuationsarisingfromshort-termdependencies.Inpractice,theexperimentsin()showthatasweincreasethespanofthedependenciesthatBengioetal.1994needtobecaptured,gradient-basedoptimizationbecomesincreasinglydiﬃcult,withtheprobabilityofsuccessfultrainingofatraditionalRNNviaSGDrapidlyreaching0forsequencesofonlylength10or20.Foradeepertreatmentofrecurrentnetworksasdynamicalsystems,seeDoya(),()and(),withareview1993Bengioetal.1994SiegelmannandSontag1995inPascanu2013etal.().Theremainingsectionsofthischapterdiscussvariousapproachesthathavebeenproposedtoreducethediﬃcultyoflearninglong-termdependencies(insomecasesallowinganRNNtolearndependenciesacross403 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETShundredsofsteps),buttheproblemoflearninglong-termdependenciesremainsoneofthemainchallengesindeeplearning.10.8EchoStateNetworksTherecurrentweightsmappingfromh(1)t−toh()tandtheinputweightsmappingfromx()ttoh()taresomeofthemostdiﬃcultparameterstolearninarecurrentnetwork.Oneproposed(,;,;,;Jaeger2003Maassetal.2002JaegerandHaas2004Jaeger2007b,)approachtoavoidingthisdiﬃcultyistosettherecurrentweightssuchthattherecurrenthiddenunitsdoagoodjobofcapturingthehistoryofpastinputs,andlearnonlytheoutputweights.ThisistheideathatwasindependentlyproposedforechostatenetworksorESNs(,;,)JaegerandHaas2004Jaeger2007bandliquidstatemachines(,).Thelatterissimilar,exceptMaassetal.2002thatitusesspikingneurons(withbinaryoutputs)insteadofthecontinuous-valuedhiddenunitsusedforESNs.BothESNsandliquidstatemachinesaretermedreservoircomputing(LukoševičiusandJaeger2009,)todenotethefactthatthehiddenunitsformofreservoiroftemporalfeatureswhichmaycapturediﬀerentaspectsofthehistoryofinputs.Onewaytothinkaboutthesereservoircomputingrecurrentnetworksisthattheyaresimilartokernelmachines:theymapanarbitrarylengthsequence(thehistoryofinputsuptotimet)intoaﬁxed-lengthvector(therecurrentstateh()t),onwhichalinearpredictor(typicallyalinearregression)canbeappliedtosolvetheproblemofinterest.Thetrainingcriterionmaythenbeeasilydesignedtobeconvexasafunctionoftheoutputweights.Forexample,iftheoutputconsistsoflinearregressionfromthehiddenunitstotheoutputtargets,andthetrainingcriterionismeansquarederror,thenitisconvexandmaybesolvedreliablywithsimplelearningalgorithms(,).Jaeger2003Theimportantquestionistherefore:howdowesettheinputandrecurrentweightssothatarichsetofhistoriescanberepresentedintherecurrentneuralnetworkstate? Theanswerproposedinthereservoircomputingliteratureistoviewtherecurrentnetasadynamicalsystem,andsettheinputandrecurrentweightssuchthatthedynamicalsystemisneartheedgeofstability.TheoriginalideawastomaketheeigenvaluesoftheJacobianofthestate-to-statetransitionfunctionbecloseto.Asexplainedinsection,animportant18.2.5characteristicofarecurrentnetworkistheeigenvaluespectrumoftheJacobiansJ()t=∂s()t∂s(1)t−.OfparticularimportanceisthespectralradiusofJ()t,deﬁnedtobethemaximumoftheabsolutevaluesofitseigenvalues.404 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETSTounderstandtheeﬀectofthespectralradius,considerthesimplecaseofback-propagationwithaJacobianmatrixJthatdoesnotchangewitht.Thiscasehappens,forexample,whenthenetworkispurelylinear.SupposethatJhasaneigenvectorvwithcorrespondingeigenvalueλ.Considerwhathappensaswepropagateagradientvectorbackwardsthroughtime.Ifwebeginwithagradientvectorg,thenafteronestepofback-propagation,wewillhaveJg,andafternstepswewillhaveJng.Nowconsiderwhathappensifweinsteadback-propagateaperturbedversionofg.Ifwebeginwithg+δv,thenafteronestep,wewillhaveJ(g+δv).Afternsteps,wewillhaveJn(g+δv).Fromthiswecanseethatback-propagationstartingfromgandback-propagationstartingfromg+δvdivergebyδJnvafternstepsofback-propagation.IfvischosentobeauniteigenvectorofJwitheigenvalueλ,thenmultiplicationbytheJacobiansimplyscalesthediﬀerenceateachstep.Thetwoexecutionsofback-propagationareseparatedbyadistanceofδλ||n.Whenvcorrespondstothelargestvalueof||λ,thisperturbationachievesthewidestpossibleseparationofaninitialperturbationofsize.δWhen||λ>1,thedeviationsizeδλ||ngrowsexponentiallylarge.When||λ<1,thedeviationsizebecomesexponentiallysmall.Ofcourse,thisexampleassumedthattheJacobianwasthesameateverytimestep,correspondingtoarecurrentnetworkwithnononlinearity.Whenanonlinearityispresent,thederivativeofthenonlinearitywillapproachzeroonmanytimesteps,andhelptopreventtheexplosionresultingfromalargespectralradius. Indeed,themostrecentworkonechostatenetworksadvocatesusingaspectralradiusmuchlargerthanunity(,;,).Yildizetal.2012Jaeger2012Everythingwehavesaidaboutback-propagationviarepeatedmatrixmultipli-cationappliesequallytoforwardpropagationinanetworkwithnononlinearity,wherethestateh(+1)t= h()tW.WhenalinearmapWalwaysshrinkshasmeasuredbytheL2norm,thenwesaythatthemapiscontractive.Whenthespectralradiusislessthanone,themappingfromh()ttoh(+1)tiscontractive,soasmallchangebecomessmalleraftereachtimestep.Thisnecessarilymakesthenetworkforgetinformationaboutthepastwhenweuseaﬁnitelevelofprecision(suchas32bitintegers)tostorethestatevector.TheJacobianmatrixtellsushowasmallchangeofh()tpropagatesonestepforward,orequivalently,howthegradientonh(+1)tpropagatesonestepbackward,duringback-propagation.NotethatneitherWnorJneedtobesymmetric(al-thoughtheyaresquareandreal),sotheycanhavecomplex-valuedeigenvaluesandeigenvectors,withimaginarycomponentscorrespondingtopotentiallyoscillatory405 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETSbehavior(ifthesameJacobianwasappliediteratively).Eventhoughh()torasmallvariationofh()tofinterestinback-propagationarereal-valued,theycanbeexpressedinsuchacomplex-valuedbasis.Whatmattersiswhathappenstothemagnitude(complexabsolutevalue)ofthesepossiblycomplex-valuedbasiscoeﬃcients, whenwemultiplythematrixbythevector.Aneigenvaluewithmagnitudegreaterthanonecorrespondstomagniﬁcation(exponentialgrowth,ifappliediteratively)orshrinking(exponentialdecay,ifappliediteratively).Withanonlinearmap, theJacobianisfreetochangeateachstep.Thedynamicsthereforebecomemorecomplicated.However,itremainstruethatasmallinitialvariationcanturnintoalargevariationafterseveralsteps.Onediﬀerencebetweenthepurelylinearcaseandthenonlinearcaseisthattheuseofasquashingnonlinearitysuchastanhcancausetherecurrentdynamicstobecomebounded.Notethat itispossible forback-propagationto retainunboundeddynamicsevenwhenforwardpropagationhasboundeddynamics,forexample,whenasequenceoftanhunitsareallinthemiddleoftheirlinearregimeandareconnectedbyweightmatriceswithspectralradiusgreaterthan.However,itis1rareforalloftheunitstosimultaneouslylieattheirlinearactivationpoint.tanhThestrategyofechostatenetworksissimplytoﬁxtheweightstohavesomespectralradiussuchas,whereinformationiscarriedforwardthroughtimebut3doesnotexplodeduetothestabilizingeﬀectofsaturatingnonlinearitiesliketanh.Morerecently,ithasbeenshownthatthetechniquesusedtosettheweightsinESNscouldbeusedtotheweightsinafullytrainablerecurrentnet-initializework(withthehidden-to-hiddenrecurrentweightstrainedusingback-propagationthroughtime),helpingtolearnlong-termdependencies(Sutskever2012Sutskever,;etal.,).Inthissetting,aninitialspectralradiusof1.2performswell,combined2013withthesparseinitializationschemedescribedinsection.8.410.9LeakyUnitsandOtherStrategiesforMultipleTimeScalesOnewaytodealwithlong-termdependenciesistodesignamodelthatoperatesatmultipletimescales,sothatsomepartsofthemodeloperateatﬁne-grainedtimescalesandcanhandlesmalldetails,whileotherpartsoperateatcoarsetimescalesandtransferinformationfromthedistantpasttothepresentmoreeﬃciently.Variousstrategiesforbuildingbothﬁneandcoarsetimescalesarepossible.Theseincludetheadditionofskipconnectionsacrosstime,“leakyunits”thatintegratesignalswithdiﬀerenttimeconstants,andtheremovalofsomeoftheconnections406 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETSusedtomodelﬁne-grainedtimescales.10.9.1AddingSkipConnectionsthroughTimeOnewaytoobtaincoarsetimescalesistoadddirectconnectionsfromvariablesinthedistantpasttovariablesinthepresent.Theideaofusingsuchskipconnectionsdatesbackto()andfollowsfromtheideaofincorporatingdelaysinLinetal.1996feedforwardneuralnetworks(,).InanordinaryrecurrentLangandHinton1988network,arecurrentconnectiongoesfromaunitattimettoaunitattimet+1.Itispossibletoconstructrecurrentnetworkswithlongerdelays(,).Bengio1991Aswehaveseeninsection,gradientsmayvanishorexplodeexponentially8.2.5withrespecttothenumberoftimesteps.()introducedrecurrentLinetal.1996connectionswithatime-delayofdtomitigatethisproblem.Gradientsnowdiminishexponentiallyasafunctionofτdratherthanτ.Sincetherearebothdelayedandsinglestepconnections,gradientsmaystillexplodeexponentiallyinτ.Thisallowsthelearningalgorithmtocapturelongerdependenciesalthoughnotalllong-termdependenciesmayberepresentedwellinthisway.10.9.2LeakyUnitsandaSpectrumofDiﬀerentTimeScalesAnotherwaytoobtainpathsonwhichtheproductofderivativesisclosetooneistohaveunitswithlinearself-connectionsandaweightnearoneontheseconnections.Whenweaccumulatearunningaverageµ()tofsomevaluev()tbyapplyingtheupdateµ()t←αµ(1)t−+(1−α)v()ttheαparameterisanexampleofalinearself-connectionfromµ(1)t−toµ()t.Whenαisnearone,therunningaverageremembersinformationaboutthepastforalongtime,andwhenαisnearzero,informationaboutthepastisrapidlydiscarded.Hiddenunitswithlinearself-connectionscanbehavesimilarlytosuchrunningaverages.Suchhiddenunitsarecalledleakyunits.Skipconnectionsthroughdtimestepsareawayofensuringthataunitcanalwayslearntobeinﬂuencedbyavaluefromdtimestepsearlier.Theuseofalinearself-connectionwithaweightnearoneisadiﬀerentwayofensuringthattheunitcanaccessvaluesfromthepast.Thelinearself-connectionapproachallowsthiseﬀecttobeadaptedmoresmoothlyandﬂexiblybyadjustingthereal-valuedαratherthanbyadjustingtheinteger-valuedskiplength.Theseideaswereproposedby()andby().Mozer1992ElHihiandBengio1996Leakyunitswerealsofoundtobeusefulinthecontextofechostatenetworks(,).Jaegeretal.2007407 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETSTherearetwobasicstrategiesforsettingthetimeconstantsusedbyleakyunits. Onestrategyistomanuallyﬁxthemtovaluesthatremainconstant,forexamplebysamplingtheirvaluesfromsomedistributiononceatinitializationtime.Anotherstrategyistomakethetimeconstantsfreeparametersandlearnthem.Havingsuchleakyunitsatdiﬀerenttimescalesappearstohelpwithlong-termdependencies(,;Mozer1992Pascanu2013etal.,).10.9.3RemovingConnectionsAnotherapproachtohandlelong-termdependenciesistheideaoforganizingthestateoftheRNNatmultipletime-scales(,),withElHihiandBengio1996informationﬂowingmoreeasilythroughlongdistancesattheslowertimescales.Thisideadiﬀersfromtheskipconnectionsthroughtimediscussedearlierbecauseitinvolvesactivelyremovinglength-oneconnectionsandreplacingthemwithlongerconnections.Unitsmodiﬁedinsuchawayareforcedtooperateonalongtimescale.Skipconnectionsthroughtimeedges.Unitsreceivingsuchaddnewconnectionsmaylearntooperateonalongtimescalebutmayalsochoosetofocusontheirothershort-termconnections.Therearediﬀerentwaysinwhichagroupofrecurrentunitscanbeforcedtooperateatdiﬀerenttimescales.Oneoptionistomaketherecurrentunitsleaky,buttohavediﬀerentgroupsofunitsassociatedwithdiﬀerentﬁxedtimescales.Thiswastheproposalin()andhasbeensuccessfullyusedinMozer1992Pascanuetal.().Anotheroptionistohaveexplicitanddiscreteupdatestakingplace2013atdiﬀerenttimes,withadiﬀerentfrequencyfordiﬀerentgroupsofunits.Thisistheapproachof()andElHihiandBengio1996Koutnik2014etal.().Itworkedwellonanumberofbenchmarkdatasets.10.10TheLongShort-TermMemoryandOtherGatedRNNsAsofthiswriting,themosteﬀectivesequencemodelsusedinpracticalapplicationsarecalledgatedRNNs.Theseincludethelongshort-termmemoryandnetworksbasedonthe.gatedrecurrentunitLikeleakyunits,gatedRNNsarebasedontheideaofcreatingpathsthroughtimethathavederivativesthatneithervanishnorexplode.Leakyunits didthiswithconnectionweightsthatwereeithermanuallychosenconstantsorwereparameters.GatedRNNsgeneralizethistoconnectionweightsthatmaychange408 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETSateachtimestep. ×inputinput gateforget gateoutput gateoutput stateself-loop×+× Figure10.16:BlockdiagramoftheLSTMrecurrentnetwork“cell.”Cellsareconnectedrecurrentlytoeachother,replacingtheusualhiddenunitsofordinaryrecurrentnetworks.Aninputfeatureiscomputedwitharegularartiﬁcialneuronunit.Itsvaluecanbeaccumulatedintothestateifthesigmoidalinputgateallowsit.Thestateunithasalinearself-loopwhoseweightiscontrolledbytheforgetgate.Theoutputofthecellcanbeshutoﬀbytheoutputgate.Allthegatingunitshaveasigmoidnonlinearity,whiletheinputunitcanhaveanysquashingnonlinearity. Thestateunitcanalsobeusedasanextrainputtothegatingunits.Theblacksquareindicatesadelayofasingletimestep.Leakyunitsallowthenetworktoaccumulateinformation(suchasevidenceforaparticularfeatureorcategory)overalongduration.However,oncethatinformationhasbeenused,itmightbeusefulfortheneuralnetworktoforgettheoldstate.Forexample,ifasequenceismadeofsub-sequencesandwewantaleakyunittoaccumulateevidenceinsideeachsub-subsequence,weneedamechanismtoforgettheoldstatebysettingittozero.Insteadofmanuallydecidingwhentoclearthestate,wewanttheneuralnetworktolearntodecidewhentodoit.This409 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETSiswhatgatedRNNsdo.10.10.1LSTMThecleverideaofintroducingself-loopstoproducepathswherethegradientcanﬂowforlongdurationsisacorecontributionoftheinitiallongshort-termmemory(LSTM)model(HochreiterandSchmidhuber1997,).Acrucialadditionhasbeentomaketheweightonthisself-loopconditionedonthecontext,ratherthanﬁxed(,).Bymakingtheweightofthisself-loopgated(controlledGersetal.2000byanotherhiddenunit),thetimescaleofintegrationcanbechangeddynamically.Inthiscase,wemeanthatevenforanLSTMwithﬁxedparameters,thetimescaleofintegrationcanchangebasedontheinputsequence,becausethetimeconstantsareoutputbythemodelitself.TheLSTMhasbeenfoundextremelysuccessfulinmanyapplications, suchasunconstrainedhandwritingrecognition(Gravesetal.,),speechrecognition(2009Graves2013GravesandJaitly2014etal.,;,),handwritinggeneration(Graves2013,),machinetranslation(Sutskever2014etal.,),imagecaptioning(,;Kirosetal.2014bVinyals2014bXu2015etal.,;etal.,)andparsing(Vinyals2014aetal.,).TheLSTMblockdiagramisillustratedinﬁgure.Thecorresponding10.16forwardpropagationequationsaregivenbelow,inthecaseofashallowrecurrentnetworkarchitecture.Deeperarchitectureshavealsobeensuccessfullyused(Gravesetal.,;2013Pascanu2014aetal.,).Insteadofaunitthatsimplyappliesanelement-wisenonlinearitytotheaﬃnetransformationofinputsandrecurrentunits,LSTMrecurrentnetworkshave“LSTMcells”thathaveaninternalrecurrence(aself-loop),inadditiontotheouterrecurrenceoftheRNN.Eachcellhasthesameinputsandoutputsasanordinaryrecurrentnetwork,buthasmoreparametersandasystemofgatingunitsthatcontrolstheﬂowofinformation.Themostimportantcomponentisthestateunits()tithathasalinearself-loopsimilartotheleakyunitsdescribedintheprevioussection.However,here,theself-loopweight(ortheassociatedtimeconstant)iscontrolledbyaforgetgateunitf()ti(fortimesteptandcell),thatsetsthisweighttoavaluebetween0and1viaasigmoidunit:if()ti= σbfi+jUfi,jx()tj+jWfi,jh(1)t−j,(10.40)wherex()tisthecurrentinputvectorandh()tisthecurrenthiddenlayervector,containingtheoutputsofalltheLSTMcells,andbf,Uf,Wfarerespectivelybiases,inputweightsandrecurrentweightsfortheforgetgates.TheLSTMcell410 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETSinternalstateisthusupdatedasfollows,butwithaconditionalself-loopweightf()ti:s()ti= f()tis(1)t−i+g()tiσbi+jUi,jx()tj+jWi,jh(1)t−j,(10.41)whereb,UandWrespectivelydenotethebiases,inputweightsandrecurrentweightsintotheLSTMcell.Theexternalinputgateunitg()tiiscomputedsimilarlytotheforgetgate(withasigmoidunittoobtainagatingvaluebetween0and1),butwithitsownparameters:g()ti= σbgi+jUgi,jx()tj+jWgi,jh(1)t−j.(10.42)Theoutputh()tioftheLSTMcellcanalsobeshutoﬀ,viatheoutputgateq()ti,whichalsousesasigmoidunitforgating:h()ti= tanhs()tiq()ti(10.43)q()ti= σboi+jUoi,jx()tj+jWoi,jh(1)t−j(10.44)whichhasparametersbo,Uo,Woforitsbiases,inputweightsandrecurrentweights,respectively.Amongthevariants,onecanchoosetousethecellstates()tiasanextrainput(withitsweight)intothethreegatesofthei-thunit,asshowninﬁgure.Thiswouldrequirethreeadditionalparameters.10.16LSTMnetworkshavebeenshowntolearnlong-termdependenciesmoreeasilythanthesimplerecurrentarchitectures,ﬁrstonartiﬁcialdatasetsdesignedfortestingtheabilitytolearnlong-termdependencies(,;Bengioetal.1994HochreiterandSchmidhuber1997Hochreiter2001,;etal.,),thenonchallengingsequenceprocessingtaskswherestate-of-the-artperformancewasobtained(Graves2012,;Graves2013Sutskever2014etal.,;etal.,).VariantsandalternativestotheLSTMhavebeenstudiedandusedandarediscussednext.10.10.2OtherGatedRNNsWhichpieces ofthe LSTMarchitectureare actuallynecessary?Whatothersuccessfularchitecturescouldbedesignedthatallowthenetworktodynamicallycontrolthetimescaleandforgettingbehaviorofdiﬀerentunits?411 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETSSomeanswerstothesequestionsaregivenwiththerecentworkongatedRNNs,whoseunitsarealsoknownasgatedrecurrentunitsorGRUs(,;Choetal.2014bChung20142015aJozefowicz2015Chrupala2015etal.,,;etal.,;etal.,).ThemaindiﬀerencewiththeLSTMisthatasinglegatingunitsimultaneouslycontrolstheforgettingfactorandthedecisiontoupdatethestateunit.Theupdateequationsarethefollowing:h()ti= u(1)t−ih(1)t−i+(1−u(1)t−i)σbi+jUi,jx(1)t−j+jWi,jr(1)t−jh(1)t−j,(10.45)whereustandsfor“update”gateandrfor“reset”gate.Theirvalueisdeﬁnedasusual:u()ti= σbui+jUui,jx()tj+jWui,jh()tj(10.46)andr()ti= σbri+jUri,jx()tj+jWri,jh()tj.(10.47)Theresetandupdatesgatescanindividually“ignore”partsofthestatevector.Theupdategatesactlikeconditionalleakyintegratorsthatcanlinearlygateanydimension,thuschoosingtocopyit(atoneextremeofthesigmoid)orcompletelyignoreit(attheotherextreme)byreplacingitbythenew“targetstate”value(towardswhichtheleakyintegratorwantstoconverge).Theresetgatescontrolwhichpartsofthestategetusedtocomputethenexttargetstate,introducinganadditionalnonlineareﬀectintherelationshipbetweenpaststateandfuturestate.Manymorevariantsaroundthisthemecanbedesigned.Forexampletheresetgate(orforgetgate)outputcouldbesharedacrossmultiplehiddenunits.Alternately,theproductofaglobalgate(coveringawholegroupofunits,suchasanentirelayer)andalocalgate(perunit)couldbeusedtocombineglobalcontrolandlocalcontrol.However,severalinvestigationsoverarchitecturalvariationsoftheLSTMandGRUfoundnovariantthatwouldclearlybeatbothoftheseacrossawiderangeoftasks(,;Greﬀetal.2015Jozefowicz2015Greﬀetal.,).etal.()foundthatacrucialingredientistheforgetgate,while2015Jozefowiczetal.()foundthataddingabiasof1totheLSTMforgetgate,apractice2015advocatedby(),makestheLSTMasstrongasthebestoftheGersetal.2000exploredarchitecturalvariants.412 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS10.11OptimizationforLong-TermDependenciesSectionandsectionhavedescribedthevanishingandexplodinggradient8.2.510.7problemsthatoccurwhenoptimizingRNNsovermanytimesteps.AninterestingideaproposedbyMartensandSutskever2011()isthatsecondderivativesmayvanishatthesametimethatﬁrstderivativesvanish.Second-orderoptimizationalgorithmsmayroughlybeunderstoodasdividingtheﬁrstderivativebythesecondderivative(inhigherdimension,multiplyingthegradientbytheinverseHessian).Ifthesecondderivativeshrinksatasimilarratetotheﬁrstderivative,thentheratioofﬁrstandsecondderivativesmayremainrelativelyconstant.Unfortunately,second-ordermethodshavemanydrawbacks,includinghighcomputationalcost,theneedforalargeminibatch,andatendencytobeattractedtosaddlepoints.MartensandSutskever2011()foundpromisingresultsusingsecond-ordermethods.Later,Sutskever2013etal.()foundthatsimplermethodssuchasNesterovmomentumwithcarefulinitializationcouldachievesimilarresults.SeeSutskever2012()formoredetail. BothoftheseapproacheshavelargelybeenreplacedbysimplyusingSGD(evenwithoutmomentum)appliedtoLSTMs.Thisispartofacontinuingthemeinmachinelearningthatitisoftenmucheasiertodesignamodelthatiseasytooptimizethanitistodesignamorepowerfuloptimizationalgorithm.10.11.1ClippingGradientsAsdiscussedinsection,stronglynonlinearfunctionssuchasthosecomputed8.2.4byarecurrentnetovermanytimestepstendtohavederivativesthatcanbeeitherverylargeorverysmallinmagnitude.Thisisillustratedinﬁgureand8.3ﬁgure,inwhichweseethattheobjectivefunction(asafunctionofthe10.17parameters)hasa“landscape” inwhichoneﬁnds“cliﬀs”:wideandratherﬂatregionsseparatedbytinyregionswheretheobjectivefunctionchangesquickly,formingakindofcliﬀ.Thediﬃcultythatarisesisthatwhentheparametergradientisverylarge,agradientdescentparameterupdatecouldthrowtheparametersveryfar,intoaregionwheretheobjectivefunctionislarger,undoingmuchoftheworkthathadbeendonetoreachthecurrentsolution.Thegradienttellsusthedirectionthatcorrespondstothesteepestdescentwithinaninﬁnitesimalregionsurroundingthecurrentparameters.Outsideofthisinﬁnitesimalregion,thecostfunctionmaybegintocurvebackupwards.Theupdatemustbechosentobesmallenoughtoavoidtraversingtoomuchupwardcurvature.Wetypicallyuselearningratesthat413 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETSdecayslowlyenoughthatconsecutivestepshaveapproximatelythesamelearningrate.Astepsizethatisappropriateforarelativelylinearpartofthelandscapeisofteninappropriateandcausesuphillmotionifweenteramorecurvedpartofthelandscapeonthenextstep.   Figure10.17:Exampleoftheeﬀectofgradientclippinginarecurrentnetworkwithtwoparameterswandb.Gradientclippingcanmakegradientdescentperformmorereasonablyinthevicinityofextremelysteepcliﬀs.Thesesteepcliﬀscommonlyoccurinrecurrentnetworksnearwherearecurrentnetworkbehavesapproximatelylinearly.Thecliﬀisexponentiallysteepinthenumberoftimestepsbecausetheweightmatrixismultipliedbyitselfonceforeachtimestep.(Left)Gradientdescentwithoutgradientclippingovershootsthebottomofthissmallravine,thenreceivesaverylargegradientfromthecliﬀface.Thelargegradientcatastrophicallypropelstheparametersoutsidetheaxesoftheplot.Gradientdescentwithgradientclippinghasamoremoderate(Right)reactiontothecliﬀ.Whileitdoesascendthecliﬀface,thestepsizeisrestrictedsothatitcannotbepropelledawayfromsteepregionnearthesolution.FigureadaptedwithpermissionfromPascanu2013etal.().Asimpletypeofsolutionhasbeeninusebypractitionersformanyyears:clippingthegradient.Therearediﬀerentinstancesofthisidea(Mikolov2012,;Pascanu2013etal.,).Oneoptionistocliptheparametergradientfromaminibatchelement-wise(Mikolov2012,)justbeforetheparameterupdate.Anotheristoclipthenorm||||gofthegradientg(Pascanu2013etal.,)justbeforetheparameterupdate:if||||g>v(10.48)g←gv||||g(10.49)414 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETSwherevisthenormthresholdandgisusedtoupdateparameters.Becausethegradientofalltheparameters(includingdiﬀerentgroupsofparameters,suchasweightsandbiases)isrenormalizedjointlywithasinglescalingfactor,thelattermethodhastheadvantagethatitguaranteesthateachstepisstillinthegradientdirection,butexperimentssuggestthatbothformsworksimilarly.Althoughtheparameterupdatehasthesamedirectionasthetruegradient,withgradientnormclipping,theparameterupdatevectornormisnowbounded.Thisboundedgradientavoidsperformingadetrimentalstepwhenthegradientexplodes.Infact,evensimplytakingarandomstepwhenthegradientmagnitudeisaboveathresholdtendstoworkalmostaswell.IftheexplosionissoseverethatthegradientisnumericallyInforNan(consideredinﬁniteornot-a-number),thenarandomstepofsizevcanbetakenandwilltypicallymoveawayfromthenumericallyunstableconﬁguration.Clippingthegradientnormper-minibatchwillnotchangethedirectionofthegradientforanindividualminibatch.However,takingtheaverageofthenorm-clippedgradientfrommanyminibatchesisnotequivalenttoclippingthenormofthetruegradient(thegradientformedfromusingallexamples).Examplesthathavelargegradientnorm,aswellasexamplesthatappearinthesameminibatchassuchexamples,willhavetheircontributiontotheﬁnaldirectiondiminished.Thisstandsincontrasttotraditionalminibatchgradientdescent,wherethetruegradientdirectionisequaltotheaverageoverallminibatchgradients.Putanotherway,traditionalstochasticgradientdescentusesanunbiasedestimateofthegradient,whilegradientdescentwithnormclippingintroducesaheuristicbiasthatweknowempiricallytobeuseful.Withelement-wiseclipping,thedirectionoftheupdateisnotalignedwiththetruegradientortheminibatchgradient,butitisstilladescentdirection.Ithasalsobeenproposed(Graves2013,)tocliptheback-propagatedgradient(withrespecttohiddenunits)butnocomparisonhasbeenpublishedbetweenthesevariants;weconjecturethatallthesemethodsbehavesimilarly.10.11.2RegularizingtoEncourageInformationFlowGradientclippinghelpstodealwithexplodinggradients,butitdoesnothelpwithvanishinggradients.Toaddressvanishinggradientsandbettercapturelong-termdependencies,wediscussedtheideaofcreatingpathsinthecomputationalgraphoftheunfoldedrecurrentarchitecturealongwhichtheproductofgradientsassociatedwitharcsisnear1.OneapproachtoachievethisiswithLSTMsandotherself-loopsandgatingmechanisms,describedaboveinsection.Anotherideais10.10toregularizeorconstraintheparameterssoastoencourage“informationﬂow.”Inparticular,wewouldlikethegradientvector∇h()tLbeingback-propagatedto415 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETSmaintainitsmagnitude,evenifthelossfunctiononlypenalizestheoutputattheendofthesequence.Formally,wewant(∇h()tL)∂h()t∂h(1)t−(10.50)tobeaslargeas∇h()tL.(10.51)Withthisobjective,Pascanu2013etal.()proposethefollowingregularizer:Ω =t|∇(h()tL)∂h()t∂h(1)t−|||∇h()tL||−12.(10.52)Computingthegradientofthisregularizermayappeardiﬃcult,butPascanuetal.()proposeanapproximationinwhichweconsidertheback-propagated2013vectors∇h()tLasiftheywereconstants(forthepurposeofthisregularizer,sothatthereisnoneedtoback-propagatethroughthem).Theexperimentswiththisregularizersuggestthat,ifcombinedwiththenormclippingheuristic(whichhandlesgradientexplosion),theregularizercanconsiderablyincreasethespanofthedependenciesthatanRNNcanlearn. BecauseitkeepstheRNNdynamicsontheedgeofexplosivegradients,thegradientclippingisparticularlyimportant.Withoutgradientclipping,gradientexplosionpreventslearningfromsucceeding.AkeyweaknessofthisapproachisthatitisnotaseﬀectiveastheLSTMfortaskswheredataisabundant,suchaslanguagemodeling.10.12ExplicitMemoryIntelligencerequiresknowledgeandacquiringknowledgecanbedonevialearning,whichhasmotivatedthedevelopmentoflarge-scaledeeparchitectures.However,therearediﬀerentkindsofknowledge.Someknowledgecanbeimplicit,sub-conscious,anddiﬃculttoverbalize—suchashowtowalk,orhowadoglooksdiﬀerentfromacat.Otherknowledgecanbeexplicit,declarative,andrelativelystraightforwardtoputintowords—everydaycommonsenseknowledge,like“acatisakindofanimal,”orveryspeciﬁcfactsthatyouneedtoknowtoaccomplishyourcurrentgoals,like“themeetingwiththesalesteamisat3:00PMinroom141.”Neuralnetworksexcelatstoringimplicitknowledge.However,theystruggletomemorizefacts. Stochasticgradientdescentrequiresmanypresentationsofthe416 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS Task network,controlling the memoryMemory cellsWritingmechanismReadingmechanism Figure10.18:Aschematicofanexampleofanetworkwithanexplicitmemory,capturingsomeofthekeydesignelementsoftheneuralTuringmachine.Inthisdiagramwedistinguishthe“representation”partofthemodel(the“tasknetwork,”herearecurrentnetinthebottom)fromthe“memory”partofthemodel(thesetofcells),whichcanstorefacts.Thetasknetworklearnsto“control”thememory,decidingwheretoreadfromandwheretowritetowithinthememory(throughthereadingandwritingmechanisms,indicatedbyboldarrowspointingatthereadingandwritingaddresses). 417 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETSsameinputbeforeitcanbestoredinaneuralnetworkparameters,andeventhen,thatinputwillnotbestoredespeciallyprecisely.Graves2014betal.()hypothesizedthatthisisbecauseneuralnetworkslacktheequivalentoftheworkingmemorysystemthatallowshumanbeingstoexplicitlyholdandmanipulatepiecesofinformationthat arerelevantto achieving somegoal.Suchexplicit memorycomponentswouldallowoursystemsnotonlytorapidlyand“intentionally”storeandretrievespeciﬁcfactsbutalsotosequentiallyreasonwiththem.Theneedforneuralnetworksthatcanprocessinformationinasequenceofsteps,changingthewaytheinputisfedintothenetworkateachstep,haslongbeenrecognizedasimportantfortheabilitytoreasonratherthantomakeautomatic,intuitiveresponsestotheinput(,).Hinton1990Toresolvethisdiﬃculty,Weston2014etal.()introducedmemorynetworksthatincludeasetofmemorycellsthatcanbeaccessedviaanaddressingmecha-nism.Memorynetworksoriginallyrequiredasupervisionsignalinstructingthemhowtousetheirmemorycells.Graves2014betal.()introducedtheneuralTuringmachine,whichisabletolearntoreadfromandwritearbitrarycontenttomemorycellswithoutexplicitsupervisionaboutwhichactionstoundertake,andallowedend-to-endtrainingwithoutthissupervisionsignal,viatheuseofacontent-basedsoftattentionmechanism(see()andsec-Bahdanauetal.2015tion).Thissoftaddressingmechanismhasbecomestandardwithother12.4.5.1relatedarchitecturesemulatingalgorithmicmechanismsinawaythatstillallowsgradient-basedoptimization(,;Sukhbaataretal.2015JoulinandMikolov2015,;Kumar2015Vinyals2015aGrefenstette2015etal.,;etal.,;etal.,).EachmemorycellcanbethoughtofasanextensionofthememorycellsinLSTMsandGRUs.Thediﬀerenceisthatthenetworkoutputsaninternalstatethatchooseswhichcelltoreadfromorwriteto,justasmemoryaccessesinadigitalcomputerreadfromorwritetoaspeciﬁcaddress.Itisdiﬃculttooptimizefunctionsthatproduceexact,integeraddresses.Toalleviatethisproblem,NTMsactuallyreadtoorwritefrommanymemorycellssimultaneously.Toread,theytakeaweightedaverageofmanycells.Towrite,theymodifymultiplecellsbydiﬀerentamounts.Thecoeﬃcientsfortheseoperationsarechosentobefocusedonasmallnumberofcells,forexample,byproducingthemviaasoftmaxfunction.Usingtheseweightswithnon-zeroderivativesallowsthefunctionscontrollingaccesstothememorytobeoptimizedusinggradientdescent.Thegradientonthesecoeﬃcientsindicateswhethereachofthemshouldbeincreasedordecreased,butthegradientwilltypicallybelargeonlyforthosememoryaddressesreceivingalargecoeﬃcient.Thesememorycellsaretypicallyaugmentedtocontainavector,ratherthan418 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETSthesinglescalarstoredbyanLSTMorGRUmemorycell.Therearetworeasonstoincreasethesizeofthememorycell.Onereasonisthatwehaveincreasedthecostofaccessingamemorycell. Wepaythecomputationalcostofproducingacoeﬃcientformanycells,butweexpectthesecoeﬃcientstoclusteraroundasmallnumberofcells.Byreadingavectorvalue,ratherthanascalarvalue,wecanoﬀsetsomeofthiscost.Anotherreasontousevector-valuedmemorycellsisthattheyallowforcontent-basedaddressing,wheretheweightusedtoreadtoorwritefromacellisafunctionofthatcell.Vector-valuedcellsallowustoretrieveacompletevector-valuedmemoryifweareabletoproduceapatternthatmatchessomebutnotallofitselements.Thisisanalogoustothewaythatpeoplecanrecallthelyricsofasongbasedonafewwords.Wecanthinkofacontent-basedreadinstructionassaying,“Retrievethelyricsofthesongthathasthechorus‘Weallliveinayellowsubmarine.’”Content-basedaddressingismoreusefulwhenwemaketheobjectstoberetrievedlarge—ifeveryletterofthesongwasstoredinaseparatememorycell,wewouldnotbeabletoﬁndthemthisway.Bycomparison,location-basedaddressingisnotallowedtorefertothecontentofthememory.Wecanthinkofalocation-basedreadinstructionassaying“Retrievethelyricsofthesonginslot347.”Location-basedaddressingcanoftenbeaperfectlysensiblemechanismevenwhenthememorycellsaresmall.Ifthecontentofamemorycelliscopied(notforgotten)atmosttimesteps,thentheinformationitcontainscanbepropagatedforwardintimeandthegradientspropagatedbackwardintimewithouteithervanishingorexploding.Theexplicitmemoryapproachisillustratedinﬁgure,whereweseethat10.18a“taskneuralnetwork” iscoupledwithamemory.Althoughthattaskneuralnetworkcouldbefeedforwardorrecurrent,theoverallsystemisarecurrentnetwork.Thetasknetworkcanchoosetoreadfromorwritetospeciﬁcmemoryaddresses.ExplicitmemoryseemstoallowmodelstolearntasksthatordinaryRNNsorLSTMRNNscannotlearn.Onereasonforthisadvantagemaybebecauseinformationandgradientscanbepropagated(forwardintimeorbackwardsintime,respectively)forverylongdurations.Asanalternativetoback-propagationthroughweightedaveragesofmemorycells,wecaninterpretthememoryaddressingcoeﬃcientsasprobabilitiesandstochasticallyreadjustonecell(ZarembaandSutskever2015,).Optimizingmodelsthatmakediscretedecisionsrequiresspecializedoptimizationalgorithms,describedinsection.Sofar,trainingthesestochasticarchitecturesthatmakediscrete20.9.1decisionsremainsharderthantrainingdeterministicalgorithmsthatmakesoftdecisions.Whetheritissoft(allowingback-propagation)orstochasticandhard,the419 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETSmechanism forchoosing anaddress isin itsform identical totheattentionmechanismwhichhadbeenpreviouslyintroducedinthecontextofmachinetranslation(,)anddiscussedinsection. TheideaBahdanauetal.201512.4.5.1ofattentionmechanismsforneuralnetworkswasintroducedevenearlier,inthecontextofhandwritinggeneration(Graves2013,),withanattentionmechanismthatwasconstrainedtomoveonlyforwardintimethroughthesequence.Inthecaseofmachinetranslationandmemorynetworks,ateachstep,thefocusofattentioncanmovetoacompletelydiﬀerentplace,comparedtothepreviousstep.Recurrentneuralnetworksprovideawaytoextenddeeplearningtosequentialdata.Theyarethelastmajortoolinourdeeplearningtoolbox.Ourdiscussionnowmovestohowtochooseandusethesetoolsandhowtoapplythemtoreal-worldtasks. 420 Chapter11PracticalMethodologySuccessfullyapplyingdeeplearningtechniquesrequiresmorethanjustagoodknowledgeofwhatalgorithmsexistandtheprinciplesthatexplainhowtheywork.Agoodmachinelearningpractitioneralsoneedstoknowhowtochooseanalgorithmforaparticularapplicationandhowtomonitorandrespondtofeedbackobtainedfromexperimentsinordertoimproveamachinelearningsystem.Duringdaytodaydevelopmentofmachinelearningsystems,practitionersneedtodecidewhethertogathermoredata,increaseordecreasemodelcapacity,addorremoveregularizingfeatures,improvetheoptimizationofamodel,improveapproximateinferenceinamodel,ordebugthesoftwareimplementationofthemodel.Alloftheseoperationsareattheveryleasttime-consumingtotryout,soitisimportanttobeabletodeterminetherightcourseofactionratherthanblindlyguessing.Mostofthisbookisaboutdiﬀerentmachinelearningmodels,trainingalgo-rithms,andobjectivefunctions.Thismaygivetheimpressionthatthemostimportantingredienttobeingamachinelearningexpertisknowingawidevarietyofmachinelearningtechniquesandbeinggoodatdiﬀerentkindsofmath.Inprac-tice,onecanusuallydomuchbetterwithacorrectapplicationofacommonplacealgorithmthanbysloppilyapplyinganobscurealgorithm.Correctapplicationofanalgorithmdependsonmasteringsomefairlysimplemethodology.Manyoftherecommendationsinthischapterareadaptedfrom().Ng2015Werecommendthefollowingpracticaldesignprocess:•Determineyourgoals—whaterrormetrictouse,andyourtargetvalueforthiserrormetric.Thesegoalsanderrormetricsshouldbedrivenbytheproblemthattheapplicationisintendedtosolve.•Establishaworkingend-to-endpipelineassoonaspossible,includingthe421 CHAPTER11.PRACTICALMETHODOLOGYestimationoftheappropriateperformancemetrics.•Instrumentthesystemwelltodeterminebottlenecksinperformance.Diag-nosewhichcomponentsareperformingworsethanexpectedandwhetheritisduetooverﬁtting,underﬁtting,oradefectinthedataorsoftware.•Repeatedlymakeincrementalchangessuchasgatheringnewdata,adjustinghyperparameters,orchangingalgorithms,basedonspeciﬁcﬁndingsfromyourinstrumentation.Asarunningexample,wewilluseStreetViewaddressnumbertranscriptionsystem(,).ThepurposeofthisapplicationistoaddGoodfellowetal.2014dbuildingstoGoogleMaps.StreetViewcarsphotographthebuildingsandrecordtheGPScoordinatesassociatedwitheachphotograph.Aconvolutionalnetworkrecognizestheaddressnumberineachphotograph,allowingtheGoogleMapsdatabasetoaddthataddressinthecorrectlocation.Thestoryofhowthiscommercialapplicationwasdevelopedgivesanexampleofhowtofollowthedesignmethodologyweadvocate.Wenowdescribeeachofthestepsinthisprocess.11.1PerformanceMetricsDeterminingyourgoals,intermsofwhicherrormetrictouse,isanecessaryﬁrststepbecauseyourerrormetricwillguideallofyourfutureactions. Youshouldalsohaveanideaofwhatlevelofperformanceyoudesire.Keepinmindthatformostapplications,itisimpossibletoachieveabsolutezeroerror.TheBayeserrordeﬁnestheminimumerrorratethatyoucanhopetoachieve,evenifyouhaveinﬁnitetrainingdataandcanrecoverthetrueprobabilitydistribution.This isbecause your inputfeatures maynot contain completeinformationabouttheoutputvariable,orbecausethesystemmightbeintrinsicallystochastic.Youwillalsobelimitedbyhavingaﬁniteamountoftrainingdata.Theamountoftrainingdatacanbelimitedforavarietyofreasons.Whenyourgoalistobuildthebestpossiblereal-worldproductorservice,youcantypicallycollectmoredatabutmustdeterminethevalueofreducingerrorfurtherandweighthisagainstthecostofcollectingmoredata.Datacollectioncanrequiretime,money,orhumansuﬀering(forexample,ifyourdatacollectionprocessinvolvesperforminginvasivemedicaltests).Whenyourgoalistoanswerascientiﬁcquestionaboutwhichalgorithmperformsbetteronaﬁxedbenchmark,thebenchmark422 CHAPTER11.PRACTICALMETHODOLOGYspeciﬁcationusuallydeterminesthetrainingsetandyouarenotallowedtocollectmoredata.Howcanonedetermineareasonablelevelofperformancetoexpect?Typically,intheacademicsetting,wehavesomeestimateoftheerrorratethatisattainablebasedonpreviouslypublishedbenchmarkresults.Inthereal-wordsetting,wehavesomeideaoftheerrorratethatisnecessaryforanapplicationtobesafe,cost-eﬀective,orappealingtoconsumers.Onceyouhavedeterminedyourrealisticdesirederrorrate,yourdesigndecisionswillbeguidedbyreachingthiserrorrate.Anotherimportantconsiderationbesidesthetargetvalueoftheperformancemetricisthechoiceofwhichmetrictouse.Severaldiﬀerentperformancemetricsmaybeusedtomeasuretheeﬀectivenessofacompleteapplicationthatincludesmachinelearningcomponents.Theseperformancemetricsareusuallydiﬀerentfromthecostfunctionusedtotrainthemodel.Asdescribedinsection,itis5.1.2commontomeasuretheaccuracy,orequivalently,theerrorrate,ofasystem.However,manyapplicationsrequiremoreadvancedmetrics.Sometimesitismuchmorecostlytomakeonekindofamistakethananother.Forexample,ane-mailspamdetectionsystemcanmaketwokindsofmistakes:incorrectlyclassifyingalegitimatemessageasspam,andincorrectlyallowingaspammessagetoappearintheinbox.Itismuchworsetoblockalegitimatemessagethantoallowaquestionablemessagetopassthrough.Ratherthanmeasuringtheerrorrateofaspamclassiﬁer,wemaywishtomeasuresomeformoftotalcost,wherethecostofblockinglegitimatemessagesishigherthanthecostofallowingspammessages.Sometimeswewishtotrainabinaryclassiﬁerthatisintendedtodetectsomerareevent.Forexample,wemightdesignamedicaltestforararedisease.Supposethatonlyoneineverymillionpeoplehasthisdisease.Wecaneasilyachieve99.9999%accuracyonthedetectiontask,bysimplyhard-codingtheclassiﬁertoalwaysreportthatthediseaseisabsent.Clearly,accuracyisapoorwaytocharacterizetheperformanceofsuchasystem.Onewaytosolvethisproblemistoinsteadmeasureprecisionandrecall.Precisionisthefractionofdetectionsreportedbythemodelthatwerecorrect,whilerecallisthefractionoftrueeventsthatweredetected.Adetectorthatsaysnoonehasthediseasewouldachieveperfectprecision,butzerorecall.Adetectorthatsayseveryonehasthediseasewouldachieveperfectrecall,butprecisionequaltothepercentageofpeoplewhohavethedisease(0.0001%inourexampleofadiseasethatonlyonepeopleinamillionhave).Whenusingprecisionandrecall,itiscommontoplotaPRcurve,withprecisiononthey-axisandrecallonthex-axis.Theclassiﬁergeneratesascorethatishigheriftheeventtobedetectedoccurred. Forexample,afeedforward423 CHAPTER11.PRACTICALMETHODOLOGYnetworkdesignedtodetectadiseaseoutputsˆy=P(y=1|x),estimatingtheprobabilitythatapersonwhosemedicalresultsaredescribedbyfeaturesxhasthedisease.Wechoosetoreportadetectionwheneverthisscoreexceedssomethreshold. Byvaryingthethreshold,wecantradeprecisionforrecall. Inmanycases,wewishtosummarizetheperformanceoftheclassiﬁerwithasinglenumberratherthanacurve.Todoso,wecanconvertprecisionpandrecallrintoanF-scoregivenbyF=2prpr+.(11.1)AnotheroptionistoreportthetotalarealyingbeneaththePRcurve.Insomeapplications,itispossibleforthemachinelearningsystemtorefusetomakeadecision.Thisisusefulwhenthemachinelearningalgorithmcanestimatehowconﬁdentitshouldbeaboutadecision,especiallyifawrongdecisioncanbeharmfulandifahumanoperatorisabletooccasionallytakeover.TheStreetViewtranscriptionsystemprovidesanexampleofthissituation.Thetaskistotranscribetheaddressnumberfromaphotographinordertoassociatethelocationwherethephotowastakenwiththecorrectaddressinamap.Becausethevalueofthemapdegradesconsiderablyifthemapisinaccurate,itisimportanttoaddanaddressonlyifthetranscriptioniscorrect.Ifthemachinelearningsystemthinksthatitislesslikelythanahumanbeingtoobtainthecorrecttranscription,thenthebestcourseofactionistoallowahumantotranscribethephotoinstead.Ofcourse,themachinelearningsystemisonlyusefulifitisabletodramaticallyreducetheamountofphotosthatthehumanoperatorsmustprocess.Anaturalperformancemetrictouseinthissituationiscoverage.Coverageisthefractionofexamplesforwhichthemachinelearningsystemisabletoproducearesponse.Itispossibletotradecoverageforaccuracy.Onecanalwaysobtain100%accuracybyrefusingtoprocessanyexample,butthisreducesthecoverageto0%.FortheStreetViewtask,thegoalfortheprojectwastoreachhuman-leveltranscriptionaccuracywhilemaintaining95%coverage.Human-levelperformanceonthistaskis98%accuracy.Manyothermetricsarepossible.Wecanforexample,measureclick-throughrates,collectusersatisfactionsurveys,andsoon. Manyspecializedapplicationareashaveapplication-speciﬁccriteriaaswell.Whatisimportantistodeterminewhichperformancemetrictoimproveaheadoftime,thenconcentrateonimprovingthismetric.Withoutclearlydeﬁnedgoals,itcanbediﬃculttotellwhetherchangestoamachinelearningsystemmakeprogressornot.424 CHAPTER11.PRACTICALMETHODOLOGY11.2DefaultBaselineModelsAfterchoosingperformancemetricsandgoals, thenextstepinanypracticalapplicationistoestablishareasonableend-to-endsystemassoonaspossible.Inthissection,weproviderecommendationsforwhichalgorithmstouseastheﬁrstbaselineapproachinvarioussituations.Keepinmindthatdeeplearningresearchprogressesquickly,sobetterdefaultalgorithmsarelikelytobecomeavailablesoonafterthiswriting.Dependingonthecomplexityofyourproblem,youmayevenwanttobeginwithoutusingdeeplearning.Ifyourproblemhasachanceofbeingsolvedbyjustchoosingafewlinearweightscorrectly,youmaywanttobeginwithasimplestatisticalmodellikelogisticregression.Ifyouknowthatyourproblemfallsintoan“AI-complete”categorylikeobjectrecognition,speechrecognition,machinetranslation,andsoon,thenyouarelikelytodowellbybeginningwithanappropriatedeeplearningmodel.First,choosethegeneralcategoryofmodelbasedonthestructureofyourdata.Ifyouwanttoperformsupervisedlearningwithﬁxed-sizevectorsasinput,useafeedforwardnetworkwithfullyconnectedlayers.Iftheinputhasknowntopologicalstructure(forexample,iftheinputisanimage),useaconvolutionalnetwork.Inthesecases,youshouldbeginbyusingsomekindofpiecewiselinearunit(ReLUsortheirgeneralizationslikeLeakyReLUs,PreLusandmaxout).Ifyourinputoroutputisasequence,useagatedrecurrentnet(LSTMorGRU).AreasonablechoiceofoptimizationalgorithmisSGDwithmomentumwithadecayinglearningrate(populardecayschemesthatperformbetterorworseondiﬀerentproblemsincludedecayinglinearlyuntilreachingaﬁxedminimumlearningrate,decayingexponentially,ordecreasingthelearningratebyafactorof2-10eachtimevalidationerrorplateaus).AnotherveryreasonablealternativeisAdam.Batchnormalizationcanhaveadramaticeﬀectonoptimizationperformance,especiallyforconvolutionalnetworksandnetworkswithsigmoidalnonlinearities.Whileitisreasonabletoomitbatchnormalizationfromtheveryﬁrstbaseline,itshouldbeintroducedquicklyifoptimizationappearstobeproblematic.Unlessyourtrainingsetcontainstensofmillionsofexamplesormore,youshouldincludesomemildformsofregularizationfromthestart.Earlystoppingshouldbeusedalmostuniversally.Dropoutisanexcellentregularizerthatiseasytoimplementandcompatiblewithmanymodelsandtrainingalgorithms.Batchnormalizationalsosometimesreducesgeneralizationerrorandallowsdropouttobeomitted,duetothenoiseintheestimateofthestatisticsusedtonormalizeeachvariable.425 CHAPTER11.PRACTICALMETHODOLOGYIfyourtaskissimilartoanothertaskthathasbeenstudiedextensively,youwillprobablydowellbyﬁrstcopyingthemodelandalgorithmthatisalreadyknowntoperformbestonthepreviouslystudiedtask.Youmayevenwanttocopyatrainedmodelfromthattask.Forexample,itiscommontousethefeaturesfromaconvolutionalnetworktrainedonImageNettosolveothercomputervisiontasks(,).Girshicketal.2015Acommonquestioniswhethertobeginbyusingunsupervisedlearning,de-scribedfurtherinpart.Thisissomewhatdomainspeciﬁc.Somedomains,suchIIIasnaturallanguageprocessing,areknowntobeneﬁttremendouslyfromunsuper-visedlearningtechniquessuchaslearningunsupervisedwordembeddings.Inotherdomains,suchascomputervision,currentunsupervisedlearningtechniquesdonotbringabeneﬁt,exceptinthesemi-supervisedsetting,whenthenumberoflabeledexamplesisverysmall(,;Kingmaetal.2014Rasmus2015etal.,).Ifyourapplicationisinacontextwhereunsupervisedlearningisknowntobeimportant,thenincludeitinyourﬁrstend-to-endbaseline.Otherwise,onlyuseunsupervisedlearninginyourﬁrstattemptifthetaskyouwanttosolveisunsupervised.Youcanalwaystryaddingunsupervisedlearninglaterifyouobservethatyourinitialbaselineoverﬁts.11.3DeterminingWhethertoGatherMoreDataAftertheﬁrstend-to-endsystemisestablished,itistimetomeasuretheperfor-manceofthealgorithmanddeterminehowtoimproveit.Manymachinelearningnovicesaretemptedtomakeimprovementsbytryingoutmanydiﬀerentalgorithms.However,itisoftenmuchbettertogathermoredatathantoimprovethelearningalgorithm.Howdoesonedecidewhethertogathermoredata?First,determinewhethertheperformanceonthetrainingsetisacceptable.Ifperformanceonthetrainingsetispoor,thelearningalgorithmisnotusingthetrainingdatathatisalreadyavailable,sothereisnoreasontogathermoredata.Instead,tryincreasingthesizeofthemodelbyaddingmorelayersoraddingmorehiddenunitstoeachlayer.Also,tryimprovingthelearningalgorithm,forexamplebytuningthelearningratehyperparameter.Iflargemodelsandcarefullytunedoptimizationalgorithmsdonotworkwell,thentheproblemmightbetheofthetrainingdata.Thequalitydatamaybetoonoisyormaynotincludetherightinputsneededtopredictthedesiredoutputs.Thissuggestsstartingover,collectingcleanerdataorcollectingarichersetoffeatures.Iftheperformanceonthetrainingsetisacceptable,thenmeasuretheper-426 CHAPTER11.PRACTICALMETHODOLOGYformanceonatestset.Iftheperformanceonthetestsetisalsoacceptable,thenthereisnothinglefttobedone.Iftestsetperformanceismuchworsethantrainingsetperformance,thengatheringmoredataisoneofthemosteﬀectivesolutions. Thekeyconsiderationsarethecostandfeasibilityofgatheringmoredata,thecostandfeasibilityofreducingthetesterrorbyothermeans,andtheamountofdatathatisexpectedtobenecessarytoimprovetestsetperformancesigniﬁcantly. Atlargeinternetcompanieswithmillionsorbillionsofusers,itisfeasibletogatherlargedatasets,andtheexpenseofdoingsocanbeconsiderablylessthantheotheralternatives,sotheanswerisalmostalwaystogathermoretrainingdata.Forexample,thedevelopmentoflargelabeleddatasetswasoneofthemostimportantfactorsinsolvingobjectrecognition.Inothercontexts,suchasmedicalapplications,itmaybecostlyorinfeasibletogathermoredata.Asimplealternativetogatheringmoredataistoreducethesizeofthemodelorimproveregularization,byadjustinghyperparameterssuchasweightdecaycoeﬃcients,orbyaddingregularizationstrategiessuchasdropout.Ifyouﬁndthatthegapbetweentrainandtestperformanceisstillunacceptableevenaftertuningtheregularizationhyperparameters,thengatheringmoredataisadvisable.Whendecidingwhethertogathermoredata,itisalsonecessarytodecidehowmuchtogather.Itishelpfultoplotcurvesshowingtherelationshipbetweentrainingsetsizeandgeneralizationerror,likeinﬁgure.Byextrapolatingsuch5.4curves,onecanpredicthowmuchadditionaltrainingdatawouldbeneededtoachieveacertainlevelofperformance.Usually,addingasmallfractionofthetotalnumberofexampleswillnothaveanoticeableimpactongeneralizationerror.Itisthereforerecommendedtoexperimentwithtrainingsetsizesonalogarithmicscale,forexampledoublingthenumberofexamplesbetweenconsecutiveexperiments.Ifgatheringmuchmoredataisnotfeasible,theonlyotherwaytoimprovegeneralizationerroristoimprovethelearningalgorithmitself.Thisbecomesthedomainofresearchandnotthedomainofadviceforappliedpractitioners.11.4SelectingHyperparametersMostdeeplearningalgorithmscomewithmanyhyperparametersthatcontrolmanyaspectsofthealgorithm’sbehavior.Someofthesehyperparametersaﬀectthetimeandmemorycostofrunningthealgorithm.Someofthesehyperparametersaﬀectthequalityofthemodelrecoveredbythetrainingprocessanditsabilitytoinfercorrectresultswhendeployedonnewinputs.Therearetwobasicapproachestochoosingthesehyperparameters:choosingthemmanuallyandchoosingthemautomatically.Choosingthehyperparameters427 CHAPTER11.PRACTICALMETHODOLOGYmanuallyrequiresunderstandingwhatthehyperparametersdoandhowmachinelearningmodelsachievegoodgeneralization.Automatichyperparameterselectionalgorithmsgreatlyreducetheneedtounderstandtheseideas,buttheyareoftenmuchmorecomputationallycostly.11.4.1ManualHyperparameterTuningTosethyperparametersmanually,onemustunderstandtherelationshipbetweenhyperparameters,trainingerror,generalizationerrorandcomputationalresources(memoryandruntime).Thismeansestablishingasolidfoundationonthefun-damentalideasconcerningtheeﬀectivecapacityofalearningalgorithmfromchapter.5Thegoalofmanualhyperparametersearchisusuallytoﬁndthelowestgeneral-izationerrorsubjecttosomeruntimeandmemorybudget.Wedonotdiscusshowtodeterminetheruntimeandmemoryimpactofvarioushyperparametersherebecausethisishighlyplatform-dependent.Theprimarygoalofmanualhyperparametersearchistoadjusttheeﬀectivecapacityofthemodeltomatchthecomplexityofthetask.Eﬀectivecapacityisconstrainedbythreefactors: therepresentationalcapacityofthemodel,theabilityofthelearningalgorithmtosuccessfullyminimizethecostfunctionusedtotrainthemodel,andthedegreetowhichthecostfunctionandtrainingprocedureregularizethemodel.Amodelwithmorelayersandmorehiddenunitsperlayerhashigherrepresentationalcapacity—itiscapableofrepresentingmorecomplicatedfunctions.Itcannotnecessarilyactuallylearnallofthesefunctionsthough,ifthetrainingalgorithmcannotdiscoverthatcertainfunctionsdoagoodjobofminimizingthetrainingcost,orifregularizationtermssuchasweightdecayforbidsomeofthesefunctions.ThegeneralizationerrortypicallyfollowsaU-shapedcurvewhenplottedasafunctionofoneofthehyperparameters,asinﬁgure. Atoneextreme,the5.3hyperparametervaluecorrespondstolowcapacity,andgeneralizationerrorishighbecausetrainingerrorishigh.Thisistheunderﬁttingregime.Attheotherextreme,thehyperparametervaluecorrespondstohighcapacity,andthegeneralizationerrorishighbecausethegapbetweentrainingandtesterrorishigh.Somewhereinthemiddleliestheoptimalmodelcapacity,whichachievesthelowestpossiblegeneralizationerror,byaddingamediumgeneralizationgaptoamediumamountoftrainingerror.Forsomehyperparameters,overﬁttingoccurswhenthevalueofthehyper-parameterislarge. Thenumberofhiddenunitsinalayerisonesuchexample,428 CHAPTER11.PRACTICALMETHODOLOGYbecauseincreasingthenumberofhiddenunitsincreasesthecapacityofthemodel.Forsomehyperparameters,overﬁttingoccurswhenthevalueofthehyperparame-terissmall.Forexample,thesmallestallowableweightdecaycoeﬃcientofzerocorrespondstothegreatesteﬀectivecapacityofthelearningalgorithm.NoteveryhyperparameterwillbeabletoexploretheentireU-shapedcurve.Manyhyperparametersarediscrete,suchasthenumberofunitsinalayerorthenumberoflinearpiecesinamaxoutunit,soitisonlypossibletovisitafewpointsalongthecurve.Somehyperparametersarebinary.Usuallythesehyperparametersareswitchesthat specify whetherornotto usesomeoptionalcomponentofthelearningalgorithm,suchasapreprocessingstepthatnormalizestheinputfeaturesbysubtractingtheirmeananddividingbytheirstandarddeviation.Thesehyperparameterscanonlyexploretwopointsonthecurve.Otherhyperparametershavesomeminimumormaximumvaluethatpreventsthemfromexploringsomepartofthecurve.Forexample,theminimumweightdecaycoeﬃcientiszero.Thismeansthatifthemodelisunderﬁttingwhenweightdecayiszero,wecannotentertheoverﬁttingregionbymodifyingtheweightdecaycoeﬃcient.Inotherwords,somehyperparameterscanonlysubtractcapacity.Thelearningrateisperhapsthemostimportanthyperparameter.Ifyouhave timeto tuneonly onehyperparameter,tune thelearning rate.It con-trolstheeﬀectivecapacityofthemodelinamorecomplicatedwaythanotherhyperparameters—theeﬀectivecapacityofthemodelishighestwhenthelearningrateiscorrectfortheoptimizationproblem,notwhenthelearningrateisespeciallylargeorespeciallysmall.ThelearningratehasaU-shapedcurvefortrainingerror,illustratedinﬁgure.Whenthelearningrateistoolarge,gradientdescent11.1caninadvertentlyincreaseratherthandecreasethetrainingerror.Intheidealizedquadraticcase,thisoccursifthelearningrateisatleasttwiceaslargeasitsoptimalvalue(,).Whenthelearningrateistoosmall,trainingLeCunetal.1998aisnotonlyslower,butmaybecomepermanentlystuckwithahightrainingerror.Thiseﬀectispoorlyunderstood(itwouldnothappenforaconvexlossfunction).Tuningtheparametersotherthanthelearningraterequiresmonitoringbothtrainingandtesterrortodiagnosewhetheryourmodelisoverﬁttingorunderﬁtting,thenadjustingitscapacityappropriately.Ifyourerroronthetrainingsetishigherthanyourtargeterrorrate,youhavenochoicebuttoincreasecapacity.Ifyouarenotusingregularizationandyouareconﬁdentthatyouroptimizationalgorithmisperformingcorrectly,thenyoumustaddmorelayerstoyournetworkoraddmorehiddenunits.Unfortunately,thisincreasesthecomputationalcostsassociatedwiththemodel.Ifyourerroronthetestsetishigherthanthanyourtargeterrorrate,youcan429 CHAPTER11.PRACTICALMETHODOLOGY 10−210−1100Learningrate(logarithmicscale)012345678Trainingerror Figure11.1:Typicalrelationshipbetweenthelearningrateandthetrainingerror.Noticethesharpriseinerrorwhenthelearningisaboveanoptimalvalue.Thisisforaﬁxedtrainingtime,asasmallerlearningratemaysometimesonlyslowdowntrainingbyafactorproportionaltothelearningratereduction. Generalizationerrorcanfollowthiscurveorbecomplicatedbyregularizationeﬀectsarisingoutofhavingatoolargeortoosmalllearningrates,sincepooroptimizationcan,tosomedegree,reduceorpreventoverﬁtting,andevenpointswithequivalenttrainingerrorcanhavediﬀerentgeneralizationerror.nowtaketwokindsofactions.Thetesterroristhesumofthetrainingerrorandthegapbetweentrainingandtesterror.Theoptimaltesterrorisfoundbytradingoﬀthesequantities.Neuralnetworkstypicallyperformbestwhenthetrainingerrorisverylow(andthus,whencapacityishigh)andthetesterrorisprimarilydrivenbythegapbetweentrainandtesterror. Yourgoalistoreducethisgapwithoutincreasingtrainingerrorfasterthanthegapdecreases.Toreducethegap,changeregularizationhyperparameterstoreduceeﬀectivemodelcapacity,suchasbyaddingdropoutorweightdecay.Usuallythebestperformancecomesfromalargemodelthatisregularizedwell,forexamplebyusingdropout.Mosthyperparameterscanbesetbyreasoningaboutwhethertheyincreaseordecreasemodelcapacity.SomeexamplesareincludedinTable.11.1Whilemanuallytuninghyperparameters,donotlosesightofyourendgoal:goodperformanceonthetestset.Addingregularizationisonlyonewaytoachievethisgoal.Aslongasyouhavelowtrainingerror,youcanalwaysreducegeneral-izationerrorbycollectingmoretrainingdata.Thebruteforcewaytopracticallyguaranteesuccessistocontinuallyincreasemodelcapacityandtrainingsetsizeuntilthetaskissolved.Thisapproachdoesofcourseincreasethecomputationalcostoftrainingandinference,soitisonlyfeasiblegivenappropriateresources.In430 CHAPTER11.PRACTICALMETHODOLOGYHyperparameterIncreasescapacitywhen...ReasonCaveatsNumberofhid-denunitsincreasedIncreasingthenumberofhiddenunitsincreasestherepresentationalcapacityofthemodel.Increasingthenumberofhiddenunits increasesboththetimeandmemorycostofessentiallyeveryop-erationonthemodel.Learningratetunedop-timallyAnimproperlearningrate,whether toohigh ortoolow,resultsinamodelwithloweﬀectivecapacityduetooptimizationfailureConvolutionker-nelwidthincreasedIncreasingthekernelwidthincreasesthenumberofpa-rametersinthemodelAwiderkernelresultsinanarroweroutputdimen-sion,reducingmodelca-pacityunlessyouuseim-plicitzeropaddingtore-ducethiseﬀect.Widerkernelsrequiremoremem-oryforparameterstorageandincreaseruntime,butanarroweroutputreducesmemorycost.ImplicitzeropaddingincreasedAddingimplicitzerosbe-foreconvolutionkeepstherepresentationsizelargeIncreasedtimeandmem-orycostofmostopera-tions.Weightdecayco-eﬃcientdecreasedDecreasingtheweightde-caycoeﬃcientfreesthemodelparameterstobe-comelargerDropoutratedecreasedDroppingunitslessoftengivestheunitsmoreoppor-tunitiesto“conspire”witheachothertoﬁtthetrain-ingsetTable11.1:Theeﬀectofvarioushyperparametersonmodelcapacity.431 CHAPTER11.PRACTICALMETHODOLOGYprinciple,thisapproachcouldfailduetooptimizationdiﬃculties,butformanyproblemsoptimizationdoesnotseemtobeasigniﬁcantbarrier,providedthatthemodelischosenappropriately.11.4.2AutomaticHyperparameterOptimizationAlgorithmsTheideallearningalgorithmjusttakesadatasetandoutputsafunction,withoutrequiringhand-tuningofhyperparameters.ThepopularityofseverallearningalgorithmssuchaslogisticregressionandSVMsstemsinpartfromtheirabilitytoperformwellwithonlyoneortwotunedhyperparameters.Neuralnetworkscansometimesperformwellwithonlyasmallnumberoftunedhyperparameters,butoftenbeneﬁtsigniﬁcantlyfromtuningoffortyormorehyperparameters.Manualhyperparametertuningcanworkverywellwhentheuserhasagoodstartingpoint,suchasonedeterminedbyothershavingworkedonthesametypeofapplicationandarchitecture,orwhentheuserhasmonthsoryearsofexperienceinexploringhyperparametervaluesforneuralnetworksappliedtosimilartasks.However,formanyapplications,thesestartingpointsarenotavailable.Inthesecases,automatedalgorithmscanﬁndusefulvaluesofthehyperparameters.Ifwethinkaboutthewayinwhichtheuserofalearningalgorithmsearchesforgoodvaluesofthehyperparameters,werealizethatanoptimizationistakingplace:wearetryingtoﬁndavalueofthehyperparametersthatoptimizesanobjectivefunction,suchasvalidationerror,sometimesunderconstraints(suchasabudgetfortrainingtime,memoryorrecognitiontime).Itisthereforepossible,inprinciple,to develophyperparameter optimizationalgorithms thatwrap a learningalgorithmandchooseitshyperparameters,thushidingthehyperparametersofthelearningalgorithmfromtheuser.Unfortunately,hyperparameteroptimizationalgorithmsoftenhavetheirownhyperparameters,suchastherangeofvaluesthatshouldbeexploredforeachofthelearningalgorithm’shyperparameters.However,thesesecondaryhyperparametersareusuallyeasiertochoose,inthesensethatacceptableperformancemaybeachievedonawiderangeoftasksusingthesamesecondaryhyperparametersforalltasks.11.4.3GridSearchWhentherearethreeorfewerhyperparameters,thecommonpracticeistoperformgridsearch.Foreachhyperparameter, theuserselectsasmallﬁnitesetofvaluestoexplore.ThegridsearchalgorithmthentrainsamodelforeveryjointspeciﬁcationofhyperparametervaluesintheCartesianproductofthesetofvaluesforeachindividualhyperparameter.Theexperimentthatyieldsthebestvalidation432 CHAPTER11.PRACTICALMETHODOLOGY GridRandomFigure11.2:Comparisonofgridsearchandrandomsearch.Forillustrationpurposeswedisplaytwohyperparametersbutwearetypicallyinterestedinhavingmanymore.(Left)Toperformgridsearch,weprovideasetofvaluesforeachhyperparameter.Thesearchalgorithmrunstrainingforeveryjointhyperparametersettinginthecrossproductofthesesets.Toperformrandomsearch,weprovideaprobabilitydistributionoverjoint(Right)hyperparameterconﬁgurations.Usuallymostofthesehyperparametersareindependentfromeachother.Commonchoicesforthedistributionoverasinglehyperparameterincludeuniformandlog-uniform(tosamplefromalog-uniformdistribution,taketheexpofasamplefromauniformdistribution).Thesearchalgorithmthenrandomlysamplesjointhyperparameterconﬁgurationsandrunstrainingwitheachofthem.Bothgridsearchandrandomsearchevaluatethevalidationseterrorandreturnthebestconﬁguration.Theﬁgureillustratesthetypicalcasewhereonlysomehyperparametershaveasigniﬁcantinﬂuenceontheresult.Inthisillustration,onlythehyperparameteronthehorizontalaxishasasigniﬁcanteﬀect.Gridsearchwastesanamountofcomputationthatisexponentialinthenumberofnon-inﬂuentialhyperparameters,whilerandomsearchtestsauniquevalueofeveryinﬂuentialhyperparameteronnearlyeverytrial.Figurereproducedwithpermissionfrom().BergstraandBengio2012 433 CHAPTER11.PRACTICALMETHODOLOGYseterroristhenchosenashavingfoundthebesthyperparameters.Seetheleftofﬁgureforanillustrationofagridofhyperparametervalues.11.2Howshouldthelistsofvaluestosearchoverbechosen?Inthecaseofnumerical(ordered)hyperparameters,thesmallestandlargestelementofeachlistischosenconservatively,basedonpriorexperiencewithsimilarexperiments,tomakesurethattheoptimalvalueisverylikelytobeintheselectedrange.Typically,agridsearchinvolvespickingvaluesapproximatelyonalogarithmicscale,e.g.,alearningratetakenwithintheset{.1,.01,10−3,10−4,10−5},oranumberofhiddenunitstakenwiththeset.{}5010020050010002000,,,,,Gridsearchusuallyperformsbestwhenitisperformedrepeatedly.Forexample,supposethatweranagridsearchoverahyperparameterαusingvaluesof{−1,0,1}.Ifthebestvaluefoundis,thenweunderestimatedtherangeinwhichthebest1αliesandweshouldshiftthegridandrunanothersearchwithαin,forexample,{1,2,3}.Ifweﬁndthatthebestvalueofαis,thenwemaywishtoreﬁneour0estimatebyzoominginandrunningagridsearchover.{−}.,,.101Theobviousproblemwithgridsearchisthatitscomputationalcostgrowsexponentiallywiththenumberofhyperparameters.Iftherearemhyperparameters,eachtakingatmostnvalues,thenthenumberoftrainingandevaluationtrialsrequiredgrowsasO(nm).Thetrialsmayberuninparallelandexploitlooseparallelism(withalmostnoneedforcommunicationbetweendiﬀerentmachinescarryingoutthesearch)Unfortunately,duetotheexponentialcostofgridsearch,evenparallelizationmaynotprovideasatisfactorysizeofsearch.11.4.4RandomSearchFortunately,thereisanalternativetogridsearchthatisassimpletoprogram,moreconvenienttouse,andconvergesmuchfastertogoodvaluesofthehyperparameters:randomsearch(,).BergstraandBengio2012Arandomsearchproceedsasfollows.Firstwedeﬁneamarginaldistributionforeachhyperparameter,e.g.,aBernoulliormultinoulliforbinaryordiscretehyperparameters,orauniformdistributiononalog-scaleforpositivereal-valuedhyperparameters.Forexample,loglearningrate__∼−−u(1,5)(11.2)learningrate_= 10loglearningrate__.(11.3)whereu(a,b)indicatesasampleoftheuniformdistributionintheinterval(a,b).Similarlythelognumberofhiddenunits____maybesampledfromu(log(50),log(2000)).434 CHAPTER11.PRACTICALMETHODOLOGYUnlikeinthecaseofagridsearch,oneshouldnotdiscretizeorbinthevaluesofthehyperparameters.Thisallowsonetoexplorealargersetofvalues,anddoesnotincuradditionalcomputationalcost. Infact,asillustratedinﬁgure,a11.2randomsearchcanbeexponentiallymoreeﬃcientthanagridsearch,whenthereareseveralhyperparametersthatdonotstronglyaﬀecttheperformancemeasure.Thisisstudiedatlengthin(),whofoundthatrandomBergstraandBengio2012searchreducesthevalidationseterrormuchfasterthangridsearch,intermsofthenumberoftrialsrunbyeachmethod.Aswithgridsearch,onemayoftenwanttorunrepeatedversionsofrandomsearch,toreﬁnethesearchbasedontheresultsoftheﬁrstrun.Themainreasonwhyrandomsearchﬁndsgoodsolutionsfasterthangridsearchisthattherearenowastedexperimentalruns,unlikeinthecaseofgridsearch,whentwovaluesofahyperparameter(givenvaluesoftheotherhyperparameters)wouldgivethesameresult.Inthecaseofgridsearch,theotherhyperparameterswouldhavethesamevaluesforthesetworuns,whereaswithrandomsearch,theywouldusuallyhavediﬀerentvalues.Henceifthechangebetweenthesetwovaluesdoesnotmarginallymakemuchdiﬀerenceintermsofvalidationseterror,gridsearchwillunnecessarilyrepeattwoequivalentexperimentswhilerandomsearchwillstillgivetwoindependentexplorationsoftheotherhyperparameters.11.4.5Model-BasedHyperparameterOptimizationThesearchforgoodhyperparameterscanbecastasanoptimizationproblem.Thedecisionvariablesarethehyperparameters.Thecosttobeoptimizedisthevalidationseterrorthatresultsfromtrainingusingthesehyperparameters.Insimpliﬁedsettingswhereitisfeasibletocomputethegradientofsomediﬀerentiableerrormeasureonthevalidationsetwithrespecttothehyperparameters,wecansimplyfollowthisgradient(,;,;,Bengioetal.1999Bengio2000Maclaurinetal.2015).Unfortunately,inmostpracticalsettings,thisgradientisunavailable,eitherduetoitshighcomputationandmemorycost,orduetohyperparametershavingintrinsicallynon-diﬀerentiableinteractionswiththevalidationseterror,asinthecaseofdiscrete-valuedhyperparameters.Tocompensateforthislackofagradient,wecanbuildamodelofthevalidationseterror,thenproposenewhyperparameterguessesbyperformingoptimizationwithinthismodel.Mostmodel-basedalgorithmsforhyperparametersearchuseaBayesianregressionmodeltoestimateboththeexpectedvalueofthevalidationseterrorforeachhyperparameterandtheuncertaintyaroundthisexpectation.Opti-mizationthusinvolvesatradeoﬀbetweenexploration(proposinghyperparameters435 CHAPTER11.PRACTICALMETHODOLOGYforwhichthereishighuncertainty,whichmayleadtoalargeimprovementbutmayalsoperformpoorly)andexploitation(proposinghyperparameterswhichthemodelisconﬁdentwillperformaswellasanyhyperparametersithasseensofar—usuallyhyperparametersthatareverysimilartoonesithasseenbefore).ContemporaryapproachestohyperparameteroptimizationincludeSpearmint(,),Snoeketal.2012TPE(,)andSMAC(,).Bergstraetal.2011Hutteretal.2011Currently,wecannotunambiguouslyrecommendBayesianhyperparameteroptimizationasanestablishedtoolforachievingbetterdeeplearningresultsorforobtainingthoseresultswithlesseﬀort.Bayesianhyperparameteroptimizationsometimesperformscomparablytohumanexperts,sometimesbetter,butfailscatastrophicallyonotherproblems.Itmaybeworthtryingtoseeifitworksonaparticularproblembutisnotyetsuﬃcientlymatureorreliable.Thatbeingsaid,hyperparameteroptimizationisanimportantﬁeldofresearchthat,whileoftendrivenprimarilybytheneedsofdeeplearning,holdsthepotentialtobeneﬁtnotonlytheentireﬁeldofmachinelearningbutthedisciplineofengineeringingeneral.Onedrawbackcommontomosthyperparameteroptimizationalgorithmswithmoresophisticationthanrandomsearchisthattheyrequireforatrainingex-perimenttoruntocompletionbeforetheyareabletoextractanyinformationfromtheexperiment.Thisismuchlesseﬃcient,inthesenseofhowmuchinfor-mationcanbegleanedearlyinanexperiment,thanmanualsearchbyahumanpractitioner,sinceonecanusuallytellearlyonifsomesetofhyperparametersiscompletelypathological.()haveintroducedanearlyversionSwerskyetal.2014ofanalgorithmthatmaintainsasetofmultipleexperiments.Atvarioustimepoints,thehyperparameteroptimizationalgorithmcanchoosetobeginanewexperiment,to“freeze”arunningexperimentthatisnotpromising,orto“thaw”andresumeanexperimentthatwasearlierfrozenbutnowappearspromisinggivenmoreinformation.11.5DebuggingStrategiesWhenamachinelearningsystemperformspoorly,itisusuallydiﬃculttotellwhetherthepoorperformanceisintrinsictothealgorithmitselforwhetherthereisabugintheimplementationofthealgorithm. Machinelearningsystemsarediﬃculttodebugforavarietyofreasons.Inmostcases,wedonotknowaprioriwhattheintendedbehaviorofthealgorithmis.Infact,theentirepointofusingmachinelearningisthatitwilldiscoverusefulbehaviorthatwewerenotabletospecifyourselves.Ifwetraina436 CHAPTER11.PRACTICALMETHODOLOGYneuralnetworkonaclassiﬁcationtaskanditachieves5%testerror,wehavenewnostraightforwardwayofknowingifthisistheexpectedbehaviororsub-optimalbehavior.Afurtherdiﬃcultyisthatmostmachinelearningmodelshavemultiplepartsthatareeachadaptive.Ifonepartisbroken,theotherpartscanadaptandstillachieveroughlyacceptableperformance.Forexample,supposethatwearetraininganeuralnetwithseverallayersparametrizedbyweightsWandbiasesb.Supposefurtherthatwehavemanuallyimplementedthegradientdescentruleforeachparameterseparately,andwemadeanerrorintheupdateforthebiases:bb←−α(11.4)whereαisthelearningrate.Thiserroneousupdatedoesnotusethegradientatall.Itcausesthebiasestoconstantlybecomenegativethroughoutlearning,whichisclearlynotacorrectimplementationofanyreasonablelearningalgorithm.Thebugmaynotbeapparentjustfromexaminingtheoutputofthemodelthough.Dependingonthedistributionoftheinput,theweightsmaybeabletoadapttocompensateforthenegativebiases.Mostdebuggingstrategiesforneuralnetsaredesignedtogetaroundoneorbothofthesetwodiﬃculties.Eitherwedesignacasethatissosimplethatthecorrectbehavioractuallycanbepredicted,orwedesignatestthatexercisesonepartoftheneuralnetimplementationinisolation.Someimportantdebuggingtestsinclude:Visualizethemodelinaction:Whentrainingamodeltodetectobjectsinimages,viewsomeimageswiththedetectionsproposedbythemodeldisplayedsuperimposedontheimage.Whentrainingagenerativemodelofspeech,listentosomeofthespeechsamplesitproduces.Thismayseemobvious,butitiseasytofallintothepracticeofonlylookingatquantitativeperformancemeasurementslikeaccuracyorlog-likelihood.Directlyobservingthemachinelearningmodelperformingitstaskwillhelptodeterminewhetherthequantitativeperformancenumbersitachievesseemreasonable.Evaluationbugscanbesomeofthemostdevastatingbugsbecausetheycanmisleadyouintobelievingyoursystemisperformingwellwhenitisnot.Visualizetheworstmistakes: Mostmodelsareabletooutputsomesortofconﬁdencemeasureforthetasktheyperform.Forexample,classiﬁersbasedonasoftmaxoutputlayerassignaprobabilitytoeachclass.Theprobabilityassignedtothemostlikelyclassthusgivesanestimateoftheconﬁdencethemodelhasinitsclassiﬁcationdecision.Typically,maximumlikelihoodtrainingresultsinthesevaluesbeingoverestimatesratherthanaccurateprobabilitiesofcorrectprediction,437 CHAPTER11.PRACTICALMETHODOLOGYbuttheyaresomewhatusefulinthesensethatexamplesthatareactuallylesslikelytobecorrectlylabeledreceivesmallerprobabilitiesunderthemodel.Byviewingthetrainingsetexamplesthatarethehardesttomodelcorrectly,onecanoftendiscoverproblemswiththewaythedatahasbeenpreprocessedorlabeled.Forexample,theStreetViewtranscriptionsystemoriginallyhadaproblemwheretheaddressnumberdetectionsystemwouldcroptheimagetootightlyandomitsomeofthedigits.Thetranscriptionnetworkthenassignedverylowprobabilitytothecorrectanswerontheseimages.Sortingtheimagestoidentifythemostconﬁdentmistakesshowedthattherewasasystematicproblemwiththecropping.Modifyingthedetectionsystemtocropmuchwiderimagesresultedinmuchbetterperformanceoftheoverallsystem,eventhoughthetranscriptionnetworkneededtobeabletoprocessgreatervariationinthepositionandscaleoftheaddressnumbers.Reasoningaboutsoftwareusingtrainandtesterror:Itisoftendiﬃculttodeterminewhethertheunderlyingsoftwareiscorrectlyimplemented.Somecluescanbeobtainedfromthetrainandtesterror.Iftrainingerrorislowbuttesterrorishigh,thenitislikelythatthatthetrainingprocedureworkscorrectly,andthemodelisoverﬁttingforfundamentalalgorithmicreasons.Analternativepossibilityisthatthetesterrorismeasuredincorrectlyduetoaproblemwithsavingthemodelaftertrainingthenreloadingitfortestsetevaluation,orifthetestdatawasprepareddiﬀerentlyfromthetrainingdata.Ifbothtrainandtesterrorarehigh,thenitisdiﬃculttodeterminewhetherthereisasoftwaredefectorwhetherthemodelisunderﬁttingduetofundamentalalgorithmicreasons.Thisscenariorequiresfurthertests,describednext.Fitatinydataset:Ifyouhavehigherroronthetrainingset,determinewhetheritisduetogenuineunderﬁttingorduetoasoftwaredefect.Usuallyevensmallmodelscanbeguaranteedtobeableﬁtasuﬃcientlysmalldataset.Forexample,aclassiﬁcationdatasetwithonlyoneexamplecanbeﬁtjustbysettingthebiasesoftheoutputlayercorrectly.Usuallyifyoucannottrainaclassiﬁertocorrectlylabelasingleexample,anautoencodertosuccessfullyreproduceasingleexamplewithhighﬁdelity,oragenerativemodeltoconsistentlyemitsamplesresemblingasingleexample,thereisasoftwaredefectpreventingsuccessfuloptimizationonthetrainingset.Thistestcanbeextendedtoasmalldatasetwithfewexamples.Compareback-propagatedderivativestonumericalderivatives:Ifyouareusingasoftwareframeworkthatrequiresyoutoimplementyourowngradientcom-putations,orifyouareaddinganewoperationtoadiﬀerentiationlibraryandmustdeﬁneitsbpropmethod,thenacommonsourceoferrorisimplementingthisgradientexpressionincorrectly.Onewaytoverifythatthesederivativesarecorrect438 CHAPTER11.PRACTICALMETHODOLOGYistocomparethederivativescomputedbyyourimplementationofautomaticdiﬀerentiationtothederivativescomputedbya.Becauseﬁnitediﬀerencesf() =limx→0fxfx(+)−(),(11.5)wecanapproximatethederivativebyusingasmall,ﬁnite:f() x≈fxfx(+)−().(11.6)Wecanimprovetheaccuracyoftheapproximationbyusingthecentereddiﬀer-ence:f() x≈fx(+12fx)−(−12).(11.7)Theperturbationsizemustchosentobelargeenoughtoensurethatthepertur-bationisnotroundeddowntoomuchbyﬁnite-precisionnumericalcomputations.Usually,wewillwanttotestthegradientorJacobianofavector-valuedfunctiong:Rm→Rn.Unfortunately,ﬁnitediﬀerencingonlyallowsustotakeasinglederivativeatatime.Wecaneitherrunﬁnitediﬀerencingmntimestoevaluateallofthepartialderivativesofg,orwecanapplythetesttoanewfunctionthatusesrandomprojectionsatboththeinputandoutputofg.Forexample,wecanapplyourtestoftheimplementationofthederivativestof(x)wheref(x) =uTg(vx),whereuandvarerandomlychosenvectors.Computingf(x)correctlyrequiresbeingabletoback-propagatethroughgcorrectly,yetiseﬃcienttodowithﬁnitediﬀerencesbecausefhasonlyasingleinputandasingleoutput.Itisusuallyagoodideatorepeatthistestformorethanonevalueofuandvtoreducethechancethatthetestoverlooksmistakesthatareorthogonaltotherandomprojection.Ifonehasaccesstonumericalcomputationoncomplexnumbers,thenthereisaveryeﬃcientwaytonumericallyestimatethegradientbyusingcomplexnumbersasinputtothefunction(SquireandTrapp1998,).Themethodisbasedontheobservationthatfxifxif(+) = ()+()+(xO2)(11.8)real((+)) = ()+(fxifxO2)imag(,fxi(+)) = f()+(xO2),(11.9)wherei=√−1.Unlikeinthereal-valuedcaseabove,thereisnocancellationeﬀectduetotakingthediﬀerencebetweenthevalueoffatdiﬀerentpoints.Thisallowstheuseoftinyvaluesoflike= 10−150,whichmaketheO(2)errorinsigniﬁcantforallpracticalpurposes.439 CHAPTER11.PRACTICALMETHODOLOGYMonitorhistogramsofactivationsandgradient:Itisoftenusefultovisualizestatisticsofneuralnetworkactivationsandgradients,collectedoveralargeamountoftrainingiterations(maybeoneepoch).Thepre-activationvalueofhiddenunitscantellusiftheunitssaturate,orhowoftentheydo.Forexample,forrectiﬁers,howoftenaretheyoﬀ?Arethereunitsthatarealwaysoﬀ?Fortanhunits,theaverageoftheabsolutevalueofthepre-activationstellsushowsaturatedtheunitis.Inadeepnetworkwherethepropagatedgradientsquicklygroworquicklyvanish,optimizationmaybehampered.Finally,itisusefultocomparethemagnitudeofparametergradientstothemagnitudeoftheparametersthemselves.Assuggestedby(),wewouldlikethemagnitudeofparameterupdatesBottou2015overaminibatchtorepresentsomethinglike1%ofthemagnitudeoftheparameter,not50%or0.001%(whichwouldmaketheparametersmovetooslowly).Itmaybethatsomegroupsofparametersaremovingatagoodpacewhileothersarestalled.Whenthedataissparse(likeinnaturallanguage),someparametersmaybeveryrarelyupdated,andthisshouldbekeptinmindwhenmonitoringtheirevolution.Finally,manydeeplearningalgorithmsprovidesomesortofguaranteeabouttheresultsproducedateachstep.Forexample,inpart,wewillseesomeapprox-IIIimateinferencealgorithmsthatworkbyusingalgebraicsolutionstooptimizationproblems. Typicallythesecanbedebuggedbytestingeachoftheirguarantees.Someguaranteesthatsomeoptimizationalgorithmsoﬀerincludethattheobjectivefunctionwillneverincreaseafteronestepofthealgorithm,thatthegradientwithrespecttosomesubsetofvariableswillbezeroaftereachstepofthealgorithm,andthatthegradientwithrespecttoallvariableswillbezeroatconvergence.Usuallyduetoroundingerror,theseconditionswillnotholdexactlyinadigitalcomputer,sothedebuggingtestshouldincludesometoleranceparameter.11.6Example:Multi-DigitNumberRecognitionToprovideanend-to-enddescriptionofhowtoapplyourdesignmethodologyinpractice,wepresentabriefaccountoftheStreetViewtranscriptionsystem,fromthepointofviewofdesigningthedeeplearningcomponents.Obviously,manyothercomponentsofthecompletesystem,suchastheStreetViewcars,thedatabaseinfrastructure,andsoon,wereofparamountimportance.Fromthepointofviewofthemachinelearningtask,theprocessbeganwithdatacollection. Thecarscollectedtherawdataandhumanoperatorsprovidedlabels.Thetranscriptiontaskwasprecededbyasigniﬁcantamountofdatasetcuration,includingusingothermachinelearningtechniquestodetectthehouse440 CHAPTER11.PRACTICALMETHODOLOGYnumberspriortotranscribingthem.Thetranscriptionprojectbeganwithachoiceofperformancemetricsanddesiredvaluesforthesemetrics. Animportantgeneralprincipleistotailorthechoiceofmetrictothebusinessgoalsfortheproject.Becausemapsareonlyusefuliftheyhavehighaccuracy,itwasimportanttosetahighaccuracyrequirementforthisproject. Speciﬁcally,thegoalwastoobtainhuman-level,98%accuracy.Thislevelofaccuracymaynotalwaysbefeasibletoobtain.Inordertoreachthislevelofaccuracy,theStreetViewtranscriptionsystemsacriﬁcescoverage.Coveragethusbecamethemainperformancemetricoptimizedduringtheproject,withaccuracyheldat98%.Astheconvolutionalnetworkimproved,itbecamepossibletoreducetheconﬁdencethresholdbelowwhichthenetworkrefusestotranscribetheinput,eventuallyexceedingthegoalof95%coverage.Afterchoosingquantitativegoals,thenextstepinourrecommendedmethodol-ogyistorapidlyestablishasensiblebaselinesystem.Forvisiontasks,thismeansaconvolutionalnetworkwithrectiﬁedlinearunits.Thetranscriptionprojectbeganwithsuchamodel.Atthetime,itwasnotcommonforaconvolutionalnetworktooutputasequenceofpredictions.Inordertobeginwiththesimplestpossiblebaseline,theﬁrstimplementationoftheoutputlayerofthemodelconsistedofndiﬀerentsoftmaxunitstopredictasequenceofncharacters.Thesesoftmaxunitsweretrainedexactlythesameasifthetaskwereclassiﬁcation,witheachsoftmaxunittrainedindependently.Ourrecommendedmethodologyistoiterativelyreﬁnethebaselineandtestwhethereachchangemakesanimprovement.TheﬁrstchangetotheStreetViewtranscriptionsystemwasmotivatedbyatheoreticalunderstandingofthecoveragemetricandthestructureofthedata.Speciﬁcally,thenetworkrefusestoclassifyaninputxwhenevertheprobabilityoftheoutputsequencep(yx|)<tforsomethresholdt.Initially,thedeﬁnitionofp(yx|)wasad-hoc,basedonsimplymultiplyingallofthesoftmaxoutputstogether.Thismotivatedthedevelopmentofaspecializedoutputlayerandcostfunctionthatactuallycomputedaprincipledlog-likelihood.Thisapproachallowedtheexamplerejectionmechanismtofunctionmuchmoreeﬀectively.Atthispoint,coveragewasstillbelow90%,yettherewerenoobvioustheoreticalproblemswiththeapproach.Ourmethodologythereforesuggeststoinstrumentthetrainandtestsetperformanceinordertodeterminewhethertheproblemisunderﬁttingoroverﬁtting.Inthiscase,trainandtestseterrorwerenearlyidentical.Indeed,themainreasonthisprojectproceededsosmoothlywastheavailabilityofadatasetwithtensofmillionsoflabeledexamples.Becausetrainandtestseterrorweresosimilar,thissuggestedthattheproblemwaseitherdue441 CHAPTER11.PRACTICALMETHODOLOGYtounderﬁttingorduetoaproblemwiththetrainingdata.Oneofthedebuggingstrategieswerecommendistovisualizethemodel’sworsterrors.Inthiscase,thatmeantvisualizingtheincorrecttrainingsettranscriptionsthatthemodelgavethehighestconﬁdence.Theseprovedtomostlyconsistofexampleswheretheinputimagehadbeencroppedtootightly,withsomeofthedigitsoftheaddressbeingremovedbythecroppingoperation.Forexample,aphotoofanaddress“1849”mightbecroppedtootightly,withonlythe“849”remainingvisible.Thisproblemcouldhavebeenresolvedbyspendingweeksimprovingtheaccuracyoftheaddressnumberdetectionsystemresponsiblefordeterminingthecroppingregions.Instead,theteamtookamuchmorepracticaldecision,tosimplyexpandthewidthofthecropregiontobesystematicallywiderthantheaddressnumberdetectionsystempredicted.Thissinglechangeaddedtenpercentagepointstothetranscriptionsystem’scoverage.Finally,thelastfewpercentagepointsofperformancecamefromadjustinghyperparameters.Thismostlyconsistedofmakingthemodellargerwhilemain-tainingsomerestrictionsonitscomputationalcost.Becausetrainandtesterrorremainedroughlyequal,itwasalwaysclearthatanyperformancedeﬁcitswereduetounderﬁtting,aswellasduetoafewremainingproblemswiththedatasetitself.Overall,thetranscriptionprojectwasagreatsuccess,andallowedhundredsofmillionsofaddressestobetranscribedbothfasterandatlowercostthanwouldhavebeenpossibleviahumaneﬀort.Wehopethatthedesignprinciplesdescribedinthischapterwillleadtomanyothersimilarsuccesses. 442 Chapter12ApplicationsInthischapter,wedescribehowtousedeeplearningtosolveapplicationsincom-putervision,speechrecognition,naturallanguageprocessing,andotherapplicationareasofcommercialinterest.WebeginbydiscussingthelargescaleneuralnetworkimplementationsrequiredformostseriousAIapplications.Next,wereviewseveralspeciﬁcapplicationareasthatdeeplearninghasbeenusedtosolve. Whileonegoalofdeeplearningistodesignalgorithmsthatarecapableofsolvingabroadvarietyoftasks,sofarsomedegreeofspecializationisneeded.Forexample,visiontasksrequireprocessingalargenumberofinputfeatures(pixels)perexample.Languagetasksrequiremodelingalargenumberofpossiblevalues(wordsinthevocabulary)perinputfeature.12.1Large-ScaleDeepLearningDeeplearningisbasedonthephilosophyofconnectionism:whileanindividualbiologicalneuronoranindividualfeatureinamachinelearningmodelisnotintelligent,alargepopulationoftheseneuronsorfeaturesactingtogethercanexhibitintelligentbehavior.Ittrulyisimportanttoemphasizethefactthatthenumberofneuronsmustbelarge.Oneofthekeyfactorsresponsiblefortheimprovementinneuralnetwork’saccuracyandtheimprovementofthecomplexityoftaskstheycansolvebetweenthe1980sandtodayisthedramaticincreaseinthesizeofthenetworksweuse.Aswesawinsection,networksizeshave1.2.3grownexponentiallyforthepastthreedecades,yetartiﬁcialneuralnetworksareonlyaslargeasthenervoussystemsofinsects.Becausethesizeofneuralnetworksisofparamountimportance,deeplearning443 CHAPTER12.APPLICATIONSrequireshighperformancehardwareandsoftwareinfrastructure.12.1.1FastCPUImplementationsTraditionally,neuralnetworksweretrainedusingtheCPUofasinglemachine.Today,thisapproachisgenerallyconsideredinsuﬃcient.WenowmostlyuseGPUcomputingortheCPUsofmanymachinesnetworkedtogether.Beforemovingtotheseexpensivesetups,researchersworkedhardtodemonstratethatCPUscouldnotmanagethehighcomputationalworkloadrequiredbyneuralnetworks.AdescriptionofhowtoimplementeﬃcientnumericalCPUcodeisbeyondthescopeofthisbook,butweemphasizeherethatcarefulimplementationforspeciﬁcCPUfamiliescanyieldlargeimprovements.Forexample,in2011,thebestCPUsavailablecouldrunneuralnetworkworkloadsfasterwhenusingﬁxed-pointarithmeticratherthanﬂoating-pointarithmetic.Bycreatingacarefullytunedﬁxed-pointimplementation,Vanhoucke2011etal.()obtainedathreefoldspeedupoverastrongﬂoating-pointsystem.EachnewmodelofCPUhasdiﬀerentperformancecharacteristics,sosometimesﬂoating-pointimplementationscanbefastertoo.Theimportantprincipleisthatcarefulspecializationofnumericalcomputationroutinescanyieldalargepayoﬀ.Otherstrategies,besideschoosingwhethertouseﬁxedorﬂoatingpoint,includeoptimizingdatastructurestoavoidcachemissesandusingvectorinstructions.Manymachinelearningresearchersneglecttheseimplementationdetails,butwhentheperformanceofanimplementationrestrictsthesizeofthemodel,theaccuracyofthemodelsuﬀers.12.1.2GPUImplementationsMostmodernneuralnetworkimplementationsarebasedongraphicsprocessingunits.Graphicsprocessingunits(GPUs)arespecializedhardwarecomponentsthatwereoriginallydevelopedforgraphicsapplications.Theconsumermarketforvideogamingsystemsspurreddevelopmentofgraphicsprocessinghardware.Theperformancecharacteristicsneededforgoodvideogamingsystemsturnouttobebeneﬁcialforneuralnetworksaswell.Videogamerenderingrequiresperformingmanyoperationsinparallelquickly.Modelsof characters andenvironments arespeciﬁed intermsof listsof 3-Dcoordinatesofvertices.Graphicscardsmustperformmatrixmultiplicationanddivisiononmanyverticesinparalleltoconvertthese3-Dcoordinatesinto2-Don-screencoordinates.Thegraphicscardmustthenperformmanycomputationsateachpixelinparalleltodeterminethecolorofeachpixel. Inbothcases,the444 CHAPTER12.APPLICATIONScomputationsarefairlysimpleanddonotinvolvemuchbranchingcomparedtothecomputationalworkloadthataCPUusuallyencounters.Forexample,eachvertexinthesamerigidobjectwillbemultipliedbythesamematrix;thereisnoneedtoevaluateanifstatementper-vertextodeterminewhichmatrixtomultiplyby.Thecomputationsarealsoentirelyindependentofeachother,andthusmaybeparallelizedeasily.Thecomputationsalsoinvolveprocessingmassivebuﬀersofmemory,containingbitmapsdescribingthetexture(colorpattern)ofeachobjecttoberendered.Together,thisresultsingraphicscardshavingbeendesignedtohaveahighdegreeofparallelismandhighmemorybandwidth,atthecostofhavingalowerclockspeedandlessbranchingcapabilityrelativetotraditionalCPUs.Neuralnetworkalgorithmsrequirethesameperformancecharacteristicsasthereal-timegraphicsalgorithmsdescribedabove.Neuralnetworksusuallyinvolvelargeandnumerousbuﬀersofparameters,activationvalues,andgradientvalues,eachofwhichmustbecompletelyupdatedduringeverystepoftraining.Thesebuﬀersarelargeenoughtofalloutsidethecacheofatraditionaldesktopcomputersothememorybandwidthofthesystemoftenbecomestheratelimitingfactor.GPUsoﬀeracompellingadvantageoverCPUsduetotheirhighmemorybandwidth.Neuralnetworktrainingalgorithmstypicallydonotinvolvemuchbranchingorsophisticatedcontrol,sotheyareappropriateforGPUhardware.Sinceneuralnetworkscanbedividedintomultipleindividual“neurons”thatcanbeprocessedindependentlyfromtheotherneuronsinthesamelayer,neuralnetworkseasilybeneﬁtfromtheparallelismofGPUcomputing.GPUhardwarewasoriginallysospecializedthatitcouldonlybeusedforgraphicstasks.Overtime,GPUhardwarebecamemoreﬂexible,allowingcustomsubroutinestobeusedtotransformthecoordinatesofverticesorassigncolorstopixels.Inprinciple,therewasnorequirementthatthesepixelvaluesactuallybebasedonarenderingtask.TheseGPUscouldbeusedforscientiﬁccomputingbywritingtheoutputofacomputationtoabuﬀerofpixelvalues.Steinkrauetal.()implementedatwo-layerfullyconnectedneuralnetworkonaGPUand2005reportedathreefoldspeedupovertheirCPU-basedbaseline.Shortlythereafter,Chellapilla2006etal.()demonstratedthatthesametechniquecouldbeusedtoacceleratesupervisedconvolutionalnetworks.ThepopularityofgraphicscardsforneuralnetworktrainingexplodedaftertheadventofgeneralpurposeGPUs.TheseGP-GPUscouldexecutearbitrarycode,notjustrenderingsubroutines. NVIDIA’sCUDAprogramminglanguageprovidedawaytowritethisarbitrarycodeinaC-likelanguage.Withtheirrelativelyconvenientprogrammingmodel,massiveparallelism,andhighmemory445 CHAPTER12.APPLICATIONSbandwidth,GP-GPUsnowoﬀeranidealplatformforneuralnetworkprogramming.Thisplatformwasrapidlyadoptedbydeeplearningresearcherssoonafteritbecameavailable(,;,).Rainaetal.2009Ciresanetal.2010WritingeﬃcientcodeforGP-GPUsremainsadiﬃculttaskbestlefttospe-cialists. ThetechniquesrequiredtoobtaingoodperformanceonGPUareverydiﬀerentfromthoseusedonCPU.Forexample,goodCPU-basedcodeisusuallydesignedtoreadinformationfromthecacheasmuchaspossible.OnGPU,mostwritablememorylocationsarenotcached,soitcanactuallybefastertocomputethesamevaluetwice,ratherthancomputeitonceandreaditbackfrommemory.GPUcodeisalsoinherentlymulti-threadedandthediﬀerentthreadsmustbecoordinatedwitheachothercarefully.Forexample,memoryoperationsarefasteriftheycanbecoalesced.Coalescedreadsorwritesoccurwhenseveralthreadscaneachreadorwriteavaluethattheyneedsimultaneously,aspartofasinglememorytransaction.DiﬀerentmodelsofGPUsareabletocoalescediﬀerentkindsofreadorwritepatterns.Typically,memoryoperationsareeasiertocoalesceifamongnthreads,threadiaccessesbytei+jofmemory,andjisamultipleofsomepowerof2. TheexactspeciﬁcationsdiﬀerbetweenmodelsofGPU.AnothercommonconsiderationforGPUsismakingsurethateachthreadinagroupexecutesthesameinstructionsimultaneously.ThismeansthatbranchingcanbediﬃcultonGPU.Threadsaredividedintosmallgroupscalledwarps.Eachthreadinawarpexecutesthesameinstructionduringeachcycle,soifdiﬀerentthreadswithinthesamewarpneedtoexecutediﬀerentcodepaths,thesediﬀerentcodepathsmustbetraversedsequentiallyratherthaninparallel.DuetothediﬃcultyofwritinghighperformanceGPUcode,researchersshouldstructuretheirworkﬂowtoavoidneedingtowritenewGPUcodeinordertotestnewmodelsoralgorithms.Typically,onecandothisbybuildingasoftwarelibraryofhighperformanceoperationslikeconvolutionandmatrixmultiplication,thenspecifyingmodelsintermsofcallstothislibraryofoperations.Forexample,themachinelearninglibraryPylearn2(Goodfellow2013cetal.,)speciﬁesallofitsmachinelearningalgorithmsintermsofcallstoTheano(,;Bergstraetal.2010Bastien2012etal.,)andcuda-convnet(,),whichprovidetheseKrizhevsky2010high-performanceoperations.Thisfactoredapproachcanalsoeasesupportformultiplekindsofhardware.Forexample,thesameTheanoprogramcanrunoneitherCPUorGPU,withoutneedingtochangeanyofthecallstoTheanoitself.OtherlibrarieslikeTensorFlow(,)andTorch(,Abadietal.2015Collobertetal.2011b)providesimilarfeatures.446 CHAPTER12.APPLICATIONS12.1.3Large-ScaleDistributedImplementationsInmanycases,thecomputationalresourcesavailableonasinglemachineareinsuﬃcient.Wethereforewanttodistributetheworkloadoftrainingandinferenceacrossmanymachines.Distributinginferenceissimple,becauseeachinputexamplewewanttoprocesscanberunbyaseparatemachine.Thisisknownas.dataparallelismItisalsopossibletogetmodelparallelism,wheremultiplemachinesworktogetheronasingledatapoint,witheachmachinerunningadiﬀerentpartofthemodel.Thisisfeasibleforbothinferenceandtraining.Dataparallelismduringtrainingissomewhatharder.WecanincreasethesizeoftheminibatchusedforasingleSGDstep,butusuallywegetlessthanlinearreturnsintermsofoptimizationperformance.Itwouldbebettertoallowmultiplemachinestocomputemultiplegradientdescentstepsinparallel.Unfortunately,thestandarddeﬁnitionofgradientdescentisasacompletelysequentialalgorithm:thegradientatstepisafunctionoftheparametersproducedbystep.tt−1Thiscanbesolvedusingasynchronousstochasticgradientdescent(Ben-gio2001Recht2011etal.,;etal.,).Inthisapproach,severalprocessorcoressharethememoryrepresentingtheparameters.Eachcorereadsparameterswithoutalock,thencomputesagradient,thenincrementstheparameterswithoutalock.Thisreducestheaverageamountofimprovementthateachgradientdescentstepyields,becausesomeofthecoresoverwriteeachother’sprogress,buttheincreasedrateofproductionofstepscausesthelearningprocesstobefasteroverall.Deanetal.()pioneeredthemulti-machineimplementationofthislock-freeapproach2012togradientdescent,wheretheparametersaremanagedbyaparameterserverratherthanstoredinsharedmemory.Distributedasynchronousgradientdescentremainstheprimarystrategyfortraininglargedeepnetworksandisusedbymostmajordeeplearninggroupsinindustry(,;Chilimbietal.2014Wuetal.,2015).Academicdeeplearningresearcherstypicallycannotaﬀordthesamescaleofdistributedlearningsystemsbutsomeresearchhasfocusedonhowtobuilddistributednetworkswithrelativelylow-costhardwareavailableintheuniversitysetting(,).Coatesetal.201312.1.4ModelCompressionInmanycommercialapplications,itismuchmoreimportantthatthetimeandmemorycostofrunninginferenceinamachinelearningmodelbelowthanthatthetimeandmemorycostoftrainingbelow.Forapplicationsthatdonotrequire447 CHAPTER12.APPLICATIONSpersonalization,itispossibletotrainamodelonce,thendeployittobeusedbybillionsofusers.Inmanycases,theenduserismoreresource-constrainedthanthedeveloper.Forexample,onemighttrainaspeechrecognitionnetworkwithapowerfulcomputercluster,thendeployitonmobilephones.Akeystrategyforreducingthecostofinferenceismodelcompression(Bu-ciluˇa2006etal.,).Thebasicideaofmodelcompressionistoreplacetheoriginal,expensivemodelwithasmallermodelthatrequireslessmemoryandruntimetostoreandevaluate.Modelcompressionisapplicablewhenthesizeoftheoriginalmodelisdrivenprimarilybyaneedtopreventoverﬁtting.Inmostcases,themodelwiththelowestgeneralizationerrorisanensembleofseveralindependentlytrainedmodels.Evaluatingallnensemblemembersisexpensive.Sometimes,evenasinglemodelgeneralizesbetterifitislarge(forexample,ifitisregularizedwithdropout).Theselargemodelslearnsomefunctionf(x),butdosousingmanymoreparametersthanarenecessaryforthetask.Theirsizeisnecessaryonlyduetothelimitednumberoftrainingexamples.Assoonaswehaveﬁtthisfunctionf(x),wecangenerateatrainingsetcontaininginﬁnitelymanyexamples,simplybyapplyingftorandomlysampledpointsx.Wethentrainthenew,smaller,modeltomatchf(x)onthesepoints.Inordertomosteﬃcientlyusethecapacityofthenew,smallmodel,itisbesttosamplethenewxpointsfromadistributionresemblingtheactualtestinputsthatwillbesuppliedtothemodellater.Thiscanbedonebycorruptingtrainingexamplesorbydrawingpointsfromagenerativemodeltrainedontheoriginaltrainingset.Alternatively,onecantrainthesmallermodelonlyontheoriginaltrainingpoints,buttrainittocopyotherfeaturesofthemodel,suchasitsposteriordistributionovertheincorrectclasses(Hinton20142015etal.,,).12.1.5DynamicStructureOnestrategyforacceleratingdataprocessingsystemsingeneralistobuildsystemsthathavedynamicstructureinthegraphdescribingthecomputationneededtoprocessaninput.Dataprocessingsystemscandynamicallydeterminewhichsubsetofmanyneuralnetworksshouldberunonagiveninput.Individualneuralnetworkscanalsoexhibitdynamicstructureinternallybydeterminingwhichsubsetoffeatures(hiddenunits)tocomputegiveninformationfromtheinput.Thisformofdynamicstructureinsideneuralnetworksissometimescalledconditionalcomputation(,;,). SincemanycomponentsofBengio2013Bengioetal.2013bthearchitecturemayberelevantonlyforasmallamountofpossibleinputs,the448 CHAPTER12.APPLICATIONSsystemcanrunfasterbycomputingthesefeaturesonlywhentheyareneeded.Dynamicstructureofcomputationsisabasiccomputerscienceprincipleappliedgenerallythroughoutthesoftwareengineeringdiscipline. Thesimplestversionsofdynamicstructureappliedtoneuralnetworksarebasedondeterminingwhichsubsetofsomegroupofneuralnetworks(orothermachinelearningmodels)shouldbeappliedtoaparticularinput.Avenerablestrategyforacceleratinginferenceinaclassiﬁeristouseacascadeofclassiﬁers.Thecascadestrategymaybeappliedwhenthegoalistodetectthepresenceofarareobject(orevent).Toknowforsurethattheobjectispresent,wemustuseasophisticatedclassiﬁerwithhighcapacity,thatisexpensivetorun.However,becausetheobjectisrare,wecanusuallyusemuchlesscomputationtorejectinputsasnotcontainingtheobject.Inthesesituations,wecantrainasequenceofclassiﬁers.Theﬁrstclassiﬁersinthesequencehavelowcapacity,andaretrainedtohavehighrecall.Inotherwords,theyaretrainedtomakesurewedonotwronglyrejectaninputwhentheobjectispresent.Theﬁnalclassiﬁeristrainedtohavehighprecision.Attesttime,weruninferencebyrunningtheclassiﬁersinasequence,abandoninganyexampleassoonasanyoneelementinthecascaderejectsit.Overall,thisallowsustoverifythepresenceofobjectswithhighconﬁdence,usingahighcapacitymodel,butdoesnotforceustopaythecostoffullinferenceforeveryexample.Therearetwodiﬀerentwaysthatthecascadecanachievehighcapacity.Onewayistomakethelatermembersofthecascadeindividuallyhavehighcapacity.Inthiscase,thesystemasawholeobviouslyhashighcapacity,becausesomeofitsindividualmembersdo. Itisalsopossibletomakeacascadeinwhicheveryindividualmodelhaslowcapacitybutthesystemasawholehashighcapacityduetothecombinationofmanysmallmodels.ViolaandJones2001()usedacascadeofboosteddecisiontreestoimplementafastandrobustfacedetectorsuitableforuseinhandhelddigitalcameras.Theirclassiﬁerlocalizesafaceusingessentiallyaslidingwindowapproachinwhichmanywindowsareexaminedandrejectediftheydonotcontainfaces.Anotherversionofcascadesusestheearliermodelstoimplementasortofhardattentionmechanism:theearlymembersofthecascadelocalizeanobjectandlatermembersofthecascadeperformfurtherprocessinggiventhelocationoftheobject.Forexample,GoogletranscribesaddressnumbersfromStreetViewimageryusingatwo-stepcascadethatﬁrstlocatestheaddressnumberwithonemachinelearningmodelandthentranscribesitwithanother(Goodfellow2014detal.,).Decisiontreesthemselvesareanexampleofdynamicstructure,becauseeachnodeinthetreedetermineswhichofitssubtreesshouldbeevaluatedforeachinput.Asimplewaytoaccomplishtheunionofdeeplearninganddynamicstructure449 CHAPTER12.APPLICATIONSistotrainadecisiontreeinwhicheachnodeusesaneuralnetworktomakethesplittingdecision(,),thoughthishastypicallynotbeenGuoandGelfand1992donewiththeprimarygoalofacceleratinginferencecomputations.Inthesamespirit,onecanuseaneuralnetwork,calledthegatertoselectwhichoneoutofseveralexpertnetworkswillbeusedtocomputetheoutput,giventhecurrentinput.Theﬁrstversionofthisideaiscalledthemixtureofexperts(Nowlan1990Jacobs1991,;etal.,),inwhichthegateroutputsasetofprobabilitiesorweights(obtainedviaasoftmaxnonlinearity),oneperexpert,andtheﬁnaloutputisobtainedbytheweightedcombinationoftheoutputoftheexperts.Inthatcase, theuseofthegaterdoesnotoﬀerareductionincomputationalcost,butifasingleexpertischosenbythegaterforeachexample,weobtainthehardmixtureofexperts(,,),whichCollobertetal.20012002canconsiderablyacceleratetrainingandinferencetime.Thisstrategyworkswellwhenthenumberofgatingdecisionsissmallbecauseitisnotcombinatorial.Butwhenwewanttoselectdiﬀerentsubsetsofunitsorparameters,itisnotpossibletousea“softswitch”becauseitrequiresenumerating(andcomputingoutputsfor)allthegaterconﬁgurations.Todealwiththisproblem,severalapproacheshavebeenexploredtotraincombinatorialgaters.()experimentwithBengioetal.2013bseveralestimatorsofthegradientonthegatingprobabilities,whileBaconetal.()and()usereinforcementlearningtechniques(policy2015Bengioetal.2015agradient)tolearnaformofconditionaldropoutonblocksofhiddenunitsandgetanactualreductionincomputationalcostwithoutimpactingnegativelyonthequalityoftheapproximation.Another kindof dynamicstructure isa switch, where ahidden unitcanreceiveinputfromdiﬀerentunitsdependingonthecontext.Thisdynamicroutingapproachcanbeinterpretedasanattentionmechanism(,).Olshausenetal.1993Sofar,theuseofahardswitchhasnotproveneﬀectiveonlarge-scaleapplications.Contemporaryapproachesinsteaduseaweightedaverageovermanypossibleinputs,andthusdonotachieveallofthepossiblecomputationalbeneﬁtsofdynamicstructure.Contemporaryattentionmechanismsaredescribedinsection.12.4.5.1Onemajorobstacletousingdynamicallystructuredsystemsisthedecreaseddegreeofparallelismthatresultsfromthesystemfollowingdiﬀerentcodebranchesfordiﬀerentinputs.Thismeansthatfewoperationsinthenetworkcanbedescribedasmatrixmultiplicationorbatchconvolutiononaminibatchofexamples.Wecanwritemorespecializedsub-routinesthatconvolveeachexamplewithdiﬀerentkernelsormultiplyeachrowofadesignmatrixbyadiﬀerentsetofcolumnsofweights.Unfortunately, thesemorespecializedsubroutinesarediﬃculttoimplementeﬃciently.CPUimplementationswillbeslowduetothelackofcache450 CHAPTER12.APPLICATIONScoherenceandGPUimplementationswillbeslowduetothelackofcoalescedmemorytransactionsandtheneedtoserializewarpswhenmembersofawarptakediﬀerentbranches.Insomecases,theseissuescanbemitigatedbypartitioningtheexamplesintogroupsthatalltakethesamebranch,andprocessingthesegroupsofexamplessimultaneously. Thiscanbeanacceptablestrategyforminimizingthetimerequiredtoprocessaﬁxedamountofexamplesinanoﬄinesetting.Inareal-timesettingwhereexamplesmustbeprocessedcontinuously,partitioningtheworkloadcanresultinload-balancingissues.Forexample,ifweassignonemachinetoprocesstheﬁrststepinacascadeandanothermachinetoprocessthelaststepinacascade,thentheﬁrstwilltendtobeoverloadedandthelastwilltendtobeunderloaded.Similarissuesariseifeachmachineisassignedtoimplementdiﬀerentnodesofaneuraldecisiontree.12.1.6SpecializedHardwareImplementationsofDeepNetworksSincetheearlydaysofneuralnetworksresearch,hardwaredesignershaveworkedonspecializedhardwareimplementationsthatcouldspeeduptrainingand/orinferenceofneuralnetworkalgorithms.Seeearlyandmorerecentreviewsofspecializedhardwarefordeepnetworks(,;,LindseyandLindblad1994Beiuetal.2003MisraandSaha2010;,).Diﬀerentformsofspecializedhardware(GrafandJackel1989Meadand,;Ismail2012Kim2009Pham2012Chen2014ab,;etal.,;etal.,;etal.,,)havebeendevelopedoverthelastdecades,eitherwithASICs(application-speciﬁcinte-gratedcircuit),eitherwithdigital(basedonbinaryrepresentationsofnumbers),analog(GrafandJackel1989MeadandIsmail2012,;,)(basedonphysicalimple-mentationsofcontinuousvaluesasvoltagesorcurrents)orhybridimplementations(combiningdigitalandanalogcomponents).InrecentyearsmoreﬂexibleFPGA(ﬁeldprogrammablegatedarray)implementations(wheretheparticularsofthecircuitcanbewrittenonthechipafterithasbeenbuilt)havebeendeveloped.Thoughsoftwareimplementationsongeneral-purposeprocessingunits(CPUsandGPUs)typicallyuse32or64bitsofprecisiontorepresentﬂoatingpointnumbers,ithaslongbeenknownthatitwaspossibletouselessprecision,atleastatinferencetime(HoltandBaker1991HoliandHwang1993Presley,;,;andHaggard1994SimardandGraf1994Wawrzynek1996Savich,;,;etal.,;etal.,2007).Thishasbecomeamorepressingissueinrecentyearsasdeeplearninghasgainedinpopularityinindustrialproducts,andasthegreatimpactoffasterhardwarewasdemonstratedwithGPUs.AnotherfactorthatmotivatescurrentresearchonspecializedhardwarefordeepnetworksisthattherateofprogressofasingleCPUorGPUcorehassloweddown,andmostrecentimprovementsin451 CHAPTER12.APPLICATIONScomputingspeedhavecomefromparallelizationacrosscores(eitherinCPUsorGPUs).Thisisverydiﬀerentfromthesituationofthe1990s(thepreviousneuralnetworkera)wherethehardwareimplementationsofneuralnetworks(whichmighttaketwoyearsfrominceptiontoavailabilityofachip)couldnotkeepupwiththerapidprogressandlowpricesofgeneral-purposeCPUs.Buildingspecializedhardwareisthusawaytopushtheenvelopefurther,atatimewhennewhardwaredesignsarebeingdevelopedforlow-powerdevicessuchasphones,aimingforgeneral-publicapplicationsofdeeplearning(e.g.,withspeech,computervisionornaturallanguage).Recentworkonlow-precisionimplementationsofbackprop-basedneuralnets(Vanhoucke2011Courbariaux2015Gupta2015etal.,;etal.,;etal.,)suggeststhatbetween8and16bitsofprecisioncansuﬃceforusingortrainingdeepneuralnetworkswithback-propagation. Whatisclearisthatmoreprecisionisrequiredduringtrainingthanatinferencetime,andthatsomeformsofdynamicﬁxedpointrepresentationofnumberscanbeusedtoreducehowmanybitsarerequiredpernumber.Traditionalﬁxedpointnumbersarerestrictedtoaﬁxedrange(whichcorrespondstoagivenexponentinaﬂoatingpointrepresentation).Dynamicﬁxedpointrepresentationssharethatrangeamongasetofnumbers(suchasalltheweightsinonelayer).Usingﬁxedpointratherthanﬂoatingpointrepresentationsandusinglessbitspernumberreducesthehardwaresurfacearea,powerrequirementsandcomputingtimeneededforperformingmultiplications,andmultiplicationsarethemostdemandingoftheoperationsneededtouseortrainamoderndeepnetworkwithbackprop.12.2ComputerVisionComputervisionhastraditionallybeenoneofthemostactiveresearchareasfordeeplearningapplications,becausevisionisataskthatiseﬀortlessforhumansandmanyanimalsbutchallengingforcomputers(,).ManyofBallardetal.1983themostpopularstandardbenchmarktasksfordeeplearningalgorithmsareformsofobjectrecognitionoropticalcharacterrecognition.Computervisionisaverybroadﬁeldencompassingawidevarietyofwaysofprocessingimages,andanamazingdiversityofapplications. Applicationsofcomputervisionrangefromreproducinghumanvisualabilities,suchasrecognizingfaces,tocreatingentirelynewcategoriesofvisualabilities.Asanexampleofthelattercategory,onerecentcomputervisionapplicationistorecognizesoundwavesfromthevibrationstheyinduceinobjectsvisibleinavideo(,Davisetal.2014).Mostdeeplearningresearchoncomputervisionhasnotfocusedonsuch452 CHAPTER12.APPLICATIONSexoticapplicationsthatexpandtherealmofwhatispossiblewithimagerybutratherasmallcoreofAIgoalsaimedatreplicatinghumanabilities.Mostdeeplearningforcomputervisionisusedforobjectrecognitionordetectionofsomeform,whetherthismeansreportingwhichobjectispresentinanimage,annotatinganimagewithboundingboxesaroundeachobject,transcribingasequenceofsymbolsfromanimage,orlabelingeachpixelinanimagewiththeidentityoftheobjectitbelongsto.Becausegenerativemodelinghasbeenaguidingprincipleofdeeplearningresearch,thereisalsoalargebodyofworkonimagesynthesisusingdeepmodels.Whileimagesynthesisisusuallynotconsideredaexnihilocomputervisionendeavor,modelscapableofimagesynthesisareusuallyusefulforimagerestoration,acomputervisiontaskinvolvingrepairingdefectsinimagesorremovingobjectsfromimages.12.2.1PreprocessingManyapplicationareasrequiresophisticatedpreprocessingbecausetheoriginalinputcomesinaformthatisdiﬃcultformanydeeplearningarchitecturestorepresent.Computervisionusuallyrequiresrelativelylittleofthiskindofpre-processing.Theimagesshouldbestandardizedsothattheirpixelsalllieinthesame,reasonablerange,like[0,1]or[-1,1]. Mixingimagesthatliein[0,1]withimagesthatliein[0,255]willusuallyresultinfailure.Formattingimagestohavethesamescaleistheonlykindofpreprocessingthatisstrictlynecessary.Manycomputervisionarchitecturesrequireimagesofastandardsize,soimagesmustbecroppedorscaledtoﬁtthatsize.Eventhisrescalingisnotalwaysstrictlynecessary.Someconvolutionalmodelsacceptvariably-sizedinputsanddynamicallyadjustthesizeoftheirpoolingregionstokeeptheoutputsizeconstant(Waibeletal.,1989).Otherconvolutionalmodelshavevariable-sizedoutputthatautomaticallyscalesinsizewiththeinput,suchasmodelsthatdenoiseorlabeleachpixelinanimage(,).Hadselletal.2007Datasetaugmentationmaybeseenasawayofpreprocessingthetrainingsetonly.Datasetaugmentationisanexcellentwaytoreducethegeneralizationerrorofmostcomputervisionmodels.Arelatedideaapplicableattesttimeistoshowthemodelmanydiﬀerentversionsofthesameinput(forexample,thesameimagecroppedatslightlydiﬀerentlocations)andhavethediﬀerentinstantiationsofthemodelvotetodeterminetheoutput.Thislatterideacanbeinterpretedasanensembleapproach,andhelpstoreducegeneralizationerror.Otherkindsofpreprocessingareappliedtoboththetrainandthetestsetwiththegoalofputtingeachexampleintoamorecanonicalforminordertoreducetheamountofvariationthatthemodelneedstoaccountfor.Reducingtheamountof453 CHAPTER12.APPLICATIONSvariationinthedatacanbothreducegeneralizationerrorandreducethesizeofthemodelneededtoﬁtthetrainingset.Simplertasksmaybesolvedbysmallermodels,andsimplersolutionsaremorelikelytogeneralizewell.Preprocessingofthiskindisusuallydesignedtoremovesomekindofvariabilityintheinputdatathatiseasyforahumandesignertodescribeandthatthehumandesignerisconﬁdenthasnorelevancetothetask.Whentrainingwithlargedatasetsandlargemodels,thiskindofpreprocessingisoftenunnecessary,anditisbesttojustletthemodellearnwhichkindsofvariabilityitshouldbecomeinvariantto.Forexample,theAlexNetsystemforclassifyingImageNetonlyhasonepreprocessingstep:subtractingthemeanacrosstrainingexamplesofeachpixel(Krizhevskyetal.,).201212.2.1.1ContrastNormalizationOneofthemostobvioussourcesofvariationthatcanbesafelyremoved formanytasksistheamountofcontrastintheimage.Contrastsimplyreferstothemagnitudeofthediﬀerencebetweenthebrightandthedarkpixelsinanimage.Therearemanywaysofquantifyingthecontrastofanimage.Inthecontextofdeeplearning,contrastusuallyreferstothestandarddeviationofthepixelsinanimageorregionofanimage.SupposewehaveanimagerepresentedbyatensorX∈Rrc××3,withXi,j,1beingtheredintensityatrowiandcolumnj,Xi,j,2givingthegreenintensityandXi,j,3givingtheblueintensity.Thenthecontrastoftheentireimageisgivenby13rcri=1cj=13k=1Xi,j,k−¯X2(12.1)where¯Xisthemeanintensityoftheentireimage:¯X=13rcri=1cj=13k=1Xi,j,k.(12.2)Globalcontrastnormalization(GCN)aimstopreventimagesfromhavingvaryingamountsofcontrastbysubtractingthemeanfromeachimage, thenrescalingitsothatthe standarddeviation acrossits pixelsis equaltosomeconstants.Thisapproachiscomplicatedbythefactthatnoscalingfactorcanchangethecontrastofazero-contrastimage(onewhosepixelsallhaveequalintensity).Imageswithverylowbutnon-zerocontrastoftenhavelittleinformationcontent.Dividingbythetruestandarddeviationusuallyaccomplishesnothing454 CHAPTER12.APPLICATIONSmorethanamplifyingsensornoiseorcompressionartifactsinsuchcases.Thismotivatesintroducingasmall,positiveregularizationparameterλtobiastheestimateofthestandarddeviation.Alternately,onecanconstrainthedenominatortobeatleast.GivenaninputimageX,GCNproducesanoutputimageX,deﬁnedsuchthatXi,j,k= sXi,j,k−¯Xmax,λ+13rcri=1cj=13k=1Xi,j,k−¯X2.(12.3)Datasetsconsistingoflargeimagescroppedtointerestingobjectsareunlikelytocontainanyimageswithnearlyconstantintensity.Inthesecases,itissafetopracticallyignorethesmalldenominatorproblembysettingλ= 0andavoiddivisionby0inextremelyrarecasesbysettingtoanextremelylowvaluelike10−8. Thisistheapproachusedby()ontheCIFAR-10Goodfellowetal.2013adataset.Smallimagescroppedrandomlyaremorelikelytohavenearlyconstantintensity,makingaggressiveregularizationmoreuseful.()usedCoatesetal.2011λ= 0and= 10onsmall,randomlyselectedpatchesdrawnfromCIFAR-10.Thescaleparameterscanusuallybesetto,asdoneby(),1Coatesetal.2011orchosentomakeeachindividualpixelhavestandarddeviationacrossexamplescloseto1,asdoneby().Goodfellowetal.2013aThestandarddeviationinequationisjustarescalingofthe12.3L2normoftheimage(assumingthemeanoftheimagehasalreadybeenremoved).ItispreferabletodeﬁneGCNintermsofstandarddeviationratherthanL2normbecausethestandarddeviationincludesdivisionbythenumberofpixels,soGCNbasedonstandarddeviationallowsthesamestobeusedregardlessofimagesize.However,theobservationthattheL2normisproportionaltothestandarddeviationcanhelpbuildausefulintuition.OnecanunderstandGCNasmappingexamplestoasphericalshell.Seeﬁgureforanillustration.Thiscanbea12.1usefulpropertybecauseneuralnetworksareoftenbetteratrespondingtodirectionsinspaceratherthanexactlocations.Respondingtomultipledistancesinthesamedirectionrequireshiddenunitswithcollinearweightvectorsbutdiﬀerentbiases.Suchcoordinationcanbediﬃcultforthelearningalgorithmtodiscover.Additionally,manyshallowgraphicalmodelshaveproblemswithrepresentingmultipleseparatedmodesalongthesameline.GCNavoidstheseproblemsbyreducingeachexampletoadirectionratherthanadirectionandadistance.Counterintuitively,thereisapreprocessingoperationknownasspheringanditisnotthesameoperationasGCN.Spheringdoesnotrefertomakingthedatalieonasphericalshell,butrathertorescalingtheprincipalcomponentstohave455 CHAPTER12.APPLICATIONS −150015...x0−15.00.15.x1Rawinput −150015...x0GCN,= 10λ−2 −150015...x0GCN,= 0λ Figure12.1:GCNmapsexamplesontoasphere.(Left)Rawinputdatamayhaveanynorm.(Center)GCNwithλ= 0mapsallnon-zeroexamplesperfectlyontoasphere.Hereweuses= 1and= 10−8.BecauseweuseGCNbasedonnormalizingthestandarddeviationratherthantheL2norm,theresultingsphereisnottheunitsphere.(Right)RegularizedGCN,withλ>0,drawsexamplestowardthespherebutdoesnotcompletelydiscardthevariationintheirnorm.Weleaveandthesameasbefore.sequalvariance,sothatthemultivariatenormaldistributionusedbyPCAhassphericalcontours.Spheringismorecommonlyknownas.whiteningGlobalcontrastnormalizationwilloftenfailtohighlightimagefeatureswewouldliketostandout,suchasedgesandcorners.Ifwehaveascenewithalargedarkareaandalargebrightarea(suchasacitysquarewithhalftheimageintheshadowofabuilding)thenglobalcontrastnormalizationwillensurethereisalargediﬀerencebetweenthebrightnessofthedarkareaandthebrightnessofthelightarea.Itwillnot,however,ensurethatedgeswithinthedarkregionstandout.Thismotivateslocalcontrastnormalization.Localcontrastnormalizationensuresthatthecontrastisnormalizedacrosseachsmallwindow,ratherthanovertheimageasawhole.Seeﬁgureforacomparisonofglobalandlocalcontrast12.2normalization.Variousdeﬁnitionsoflocalcontrastnormalizationarepossible.Inallcases,onemodiﬁeseachpixelbysubtractingameanofnearbypixelsanddividingbyastandarddeviationofnearbypixels.Insomecases,thisisliterallythemeanandstandarddeviationofallpixelsinarectangularwindowcenteredonthepixeltobemodiﬁed(,).Inothercases,thisisaweightedmeanPintoetal.2008andweightedstandarddeviationusingGaussianweightscenteredonthepixeltobemodiﬁed. Inthecaseofcolorimages,somestrategiesprocessdiﬀerentcolor456 CHAPTER12.APPLICATIONS InputimageGCNLCNFigure12.2:Acomparisonofglobalandlocalcontrastnormalization.Visually,theeﬀectsofglobalcontrastnormalizationaresubtle.Itplacesallimagesonroughlythesamescale,whichreducestheburdenonthelearningalgorithmtohandlemultiplescales.Localcontrastnormalizationmodiﬁestheimagemuchmore,discardingallregionsofconstantintensity.Thisallowsthemodeltofocusonjusttheedges.Regionsofﬁnetexture,suchasthehousesinthesecondrow,maylosesomedetailduetothebandwidthofthenormalizationkernelbeingtoohigh.channelsseparatelywhileotherscombineinformationfromdiﬀerentchannelstonormalizeeachpixel(,).Sermanetetal.2012Localcontrastnormalizationcanusuallybeimplementedeﬃcientlybyusingseparableconvolution(seesection)tocomputefeaturemapsoflocalmeansand9.8localstandarddeviations,thenusingelement-wisesubtractionandelement-wisedivisionondiﬀerentfeaturemaps.Localcontrastnormalizationisadiﬀerentiableoperationandcanalsobeusedasanonlinearityappliedtothehiddenlayersofanetwork,aswellasapreprocessingoperationappliedtotheinput.Aswithglobalcontrastnormalization,wetypicallyneedtoregularizelocalcontrastnormalizationtoavoiddivisionbyzero.Infact,becauselocalcontrastnormalizationtypicallyactsonsmallerwindows,itisevenmoreimportanttoregularize.Smallerwindowsaremorelikelytocontainvaluesthatareallnearlythesameaseachother,andthusmorelikelytohavezerostandarddeviation.457 CHAPTER12.APPLICATIONS12.2.1.2DatasetAugmentationAsdescribedinsection,itiseasytoimprovethegeneralizationofaclassiﬁer7.4byincreasingthesizeofthetrainingsetbyaddingextracopiesofthetrainingexamplesthathavebeenmodiﬁedwithtransformationsthatdonotchangetheclass.Objectrecognitionisaclassiﬁcationtaskthatisespeciallyamenabletothisform ofdataset augmentationbecause theclass isinvariant toso manytransformationsandtheinputcanbeeasilytransformedwithmanygeometricoperations.Asdescribedbefore,classiﬁerscanbeneﬁtfromrandomtranslations,rotations,andinsomecases,ﬂipsoftheinputtoaugmentthedataset.Inspecializedcomputervisionapplications,moreadvancedtransformationsarecommonlyusedfordatasetaugmentation.Theseschemesincluderandomperturbationofthecolorsinanimage(,)andnonlineargeometricdistortionsofKrizhevskyetal.2012theinput(,).LeCunetal.1998b12.3SpeechRecognitionThetaskofspeechrecognitionistomapanacousticsignalcontainingaspokennaturallanguageutteranceintothecorrespondingsequenceofwordsintendedbythespeaker.LetX= (x(1),x(2),...,x()T)denotethesequenceofacousticinputvectors(traditionallyproducedbysplittingtheaudiointo20msframes).Mostspeechrecognitionsystemspreprocesstheinputusingspecializedhand-designedfeatures,butsome(,)deeplearningsystemslearnfeaturesJaitlyandHinton2011fromrawinput.Lety= (y1,y2,...,yN)denotethetargetoutputsequence(usuallyasequenceofwordsorcharacters).Theautomaticspeechrecognition(ASR)taskconsistsofcreatingafunctionf∗ASRthatcomputesthemostprobablelinguisticsequencegiventheacousticsequence:yXf∗ASR() = argmaxXyP∗(= )yX|X(12.4)whereP∗isthetrueconditionaldistributionrelatingtheinputsXtothetargetsy.Sincethe1980sanduntilabout2009–2012,state-of-theartspeechrecognitionsystemsprimarilycombinedhiddenMarkovmodels(HMMs)andGaussianmixturemodels(GMMs).GMMsmodeledtheassociationbetweenacousticfeaturesandphonemes(,),whileHMMsmodeledthesequenceofphonemes.Bahletal.1987TheGMM-HMM modelfamilytreats acousticwaveformsasbeinggeneratedbythefollowingprocess: ﬁrstanHMMgeneratesasequenceofphonemesanddiscretesub-phonemicstates(suchasthebeginning,middle,andendofeach458 CHAPTER12.APPLICATIONSphoneme),thenaGMMtransformseachdiscretesymbolintoabriefsegmentofaudiowaveform.AlthoughGMM-HMMsystemsdominatedASRuntilrecently,speechrecognitionwasactuallyoneoftheﬁrstareaswhereneuralnetworkswereapplied,andnumerousASRsystemsfromthelate1980sandearly1990susedneuralnets(BourlardandWellekens1989Waibel1989Robinsonand,;etal.,;Fallside1991Bengio19911992Konig1996,;etal.,,;etal.,).Atthetime,theperformanceofASRbasedonneuralnetsapproximatelymatchedtheperformanceofGMM-HMMsystems.Forexample,RobinsonandFallside1991()achieved26%phonemeerrorrateontheTIMIT(,)corpus(with39Garofoloetal.1993phonemestodiscriminatebetween), whichwasbetterthanorcomparabletoHMM-basedsystems.Sincethen,TIMIThasbeenabenchmarkforphonemerecognition,playingarolesimilartotheroleMNISTplaysforobjectrecognition.However,becauseofthecomplexengineeringinvolvedinsoftwaresystemsforspeechrecognitionandtheeﬀortthathadbeeninvestedinbuildingthesesystemsonthebasisofGMM-HMMs,theindustrydidnotseeacompellingargumentforswitchingtoneuralnetworks.Asaconsequence,untilthelate2000s,bothacademicandindustrialresearchinusingneuralnetsforspeechrecognitionmostlyfocusedonusingneuralnetstolearnextrafeaturesforGMM-HMMsystems.Later,withmuchlargeranddeepermodelsandmuchlargerdatasets,recognitionaccuracywasdramaticallyimprovedbyusingneuralnetworkstoreplaceGMMsforthetaskofassociatingacousticfeaturestophonemes(orsub-phonemicstates).Startingin2009,speechresearchersappliedaformofdeeplearningbasedonunsupervisedlearningtospeechrecognition.ThisapproachtodeeplearningwasbasedontrainingundirectedprobabilisticmodelscalledrestrictedBoltzmannmachines(RBMs)tomodeltheinputdata.RBMswillbedescribedinpart.IIITosolvespeechrecognitiontasks,unsupervisedpretrainingwasusedtobuilddeepfeedforwardnetworkswhoselayerswereeachinitializedbytraininganRBM.Thesenetworkstakespectralacousticrepresentationsinaﬁxed-sizeinputwindow(aroundacenterframe)andpredicttheconditionalprobabilitiesofHMMstatesforthatcenterframe.TrainingsuchdeepnetworkshelpedtosigniﬁcantlyimprovetherecognitionrateonTIMIT(,,),bringingdowntheMohamedetal.20092012aphonemeerrorratefromabout26%to20.7%.See()foranMohamedetal.2012banalysisofreasonsforthesuccessofthesemodels.Extensionstothebasicphonerecognitionpipelineincludedtheadditionofspeaker-adaptivefeatures(Mohamedetal.,)thatfurtherreducedtheerrorrate.Thiswasquicklyfollowedup2011byworktoexpandthearchitecturefromphonemerecognition(whichiswhatTIMITisfocusedon)tolarge-vocabularyspeechrecognition(,),Dahletal.2012whichinvolvesnotjustrecognizingphonemesbutalsorecognizingsequencesofwordsfromalargevocabulary.Deepnetworksforspeechrecognitioneventually459 CHAPTER12.APPLICATIONSshiftedfrombeingbasedonpretrainingandBoltzmannmachinestobeingbasedontechniquessuchasrectiﬁedlinearunitsanddropout(,;Zeileretal.2013Dahletal.,). Bythattime,severalofthemajorspeechgroupsinindustryhad2013startedexploringdeeplearningincollaborationwithacademicresearchers.Hintonetal.()describethebreakthroughsachievedbythesecollaborators,which2012aarenowdeployedinproductssuchasmobilephones.Later,asthesegroupsexploredlargerandlargerlabeleddatasetsandincorpo-ratedsomeofthemethodsforinitializing,training,andsettingupthearchitectureofdeepnets,theyrealizedthattheunsupervisedpretrainingphasewaseitherunnecessaryordidnotbringanysigniﬁcantimprovement.Thesebreakthroughsinrecognitionperformanceforworderrorrateinspeechrecognitionwereunprecedented(around30%improvement)andwerefollowingalongperiodofabouttenyearsduringwhicherrorratesdidnotimprovemuchwiththetraditionalGMM-HMMtechnology,inspiteofthecontinuouslygrowingsizeoftrainingsets(seeﬁgure2.4ofDengandYu2014()).Thiscreatedarapidshiftinthespeechrecognitioncommunitytowardsdeeplearning.Inamatterofroughlytwoyears,mostoftheindustrialproductsforspeechrecognitionincorporateddeepneuralnetworksandthissuccessspurredanewwaveofresearchintodeeplearningalgorithmsandarchitecturesforASR,whichisstillongoingtoday.Oneoftheseinnovationswastheuseofconvolutionalnetworks(,Sainathetal.2013)thatreplicateweightsacrosstimeandfrequency,improvingovertheearliertime-delayneuralnetworksthatreplicatedweightsonlyacrosstime.Thenewtwo-dimensionalconvolutionalmodelsregardtheinputspectrogramnotasonelongvectorbutasanimage,withoneaxiscorrespondingtotimeandtheothertofrequencyofspectralcomponents.Anotherimportantpush, stillongoing,hasbeentowardsend-to-enddeeplearningspeechrecognitionsystemsthatcompletelyremovetheHMM.TheﬁrstmajorbreakthroughinthisdirectioncamefromGraves2013etal.()whotrainedadeepLSTMRNN(seesection),usingMAPinferenceovertheframe-to-10.10phonemealignment,asin()andintheCTCframework(LeCunetal.1998bGravesetal.,;2006Graves2012Graves2013,).AdeepRNN(etal.,)hasstatevariablesfromseverallayersateachtimestep,givingtheunfoldedgraphtwokindsofdepth:ordinarydepthduetoastackoflayers,anddepthduetotimeunfolding. ThisworkbroughtthephonemeerrorrateonTIMITtoarecordlowof17.7%.SeePascanu2014aChung2014etal.()andetal.()forothervariantsofdeepRNNs,appliedinothersettings.Anothercontemporarysteptowardend-to-enddeeplearningASRistoletthesystemlearnhowto“align”theacoustic-levelinformationwiththephonetic-level460 CHAPTER12.APPLICATIONSinformation(,;,).Chorowskietal.2014Luetal.201512.4NaturalLanguageProcessingNaturallanguageprocessing(NLP)istheuseofhumanlanguages,suchasEnglishorFrench,byacomputer.Computerprogramstypicallyreadandemitspecializedlanguagesdesignedtoalloweﬃcientandunambiguousparsingbysimpleprograms.Morenaturallyoccurringlanguagesareoftenambiguousanddefyformaldescription. Naturallanguageprocessingincludesapplicationssuchasmachinetranslation,inwhichthelearnermustreadasentenceinonehumanlanguageandemitanequivalentsentenceinanotherhumanlanguage.ManyNLPapplicationsarebasedonlanguagemodelsthatdeﬁneaprobabilitydistributionoversequencesofwords,charactersorbytesinanaturallanguage.Aswiththeotherapplicationsdiscussedinthischapter,verygenericneuralnetworktechniquescanbesuccessfullyappliedtonaturallanguageprocessing.However,toachieveexcellentperformanceandtoscalewelltolargeapplications,somedomain-speciﬁcstrategiesbecomeimportant.Tobuildaneﬃcientmodelofnaturallanguage,wemustusuallyusetechniquesthatarespecializedforprocessingsequentialdata.Inmanycases,wechoosetoregardnaturallanguageasasequenceofwords,ratherthanasequenceofindividualcharactersorbytes.Becausethetotalnumberofpossiblewordsissolarge,word-basedlanguagemodelsmustoperateonanextremelyhigh-dimensionalandsparsediscretespace.Severalstrategieshavebeendevelopedtomakemodelsofsuchaspaceeﬃcient,bothinacomputationalandinastatisticalsense.12.4.1-gramsnAlanguagemodeldeﬁnesaprobabilitydistributionoversequencesoftokensinanaturallanguage.Dependingonhowthemodelisdesigned,atokenmaybeaword,acharacter,orevenabyte.Tokensarealwaysdiscreteentities.Theearliestsuccessfullanguagemodelswerebasedonmodelsofﬁxed-lengthsequencesoftokenscalled-grams.An-gramisasequenceoftokens.nnnModelsbasedonn-gramsdeﬁnetheconditionalprobabilityofthen-thtokengiventheprecedingn−1tokens.Themodelusesproductsoftheseconditionaldistributionstodeﬁnetheprobabilitydistributionoverlongersequences:Px(1,...,xτ) = (Px1,...,xn−1)τtn=Px(t|xtn−+1,...,xt−1).(12.5)461 CHAPTER12.APPLICATIONSThisdecompositionisjustiﬁedbythechainruleofprobability.TheprobabilitydistributionovertheinitialsequenceP(x1,...,xn−1)maybemodeledbyadiﬀerentmodelwithasmallervalueof.nTrainingn-grammodelsisstraightforwardbecausethemaximumlikelihoodestimatecanbecomputedsimplybycountinghowmanytimeseachpossiblengramoccursinthetrainingset.Modelsbasedonn-gramshavebeenthecorebuildingblockofstatisticallanguagemodelingformanydecades(JelinekandMercer1980Katz1987ChenandGoodman1999,;,;,).Forsmallvaluesofn,modelshaveparticularnames:unigramforn=1,bigramforn=2,andtrigramforn=3. ThesenamesderivefromtheLatinpreﬁxesforthecorrespondingnumbersandtheGreeksuﬃx“-gram”denotingsomethingthatiswritten.Usuallywetrainbothann-grammodelandann−1 grammodelsimultaneously.ThismakesiteasytocomputePx(t|xtn−+1,...,xt−1) =Pn(xtn−+1,...,xt)Pn−1(xtn−+1,...,xt−1)(12.6)simplybylookinguptwostoredprobabilities.ForthistoexactlyreproduceinferenceinPn,wemustomittheﬁnalcharacterfromeachsequencewhenwetrainPn−1.Asanexample,wedemonstratehowatrigrammodelcomputestheprobabilityofthesentence“THEDOGRANAWAY.”Theﬁrstwordsofthesentencecannotbehandledbythedefaultformulabasedonconditionalprobabilitybecausethereisnocontextatthebeginningofthesentence.Instead,wemustusethemarginalprob-abilityoverwordsatthestartofthesentence.WethusevaluateP3(THEDOGRAN).Finally,thelastwordmaybepredictedusingthetypicalcase,ofusingthecondi-tionaldistributionP(AWAYDOGRAN|).Puttingthistogetherwithequation,12.6weobtain:PP() = THEDOGRANAWAY3()THEDOGRANP3()DOGRANAWAY/P2()DOGRAN.(12.7)Afundamentallimitationofmaximumlikelihoodforn-grammodelsisthatPnasestimatedfromtrainingsetcountsisverylikelytobezeroinmanycases,eventhoughthetuple(xtn−+1,...,xt)mayappearinthetestset.Thiscancausetwodiﬀerentkindsofcatastrophicoutcomes.WhenPn−1iszero,theratioisundeﬁned,sothemodeldoesnotevenproduceasensibleoutput.WhenPn−1isnon-zerobutPniszero,thetestlog-likelihoodis−∞. Toavoidsuchcatastrophicoutcomes,mostn-grammodelsemploysomeformofsmoothing.Smoothingtechniques462 CHAPTER12.APPLICATIONSshiftprobabilitymassfromtheobservedtuplestounobservedonesthataresimilar.See()forareviewandempiricalcomparisons.OnebasicChenandGoodman1999techniqueconsistsofaddingnon-zeroprobabilitymasstoallofthepossiblenextsymbolvalues.ThismethodcanbejustiﬁedasBayesianinferencewithauniformorDirichletprioroverthecountparameters.Anotherverypopularideaistoformamixturemodelcontaininghigher-orderandlower-ordern-grammodels,withthehigher-ordermodelsprovidingmorecapacityandthelower-ordermodelsbeingmorelikelytoavoidcountsofzero.Back-oﬀmethodslook-upthelower-ordern-gramsifthefrequencyofthecontextxt−1,...,xtn−+1istoosmalltousethehigher-ordermodel.Moreformally,theyestimatethedistributionoverxtbyusingcontextsxtnk−+,...,xt−1,forincreasingk,untilasuﬃcientlyreliableestimateisfound.Classicaln-grammodelsareparticularlyvulnerabletothecurseofdimension-ality.Thereare||Vnpossiblen-gramsand||Visoftenverylarge.Evenwithamassivetrainingsetandmodestn,mostn-gramswillnotoccurinthetrainingset.Onewaytoviewaclassicaln-grammodelisthatitisperformingnearest-neighborlookup.Inotherwords,itcanbeviewedasalocalnon-parametricpredictor,similartok-nearestneighbors.Thestatisticalproblemsfacingtheseextremelylocalpredictorsaredescribedinsection.Theproblemforalanguagemodel5.11.2isevenmoreseverethanusual,becauseanytwodiﬀerentwordshavethesamedis-tancefromeachotherinone-hotvectorspace.Itisthusdiﬃculttoleveragemuchinformationfromany“neighbors”—onlytrainingexamplesthatrepeatliterallythesamecontextareusefulforlocalgeneralization. Toovercometheseproblems,alanguagemodelmustbeabletoshareknowledgebetweenonewordandothersemanticallysimilarwords.Toimprovethestatisticaleﬃciencyofn-grammodels,class-basedlanguagemodels(Brown1992NeyandKneser1993Niesler1998etal.,;,;etal.,)introducethenotionofwordcategoriesandthensharestatisticalstrengthbetweenwordsthatareinthesamecategory.Theideaistouseaclusteringalgorithmtopartitionthesetofwordsintoclustersorclasses,basedontheirco-occurrencefrequencieswithotherwords.ThemodelcanthenusewordclassIDsratherthanindividualwordIDstorepresentthecontextontherightsideoftheconditioningbar.Compositemodelscombiningword-basedandclass-basedmodelsviamixingorback-oﬀarealsopossible.Althoughwordclassesprovideawaytogeneralizebetweensequencesinwhichsomewordisreplacedbyanotherofthesameclass,muchinformationislostinthisrepresentation.463 CHAPTER12.APPLICATIONS12.4.2NeuralLanguageModelsNeurallanguagemodelsorNLMsare aclassoflanguagemodeldesignedtoovercomethecurseofdimensionalityproblemformodelingnaturallanguagesequencesbyusingadistributedrepresentationofwords(,).Bengioetal.2001Unlikeclass-basedn-grammodels,neurallanguagemodelsareabletorecognizethattwowordsaresimilarwithoutlosingtheabilitytoencodeeachwordasdistinctfromtheother.Neurallanguagemodelssharestatisticalstrengthbetweenoneword(anditscontext)andothersimilarwordsandcontexts.Thedistributedrepresentationthemodellearnsforeachwordenablesthissharingbyallowingthemodeltotreatwordsthathavefeaturesincommonsimilarly.Forexample,iftheworddogandthewordcatmaptorepresentationsthatsharemanyattributes,thensentencesthatcontainthewordcatcaninformthepredictionsthatwillbemadebythemodelforsentencesthatcontaintheworddog,andvice-versa.Becausetherearemanysuchattributes,therearemanywaysinwhichgeneralizationcanhappen,transferringinformationfromeachtrainingsentencetoanexponentiallylargenumberofsemanticallyrelatedsentences.Thecurseofdimensionalityrequiresthemodeltogeneralizetoanumberofsentencesthatisexponentialinthesentencelength.Themodelcountersthiscursebyrelatingeachtrainingsentencetoanexponentialnumberofsimilarsentences.Wesometimescallthesewordrepresentationswordembeddings.Inthisinterpretation,weviewtherawsymbolsaspointsinaspaceofdimensionequaltothevocabularysize.Thewordrepresentationsembedthosepointsinafeaturespaceoflowerdimension.Intheoriginalspace,everywordisrepresentedbyaone-hotvector,soeverypairofwordsisatEuclideandistance√2fromeachother.Intheembeddingspace,wordsthatfrequentlyappearinsimilarcontexts(oranypairofwordssharingsome“features”learnedbythemodel)areclosetoeachother.Thisoftenresultsinwordswithsimilarmeaningsbeingneighbors.Figurezoomsinonspeciﬁcareasofalearnedwordembeddingspacetoshow12.3howsemanticallysimilarwordsmaptorepresentationsthatareclosetoeachother.Neuralnetworksinotherdomainsalsodeﬁneembeddings.Forexample,ahiddenlayerofaconvolutionalnetworkprovidesan“imageembedding.”UsuallyNLPpractitionersaremuchmoreinterestedinthisideaofembeddingsbecausenaturallanguagedoesnotoriginallylieinareal-valuedvectorspace.Thehiddenlayerhasprovidedamorequalitativelydramaticchangeinthewaythedataisrepresented.Thebasicideaofusingdistributedrepresentationstoimprovemodelsfornaturallanguageprocessingisnotrestrictedtoneuralnetworks.Itmayalsobeusedwithgraphicalmodelsthathavedistributedrepresentationsintheformof464 CHAPTER12.APPLICATIONSmultiplelatentvariables(MnihandHinton2007,). −−−−−3432302826−14−13−12−11−10−9−8−7−6 CanadaEuropeOntarioNorthEnglish CanadianUnionAfricanAfricaBritishFranceRussianChinaGermanyFrenchAssemblyEUJapanIraqSouthEuropean350355360365370375380.......171819202122 199519961997199819992000200120022003200420052006200720082009 Figure12.3:Two-dimensionalvisualizationsofwordembeddingsobtainedfromaneuralmachinetranslationmodel(,),zoominginonspeciﬁcareaswhereBahdanauetal.2015semanticallyrelatedwordshaveembeddingvectorsthatareclosetoeachother.Countriesappearontheleftandnumbersontheright.Keepinmindthattheseembeddingsare2-Dforthepurposeofvisualization.Inrealapplications,embeddingstypicallyhavehigherdimensionalityandcansimultaneouslycapturemanykindsofsimilaritybetweenwords.12.4.3High-DimensionalOutputsInmanynaturallanguageapplications,weoftenwantourmodelstoproducewords(ratherthancharacters)asthefundamentalunitoftheoutput.Forlargevocabularies,itcanbeverycomputationallyexpensivetorepresentanoutputdistributionoverthechoiceofaword,becausethevocabularysizeislarge.Inmanyapplications,Vcontainshundredsofthousandsofwords.Thenaiveapproachtorepresentingsuchadistributionistoapplyanaﬃnetransformationfromahiddenrepresentationtotheoutputspace,thenapplythesoftmaxfunction.SupposewehaveavocabularyVwithsize||V.Theweightmatrixdescribingthelinearcomponentofthisaﬃnetransformationisverylarge,becauseitsoutputdimensionis||V.Thisimposesahighmemorycosttorepresentthematrix,andahighcomputationalcosttomultiplybyit.Becausethesoftmaxisnormalizedacrossall||Voutputs,itisnecessarytoperformthefullmatrixmultiplicationattrainingtimeaswellastesttime—wecannotcalculateonlythedotproductwiththeweightvectorforthecorrectoutput.Thehighcomputationalcostsoftheoutputlayerthusarisebothattrainingtime(tocomputethelikelihoodanditsgradient)andattesttime(tocomputeprobabilitiesforallorselectedwords).Forspecialized465 CHAPTER12.APPLICATIONSlossfunctions,thegradientcanbecomputedeﬃciently(,),butVincentetal.2015thestandardcross-entropylossappliedtoatraditionalsoftmaxoutputlayerposesmanydiﬃculties.Supposethathisthetophiddenlayerusedtopredicttheoutputprobabilitiesˆy.IfweparametrizethetransformationfromhtoˆywithlearnedweightsWandlearnedbiasesb,thentheaﬃne-softmaxoutputlayerperformsthefollowingcomputations:ai= bi+jWijhj∀∈{||}i1,...,V,(12.8)ˆyi=eai||Vi=1eai.(12.9)IfhcontainsnhelementsthentheaboveoperationisO(||Vnh).Withnhinthethousandsand||Vinthehundredsofthousands,thisoperationdominatesthecomputationofmostneurallanguagemodels.12.4.3.1UseofaShortListTheﬁrstneurallanguagemodels(,,)dealtwiththehighcostBengioetal.20012003ofusingasoftmaxoveralargenumberofoutputwordsbylimitingthevocabularysizeto10,000or20,000words.SchwenkandGauvain2002Schwenk2007()and()builtuponthisapproachbysplittingthevocabularyVintoashortlistLofmostfrequentwords(handledbytheneuralnet)andatailT=VL\\ofmorerarewords(handledbyann-grammodel). Tobeabletocombinethetwopredictions,theneuralnetalsohastopredicttheprobabilitythatawordappearingaftercontextCbelongstothetaillist.ThismaybeachievedbyaddinganextrasigmoidoutputunittoprovideanestimateofP(iC∈|T).Theextraoutputcanthenbeusedtoachieveanestimateoftheprobabilitydistributionoverallwordsinasfollows:VPyiC(= |) =1i∈LPyiC,iPiC(= |∈−L)(1(∈|T))+1i∈TPyiC,iPiC(= |∈T)(∈|T)(12.10)whereP(y=iC,i|∈L)isprovidedbytheneurallanguagemodelandP(y=i|C,i∈T) isprovidedbythen-grammodel.Withslightmodiﬁcation,thisapproachcanalsoworkusinganextraoutputvalueintheneurallanguagemodel’ssoftmaxlayer,ratherthanaseparatesigmoidunit.Anobviousdisadvantageoftheshortlistapproachisthatthepotentialgener-alizationadvantageoftheneurallanguagemodelsislimitedtothemostfrequent466 CHAPTER12.APPLICATIONSwords,where,arguably,itistheleastuseful. Thisdisadvantagehasstimulatedtheexplorationofalternativemethodstodealwithhigh-dimensionaloutputs,describedbelow.12.4.3.2HierarchicalSoftmaxAclassicalapproach(,)toreducingthecomputationalburdenGoodman2001ofhigh-dimensionaloutputlayersoverlargevocabularysetsVistodecomposeprobabilitieshierarchically.Insteadofnecessitatinganumberofcomputationsproportionalto||V(andalsoproportionaltothenumberofhiddenunits,nh),the||Vfactorcanbereducedtoaslowaslog||V.()andBengio2002MorinandBengio2005()introducedthisfactorizedapproachtothecontextofneurallanguagemodels.Onecanthinkofthishierarchyasbuildingcategoriesofwords,thencategoriesofcategoriesofwords,thencategoriesofcategoriesofcategoriesofwords,etc.Thesenestedcategoriesformatree,withwordsattheleaves.Inabalancedtree,thetreehasdepthO(log||V). Theprobabilityofachoosingawordisgivenbytheproductoftheprobabilitiesofchoosingthebranchleadingtothatwordateverynodeonapathfromtherootofthetreetotheleafcontainingtheword.Figureillustratesasimpleexample.()alsodescribe12.4MnihandHinton2009howtousemultiplepathstoidentifyasinglewordinordertobettermodelwordsthathavemultiplemeanings.Computingtheprobabilityofawordtheninvolvessummationoverallofthepathsthatleadtothatword.Topredicttheconditionalprobabilitiesrequiredateachnodeofthetree,wetypicallyusealogisticregressionmodelateachnodeofthetree,andprovidethesamecontextCasinputtoallofthesemodels.Becausethecorrectoutputisencodedinthetrainingset,wecanusesupervisedlearningtotrainthelogisticregressionmodels.Thisistypicallydoneusingastandardcross-entropyloss,correspondingtomaximizingthelog-likelihoodofthecorrectsequenceofdecisions.Becausetheoutputlog-likelihoodcanbecomputedeﬃciently(aslowaslog||Vratherthan||V),itsgradientsmayalsobecomputedeﬃciently.Thisincludesnotonlythegradientwithrespecttotheoutputparametersbutalsothegradientswithrespecttothehiddenlayeractivations.Itispossiblebutusuallynotpracticaltooptimizethetreestructuretominimizetheexpectednumberofcomputations.Toolsfrominformationtheoryspecifyhowtochoosetheoptimalbinarycodegiventherelativefrequenciesofthewords.Todoso,wecouldstructurethetreesothatthenumberofbitsassociatedwithawordisapproximatelyequaltothelogarithmofthefrequencyofthatword.However,in467 CHAPTER12.APPLICATIONS (1)(0) (0,0,0)(0,0,1)(0,1,0)(0,1,1)(1,0,0)(1,0,1)(1,1,0)(1,1,1)(1,1)(1,0)(0,1)(0,0)w0w0w1w1w2w2w3w3w4w4w5w5w6w6w7w7Figure12.4:Illustrationofasimplehierarchyofwordcategories,with8wordsw0,...,w7organizedintoathreelevelhierarchy.Theleavesofthetreerepresentactualspeciﬁcwords.Internalnodesrepresentgroupsofwords.Anynodecanbeindexedbythesequenceofbinarydecisions(0=left,1=right)toreachthenodefromtheroot.Super-class(0)containstheclasses(0,0)(0and,1),whichrespectivelycontainthesetsofwords{w0,w1}and{w2,w3},andsimilarlysuper-classcontainstheclasses(1)(1,0)(1and,1),whichrespectivelycontainthewords(w4,w5)(andw6,w7).Ifthetreeissuﬃcientlybalanced,themaximumdepth(numberofbinarydecisions)isontheorderofthelogarithmofthenumberofwords||V: thechoiceofoneoutof||VwordscanbeobtainedbydoingO(log||V)operations(oneforeachofthenodesonthepathfromtheroot).Inthisexample,computingtheprobabilityofawordycanbedonebymultiplyingthreeprobabilities,associatedwiththebinarydecisionstomoveleftorrightateachnodeonthepathfromtheroottoanodey.Letbi(y)bethei-thbinarydecisionwhentraversingthetreetowardsthevaluey.Theprobabilityofsamplinganoutputydecomposesintoaproductofconditionalprobabilities,usingthechainruleforconditionalprobabilities,witheachnodeindexedbythepreﬁxofthesebits.Forexample,node(1,0)correspondstothepreﬁx(b0(w4) = 1,b1(w4) = 0),andtheprobabilityofw4canbedecomposedasfollows:Pw(= y4) = (Pb0= 1,b1= 0,b2= 0)(12.11)= (Pb0= 1)(Pb1= 0 |b0= 1)(Pb2= 0 |b0= 1,b1= 0).(12.12)468 CHAPTER12.APPLICATIONSpractice,thecomputationalsavingsaretypicallynotworththeeﬀortbecausethecomputationoftheoutputprobabilitiesisonlyonepartofthetotalcomputationintheneurallanguagemodel.Forexample,supposetherearelfullyconnectedhiddenlayersofwidthnh.Letnbbetheweightedaverageofthenumberofbitsrequiredtoidentifyaword,withtheweightinggivenbythefrequencyofthesewords.Inthisexample,thenumberofoperationsneededtocomputethehiddenactivationsgrowsasasO(ln2h)whiletheoutputcomputationsgrowasO(nhnb).Aslongasnb≤lnh,wecanreducecomputationmorebyshrinkingnhthanbyshrinkingnb.Indeed,nbisoftensmall.Becausethesizeofthevocabularyrarelyexceedsamillionwordsandlog2(106)≈20,itispossibletoreducenbtoabout,20butnhisoftenmuchlarger,around103ormore.Ratherthancarefullyoptimizingatreewithabranchingfactorof,onecaninsteaddeﬁneatreewithdepthtwo2andabranchingfactorof||V.Suchatreecorrespondstosimplydeﬁningasetofmutuallyexclusivewordclasses.Thesimpleapproachbasedonatreeofdepthtwocapturesmostofthecomputationalbeneﬁtofthehierarchicalstrategy.Onequestionthatremainssomewhatopenishowtobestdeﬁnethesewordclasses,orhowtodeﬁnethewordhierarchyingeneral.Earlyworkusedexistinghierarchies(,)butthehierarchycanalsobelearned,ideallyMorinandBengio2005jointlywiththeneurallanguagemodel.Learningthehierarchyisdiﬃcult.Anexactoptimizationofthelog-likelihoodappearsintractablebecausethechoiceofawordhierarchyisadiscreteone,notamenabletogradient-basedoptimization.However,onecouldusediscreteoptimizationtoapproximatelyoptimizethepartitionofwordsintowordclasses.Animportantadvantageofthehierarchicalsoftmaxisthatitbringscomputa-tionalbeneﬁtsbothattrainingtimeandattesttime,ifattesttimewewanttocomputetheprobabilityofspeciﬁcwords.Ofcourse,computingtheprobabilityofall||Vwordswillremainexpensiveevenwiththehierarchicalsoftmax.Anotherimportantoperationisselectingthemostlikelywordinagivencontext.Unfortunatelythetreestructuredoesnotprovideaneﬃcientandexactsolutiontothisproblem.Adisadvantageisthatinpracticethehierarchicalsoftmaxtendstogiveworsetestresultsthansampling-basedmethodswewilldescribenext.Thismaybeduetoapoorchoiceofwordclasses.12.4.3.3ImportanceSamplingOnewaytospeedupthetrainingofneurallanguagemodelsistoavoidexplicitlycomputingthecontributionofthegradientfromallofthewordsthatdonotappear469 CHAPTER12.APPLICATIONSinthenextposition.Everyincorrectwordshouldhavelowprobabilityunderthemodel.Itcanbecomputationallycostlytoenumerateallofthesewords.Instead,itispossibletosampleonlyasubsetofthewords.Usingthenotationintroducedinequation,thegradientcanbewrittenasfollows:12.8∂PyClog(|)∂θ=∂logsoftmaxy()a∂θ(12.13)=∂∂θlogeayieai(12.14)=∂∂θ(ay−logieai)(12.15)=∂ay∂θ−iPyiC(= |)∂ai∂θ(12.16)whereaisthevectorofpre-softmaxactivations(orscores),withoneelementperword.Theﬁrsttermisthepositivephaseterm(pushingayup)whilethesecondtermisthenegativephaseterm(pushingaidownforalli,withweightP(iC|).Sincethenegativephasetermisanexpectation,wecanestimateitwithaMonteCarlosample.However,thatwouldrequiresamplingfromthemodelitself.SamplingfromthemodelrequirescomputingP(iC|)foralliinthevocabulary,whichispreciselywhatwearetryingtoavoid.Insteadofsamplingfromthemodel,onecansamplefromanotherdistribution,calledtheproposaldistribution(denotedq),anduseappropriateweightstocorrectforthebiasintroducedbysamplingfromthewrongdistribution(BengioandSénécal2003BengioandSénécal2008,;,).Thisisanapplicationofamoregeneraltechniquecalledimportancesampling,whichwillbedescribedinmoredetailinsection.Unfortunately,evenexactimportancesamplingisnoteﬃcient17.2becauseitrequirescomputingweightspi/qi,wherepi=P(iC|),whichcanonlybecomputedifallthescoresaiarecomputed.Thesolutionadoptedforthisapplicationiscalledbiasedimportancesampling,wheretheimportanceweightsarenormalizedtosumto1.Whennegativewordniissampled,theassociatedgradientisweightedbywi=pni/qniNj=1pnj/qnj.(12.17)Theseweightsareusedtogivetheappropriateimportancetothemnegativesamplesfromqusedtoformtheestimatednegativephasecontributiontothe470 CHAPTER12.APPLICATIONSgradient:||Vi=1PiC(|)∂ai∂θ≈1mmi=1wi∂ani∂θ.(12.18)Aunigramorabigramdistributionworkswellastheproposaldistributionq.Itiseasytoestimatetheparametersofsuchadistributionfromdata.Afterestimatingtheparameters,itisalsopossibletosamplefromsuchadistributionveryeﬃciently.Importancesamplingisnotonlyusefulforspeedingupmodelswithlargesoftmaxoutputs.Moregenerally,itisusefulforacceleratingtrainingwithlargesparseoutputlayers,wheretheoutputisasparsevectorratherthana-of-1nchoice.Anexampleisabagofwords.Abagofwordsisasparsevectorvwhereviindicatesthepresenceorabsenceofwordifromthevocabularyinthedocument.Alternately,vicanindicatethenumberoftimesthatwordiappears.Machinelearningmodelsthatemitsuchsparsevectorscanbeexpensivetotrainforavarietyofreasons.Earlyinlearning,themodelmaynotactuallychoosetomaketheoutputtrulysparse.Moreover,thelossfunctionweusefortrainingmightmostnaturallybedescribedintermsofcomparingeveryelementoftheoutputtoeveryelementofthetarget.Thismeansthatitisnotalwaysclearthatthereisacomputationalbeneﬁttousingsparseoutputs,becausethemodelmaychoosetomakethemajorityoftheoutputnon-zeroandallofthesenon-zerovaluesneedtobecomparedtothecorrespondingtrainingtarget,evenifthetrainingtargetiszero.Dauphin2011etal.()demonstratedthatsuchmodelscanbeacceleratedusingimportancesampling.Theeﬃcientalgorithmminimizesthelossreconstructionforthe“positivewords”(thosethatarenon-zerointhetarget)andanequalnumberof“negativewords.”Thenegativewordsarechosenrandomly,usingaheuristictosamplewordsthataremorelikelytobemistaken. Thebiasintroducedbythisheuristicoversamplingcanthenbecorrectedusingimportanceweights.Inallofthesecases,thecomputationalcomplexityofgradientestimationfortheoutputlayerisreducedtobeproportionaltothenumberofnegativesamplesratherthanproportionaltothesizeoftheoutputvector.12.4.3.4Noise-ContrastiveEstimationandRankingLossOtherapproachesbasedonsamplinghavebeenproposedtoreducethecomputa-tionalcostoftrainingneurallanguagemodelswithlargevocabularies.AnearlyexampleistherankinglossproposedbyCollobertandWeston2008a(),whichviewstheoutputoftheneurallanguagemodelforeachwordasascoreandtriestomakethescoreofthecorrectwordayberankedhighincomparisontotheother471 CHAPTER12.APPLICATIONSscoresai.TherankinglossproposedthenisL=imax(01,−ay+ai).(12.19)Thegradientiszeroforthei-thtermifthescoreoftheobservedword,ay,isgreaterthanthescoreofthenegativewordaibyamarginof1.Oneissuewiththiscriterionisthatitdoesnotprovideestimatedconditionalprobabilities,whichareusefulinsomeapplications,includingspeechrecognitionandtextgeneration(includingconditionaltextgenerationtaskssuchastranslation).Amorerecentlyusedtrainingobjectiveforneurallanguagemodelisnoise-contrastiveestimation,whichisintroducedinsection.Thisapproachhas18.6beensuccessfullyappliedtoneurallanguagemodels(MnihandTeh2012Mnih,;andKavukcuoglu2013,).12.4.4CombiningNeuralLanguageModelswith-gramsnAmajoradvantageofn-grammodelsoverneuralnetworksisthatn-grammodelsachievehighmodelcapacity(bystoringthefrequenciesofverymanytuples)whilerequiringverylittlecomputationtoprocessanexample(bylookinguponlyafewtuplesthatmatchthecurrentcontext).Ifweusehashtablesortreestoaccessthecounts,thecomputationusedforn-gramsisalmostindependentofcapacity.Incomparison,doublinganeuralnetwork’snumberofparameterstypicallyalsoroughlydoublesitscomputationtime.Exceptionsincludemodelsthatavoidusingallparametersoneachpass.Embeddinglayersindexonlyasingleembeddingineachpass,sowecanincreasethevocabularysizewithoutincreasingthecomputationtimeperexample.Someothermodels,suchastiledconvolutionalnetworks,canaddparameterswhilereducingthedegreeofparametersharinginordertomaintainthesameamountofcomputation.However,typicalneuralnetworklayersbasedonmatrixmultiplicationuseanamountofcomputationproportionaltothenumberofparameters.Oneeasywaytoaddcapacityisthustocombinebothapproachesinanensembleconsistingofaneurallanguagemodelandann-gramlanguagemodel(Bengioetal.,,).Aswithanyensemble,thistechniquecanreducetesterrorif20012003theensemblemembersmakeindependentmistakes.Theﬁeldofensemblelearningprovidesmanywaysofcombiningtheensemblemembers’predictions,includinguniformweightingandweightschosenonavalidationset.Mikolov2011aetal.()extendedtheensembletoincludenotjusttwomodelsbutalargearrayofmodels.Itisalsopossibletopairaneuralnetworkwithamaximumentropymodelandtrainbothjointly(Mikolov2011betal.,).Thisapproachcanbeviewedastraining472 CHAPTER12.APPLICATIONSaneuralnetworkwithanextrasetofinputsthatareconnecteddirectlytotheoutput,andnotconnectedtoanyotherpartofthemodel.Theextrainputsareindicatorsforthepresenceofparticularn-gramsintheinputcontext,sothesevariablesareveryhigh-dimensionalandverysparse.Theincreaseinmodelcapacityishuge—thenewportionofthearchitecturecontainsupto||sVnparameters—buttheamountofaddedcomputationneededtoprocessaninputisminimalbecausetheextrainputsareverysparse.12.4.5NeuralMachineTranslationMachinetranslationisthetaskofreadingasentenceinonenaturallanguageandemittingasentencewiththeequivalentmeaninginanotherlanguage. Machinetranslationsystemsofteninvolvemanycomponents.Atahighlevel,thereisoftenonecomponentthatproposesmanycandidatetranslations.Manyofthesetranslationswillnotbegrammaticalduetodiﬀerencesbetweenthelanguages.Forexample,manylanguagesputadjectivesafternouns,sowhentranslatedtoEnglishdirectlytheyyieldphrasessuchas“applered.”Theproposalmechanismsuggestsmanyvariantsofthesuggestedtranslation,ideallyincluding“redapple.”Asecondcomponentofthetranslationsystem,alanguagemodel,evaluatestheproposedtranslations,andcanscore“redapple”asbetterthan“applered.”Theearliestuseofneuralnetworksformachinetranslationwastoupgradethelanguagemodelofatranslationsystembyusinganeurallanguagemodel(Schwenketal.,;2006Schwenk2010,).Previously,mostmachinetranslationsystemshadusedann-grammodelforthiscomponent.Then-grambasedmodelsusedformachinetranslationincludenotjusttraditionalback-oﬀn-grammodels(JelinekandMercer1980Katz1987ChenandGoodman1999,;,;,)butalsomaximumentropylanguagemodels(,),inwhichanaﬃne-softmaxlayerBergeretal.1996predictsthenextwordgiventhepresenceoffrequent-gramsinthecontext.nTraditionallanguagemodelssimplyreporttheprobabilityofanaturallanguagesentence.Becausemachinetranslationinvolvesproducinganoutputsentencegivenaninputsentence,itmakessensetoextendthenaturallanguagemodeltobeconditional.Asdescribedinsection,itisstraightforwardtoextendamodel6.2.1.1thatdeﬁnesamarginaldistributionoversomevariabletodeﬁneaconditionaldistributionoverthatvariablegivenacontextC,whereCmightbeasinglevariableoralistofvariables.()beatthestate-of-the-artinsomestatisticalDevlinetal.2014machinetranslationbenchmarksbyusinganMLPtoscoreaphraset1,t2,...,tkinthetargetlanguagegivenaphrases1,s2,...,sninthesourcelanguage.TheMLPestimatesP(t1,t2,...,tk|s1,s2,...,sn).TheestimateformedbythisMLPreplacestheestimateprovidedbyconditional-grammodels.n473 CHAPTER12.APPLICATIONS DecoderOutput object (English sentence)Intermediate, semantic representationSource object (French sentence or image)EncoderFigure12.5:Theencoder-decoderarchitecturetomapbackandforthbetweenasurfacerepresentation(suchasasequenceofwordsoranimage)andasemanticrepresentation.Byusingtheoutputofanencoderofdatafromonemodality(suchastheencodermappingfromFrenchsentencestohiddenrepresentationscapturingthemeaningofsentences)astheinputtoadecoderforanothermodality(suchasthedecodermappingfromhiddenrepresentationscapturingthemeaningofsentencestoEnglish),wecantrainsystemsthattranslatefromonemodalitytoanother.Thisideahasbeenappliedsuccessfullynotjusttomachinetranslationbutalsotocaptiongenerationfromimages.AdrawbackoftheMLP-basedapproachisthatitrequiresthesequencestobepreprocessedtobeofﬁxedlength.Tomakethetranslationmoreﬂexible,wewouldliketouseamodelthatcanaccommodatevariablelengthinputsandvariablelengthoutputs.AnRNNprovidesthisability.Sectiondescribesseveralways10.2.4ofconstructinganRNNthatrepresentsaconditionaldistributionoverasequencegivensomeinput,andsectiondescribeshowtoaccomplishthisconditioning10.4whentheinputisasequence.Inallcases,onemodelﬁrstreadstheinputsequenceandemitsadatastructurethatsummarizestheinputsequence.Wecallthissummarythe“context”C.ThecontextCmaybealistofvectors,oritmaybeavectorortensor.ThemodelthatreadstheinputtoproduceCmaybeanRNN(,;Choetal.2014aSutskever2014Jean2014etal.,;etal.,)oraconvolutionalnetwork(KalchbrennerandBlunsom2013,). Asecondmodel,usuallyanRNN,thenreadsthecontextCandgeneratesasentenceinthetargetlanguage.Thisgeneralideaofanencoder-decoderframeworkformachinetranslationisillustratedinﬁgure.12.5Inordertogenerateanentiresentenceconditionedonthesourcesentence,themodelmusthaveawaytorepresenttheentiresourcesentence. Earliermodelswereonlyabletorepresentindividualwordsorphrases. Fromarepresentation474 CHAPTER12.APPLICATIONSlearningpointofview,itcanbeusefultolearnarepresentationinwhichsentencesthathavethesamemeaninghavesimilarrepresentationsregardlessofwhethertheywerewritteninthesourcelanguageorthetargetlanguage.ThisstrategywasexploredﬁrstusingacombinationofconvolutionsandRNNs(KalchbrennerandBlunsom2013,).LaterworkintroducedtheuseofanRNNforscoringproposedtranslations(,)andforgeneratingtranslatedsentences(Choetal.2014aSutskeveretal.etal.,).2014Jean()scaledthesemodelstolargervocabularies.201412.4.5.1UsinganAttentionMechanismandAligningPiecesofData α(t−1)α(t−1)α()tα()tα(+1)tα(+1)th(t−1)h(t−1)h()th()th(+1)th(+1)tc c × ×× ×× ×+ Figure12.6:Amodernattentionmechanism,asintroducedby(),isBahdanauetal.2015essentiallyaweightedaverage.Acontextvectorcisformedbytakingaweightedaverageoffeaturevectorsh()twithweightsα()t.Insomeapplications,thefeaturevectorsharehiddenunitsofaneuralnetwork,buttheymayalsoberawinputtothemodel.Theweightsα()tareproducedbythemodelitself.Theyareusuallyvaluesintheinterval[0,1]andareintendedtoconcentratearoundjustoneh()tsothattheweightedaverageapproximatesreadingthatonespeciﬁctimestepprecisely.Theweightsα()tareusuallyproducedbyapplyingasoftmaxfunctiontorelevancescoresemittedbyanotherportionofthemodel.Theattentionmechanismismoreexpensivecomputationallythandirectlyindexingthedesiredh()t,butdirectindexingcannotbetrainedwithgradientdescent.Theattentionmechanismbasedonweightedaveragesisasmooth,diﬀerentiableapproximationthatcanbetrainedwithexistingoptimizationalgorithms.Usingaﬁxed-sizerepresentationtocaptureallthesemanticdetailsofaverylongsentenceofsay60wordsisverydiﬃcult. ItcanbeachievedbytrainingasuﬃcientlylargeRNNwellenoughandforlongenough,asdemonstratedbyChoetal.()and2014aSutskever2014etal.().However,amoreeﬃcientapproachistoreadthewholesentenceorparagraph(togetthecontextandthegistofwhat475 CHAPTER12.APPLICATIONSisbeingexpressed),thenproducethetranslatedwordsoneatatime,eachtimefocusingonadiﬀerentpartoftheinputsentenceinordertogatherthesemanticdetailsthatarerequiredtoproducethenextoutputword. Thatisexactlytheideathat()ﬁrstintroduced.TheattentionmechanismusedBahdanauetal.2015tofocusonspeciﬁcpartsoftheinputsequenceateachtimestepisillustratedinﬁgure.12.6Wecanthinkofanattention-basedsystemashavingthreecomponents:1.Aprocessthat“reads”rawdata(suchassourcewordsinasourcesentence),andconvertsthemintodistributedrepresentations,withonefeaturevectorassociatedwitheachwordposition.2.Alistoffeaturevectorsstoringtheoutputofthereader.Thiscanbeunderstoodasa“” containingasequenceoffacts,whichcanbememoryretrievedlater,notnecessarilyinthesameorder,withouthavingtovisitallofthem.3.Aprocessthat“”thecontentofthememorytosequentiallyperformexploitsatask,ateachtimestephavingtheabilityputattentiononthecontentofonememoryelement(orafew,withadiﬀerentweight).Thethirdcomponentgeneratesthetranslatedsentence.Whenwordsinasentencewritteninonelanguagearealignedwithcorrespond-ingwordsinatranslatedsentenceinanotherlanguage,itbecomespossibletorelatethecorrespondingwordembeddings.Earlierworkshowedthatonecouldlearnakindoftranslationmatrixrelatingthewordembeddingsinonelanguagewiththewordembeddingsinanother(Kočiský2014etal.,),yieldingloweralignmenterrorratesthantraditionalapproachesbasedonthefrequencycountsinthephrasetable.Thereisevenearlierworkonlearningcross-lingualwordvectors(Klementievetal.,2012).Manyextensionstothisapproacharepossible.Forexample,moreeﬃcientcross-lingualalignment(,)allowstrainingonlargerdatasets.Gouwsetal.201412.4.6HistoricalPerspectiveTheideaofdistributedrepresentationsforsymbolswasintroducedbyRumelhartetal.()inoneoftheﬁrstexplorationsofback-propagation,withsymbols1986acorrespondingtotheidentityoffamilymembersandtheneuralnetworkcapturingtherelationshipsbetweenfamilymembers,withtrainingexamplesformingtripletssuchas(Colin,Mother,Victoria). Theﬁrstlayeroftheneuralnetworklearnedarepresentationofeachfamilymember.Forexample, thefeaturesforColin476 CHAPTER12.APPLICATIONSmightrepresentwhichfamilytreeColinwasin,whatbranchofthattreehewasin,whatgenerationhewasfrom,etc.Onecanthinkoftheneuralnetworkascomputinglearnedrulesrelatingtheseattributestogetherinordertoobtainthedesiredpredictions.ThemodelcanthenmakepredictionssuchasinferringwhoisthemotherofColin.TheideaofforminganembeddingforasymbolwasextendedtotheideaofanembeddingforawordbyDeerwester1990etal.().TheseembeddingswerelearnedusingtheSVD.Later,embeddingswouldbelearnedbyneuralnetworks.Thehistoryofnaturallanguageprocessingismarkedbytransitionsinthepopularityofdiﬀerentwaysofrepresentingtheinputtothemodel.Followingthisearlyworkonsymbolsorwords,someoftheearliestapplicationsofneuralnetworkstoNLP(,;MiikkulainenandDyer1991Schmidhuber1996,)representedtheinputasasequenceofcharacters.Bengio2001etal.()returnedthefocustomodelingwordsandintroducedneurallanguagemodels,whichproduceinterpretablewordembeddings.Theseneuralmodelshavescaledupfromdeﬁningrepresentationsofasmallsetofsymbolsinthe1980stomillionsofwords(includingpropernounsandmisspellings)inmodernapplications.Thiscomputationalscalingeﬀortledtotheinventionofthetechniquesdescribedaboveinsection.12.4.3Initially,theuseofwordsasthefundamentalunitsoflanguagemodelsyieldedimprovedlanguage modelingperformance(,).Tothisday,Bengioetal.2001newtechniquescontinuallypushbothcharacter-basedmodels(Sutskeveretal.,2011)andword-basedmodelsforward,withrecentwork(,)evenGillicketal.2015modelingindividualbytesofUnicodecharacters.Theideasbehindneurallanguagemodelshavebeenextendedintoseveralnaturallanguageprocessingapplications,suchasparsing(,,;Henderson20032004Collobert2011,),part-of-speechtagging,semanticrolelabeling,chunking,etc,sometimesusingasinglemulti-tasklearningarchitecture(CollobertandWeston,2008aCollobert2011a;etal.,)inwhichthewordembeddingsaresharedacrosstasks.Two-dimensionalvisualizationsofembeddingsbecameapopulartoolforan-alyzinglanguagemodelsfollowingthedevelopmentofthet-SNEdimensionalityreductionalgorithm(vanderMaatenandHinton2008,)anditshigh-proﬁleappli-cationtovisualizationwordembeddingsbyJosephTurianin2009.477 CHAPTER12.APPLICATIONS12.5OtherApplicationsInthissectionwecoverafewothertypesofapplicationsofdeeplearningthatarediﬀerentfromthestandardobjectrecognition,speechrecognitionandnaturallanguageprocessingtasksdiscussedabove.PartofthisbookwillexpandthatIIIscopeevenfurthertotasksthatremainprimarilyresearchareas.12.5.1RecommenderSystemsOneofthemajorfamiliesofapplicationsofmachinelearningintheinformationtechnologysectoristheabilitytomakerecommendationsofitemstopotentialusersorcustomers.Twomajortypesofapplicationscanbedistinguished:onlineadvertisinganditemrecommendations(oftentheserecommendationsarestillforthepurposeofsellingaproduct).Bothrelyonpredictingtheassociationbetweenauserandanitem,eithertopredicttheprobabilityofsomeaction(theuserbuyingtheproduct,orsomeproxyforthisaction)ortheexpectedgain(whichmaydependonthevalueoftheproduct)ifanadisshownorarecommendationismaderegardingthatproducttothatuser.Theinternetiscurrentlyﬁnancedingreatpartbyvariousformsofonlineadvertising. Therearemajorpartsoftheeconomythatrelyononlineshopping. CompaniesincludingAmazonandeBayusemachinelearning,includingdeeplearning,fortheirproductrecommendations.Sometimes,theitemsarenotproductsthatareactuallyforsale.Examplesincludeselectingpoststodisplayonsocialnetworknewsfeeds,recommendingmoviestowatch,recommendingjokes,recommendingadvicefromexperts,matchingplayersforvideogames,ormatchingpeopleindatingservices.Often,thisassociationproblemishandledlikeasupervisedlearningproblem:givensomeinformationabouttheitemandabouttheuser,predicttheproxyofinterest(userclicksonad,userentersarating,userclicksona“like”button,userbuysproduct,userspendssomeamountofmoneyontheproduct,userspendstimevisitingapagefortheproduct,etc).Thisoftenendsupbeingeitheraregressionproblem(predictingsomeconditionalexpectedvalue)oraprobabilisticclassiﬁcationproblem(predictingtheconditionalprobabilityofsomediscreteevent).Theearlyworkonrecommendersystemsreliedonminimalinformationasinputsforthesepredictions:theuserIDandtheitemID.Inthiscontext,theonlywaytogeneralizeistorelyonthesimilaritybetweenthepatternsofvaluesofthetargetvariablefordiﬀerentusersorfordiﬀerentitems.Supposethatuser1anduser2bothlikeitemsA,BandC.Fromthis,wemayinferthatuser1and478 CHAPTER12.APPLICATIONSuser2havesimilartastes.Ifuser1likesitemD,thenthisshouldbeastrongcuethatuser2willalsolikeD.Algorithmsbasedonthisprinciplecomeunderthenameofcollaborativeﬁltering.Bothnon-parametricapproaches(suchasnearest-neighbormethodsbasedontheestimatedsimilaritybetweenpatternsofpreferences)andparametricmethodsarepossible.Parametricmethodsoftenrelyonlearningadistributedrepresentation(alsocalledanembedding)foreachuserandforeachitem.Bilinearpredictionofthetargetvariable(suchasarating)isasimpleparametricmethodthatishighlysuccessfulandoftenfoundasacomponentofstate-of-the-artsystems.Thepredictionisobtainedbythedotproductbetweentheuserembeddingandtheitemembedding(possiblycorrectedbyconstantsthatdependonlyoneithertheuserIDortheitemID).LetˆRbethematrixcontainingourpredictions,AamatrixwithuserembeddingsinitsrowsandBamatrixwithitemembeddingsinitscolumns.Letbandcbevectorsthatcontainrespectivelyakindofbiasforeachuser(representinghowgrumpyorpositivethatuserisingeneral)andforeachitem(representingitsgeneralpopularity).Thebilinearpredictionisthusobtainedasfollows:ˆRu,i= bu+ci+jAu,jBj,i.(12.20)TypicallyonewantstominimizethesquarederrorbetweenpredictedratingsˆRu,iandactualratingsRu,i.Userembeddingsanditemembeddingscanthenbeconvenientlyvisualizedwhentheyareﬁrstreducedtoalowdimension(twoorthree),ortheycanbeusedtocompareusersoritemsagainsteachother,justlikewordembeddings. OnewaytoobtaintheseembeddingsisbyperformingasingularvaluedecompositionofthematrixRofactualtargets(suchasratings).ThiscorrespondstofactorizingR=UDV(oranormalizedvariant)intotheproductoftwofactors,thelowerrankmatricesA=UDandB=V.OneproblemwiththeSVDisthatittreatsthemissingentriesinanarbitraryway,asiftheycorrespondedtoatargetvalueof0.Insteadwewouldliketoavoidpayinganycostforthepredictionsmadeonmissingentries.Fortunately,thesumofsquarederrorsontheobservedratingscanalsobeeasilyminimizedbygradient-basedoptimization.TheSVDandthebilinearpredictionofequationboth12.20performedverywellinthecompetitionfortheNetﬂixprize(,BennettandLanning2007),aimingatpredictingratingsforﬁlms,basedonlyonpreviousratingsbyalargesetofanonymoususers. Manymachinelearningexpertsparticipatedinthiscompetition,whichtookplacebetween2006and2009.Itraisedthelevelofresearchinrecommendersystemsusingadvancedmachinelearningandyieldedimprovementsinrecommendersystems.Eventhoughitdidnotwinbyitself,thesimplebilinearpredictionorSVDwasacomponentoftheensemblemodels479 CHAPTER12.APPLICATIONSpresentedbymostofthecompetitors,includingthewinners(,;Töscheretal.2009Koren2009,).Beyondthesebilinearmodelswithdistributedrepresentations,oneoftheﬁrstusesofneuralnetworksforcollaborativeﬁlteringisbasedontheRBMundirectedprobabilisticmodel(Salakhutdinov2007etal.,).RBMswereanimportantelementoftheensembleofmethodsthatwontheNetﬂixcompetition(Töscher2009etal.,;Koren2009,).Moreadvancedvariantsontheideaoffactorizingtheratingsmatrixhavealsobeenexploredintheneuralnetworkscommunity(SalakhutdinovandMnih2008,).However,thereisabasiclimitationofcollaborativeﬁlteringsystems:whenanewitemoranewuserisintroduced,itslackofratinghistorymeansthatthereisnowaytoevaluateitssimilaritywithotheritemsorusers(respectively),orthedegreeofassociationbetween,say,thatnewuserandexistingitems.Thisiscalledtheproblemofcold-startrecommendations.Ageneralwayofsolvingthecold-startrecommendationproblemistointroduceextrainformationabouttheindividualusersanditems.Forexample,thisextrainformationcouldbeuserproﬁleinformationorfeaturesofeachitem. Systemsthatusesuchinformationarecalledcontent-basedrecommendersystems.Themappingfromarichsetofuserfeaturesoritemfeaturestoanembeddingcanbelearnedthroughadeeplearningarchitecture(,;Huangetal.2013Elkahky2015etal.,).Specializeddeeplearningarchitecturessuchasconvolutionalnetworkshavealsobeenappliedtolearntoextractfeaturesfromrichcontentsuchasfrommusicalaudiotracks,formusicrecommendation(vandenOörd2013etal.,).Inthatwork,theconvolutionalnettakesacousticfeaturesasinputandcomputesanembeddingfortheassociatedsong.Thedotproductbetweenthissongembeddingandtheembeddingforauseristhenusedtopredictwhetherauserwilllistentothesong.12.5.1.1ExplorationVersusExploitationWhenmakingrecommendationstousers,anissuearisesthatgoesbeyondordinarysupervisedlearningandintotherealmofreinforcementlearning.Manyrecom-mendationproblemsaremostaccuratelydescribedtheoreticallyascontextualbandits(,;,).TheissueisthatwhenweLangfordandZhang2008Luetal.2010usetherecommendationsystemtocollectdata,wegetabiasedandincompleteviewofthepreferencesofusers:weonlyseetheresponsesofuserstotheitemstheywererecommendedandnottotheotheritems. Inaddition,insomecaseswemaynotgetanyinformationonusersforwhomnorecommendationhasbeenmade(forexample,withadauctions,itmaybethatthepriceproposedforan480 CHAPTER12.APPLICATIONSadwasbelowaminimumpricethreshold,ordoesnotwintheauction,sotheadisnotshownatall).Moreimportantly,wegetnoinformationaboutwhatoutcomewouldhaveresultedfromrecommendinganyoftheotheritems.Thiswouldbeliketrainingaclassiﬁerbypickingoneclassˆyforeachtrainingexamplex(typicallytheclasswiththehighestprobabilityaccordingtothemodel)andthenonlygettingasfeedbackwhetherthiswasthecorrectclassornot.Clearly,eachexampleconveyslessinformationthaninthesupervisedcasewherethetruelabelyisdirectlyaccessible,somoreexamplesarenecessary.Worse,ifwearenotcareful,wecouldendupwithasystemthatcontinuespickingthewrongdecisionsevenasmoreandmoredataiscollected,becausethecorrectdecisioninitiallyhadaverylowprobability:untilthelearnerpicksthatcorrectdecision,itdoesnotlearnaboutthecorrectdecision.Thisissimilartothesituationinreinforcementlearningwhereonlytherewardfortheselectedactionisobserved.Ingeneral,reinforcementlearningcaninvolveasequenceofmanyactionsandmanyrewards.Thebanditsscenarioisaspecialcaseofreinforcementlearning,inwhichthelearnertakesonlyasingleactionandreceivesasinglereward.Thebanditproblemiseasierinthesensethatthelearnerknowswhichrewardisassociatedwithwhichaction.Inthegeneralreinforcementlearningscenario,ahighrewardoralowrewardmighthavebeencausedbyarecentactionorbyanactioninthedistantpast.Thetermcontextualbanditsreferstothecasewheretheactionistakeninthecontextofsomeinputvariablethatcaninformthedecision.Forexample,weatleastknowtheuseridentity,andwewanttopickanitem.Themappingfromcontexttoactionisalsocalledapolicy.Thefeedbackloopbetweenthelearnerandthedatadistribution(whichnowdependsontheactionsofthelearner)isacentralresearchissueinthereinforcementlearningandbanditsliterature.Reinforcementlearningrequireschoosingatradeoﬀbetweenexplorationandexploitation.Exploitationreferstotakingactionsthatcomefromthecurrent,bestversionofthelearnedpolicy—actionsthatweknowwillachieveahighreward.Explorationreferstotakingactionsspeciﬁcallyinordertoobtainmoretrainingdata.Ifweknowthatgivencontextx,actionagivesusarewardof1,wedonotknowwhetherthatisthebestpossiblereward.Wemaywanttoexploitourcurrentpolicyandcontinuetakingactionainordertoberelativelysureofobtainingarewardof1.However,wemayalsowanttoexplorebytryingactiona.Wedonotknowwhatwillhappenifwetryactiona.Wehopetogetarewardof,butwe2runtheriskofgettingarewardof.Eitherway,weatleastgainsomeknowledge.0Explorationcanbeimplementedinmanyways,rangingfromoccasionallytakingrandomactionsintendedtocovertheentirespaceofpossibleactions,tomodel-basedapproachesthatcomputeachoiceofactionbasedonitsexpectedrewardandthemodel’samountofuncertaintyaboutthatreward.481 CHAPTER12.APPLICATIONSManyfactorsdeterminetheextenttowhichwepreferexplorationorexploitation.Oneofthemostprominentfactorsisthetimescaleweareinterestedin. Iftheagenthasonlyashortamountoftimetoaccruereward,thenweprefermoreexploitation.Iftheagenthasalongtimetoaccruereward,thenwebeginwithmoreexplorationsothatfutureactionscanbeplannedmoreeﬀectivelywithmoreknowledge.Astimeprogressesandourlearnedpolicyimproves,wemovetowardmoreexploitation.Supervised learninghas notradeoﬀ between explorationand exploitationbecausethesupervisionsignalalwaysspeciﬁeswhichoutputiscorrectforeachinput.Thereisnoneedtotryoutdiﬀerentoutputstodetermineifoneisbetterthanthemodel’scurrentoutput—wealwaysknowthatthelabelisthebestoutput.Anotherdiﬃcultyarisinginthecontextofreinforcementlearning,besidestheexploration-exploitationtrade-oﬀ,isthediﬃcultyofevaluatingandcomparingdiﬀerentpolicies.Reinforcementlearninginvolvesinteractionbetweenthelearnerandtheenvironment.Thisfeedbackloopmeansthatitisnotstraightforwardtoevaluatethelearner’sperformanceusingaﬁxedsetoftestsetinputvalues.Thepolicyitselfdetermineswhichinputswillbeseen.()presentDudiketal.2011techniquesforevaluatingcontextualbandits.12.5.2KnowledgeRepresentation,ReasoningandQuestionAn-sweringDeeplearningapproacheshavebeenverysuccessfulinlanguagemodeling,machinetranslationandnaturallanguageprocessingduetotheuseofembeddingsforsymbols(,)andwords(Rumelhartetal.1986aDeerwester1990Bengioetal.,;etal.,2001).Theseembeddingsrepresentsemanticknowledgeaboutindividualwordsandconcepts.Aresearchfrontieristodevelopembeddingsforphrasesandforrelationsbetweenwordsandfacts.Searchenginesalreadyusemachinelearningforthispurposebutmuchmoreremainstobedonetoimprovethesemoreadvancedrepresentations.12.5.2.1Knowledge,RelationsandQuestionAnsweringOneinterestingresearchdirectionisdetermininghowdistributedrepresentationscanbetrainedtocapturetherelationsbetweentwoentities.Theserelationsallowustoformalizefactsaboutobjectsandhowobjectsinteractwitheachother.Inmathematics,abinaryrelationisasetoforderedpairsofobjects.Pairsthatareinthesetaresaidtohavetherelationwhilethosewhoarenotintheset482 CHAPTER12.APPLICATIONSdonot.Forexample,wecandeﬁnetherelation“islessthan”onthesetofentities{1,2,3}bydeﬁningthesetoforderedpairsS={(1,2),(1,3),(2,3)}.Oncethisrelationisdeﬁned,wecanuseitlikeaverb.Because(1,2)∈S,wesaythat1islessthan2.Because(2,1)∈S,wecannotsaythat2islessthan1.Ofcourse,theentitiesthatarerelatedtooneanotherneednotbenumbers.Wecoulddeﬁnearelationcontainingtupleslike(,).is_a_type_ofdogmammalInthecontextofAI,wethinkofarelationasasentenceinasyntacticallysimpleandhighlystructuredlanguage.Therelationplaystheroleofaverb,whiletwoargumentstotherelationplaytheroleofitssubjectandobject.Thesesentencestaketheformofatripletoftokens(subjectverbobject),,(12.21)withvalues(entityi,relationj,entityk).(12.22)Wecanalsodeﬁneanattribute,aconceptanalogoustoarelation,buttakingonlyoneargument:(entityi,attributej).(12.23)Forexample,wecoulddeﬁnethehas_furattribute,andapplyittoentitieslikedog.Manyapplicationsrequirerepresentingrelationsandreasoningaboutthem.Howshouldwebestdothiswithinthecontextofneuralnetworks?Machinelearningmodelsofcourserequiretrainingdata.Wecaninferrelationsbetweenentitiesfromtrainingdatasetsconsistingofunstructurednaturallanguage.Therearealsostructureddatabasesthatidentifyrelationsexplicitly.Acommonstructureforthesedatabasesistherelationaldatabase,whichstoresthissamekindofinformation, albeit notformattedasthreetokensentences.Whenadatabaseisintendedtoconveycommonsenseknowledgeabouteverydaylifeorexpertknowledgeaboutanapplicationareatoanartiﬁcialintelligencesystem,wecallthedatabaseaknowledgebase.KnowledgebasesrangefromgeneraloneslikeFreebase,OpenCyc,WordNet,orWikibase,1etc.tomorespecializedknowledgebases,likeGeneOntology.2Representationsforentitiesandrelationscanbelearnedbyconsideringeachtripletinaknowledgebaseasatrainingexampleandmaximizingatrainingobjectivethatcapturestheirjointdistribution(Bordesetal.,).2013a1Respectivelyavailable fromthese web sites:freebase.com,cyc.com/opencyc,wordnet.princeton.eduwikiba.se,2geneontology.org483 CHAPTER12.APPLICATIONSInadditiontotrainingdata,wealsoneedtodeﬁneamodelfamilytotrain.Acommonapproachistoextendneurallanguagemodelstomodelentitiesandrelations.Neurallanguagemodelslearnavectorthatprovidesadistributedrepresentationofeachword.Theyalsolearnaboutinteractionsbetweenwords,suchaswhichwordislikelytocomeafterasequenceofwords,bylearningfunctionsofthesevectors.Wecanextendthisapproachtoentitiesandrelationsbylearninganembeddingvectorforeachrelation.Infact,theparallelbetweenmodelinglanguageandmodelingknowledgeencodedasrelationsissoclosethatresearchershavetrainedrepresentationsofsuchentitiesbyusingbothandknowledgebasesnaturallanguagesentences(,,;Bordesetal.20112012Wang2014aetal.,)orcombiningdatafrommultiplerelationaldatabases(,).ManyBordesetal.2013bpossibilitiesexistfortheparticularparametrizationassociatedwithsuchamodel.Earlyworkonlearningaboutrelationsbetweenentities(,PaccanaroandHinton2000)positedhighlyconstrainedparametricforms(“linearrelationalembeddings”),oftenusingadiﬀerentformofrepresentationfortherelationthanfortheentities.Forexample,PaccanaroandHinton2000Bordes2011()andetal.()usedvectorsforentitiesandmatricesforrelations,withtheideathatarelationactslikeanoperatoronentities.Alternatively,relationscanbeconsideredasanyotherentity(Bordesetal.,),allowingustomakestatementsaboutrelations,butmoreﬂexibilityis2012putinthemachinerythatcombinestheminordertomodeltheirjointdistribution.Apracticalshort-termapplicationofsuchmodelsislinkprediction:predict-ingmissingarcsintheknowledgegraph.Thisisaformofgeneralizationtonewfacts,basedonoldfacts.Mostoftheknowledgebasesthatcurrentlyexisthavebeenconstructedthroughmanuallabor,whichtendstoleavemanyandprobablythemajorityoftruerelationsabsentfromtheknowledgebase.SeeWangetal.(),()and()forexamplesofsuchan2014bLinetal.2015Garcia-Duranetal.2015application.Evaluatingtheperformanceofamodelonalinkpredictiontaskisdiﬃcultbecausewehaveonlyadatasetofpositiveexamples(factsthatareknowntobetrue). Ifthemodelproposesafactthatisnotinthedataset,weareunsurewhetherthemodelhasmadeamistakeordiscoveredanew,previouslyunknownfact.Themetricsarethussomewhatimpreciseandarebasedontestinghowthemodelranksaheld-outofsetofknowntruepositivefactscomparedtootherfactsthatarelesslikelytobetrue.Acommonwaytoconstructinterestingexamplesthatareprobablynegative(factsthatareprobablyfalse)istobeginwithatruefactandcreatecorruptedversionsofthatfact,forexamplebyreplacingoneentityintherelationwithadiﬀerententityselectedatrandom.Thepopularprecisionat10%metriccountshowmanytimesthemodelranksa“correct”factamongthetop10%ofallcorruptedversionsofthatfact.484 CHAPTER12.APPLICATIONSAnotherapplicationofknowledgebasesanddistributedrepresentationsforthemisword-sensedisambiguation(NavigliandVelardi2005Bordes,;etal.,2012),whichisthetaskofdecidingwhichofthesensesofawordistheappropriateone,insomecontext.Eventually,knowledgeofrelationscombinedwithareasoningprocessandunderstandingofnaturallanguagecouldallowustobuildageneralquestionansweringsystem.Ageneralquestionansweringsystemmustbeabletoprocessinputinformationandrememberimportantfacts,organizedinawaythatenablesittoretrieveandreasonaboutthemlater.Thisremainsadiﬃcultopenproblemwhichcanonlybesolvedinrestricted“toy”environments.Currently,thebestapproachtorememberingandretrievingspeciﬁcdeclarativefactsistouseanexplicitmemorymechanism,asdescribedinsection.Memorynetworkswere10.12ﬁrstproposedtosolveatoyquestionansweringtask(Weston2014Kumaretal.,).etal.()haveproposedanextensionthatusesGRUrecurrentnetstoread2015theinputintothememoryandtoproducetheanswergiventhecontentsofthememory.Deeplearninghasbeenappliedtomanyotherapplicationsbesidestheonesdescribedhere,andwillsurelybeappliedtoevenmoreafterthiswriting.Itwouldbeimpossibletodescribeanythingremotelyresemblingacomprehensivecoverageofsuchatopic.Thissurveyprovidesarepresentativesampleofwhatispossibleasofthiswriting.Thisconcludespart,whichhasdescribedmodernpracticesinvolvingdeepIInetworks,comprisingallofthemostsuccessfulmethods.Generallyspeaking,thesemethodsinvolveusingthegradientofacostfunctiontoﬁndtheparametersofamodelthatapproximatessomedesiredfunction.Withenoughtrainingdata,thisapproachisextremelypowerful.Wenowturntopart,inwhichwestepintotheIIIterritoryofresearch—methodsthataredesignedtoworkwithlesstrainingdataortoperformagreatervarietyoftasks,wherethechallengesaremorediﬃcultandnotasclosetobeingsolvedasthesituationswehavedescribedsofar. 485 PartIIIDeepLearningResearch 486 Thispartofthebookdescribesthemoreambitiousandadvancedapproachestodeeplearning,currentlypursuedbytheresearchcommunity.Inthepreviouspartsofthebook,wehaveshownhowtosolvesupervisedlearningproblems—howtolearntomaponevectortoanother,givenenoughexamplesofthemapping.Notallproblemswemightwanttosolvefallintothiscategory.Wemaywishtogeneratenewexamples,ordeterminehowlikelysomepointis,orhandlemissingvaluesandtakeadvantageofalargesetofunlabeledexamplesorexamplesfromrelatedtasks.Ashortcomingofthecurrentstateoftheartforindustrialapplicationsisthatourlearningalgorithmsrequirelargeamountsofsuperviseddatatoachievegoodaccuracy.Inthispartofthebook,wediscusssomeofthespeculativeapproachestoreducingtheamountoflabeleddatanecessaryforexistingmodelstoworkwellandbeapplicableacrossabroaderrangeoftasks.Accomplishingthesegoalsusuallyrequiressomeformofunsupervisedorsemi-supervisedlearning.Manydeeplearningalgorithmshavebeendesignedtotackleunsupervisedlearningproblems,butnonehavetrulysolvedtheprobleminthesamewaythatdeeplearninghaslargelysolvedthesupervisedlearningproblemforawidevarietyoftasks.Inthispartofthebook,wedescribetheexistingapproachestounsupervisedlearningandsomeofthepopularthoughtabouthowwecanmakeprogressinthisﬁeld.Acentralcauseofthediﬃcultieswithunsupervisedlearningisthehighdi-mensionalityoftherandomvariablesbeingmodeled.Thisbringstwodistinctchallenges:astatisticalchallengeandacomputationalchallenge.Thestatisticalchallengeregardsgeneralization:thenumberofconﬁgurationswemaywanttodistinguishcangrowexponentiallywiththenumberofdimensionsofinterest,andthisquicklybecomesmuchlargerthanthenumberofexamplesonecanpossiblyhave(orusewithboundedcomputationalresources).Thecomputationalchallengeassociatedwithhigh-dimensionaldistributionsarisesbecausemanyalgorithmsforlearningorusingatrainedmodel(especiallythosebasedonestimatinganexplicitprobabilityfunction)involveintractablecomputationsthatgrowexponentiallywiththenumberofdimensions.Withprobabilisticmodels,thiscomputationalchallengearisesfromtheneedtoperformintractableinferenceorsimplyfromtheneedtonormalizethedistribution.•Intractableinference:inferenceisdiscussedmostlyinchapter.Itregards19thequestionofguessingtheprobablevaluesofsomevariablesa,givenothervariablesb,withrespecttoamodelthatcapturesthejointdistributionover487 a,bandc.Inordertoevencomputesuchconditionalprobabilitiesoneneedstosumoverthevaluesofthevariablesc,aswellascomputeanormalizationconstantwhichsumsoverthevaluesofaandc.•Intractablenormalizationconstants(thepartitionfunction):thepartitionfunctionisdiscussedmostlyinchapter.Normalizingconstantsofproba-18bilityfunctionscomeupininference(above)aswellasinlearning. Manyprobabilisticmodelsinvolvesuchanormalizingconstant.Unfortunately,learningsuchamodeloftenrequirescomputingthegradientoftheloga-rithmofthepartitionfunctionwithrespecttothemodelparameters.Thatcomputationisgenerallyasintractableascomputingthepartitionfunctionitself.MonteCarloMarkovchain(MCMC)methods(chapter)areof-17tenusedtodealwiththepartitionfunction(computingitoritsgradient).Unfortunately,MCMCmethodssuﬀerwhenthemodesofthemodeldistribu-tionarenumerousandwell-separated,especiallyinhigh-dimensionalspaces(section).17.5Onewaytoconfronttheseintractablecomputationsistoapproximatethem,andmanyapproacheshavebeenproposedasdiscussedinthisthirdpartofthebook.Anotherinterestingway, alsodiscussedhere, wouldbetoavoidtheseintractablecomputationsaltogetherbydesign,andmethodsthatdonotrequiresuchcomputationsarethusveryappealing.Severalgenerativemodelshavebeenproposedinrecentyears,withthatmotivation.Awidevarietyofcontemporaryapproachestogenerativemodelingarediscussedinchapter.20Partisthemostimportantforaresearcher—someonewhowantstoun-IIIderstandthebreadthofperspectivesthathavebeenbroughttotheﬁeldofdeeplearning,andpushtheﬁeldforwardtowardstrueartiﬁcialintelligence. 488 Chapter13LinearFactorModelsManyoftheresearchfrontiersindeeplearninginvolvebuildingaprobabilisticmodeloftheinput,pmodel(x).Suchamodelcan,inprinciple,useprobabilisticinferencetopredictanyofthevariablesinitsenvironmentgivenanyoftheothervariables.Manyofthesemodelsalsohavelatentvariablesh,withpmodel() = xEhpmodel()xh|.Theselatentvariablesprovideanothermeansofrepresentingthedata.Distributedrepresentationsbased onlatent variablescanobtain alloftheadvantagesofrepresentationlearningthatwehaveseenwithdeepfeedforwardandrecurrentnetworks.Inthischapter,wedescribesomeofthesimplestprobabilisticmodelswithlatentvariables:linearfactormodels.Thesemodelsaresometimesusedasbuildingblocksofmixturemodels(Hinton1995aGhahramaniandHinton1996etal.,;,;Roweis2002Tang2012etal.,)orlarger,deepprobabilisticmodels(etal.,).Theyalsoshowmanyofthebasicapproachesnecessarytobuildgenerativemodelsthatthemoreadvanceddeepmodelswillextendfurther.Alinearfactormodelisdeﬁnedbytheuseofastochastic,lineardecoderfunctionthatgeneratesbyaddingnoisetoalineartransformationof.xhThesemodelsareinterestingbecausetheyallowustodiscoverexplanatoryfactorsthathaveasimplejointdistribution.Thesimplicityofusingalineardecodermadethesemodelssomeoftheﬁrstlatentvariablemodelstobeextensivelystudied.Alinearfactormodeldescribesthedatagenerationprocessasfollows.First,wesampletheexplanatoryfactorsfromadistributionhh∼p,()h(13.1)wherep(h)isafactorialdistribution,withp(h) =ip(hi),sothatitiseasyto489 CHAPTER13.LINEARFACTORMODELSsamplefrom.Nextwesamplethereal-valuedobservablevariablesgiventhefactors:xWhb= ++noise(13.2)wherethenoiseistypicallyGaussiananddiagonal(independentacrossdimensions).Thisisillustratedinﬁgure.13.1h1h1h2h2h3h3x1x1x2x2x3x3xhn o i s exhn o i s e=W++b=W++bFigure13.1:Thedirectedgraphicalmodeldescribingthelinearfactormodelfamily,inwhichweassumethatanobserveddatavectorxisobtainedbyalinearcombinationofindependentlatentfactorsh,plussomenoise.Diﬀerentmodels,suchasprobabilisticPCA,factoranalysisorICA,makediﬀerentchoicesabouttheformofthenoiseandoftheprior.p()h13.1ProbabilisticPCAandFactorAnalysisProbabilisticPCA(principalcomponentsanalysis),factoranalysisandotherlinearfactormodelsarespecialcasesoftheaboveequations(and)andonly13.113.2diﬀerinthechoicesmadeforthenoisedistributionandthemodel’sprioroverlatentvariablesbeforeobserving.hxInfactoranalysis(,;,),thelatentvariableBartholomew1987Basilevsky1994priorisjusttheunitvarianceGaussianh0∼N(;h,I)(13.3)whiletheobservedvariablesxiareassumedtobeconditionallyindependent,givenh.Speciﬁcally, the noiseisassumed tobedrawnfroma diagonalco-variance Gaussiandistribution,with covariancematrixψ=diag(σ2),withσ2= [σ21,σ22,...,σ2n]avectorofper-variablevariances.Theroleofthelatentvariablesisthustocapturethedependenciesbetweenthediﬀerentobservedvariablesxi.Indeed,itcaneasilybeshownthatxisjustamultivariatenormalrandomvariable,withx∼N(;xbWW,+)ψ.(13.4)490 CHAPTER13.LINEARFACTORMODELSInordertocastPCAinaprobabilisticframework, wecanmakeaslightmodiﬁcationtothefactoranalysismodel,makingtheconditionalvariancesσ2iequaltoeachother.InthatcasethecovarianceofxisjustWW+σ2I,whereσ2isnowascalar.Thisyieldstheconditionaldistributionx∼N(;xbWW,+σ2I)(13.5)orequivalentlyxhz= W++bσ(13.6)wherez∼N(z;0,I)isGaussiannoise.()thenshowanTippingandBishop1999iterativeEMalgorithmforestimatingtheparametersandWσ2.ThisprobabilisticPCAmodeltakesadvantageoftheobservationthatmostvariationsinthedatacanbecapturedbythelatentvariablesh,uptosomesmallresidualreconstructionerrorσ2.Asshownby(),TippingandBishop1999probabilisticPCAbecomesPCAasσ→0.Inthatcase,theconditionalexpectedvalueofhgivenxbecomesanorthogonalprojectionofxb−ontothespacespannedbythecolumnsof,likeinPCA.dWAsσ→0,thedensitymodeldeﬁnedbyprobabilisticPCAbecomesverysharparoundtheseddimensionsspannedbythecolumnsofW.Thiscanmakethemodelassignverylowlikelihoodtothedataifthedatadoesnotactuallyclusternearahyperplane.13.2IndependentComponentAnalysis(ICA)Independentcomponentanalysis(ICA)isamongtheoldestrepresentationlearningalgorithms(,;,; ,;Herault andAns1984Jutten andHerault1991Comon1994Hyvärinen1999Hyvärinen2001aHinton2001Teh2003,;etal.,;etal.,;etal.,).Itisanapproachtomodelinglinearfactorsthatseekstoseparateanobservedsignalintomanyunderlyingsignalsthatarescaledandaddedtogethertoformtheobserveddata.Thesesignalsareintendedtobefullyindependent,ratherthanmerelydecorrelatedfromeachother.1ManydiﬀerentspeciﬁcmethodologiesarereferredtoasICA.Thevariantthatismostsimilartotheothergenerativemodelswehavedescribedhereisavariant(,)thattrainsafullyparametricgenerativemodel.ThePhametal.1992priordistributionovertheunderlyingfactors,p(h),mustbeﬁxedaheadoftimebytheuser.Themodelthendeterministicallygeneratesx=Wh.Wecanperforma1Seesectionforadiscussionofthediﬀerencebetweenuncorrelatedvariablesandindepen-3.8dentvariables.491 CHAPTER13.LINEARFACTORMODELSnonlinearchangeofvariables(usingequation)todetermine3.47p(x).Learningthemodelthenproceedsasusual,usingmaximumlikelihood.Themotivationforthisapproachisthatbychoosingp(h)tobeindependent,wecanrecoverunderlyingfactorsthatareascloseaspossibletoindependent.Thisiscommonlyused,nottocapturehigh-levelabstractcausalfactors,buttorecoverlow-levelsignalsthathavebeenmixedtogether.Inthissetting,eachtrainingexampleisonemomentintime,eachxiisonesensor’sobservationofthemixedsignals,andeachhiisoneestimateofoneoftheoriginalsignals.Forexample,wemighthavenpeoplespeakingsimultaneously.Ifwehavendiﬀerentmicrophonesplacedindiﬀerentlocations,ICAcandetectthechangesinthevolumebetweeneachspeakerasheardbyeachmicrophone,andseparatethesignalssothateachhicontainsonlyonepersonspeakingclearly.Thisiscommonlyusedinneuroscienceforelectroencephalography,atechnologyforrecordingelectricalsignalsoriginatinginthebrain.Manyelectrodesensorsplacedonthesubject’sheadareusedtomeasuremanyelectricalsignalscomingfromthebody.Theexperimenteristypicallyonlyinterestedinsignalsfromthebrain,butsignalsfromthesubject’sheartandeyesarestrongenoughtoconfoundmeasurementstakenatthesubject’sscalp.Thesignalsarriveattheelectrodesmixedtogether,soICAisnecessarytoseparatetheelectricalsignatureoftheheartfromthesignalsoriginatinginthebrain,andtoseparatesignalsindiﬀerentbrainregionsfromeachother.Asmentionedbefore,manyvariantsofICAarepossible.Someaddsomenoiseinthegenerationofxratherthanusingadeterministicdecoder.Mostdonotusethemaximumlikelihoodcriterion,butinsteadaimtomaketheelementsofh=W−1xindependentfromeachother.Manycriteriathataccomplishthisgoalarepossible.Equationrequirestakingthedeterminantof3.47W,whichcanbeanexpensiveandnumericallyunstableoperation.SomevariantsofICAavoidthisproblematicoperationbyconstrainingtobeorthogonal.WAllvariantsofICArequirethatp(h)benon-Gaussian.Thisisbecauseifp(h)isanindependentpriorwithGaussiancomponents,thenWisnotidentiﬁable.Wecanobtainthesamedistributionoverp(x)formanyvaluesofW.ThisisverydiﬀerentfromotherlinearfactormodelslikeprobabilisticPCAandfactoranalysis,thatoftenrequirep(h)tobeGaussianinordertomakemanyoperationsonthemodelhaveclosedformsolutions.Inthemaximumlikelihoodapproachwheretheuserexplicitlyspeciﬁesthedistribution,atypicalchoiceistousep(hi) =ddhiσ(hi).Typicalchoicesofthesenon-Gaussiandistributionshavelargerpeaksnear0thandoestheGaussiandistribution,sowecanalsoseemostimplementationsofICAaslearningsparsefeatures.492 CHAPTER13.LINEARFACTORMODELSManyvariantsofICAarenotgenerativemodelsinthesensethatweusethephrase.Inthisbook,agenerativemodeleitherrepresentsp(x) orcandrawsamplesfromit.ManyvariantsofICAonlyknowhowtotransformbetweenxandh,butdonothaveanywayofrepresentingp(h),andthusdonotimposeadistributionoverp(x).Forexample,manyICAvariantsaimtoincreasethesamplekurtosisofh=W−1x,becausehighkurtosisindicatesthatp(h)isnon-Gaussian,butthisisaccomplishedwithoutexplicitlyrepresentingp(h).ThisisbecauseICAismoreoftenusedasananalysistoolforseparatingsignals,ratherthanforgeneratingdataorestimatingitsdensity.JustasPCAcanbegeneralizedtothenonlinearautoencodersdescribedinchapter,ICAcanbegeneralizedtoanonlineargenerativemodel,inwhich14weuseanonlinearfunctionftogeneratetheobserveddata.SeeHyvärinenandPajunen1999()fortheinitialworkonnonlinearICAanditssuccessfulusewithensemblelearningby()and().RobertsandEverson2001Lappalainenetal.2000AnothernonlinearextensionofICAistheapproachofnonlinearindependentcomponentsestimation,orNICE(,),whichstacksaseriesDinhetal.2014ofinvertibletransformations(encoderstages)thathavethepropertythatthedeterminantoftheJacobianofeachtransformationcanbecomputedeﬃciently.Thismakesitpossibletocomputethelikelihoodexactlyand,likeICA,attemptstotransformthedataintoaspacewhereithasafactorizedmarginaldistribution,butismorelikelytosucceedthankstothenonlinearencoder.Becausetheencoderisassociatedwithadecoderthatisitsperfectinverse,itisstraightforwardtogeneratesamplesfromthemodel(byﬁrstsamplingfromp(h)andthenapplyingthedecoder).AnothergeneralizationofICAistolearngroupsoffeatures,withstatisticaldependenceallowedwithinagroupbutdiscouragedbetweengroups(HyvärinenandHoyer1999Hyvärinen2001b,;etal.,).Whenthegroupsofrelatedunitsarechosentobenon-overlapping,thisiscalledindependentsubspaceanalysis.Itisalsopossibletoassignspatialcoordinatestoeachhiddenunitandformoverlappinggroupsofspatiallyneighboringunits.Thisencouragesnearbyunitstolearnsimilarfeatures.Whenappliedtonaturalimages,thistopographicICAapproachlearnsGaborﬁlters,suchthatneighboringfeatureshavesimilarorientation,locationorfrequency.ManydiﬀerentphaseoﬀsetsofsimilarGaborfunctionsoccurwithineachregion,sothatpoolingoversmallregionsyieldstranslationinvariance.13.3SlowFeatureAnalysisSlowfeatureanalysis(SFA)isalinearfactormodelthatusesinformationfrom493 CHAPTER13.LINEARFACTORMODELStimesignalstolearninvariantfeatures(,).WiskottandSejnowski2002Slowfeatureanalysisismotivatedbyageneralprinciplecalledtheslownessprinciple.Theideaisthattheimportantcharacteristicsofsceneschangeveryslowlycomparedtotheindividualmeasurementsthatmakeupadescriptionofascene.Forexample,incomputervision,individualpixelvaluescanchangeveryrapidly.Ifazebramovesfromlefttorightacrosstheimage,anindividualpixelwillrapidlychangefromblacktowhiteandbackagainasthezebra’sstripespassoverthepixel.Bycomparison,thefeatureindicatingwhetherazebraisintheimagewillnotchangeatall,andthefeaturedescribingthezebra’spositionwillchangeslowly. Wethereforemaywishtoregularizeourmodeltolearnfeaturesthatchangeslowlyovertime.Theslownessprinciplepredatesslowfeatureanalysisandhasbeenappliedtoawidevarietyofmodels(,;,;,;Hinton1989Földiák1989Mobahietal.2009BergstraandBengio2009,).Ingeneral,wecanapplytheslownessprincipletoanydiﬀerentiablemodeltrainedwithgradientdescent.TheslownessprinciplemaybeintroducedbyaddingatermtothecostfunctionoftheformλtLf((x(+1)t)(,fx()t))(13.7)whereλisahyperparameterdeterminingthestrengthoftheslownessregularizationterm,tistheindexintoatimesequenceofexamples,fisthefeatureextractortoberegularized,andLisalossfunctionmeasuringthedistancebetweenf(x()t)andf(x(+1)t).Acommonchoiceforisthemeansquareddiﬀerence.LSlowfeatureanalysisisaparticularlyeﬃcientapplicationoftheslownessprinciple.Itiseﬃcientbecauseitisappliedtoalinearfeatureextractor,andcanthusbetrainedinclosedform.LikesomevariantsofICA,SFAisnotquiteagenerativemodelperse,inthesensethatitdeﬁnesalinearmapbetweeninputspaceandfeaturespacebutdoesnotdeﬁneaprioroverfeaturespaceandthusdoesnotimposeadistributiononinputspace.p()xTheSFAalgorithm(WiskottandSejnowski2002,)consistsofdeﬁningf(x;θ)tobealineartransformation,andsolvingtheoptimizationproblemminθEt((fx(+1)t)i−f(x()t)i)2(13.8)subjecttotheconstraintsEtf(x()t)i= 0(13.9)andEt[(fx()t)2i] = 1.(13.10)494 CHAPTER13.LINEARFACTORMODELSTheconstraintthatthelearnedfeaturehavezeromeanisnecessarytomaketheproblemhaveauniquesolution;otherwisewecouldaddaconstanttoallfeaturevaluesandobtainadiﬀerentsolutionwithequalvalueoftheslownessobjective.Theconstraintthatthefeatureshaveunitvarianceisnecessarytopreventthepathologicalsolutionwhereallfeaturescollapseto.LikePCA,theSFAfeatures0areordered,withtheﬁrstfeaturebeingtheslowest.Tolearnmultiplefeatures,wemustalsoaddtheconstraint∀i<j,Et[(fx()t)if(x()t)j] = 0.(13.11)Thisspeciﬁesthatthelearnedfeaturesmustbelinearlydecorrelatedfromeachother.Withoutthisconstraint,allofthelearnedfeatureswouldsimplycapturetheoneslowestsignal.Onecouldimagineusingothermechanisms,suchasminimizingreconstructionerror, to forcethe featurestodiversify, but thisdecorrelationmechanismadmitsasimplesolutionduetothelinearityofSFAfeatures.TheSFAproblemmaybesolvedinclosedformbyalinearalgebrapackage.SFAistypicallyusedtolearnnonlinearfeaturesbyapplyinganonlinearbasisexpansiontoxbeforerunningSFA.Forexample,itiscommontoreplacexbythequadraticbasisexpansion,avectorcontainingelementsxixjforalliandj.LinearSFAmodulesmaythenbecomposedtolearndeepnonlinearslowfeatureextractorsbyrepeatedlylearningalinearSFAfeatureextractor,applyinganonlinearbasisexpansiontoitsoutput,andthenlearninganotherlinearSFAfeatureextractorontopofthatexpansion.Whentrainedonsmallspatialpatchesofvideosofnaturalscenes,SFAwithquadraticbasisexpansionslearnsfeaturesthatsharemanycharacteristicswiththoseofcomplexcellsinV1cortex(BerkesandWiskott2005,).Whentrainedonvideosofrandommotionwithin3-Dcomputerrenderedenvironments,deepSFAlearnsfeaturesthatsharemanycharacteristicswiththefeaturesrepresentedbyneuronsinratbrainsthatareusedfornavigation(Franzius2007etal.,).SFAthusseemstobeareasonablybiologicallyplausiblemodel.AmajoradvantageofSFAisthatitispossiblytotheoreticallypredictwhichfeaturesSFAwilllearn,eveninthedeep,nonlinearsetting.Tomakesuchtheoreticalpredictions,onemustknowaboutthedynamicsoftheenvironmentintermsofconﬁgurationspace (e.g., inthe caseofrandom motioninthe 3-Drenderedenvironment,thetheoreticalanalysisproceedsfromknowledgeoftheprobabilitydistributionoverpositionandvelocityofthecamera).Giventheknowledgeofhowtheunderlyingfactorsactuallychange,itispossibletoanalyticallysolvefortheoptimalfunctionsexpressingthesefactors.Inpractice,experimentswithdeepSFAappliedtosimulateddataseemtorecoverthetheoreticallypredictedfunctions.495 CHAPTER13.LINEARFACTORMODELSThisisincomparisontootherlearningalgorithmswherethecostfunctiondependshighlyonspeciﬁcpixelvalues,makingitmuchmorediﬃculttodeterminewhatfeaturesthemodelwilllearn.DeepSFAhasalsobeenusedtolearnfeaturesforobjectrecognitionandposeestimation(Franzius2008etal.,).Sofar,theslownessprinciplehasnotbecomethebasisforanystateoftheartapplications.Itisunclearwhatfactorhaslimiteditsperformance.Wespeculatethatperhapstheslownessprioristoostrong,andthat,ratherthanimposingapriorthatfeaturesshouldbeapproximatelyconstant,itwouldbebettertoimposeapriorthatfeaturesshouldbeeasytopredictfromonetimesteptothenext.Thepositionofanobjectisausefulfeatureregardlessofwhethertheobject’svelocityishighorlow,buttheslownessprincipleencouragesthemodeltoignorethepositionofobjectsthathavehighvelocity.13.4SparseCodingSparsecoding(,)isalinearfactormodelthathasOlshausenandField1996beenheavilystudiedasanunsupervisedfeaturelearningandfeatureextractionmechanism. Strictlyspeaking,theterm“sparsecoding”referstotheprocessofinferringthevalueofhinthismodel,while“sparsemodeling”referstotheprocessofdesigningandlearningthemodel,buttheterm“sparsecoding”isoftenusedtorefertoboth.Likemostotherlinearfactormodels,itusesalineardecoderplusnoisetoobtainreconstructionsofx,asspeciﬁedinequation.Morespeciﬁcally,sparse13.2codingmodelstypicallyassumethatthelinearfactorshaveGaussiannoisewithisotropicprecision:βp,() = (;+xh|NxWhb1βI).(13.12)Thedistributionp(h)ischosentobeonewithsharppeaksnear0(OlshausenandField1996,).CommonchoicesincludefactorizedLaplace,CauchyorfactorizedStudent-tdistributions.Forexample,theLaplacepriorparametrizedintermsofthesparsitypenaltycoeﬃcientisgivenbyλph(i) = Laplace(hi;0,2λ) =λ4e−12λh|i|(13.13)andtheStudent-priorbytph(i) ∝1(1+h2iν)ν+12.(13.14)496 CHAPTER13.LINEARFACTORMODELSTrainingsparsecodingwithmaximumlikelihoodisintractable.Instead,thetrainingalternatesbetweenencodingthedataandtrainingthedecodertobetterreconstructthedatagiventheencoding.Thisapproachwillbejustiﬁedfurtherasaprincipledapproximationtomaximumlikelihoodlater,insection.19.3FormodelssuchasPCA,wehaveseentheuseofaparametricencoderfunctionthatpredictshandconsistsonlyofmultiplicationbyaweightmatrix.Theencoderthatweusewithsparsecodingisnotaparametricencoder.Instead,theencoderisanoptimizationalgorithm,thatsolvesanoptimizationprobleminwhichweseekthesinglemostlikelycodevalue:h∗= () = argmaxfxhp.()hx|(13.15)Whencombinedwithequationandequation,thisyieldsthefollowing13.1313.12optimizationproblem:argmaxhp()hx|(13.16)=argmaxhlog()phx|(13.17)=argminhλ||||h1+β||−||xWh22,(13.18)wherewehavedroppedtermsnotdependingonhanddividedbypositivescalingfactorstosimplifytheequation.DuetotheimpositionofanL1normonh,thisprocedurewillyieldasparseh∗(Seesection).7.1.2Totrainthemodelratherthanjustperforminference,wealternatebetweenminimizationwithrespecttohandminimizationwithrespecttoW.Inthispresentation,wetreatβasahyperparameter.Typicallyitissetto1becauseitsroleinthisoptimizationproblemissharedwithλandthereisnoneedforbothhyperparameters.Inprinciple,wecouldalsotreatβasaparameterofthemodelandlearnit.Ourpresentationherehasdiscardedsometermsthatdonotdependonhbutdodependonβ.Tolearnβ,thesetermsmustbeincluded,orβwillcollapseto.0Notallapproachestosparsecodingexplicitlybuildap(h)andap(xh|).Oftenwearejustinterestedinlearningadictionaryoffeatureswithactivationvaluesthatwilloftenbezerowhenextractedusingthisinferenceprocedure.IfwesamplehfromaLaplaceprior,itisinfactazeroprobabilityeventforanelementofhtoactuallybezero.Thegenerativemodelitselfisnotespeciallysparse,onlythefeatureextractoris.()describeapproximateGoodfellowetal.2013d497 CHAPTER13.LINEARFACTORMODELSinferenceinadiﬀerentmodelfamily,thespikeandslabsparsecodingmodel,forwhichsamplesfromthepriorusuallycontaintruezeros.Thesparsecodingapproachcombinedwiththeuseofthenon-parametricencodercaninprincipleminimizethecombinationofreconstructionerrorandlog-priorbetterthananyspeciﬁcparametricencoder.Anotheradvantageisthatthereisnogeneralizationerrortotheencoder.Aparametricencodermustlearnhowtomapxtohinawaythatgeneralizes.Forunusualxthatdonotresemblethetrainingdata,alearned,parametricencodermayfailtoﬁndanhthatresultsinaccuratereconstructionorasparsecode.Forthevastmajorityofformulationsofsparsecodingmodels,wheretheinferenceproblemisconvex,theoptimizationprocedurewillalwaysﬁndtheoptimalcode(unlessdegeneratecasessuchasreplicatedweightvectorsoccur).Obviously,thesparsityandreconstructioncostscanstillriseonunfamiliarpoints,butthisisduetogeneralizationerrorinthedecoderweights,ratherthangeneralizationerrorintheencoder.Thelackofgeneralizationerrorinsparsecoding’soptimization-basedencodingprocessmayresultinbettergeneralizationwhensparsecodingisusedasafeatureextractorforaclassiﬁerthanwhenaparametricfunctionisusedtopredictthecode.CoatesandNg2011()demonstratedthatsparsecodingfeaturesgeneralizebetterforobjectrecognitiontasksthanthefeaturesofarelatedmodelbasedonaparametricencoder,thelinear-sigmoidautoencoder.Inspiredbytheirwork,Goodfellowetal.()showedthatavariantofsparsecodinggeneralizesbetterthanotherfeature2013dextractorsintheregimewhereextremelyfewlabelsareavailable(twentyorfewerlabelsperclass).Theprimarydisadvantageofthenon-parametricencoderisthatitrequiresgreatertimetocomputehgivenxbecausethenon-parametricapproachrequiresrunninganiterativealgorithm.Theparametricautoencoderapproach,developedin chapter ,usesonly a ﬁxed number of layers, often only one.Another14disadvantageisthatitisnotstraight-forwardtoback-propagatethroughthenon-parametricencoder,whichmakesitdiﬃculttopretrainasparsecodingmodelwithanunsupervisedcriterionandthenﬁne-tuneitusingasupervisedcriterion.Modiﬁedversionsofsparsecodingthatpermitapproximatederivativesdoexistbutarenotwidelyused(,).BagnellandBradley2009Sparsecoding,likeotherlinearfactormodels,oftenproducespoorsamples,asshowninﬁgure.Thishappensevenwhenthemodelisabletoreconstruct13.2thedatawellandprovideusefulfeaturesforaclassiﬁer.Thereasonisthateachindividualfeaturemaybelearnedwell,butthefactorialprioronthehiddencoderesultsinthemodelincludingrandomsubsetsofallofthefeaturesineachgeneratedsample.Thismotivatesthedevelopmentofdeepermodelsthatcanimposeanon-498 CHAPTER13.LINEARFACTORMODELS Figure13.2: ExamplesamplesandweightsfromaspikeandslabsparsecodingmodeltrainedontheMNISTdataset.(Left)Thesamplesfromthemodeldonotresemblethetrainingexamples.Atﬁrstglance,onemightassumethemodelispoorlyﬁt.The(Right)weightvectorsofthemodelhavelearnedtorepresentpenstrokesandsometimescompletedigits.Themodelhasthuslearnedusefulfeatures.Theproblemisthatthefactorialprioroverfeaturesresultsinrandomsubsetsoffeaturesbeingcombined.FewsuchsubsetsareappropriatetoformarecognizableMNISTdigit.Thismotivatesthedevelopmentofgenerativemodelsthathavemorepowerfuldistributionsovertheirlatentcodes.FigurereproducedwithpermissionfromGoodfellow2013detal.().factorialdistributiononthedeepestcodelayer,aswellasthedevelopmentofmoresophisticatedshallowmodels.13.5ManifoldInterpretationofPCALinearfactormodelsincludingPCAandfactoranalysiscanbeinterpretedaslearningamanifold(,).WecanviewprobabilisticPCAasHintonetal.1997deﬁningathinpancake-shapedregionofhighprobability—aGaussiandistributionthatisverynarrowalongsomeaxes,justasapancakeisveryﬂatalongitsverticalaxis,butiselongatedalongotheraxes,justasapancakeiswidealongitshorizontalaxes. Thisisillustratedinﬁgure. PCAcanbeinterpretedasaligningthis13.3pancakewithalinearmanifoldinahigher-dimensionalspace.ThisinterpretationappliesnotjusttotraditionalPCAbutalsotoanylinearautoencoderthatlearnsmatricesWandVwiththegoalofmakingthereconstructionofxlieasclosetoxaspossible,LettheencoderbehxW= (f) = ()xµ−.(13.19)499 CHAPTER13.LINEARFACTORMODELSTheencodercomputesalow-dimensionalrepresentationofh.Withtheautoencoderview,wehaveadecodercomputingthereconstructionˆxhbVh= (g) = +.(13.20) Figure13.3:FlatGaussiancapturingprobabilityconcentrationnearalow-dimensionalmanifold.Theﬁgureshowstheupperhalfofthe“pancake”abovethe“manifoldplane”whichgoesthroughitsmiddle.Thevarianceinthedirectionorthogonaltothemanifoldisverysmall(arrowpointingoutofplane)andcanbeconsideredlike“noise,”whiletheothervariancesarelarge(arrowsintheplane)andcorrespondto“signal,”andacoordinatesystemforthereduced-dimensiondata.ThechoicesoflinearencoderanddecoderthatminimizereconstructionerrorE[||−xˆx||2](13.21)correspondtoV=W,µ=b=E[x]andthecolumnsofWformanorthonormalbasiswhichspansthesamesubspaceastheprincipaleigenvectorsofthecovariancematrixCxµxµ= [(E−)(−)].(13.22)InthecaseofPCA,thecolumnsofWaretheseeigenvectors,orderedbythemagnitudeofthecorrespondingeigenvalues(whichareallrealandnon-negative).OnecanalsoshowthateigenvalueλiofCcorrespondstothevarianceofxinthedirectionofeigenvectorv()i.Ifx∈RDandh∈Rdwithd<D,thenthe500 CHAPTER13.LINEARFACTORMODELSoptimalreconstructionerror(choosing,,andasabove)isµbVWmin[E||−xˆx||2] =Did=+1λi.(13.23)Hence,ifthecovariancehasrankd,theeigenvaluesλd+1toλDare0andrecon-structionerroris0.Furthermore,onecanalsoshowthattheabovesolutioncanbeobtainedbymaximizingthevariancesoftheelementsofh,underorthogonalW,insteadofminimizingreconstructionerror.Linearfactormodelsaresomeofthesimplestgenerativemodelsandsomeofthesimplestmodelsthatlearnarepresentationofdata.Muchaslinearclassiﬁersandlinearregressionmodelsmaybeextendedtodeepfeedforwardnetworks,theselinearfactormodelsmaybeextendedtoautoencodernetworksanddeepprobabilisticmodelsthatperformthesametasksbutwithamuchmorepowerfulandﬂexiblemodelfamily. 501 Chapter14AutoencodersAnautoencoderisaneuralnetworkthatistrainedtoattempttocopyitsinputtoitsoutput. Internally,ithasahiddenlayerhthatdescribesacodeusedtorepresenttheinput.Thenetworkmaybeviewedasconsistingoftwoparts:anencoderfunctionh=f(x)andadecoderthatproducesareconstructionr=g(h).Thisarchitectureispresentedinﬁgure.Ifanautoencodersucceedsinsimply14.1learningtosetg(f(x)) =xeverywhere,thenitisnotespeciallyuseful.Instead,autoencodersaredesignedtobeunabletolearntocopyperfectly.Usuallytheyarerestrictedinwaysthatallowthemtocopyonlyapproximately,andtocopyonlyinputthatresemblesthetrainingdata.Becausethemodelisforcedtoprioritizewhichaspectsoftheinputshouldbecopied,itoftenlearnsusefulpropertiesofthedata.Modern autoencoders havegeneralized the ideaof anencoder and ade-coderbeyonddeterministicfunctionstostochasticmappingspencoder(hx|)andpdecoder()xh|.Theideaofautoencodershasbeenpartofthehistoricallandscapeofneuralnetworksfordecades(,;,;,LeCun1987BourlardandKamp1988HintonandZemel1994).Traditionally, autoencoderswereused fordimensionalityreductionorfeaturelearning.Recently,theoreticalconnectionsbetweenautoencodersandlatentvariablemodelshavebroughtautoencoderstotheforefrontofgenerativemodeling,aswewillseeinchapter.Autoencodersmaybethoughtofasbeing20aspecialcaseoffeedforwardnetworks,andmaybetrainedwithallofthesametechniques,typicallyminibatchgradientdescentfollowinggradientscomputedbyback-propagation.Unlikegeneralfeedforwardnetworks,autoencodersmayalsobetrainedusingrecirculation(HintonandMcClelland1988,),alearningalgorithmbasedoncomparingtheactivationsofthenetworkontheoriginalinput502 CHAPTER14.AUTOENCODERStotheactivationsonthereconstructedinput.Recirculationisregardedasmorebiologicallyplausiblethanback-propagation,butisrarelyusedformachinelearningapplications. x xr rh hfgFigure14.1:Thegeneralstructureofanautoencoder,mappinganinputtoanoutputx(calledreconstruction)rthroughaninternalrepresentationorcodeh.Theautoencoderhastwocomponents:theencoderf(mappingxtoh)andthedecoderg(mappinghtor).14.1UndercompleteAutoencodersCopyingtheinputtotheoutputmaysounduseless,butwearetypicallynotinterestedintheoutputofthe decoder.Instead, wehope thattrainingtheautoencodertoperformtheinputcopyingtaskwillresultinhtakingonusefulproperties.Onewaytoobtainusefulfeaturesfromtheautoencoderistoconstrainhtohavesmallerdimensionthanx.Anautoencoderwhosecodedimensionislessthantheinputdimensioniscalledundercomplete.Learninganundercompleterepresentationforcestheautoencodertocapturethemostsalientfeaturesofthetrainingdata.ThelearningprocessisdescribedsimplyasminimizingalossfunctionL,gf(x(()))x(14.1)whereLisalossfunctionpenalizingg(f(x))forbeingdissimilarfromx,suchasthemeansquarederror.WhenthedecoderislinearandListhemeansquarederror,anundercompleteautoencoderlearnstospanthesamesubspaceasPCA.Inthiscase,anautoencodertrainedtoperformthecopyingtaskhaslearnedtheprincipalsubspaceofthetrainingdataasaside-eﬀect.Autoencoderswithnonlinearencoderfunctionsfandnonlineardecoderfunc-tionsgcanthuslearnamorepowerfulnonlineargeneralizationofPCA.Unfortu-503 CHAPTER14.AUTOENCODERSnately,iftheencoderanddecoderareallowedtoomuchcapacity,theautoencodercanlearntoperformthecopyingtaskwithoutextractingusefulinformationaboutthedistributionofthedata.Theoretically,onecouldimaginethatanautoencoderwithaone-dimensionalcodebutaverypowerfulnonlinearencodercouldlearntorepresenteachtrainingexamplex()iwiththecodei.Thedecodercouldlearntomaptheseintegerindicesbacktothevaluesofspeciﬁctrainingexamples.Thisspeciﬁcscenariodoesnotoccurinpractice,butitillustratesclearlythatanautoen-codertrainedtoperformthecopyingtaskcanfailtolearnanythingusefulaboutthedatasetifthecapacityoftheautoencoderisallowedtobecometoogreat.14.2RegularizedAutoencodersUndercompleteautoencoders,withcodedimensionlessthantheinputdimension,canlearnthemostsalientfeaturesofthedatadistribution.Wehaveseenthattheseautoencodersfailtolearnanythingusefuliftheencoderanddecoderaregiventoomuchcapacity.Asimilarproblemoccursifthehiddencodeisallowedtohavedimensionequaltotheinput,andintheovercompletecaseinwhichthehiddencodehasdimensiongreaterthantheinput.Inthesecases,evenalinearencoderandlineardecodercanlearntocopytheinputtotheoutputwithoutlearninganythingusefulaboutthedatadistribution.Ideally,onecouldtrainanyarchitectureofautoencodersuccessfully,choosingthecodedimensionandthecapacityoftheencoderanddecoderbasedonthecomplexityofdistributiontobemodeled.Regularizedautoencodersprovidetheabilitytodoso.Ratherthanlimitingthemodelcapacitybykeepingtheencoderanddecodershallowandthecodesizesmall,regularizedautoencodersusealossfunctionthatencouragesthemodeltohaveotherpropertiesbesidestheabilitytocopyitsinputtoitsoutput.Theseotherpropertiesincludesparsityoftherepresentation,smallnessofthederivativeoftherepresentation,androbustnesstonoiseortomissinginputs.Aregularizedautoencodercanbenonlinearandovercompletebutstilllearnsomethingusefulaboutthedatadistributionevenifthemodelcapacityisgreatenoughtolearnatrivialidentityfunction.Inadditiontothemethodsdescribedherewhicharemostnaturallyinterpretedasregularizedautoencoders,nearlyanygenerativemodelwithlatentvariablesandequippedwithaninferenceprocedure(forcomputinglatentrepresentationsgiveninput)maybeviewedasaparticularformofautoencoder.TwogenerativemodelingapproachesthatemphasizethisconnectionwithautoencodersarethedescendantsoftheHelmholtzmachine(,),suchasthevariationalHintonetal.1995b504 CHAPTER14.AUTOENCODERSautoencoder(section)andthegenerativestochasticnetworks(section).20.10.320.12Thesemodelsnaturallylearnhigh-capacity,overcompleteencodingsoftheinputanddonotrequireregularizationfortheseencodingstobeuseful.Theirencodingsarenaturallyusefulbecausethemodelsweretrainedtoapproximatelymaximizetheprobabilityofthetrainingdataratherthantocopytheinputtotheoutput.14.2.1SparseAutoencodersAsparseautoencoderissimplyanautoencoderwhosetrainingcriterioninvolvesasparsitypenaltyΩ(h)onthecodelayerh,inadditiontothereconstructionerror:L,gf(x(()))+Ω()xh(14.2)whereg(h)isthedecoderoutputandtypicallywehaveh=f(x),theencoderoutput.Sparseautoencodersaretypicallyusedtolearnfeaturesforanothertasksuchasclassiﬁcation.Anautoencoderthathasbeenregularizedtobesparsemustrespondtouniquestatisticalfeaturesofthedatasetithasbeentrainedon,ratherthansimplyactingasanidentityfunction.Inthisway,trainingtoperformthecopyingtaskwithasparsitypenaltycanyieldamodelthathaslearnedusefulfeaturesasabyproduct.Wecanthink ofthepenaltyΩ(h)simplyasaregularizertermaddedtoafeedforwardnetworkwhoseprimarytaskistocopytheinputtotheoutput(unsupervisedlearningobjective)andpossiblyalsoperformsomesupervisedtask(with asupervised learning objective) thatdepends on thesesparsefeatures.Unlikeotherregularizerssuchasweightdecay,thereisnotastraightforwardBayesianinterpretationtothisregularizer.Asdescribedinsection,training5.6.1withweightdecayandotherregularizationpenaltiescanbeinterpretedasaMAPapproximationtoBayesianinference,withtheaddedregularizingpenaltycorrespondingtoapriorprobabilitydistributionoverthemodelparameters.Inthisview,regularizedmaximumlikelihoodcorrespondstomaximizingp(θx|),whichisequivalenttomaximizinglogp(xθ|)+logp(θ). Thelogp(xθ|)termistheusualdatalog-likelihoodtermandthelogp(θ)term,thelog-prioroverparameters,incorporatesthepreferenceoverparticularvaluesofθ.Thisviewwasdescribedinsection.Regularizedautoencodersdefysuchaninterpretation5.6becausetheregularizerdependsonthedataandisthereforebydeﬁnitionnotapriorintheformalsenseoftheword.Wecanstillthinkoftheseregularizationtermsasimplicitlyexpressingapreferenceoverfunctions.Ratherthanthinkingofthesparsitypenaltyasaregularizerforthecopyingtask,wecanthinkoftheentiresparseautoencoderframeworkasapproximating505 CHAPTER14.AUTOENCODERSmaximumlikelihood trainingofagenerativemodel thathaslatentvariables.Supposewehaveamodelwithvisiblevariablesxandlatentvariablesh,withanexplicitjointdistributionpmodel(xh,)=pmodel(h)pmodel(xh|).Werefertopmodel(h)asthemodel’spriordistributionoverthelatentvariables,representingthemodel’sbeliefspriortoseeingx.Thisisdiﬀerentfromthewaywehavepreviouslyusedtheword“prior,”torefertothedistributionp(θ)encodingourbeliefsaboutthemodel’sparametersbeforewehaveseenthetrainingdata.Thelog-likelihoodcanbedecomposedaslogpmodel() = logxhpmodel()hx,.(14.3)Wecanthinkoftheautoencoderasapproximatingthissumwithapointestimateforjustonehighlylikelyvalueforh.Thisissimilartothesparsecodinggenerativemodel(section),butwith13.4hbeingtheoutputoftheparametricencoderratherthantheresultofanoptimizationthatinfersthemostlikelyh.Fromthispointofview,withthischosen,wearemaximizinghlogpmodel() = loghx,pmodel()+loghpmodel()xh|.(14.4)Thelogpmodel()htermcanbesparsity-inducing.Forexample,theLaplaceprior,pmodel(hi) =λ2e−|λhi|,(14.5)correspondstoanabsolutevaluesparsitypenalty.Expressingthelog-priorasanabsolutevaluepenalty,weobtainΩ() = hλi|hi|(14.6)−logpmodel() =hiλh|i|−logλ2= Ω()+consth(14.7)wheretheconstanttermdependsonlyonλandnoth.Wetypicallytreatλasahyperparameteranddiscardtheconstanttermsinceitdoesnotaﬀecttheparameterlearning.OtherpriorssuchastheStudent-tpriorcanalsoinducesparsity.Fromthispointofviewofsparsityasresultingfromtheeﬀectofpmodel(h)onapproximatemaximumlikelihoodlearning,thesparsitypenaltyisnotaregularizationtermatall. Itisjustaconsequenceofthemodel’sdistributionoveritslatentvariables.Thisviewprovidesadiﬀerentmotivationfortraininganautoencoder:itisawayofapproximatelytrainingagenerativemodel.Italsoprovidesadiﬀerentreasonfor506 CHAPTER14.AUTOENCODERSwhythefeatureslearnedbytheautoencoderareuseful:theydescribethelatentvariablesthatexplaintheinput.Earlyworkonsparseautoencoders(,,)exploredRanzatoetal.2007a2008variousformsofsparsityandproposedaconnectionbetweenthesparsitypenaltyandthelogZtermthatariseswhenapplyingmaximumlikelihoodtoanundirectedprobabilisticmodelp(x) =1Z˜p(x).TheideaisthatminimizinglogZpreventsaprobabilisticmodelfromhavinghighprobabilityeverywhere,andimposingsparsityon anautoencoder preventstheautoencoderfrom having lowreconstructionerroreverywhere.Inthiscase, theconnectionisonthelevelofanintuitiveunderstandingofageneralmechanismratherthanamathematicalcorrespondence.Theinterpretationofthesparsitypenaltyascorrespondingtologpmodel(h)inadirectedmodelpmodel()hpmodel()xh|ismoremathematicallystraightforward.Onewaytoachieveactualzerosinhforsparse(anddenoising)autoencoderswasintroducedin().TheideaistouserectiﬁedlinearunitstoGlorotetal.2011bproducethecodelayer.Withapriorthatactuallypushestherepresentationstozero(liketheabsolutevaluepenalty),onecanthusindirectlycontroltheaveragenumberofzerosintherepresentation.14.2.2DenoisingAutoencodersRatherthanaddingapenaltytothecostfunction,wecanobtainanautoencoderΩ thatlearnssomethingusefulbychangingthereconstructionerrortermofthecostfunction.Traditionally,autoencodersminimizesomefunctionL,gf(x(()))x(14.8)whereLisalossfunctionpenalizingg(f(x))forbeingdissimilarfromx,suchastheL2normoftheirdiﬀerence. Thisencouragesgf◦tolearntobemerelyanidentityfunctioniftheyhavethecapacitytodoso.AorDAEinsteadminimizesdenoisingautoencoderL,gf(x((˜x))),(14.9)where˜xisacopyofxthathasbeencorruptedbysomeformofnoise.Denoisingautoencodersmustthereforeundothiscorruptionratherthansimplycopyingtheirinput.Denoisingtrainingforcesfandgtoimplicitlylearnthestructureofpdata(x),asshown by () and().DenoisingAlainandBengio2013Bengio etal.2013c507 CHAPTER14.AUTOENCODERSautoencodersthusprovideyetanotherexampleofhowusefulpropertiescanemergeasabyproductofminimizingreconstructionerror.Theyarealsoanexampleofhowovercomplete,high-capacitymodelsmaybeusedasautoencoderssolongascareistakentopreventthemfromlearningtheidentityfunction. Denoisingautoencodersarepresentedinmoredetailinsection.14.514.2.3RegularizingbyPenalizingDerivativesAnotherstrategyforregularizinganautoencoderistouseapenaltyasinsparseΩautoencoders,L,gf,,(x(()))+Ω(xhx)(14.10)butwithadiﬀerentformof:ΩΩ() = hx,λi||∇xhi||2.(14.11)Thisforcesthemodeltolearnafunctionthatdoesnotchangemuchwhenxchangesslightly.Becausethispenaltyisappliedonlyattrainingexamples,itforcestheautoencodertolearnfeaturesthatcaptureinformationaboutthetrainingdistribution.AnautoencoderregularizedinthiswayiscalledacontractiveautoencoderorCAE.Thisapproachhastheoreticalconnectionstodenoisingautoencoders,manifoldlearningandprobabilisticmodeling.TheCAEisdescribedinmoredetailinsection.14.714.3RepresentationalPower,LayerSizeandDepthAutoencodersareoftentrainedwithonlyasinglelayerencoderandasinglelayerdecoder.However,thisisnotarequirement.Infact,usingdeepencodersanddecodersoﬀersmanyadvantages.Recallfromsectionthattherearemanyadvantagestodepthinafeedfor-6.4.1wardnetwork.Becauseautoencodersarefeedforwardnetworks,theseadvantagesalsoapplytoautoencoders.Moreover,theencoderisitselfafeedforwardnetworkasisthedecoder,soeachofthesecomponentsoftheautoencodercanindividuallybeneﬁtfromdepth.Onemajoradvantageofnon-trivialdepthisthattheuniversalapproximatortheoremguaranteesthatafeedforwardneuralnetworkwithatleastonehiddenlayercanrepresentanapproximationofanyfunction(withinabroadclass)toan508 CHAPTER14.AUTOENCODERSarbitrarydegreeofaccuracy,providedthatithasenoughhiddenunits.Thismeansthatanautoencoderwithasinglehiddenlayerisabletorepresenttheidentityfunctionalongthedomainofthedataarbitrarilywell.However,themappingfrominputtocodeisshallow.Thismeansthatwearenotabletoenforcearbitraryconstraints,suchasthatthecodeshouldbesparse.Adeepautoencoder,withatleastoneadditionalhiddenlayerinsidetheencoderitself,canapproximateanymappingfrominputtocodearbitrarilywell,givenenoughhiddenunits.Depthcanexponentiallyreducethecomputationalcostofrepresentingsomefunctions.Depthcanalsoexponentiallydecreasetheamountoftrainingdataneededtolearnsomefunctions.Seesectionforareviewoftheadvantagesof6.4.1depthinfeedforwardnetworks.Experimentally,deepautoencodersyieldmuchbettercompressionthancorre-spondingshalloworlinearautoencoders(HintonandSalakhutdinov2006,).Acommonstrategyfortrainingadeepautoencoderistogreedilypretrainthedeeparchitecturebytrainingastackofshallowautoencoders,soweoftenencountershallowautoencoders,evenwhentheultimategoalistotrainadeepautoencoder.14.4StochasticEncodersandDecodersAutoencodersarejustfeedforwardnetworks.Thesamelossfunctionsandoutputunittypesthatcanbeusedfortraditionalfeedforwardnetworksarealsousedforautoencoders.Asdescribedinsection,ageneralstrategyfordesigningtheoutputunits6.2.2.4andthelossfunctionofafeedforwardnetworkistodeﬁneanoutputdistributionp(yx|)andminimizethenegativelog-likelihood−logp(yx|).Inthatsetting,ywasavectoroftargets,suchasclasslabels.Inthecaseofanautoencoder,xisnowthetargetaswellastheinput.However,wecanstillapplythesamemachineryasbefore.Givenahiddencodeh,wemaythinkofthedecoderasprovidingaconditionaldistributionpdecoder(xh|). Wemaythentraintheautoencoderbyminimizing−logpdecoder()xh|.Theexactformofthislossfunctionwillchangedependingontheformofpdecoder.Aswithtraditionalfeedforwardnetworks,weusuallyuselinearoutputunitstoparametrizethemeanofaGaussiandistributionifxisreal-valued.Inthatcase,thenegativelog-likelihoodyieldsameansquarederrorcriterion.Similarly,binaryxvaluescorrespondtoaBernoullidistributionwhoseparametersaregivenbyasigmoidoutputunit,discretexvaluescorrespondtoasoftmaxdistribution,andsoon.509 CHAPTER14.AUTOENCODERSTypically,theoutputvariablesaretreatedasbeingconditionallyindependentgivenhsothatthisprobabilitydistributionisinexpensivetoevaluate,butsometechniquessuchasmixturedensityoutputsallowtractablemodelingofoutputswithcorrelations. x xr rh hpencoder()hx|pdecoder()xh|Figure14.2:Thestructureofastochasticautoencoder,inwhichboththeencoderandthedecoderarenotsimplefunctionsbutinsteadinvolvesomenoiseinjection,meaningthattheiroutputcanbeseenassampledfromadistribution,pencoder(hx|)fortheencoderandpdecoder()xh|forthedecoder.Tomakeamoreradicaldeparturefromthefeedforwardnetworkswehaveseenpreviously,wecanalsogeneralizethenotionofanencodingfunctionf(x)toanencodingdistributionpencoder()hx|,asillustratedinﬁgure.14.2Anylatentvariablemodelpmodel()hx,deﬁnesastochasticencoderpencoder() = hx|pmodel()hx|(14.12)andastochasticdecoderpdecoder() = xh|pmodel()xh|.(14.13)Ingeneral,theencoderanddecoderdistributionsarenotnecessarilyconditionaldistributionscompatiblewithauniquejointdistributionpmodel(xh,).Alainetal.()showedthattrainingtheencoderanddecoderasadenoisingautoencoder2015willtendtomakethemcompatibleasymptotically(withenoughcapacityandexamples).14.5DenoisingAutoencodersThedenoisingautoencoder(DAE)isanautoencoderthatreceivesacorrupteddatapointasinputandistrainedtopredicttheoriginal,uncorrupteddatapointasitsoutput.TheDAEtrainingprocedureisillustratedinﬁgure.Weintroducea14.3corruptionprocessC(˜xx|)whichrepresentsaconditional distributionover510 CHAPTER14.AUTOENCODERS ˜x˜xLLh hfg x xC(˜xx|)Figure14.3:Thecomputationalgraphofthecostfunctionforadenoisingautoencoder,whichistrainedtoreconstructthecleandatapointxfromitscorruptedversion˜x.ThisisaccomplishedbyminimizingthelossL=−logpdecoder(xh|=f(˜x)),where˜xisacorruptedversionofthedataexamplex,obtainedthroughagivencorruptionprocessC(˜xx|).Typicallythedistributionpdecoderisafactorialdistributionwhosemeanparametersareemittedbyafeedforwardnetwork.gcorruptedsamples˜x,givenadatasamplex.Theautoencoderthenlearnsareconstructiondistributionpreconstruct(x|˜x)estimatedfromtrainingpairs(x,˜x),asfollows:1. Sampleatrainingexamplefromthetrainingdata.x2. Sampleacorruptedversion˜xfromC(˜xx|= )x.3.Use(x,˜x)asatrainingexampleforestimatingtheautoencoderreconstructiondistributionpreconstruct(x|˜x) =pdecoder(xh|)withhtheoutputofencoderf(˜x)andpdecodertypicallydeﬁnedbyadecoder.g()hTypicallywecansimplyperformgradient-basedapproximateminimization(suchasminibatchgradientdescent)onthenegativelog-likelihood−logpdecoder(xh|).Solongastheencoderisdeterministic,thedenoisingautoencoderisafeedforwardnetwork andmay be trainedwith exactlythesame techniques as anyotherfeedforwardnetwork.WecanthereforeviewtheDAEasperformingstochasticgradientdescentonthefollowingexpectation:−Ex∼ˆpdata()xE˜x∼C(˜x|x)logpdecoder(= (xh|f˜x))(14.14)whereˆpdata()xisthetrainingdistribution.511 CHAPTER14.AUTOENCODERS x˜xgf◦˜xC(˜xx|)xFigure14.4:Adenoisingautoencoderistrainedtomapacorrupteddatapoint˜xbacktotheoriginaldatapointx.Weillustratetrainingexamplesxasredcrosseslyingnearalow-dimensionalmanifoldillustratedwiththeboldblackline.WeillustratethecorruptionprocessC(˜xx|) withagraycircleofequiprobablecorruptions.Agrayarrowdemonstrateshowonetrainingexampleistransformedintoonesamplefromthiscorruptionprocess.Whenthedenoisingautoencoderistrainedtominimizetheaverageofsquarederrors||g(f(˜x))−||x2,thereconstructiong(f(˜x)) estimatesEx,˜x∼pdata()(xC˜xx|)[x|˜x].Thevectorg(f(˜x))−˜xpointsapproximatelytowardsthenearestpointonthemanifold,sinceg(f(˜x))estimatesthecenterofmassofthecleanpointsxwhichcouldhavegivenriseto˜x.Theautoencoderthuslearnsavectorﬁeldg(f(x))−xindicatedbythegreenarrows.Thisvectorﬁeldestimatesthescore∇xlogpdata(x)uptoamultiplicativefactorthatistheaveragerootmeansquarereconstructionerror. 512 CHAPTER14.AUTOENCODERS14.5.1EstimatingtheScoreScorematching(,)isanalternativetomaximumlikelihood.ItHyvärinen2005providesaconsistentestimatorofprobabilitydistributionsbasedonencouragingthemodeltohavethesamescoreasthedatadistributionateverytrainingpointx.Inthiscontext,thescoreisaparticulargradientﬁeld:∇xlog()px.(14.15)Scorematchingisdiscussedfurtherinsection.Forthepresentdiscussion18.4regardingautoencoders,itissuﬃcienttounderstandthatlearningthegradientﬁeldoflogpdataisonewaytolearnthestructureofpdataitself.AveryimportantpropertyofDAEsisthat theirtrainingcriterion(withconditionallyGaussianp(xh|))makes theautoencoder learnavectorﬁeld(g(f(x))−x)thatestimatesthescoreofthedatadistribution.Thisisillustratedinﬁgure.14.4Denoisingtrainingofaspeciﬁckindofautoencoder(sigmoidalhiddenunits,linear reconstruction units)usingGaussiannoiseand meansquared errorasthereconstructioncostisequivalent(,)totrainingaspeciﬁckindVincent2011ofundirectedprobabilisticmodelcalledanRBMwithGaussianvisibleunits.Thiskindofmodelwillbedescribedindetailinsection;forthepresent20.5.1discussionitsuﬃcestoknowthatitisamodelthatprovidesanexplicitpmodel(x;θ).WhentheRBMistrainedusingdenoisingscorematching(,KingmaandLeCun2010),itslearningalgorithmisequivalenttodenoisingtraininginthecorrespondingautoencoder.Withaﬁxednoiselevel,regularizedscorematchingisnotaconsistentestimator;itinsteadrecoversablurredversionofthedistribution.However,ifthenoiselevelischosentoapproach0whenthenumberofexamplesapproachesinﬁnity,thenconsistencyisrecovered.Denoisingscorematchingisdiscussedinmoredetailinsection.18.5OtherconnectionsbetweenautoencodersandRBMsexist.ScorematchingappliedtoRBMsyieldsacostfunctionthatisidenticaltoreconstructionerrorcombinedwitharegularizationtermsimilartothecontractivepenaltyoftheCAE(Swersky2011BengioandDelalleau2009etal.,).()showedthatanautoen-codergradientprovidesanapproximationtocontrastivedivergencetrainingofRBMs.Forcontinuous-valuedx,thedenoisingcriterionwithGaussiancorruptionandreconstructiondistributionyieldsanestimatorofthescorethatisapplicabletogeneralencoderanddecoderparametrizations(,).ThisAlainandBengio2013meansagenericencoder-decoderarchitecturemaybemadetoestimatethescore513 CHAPTER14.AUTOENCODERSbytrainingwiththesquarederrorcriterion||gf((˜xx))−||2(14.16)andcorruptionC(˜x=˜xx|) = (N˜xx;= µ,σΣ = 2I)(14.17)withnoisevarianceσ2.Seeﬁgureforanillustrationofhowthisworks.14.5 Figure14.5:Vectorﬁeldlearnedbyadenoisingautoencoderarounda1-Dcurvedmanifoldnearwhichthedataconcentratesina2-Dspace.Eacharrowisproportionaltothereconstructionminusinputvectoroftheautoencoderandpointstowardshigherprobabilityaccordingtotheimplicitlyestimatedprobabilitydistribution.Thevectorﬁeldhaszerosatbothmaximaoftheestimateddensityfunction(onthedatamanifolds)andatminimaofthatdensityfunction.Forexample,thespiralarmformsaone-dimensionalmanifoldoflocalmaximathatareconnectedtoeachother.Localminimaappearnearthemiddleofthegapbetweentwoarms.Whenthenormofreconstructionerror(shownbythelengthofthearrows)islarge,itmeansthatprobabilitycanbesigniﬁcantlyincreasedbymovinginthedirectionofthearrow,andthatismostlythecaseinplacesoflowprobability.Theautoencodermapstheselowprobabilitypointstohigherprobabilityreconstructions.Whereprobabilityismaximal,thearrowsshrinkbecausethereconstructionbecomesmoreaccurate.Figurereproducedwithpermissionfrom().AlainandBengio2013Ingeneral,thereisnoguaranteethatthereconstructiong(f(x))minustheinputxcorrespondstothegradientofanyfunction,letalonetothescore.Thatis514 CHAPTER14.AUTOENCODERSwhytheearlyresults(,)arespecializedtoparticularparametrizationsVincent2011whereg(f(x))−xmaybeobtainedbytakingthederivativeofanotherfunction.KamyshanskaandMemisevic2015Vincent2011()generalizedtheresultsof()byidentifyingafamilyofshallowautoencoderssuchthatg(f(x))−xcorrespondstoascoreforallmembersofthefamily.Sofarwehavedescribedonlyhowthedenoisingautoencoderlearnstorepresentaprobabilitydistribution.Moregenerally,onemaywanttousetheautoencoderasagenerativemodelanddrawsamplesfromthisdistribution.Thiswillbedescribedlater,insection.20.1114.5.1.1HistoricalPerspectiveTheideaofusingMLPsfordenoisingdatesbacktotheworkof()LeCun1987and().()alsousedrecurrentnetworkstodenoiseGallinarietal.1987Behnke2001images.Denoisingautoencodersare,insomesense,justMLPstrainedtodenoise.However,thename“denoisingautoencoder”referstoamodelthatisintendednotmerelytolearntodenoiseitsinputbuttolearnagoodinternalrepresentationas asideeﬀect oflearningto denoise.This ideacame muchlater (Vincentetal.,,).Thelearnedrepresentationmaythenbeusedtopretraina20082010deeperunsupervisednetworkorasupervisednetwork.Likesparseautoencoders,sparsecoding,contractiveautoencodersandotherregularizedautoencoders,themotivationforDAEswastoallowthelearningofaveryhigh-capacityencoderwhilepreventingtheencoderanddecoderfromlearningauselessidentityfunction.PriortotheintroductionofthemodernDAE,InayoshiandKurita2005()exploredsomeofthesamegoalswithsomeofthesamemethods.TheirapproachminimizesreconstructionerrorinadditiontoasupervisedobjectivewhileinjectingnoiseinthehiddenlayerofasupervisedMLP,withtheobjectivetoimprovegeneralizationbyintroducing thereconstructionerror andtheinjectednoise.However,theirmethodwasbasedonalinearencoderandcouldnotlearnfunctionfamiliesaspowerfulascanthemodernDAE.14.6LearningManifoldswithAutoencodersLike many other machine learning algorithms, autoencoders exploittheideathatdataconcentratesaroundalow-dimensionalmanifoldorasmallsetofsuchmanifolds,asdescribedinsection.Somemachinelearningalgorithmsexploit5.11.3thisideaonlyinsofarasthattheylearnafunctionthatbehavescorrectlyonthemanifoldbutmayhaveunusualbehaviorifgivenaninputthatisoﬀthemanifold.515 CHAPTER14.AUTOENCODERSAutoencoderstakethisideafurtherandaimtolearnthestructureofthemanifold.Tounderstandhowautoencodersdothis,wemustpresentsomeimportantcharacteristicsofmanifolds.Animportantcharacterizationofamanifoldisthesetofitstangentplanes.Atapointxonad-dimensionalmanifold,thetangentplaneisgivenbydbasisvectorsthatspanthelocaldirectionsofvariationallowedonthemanifold.Asillustratedinﬁgure,theselocaldirectionsspecifyhowonecanchange14.6xinﬁnitesimallywhilestayingonthemanifold.Allautoencodertrainingproceduresinvolveacompromisebetweentwoforces:1.Learningarepresentationhofatrainingexamplexsuchthatxcanbeapproximatelyrecoveredfromhthroughadecoder.Thefactthatxisdrawnfromthetrainingdataiscrucial,becauseitmeanstheautoencoderneednotsuccessfullyreconstructinputsthatarenotprobableunderthedatageneratingdistribution.2. Satisfyingtheconstraintorregularizationpenalty.Thiscanbeanarchitec-turalconstraintthatlimitsthecapacityoftheautoencoder,oritcanbearegularizationtermaddedtothereconstructioncost.Thesetechniquesgenerallyprefersolutionsthatarelesssensitivetotheinput.Clearly,neitherforcealonewouldbeuseful—copyingtheinputtotheoutputisnotusefulonitsown,norisignoringtheinput.Instead,thetwoforcestogetherareusefulbecausetheyforcethehiddenrepresentationtocaptureinformationaboutthestructureofthedatageneratingdistribution.Theimportantprincipleisthattheautoencodercanaﬀordtorepresentonlythevariationsthatareneededtoreconstructtrainingexamples.Ifthedatageneratingdistributionconcentratesnearalow-dimensionalmanifold,thisyieldsrepresentationsthatimplicitlycapturealocalcoordinatesystemforthismanifold:onlythevariationstangenttothemanifoldaroundxneedtocorrespondtochangesinh=f(x).Hencetheencoderlearnsamappingfromtheinputspacextoarepresentationspace,amappingthatisonlysensitivetochangesalongthemanifolddirections,butthatisinsensitivetochangesorthogonaltothemanifold.Aone-dimensionalexampleisillustratedinﬁgure,showingthat,bymaking14.7thereconstructionfunctioninsensitivetoperturbationsoftheinputaroundthedatapoints,wecausetheautoencodertorecoverthemanifoldstructure.Tounderstandwhyautoencodersareusefulformanifoldlearning,itisin-structivetocomparethemtootherapproaches.Whatismostcommonlylearnedtocharacterizeamanifoldisarepresentationofthedatapointson(ornear)516 CHAPTER14.AUTOENCODERS Figure14.6: Anillustrationoftheconceptofatangenthyperplane.Herewecreateaone-dimensionalmanifoldin784-dimensionalspace.WetakeanMNISTimagewith784pixelsandtransformitbytranslatingitvertically. Theamountofverticaltranslationdeﬁnesacoordinatealongaone-dimensionalmanifoldthattracesoutacurvedpaththroughimagespace.Thisplotshowsafewpointsalongthismanifold. Forvisualization,wehaveprojectedthemanifoldintotwodimensionalspaceusingPCA.Ann-dimensionalmanifoldhasann-dimensionaltangentplaneateverypoint.Thistangentplanetouchesthemanifoldexactlyatthatpointandisorientedparalleltothesurfaceatthatpoint.Itdeﬁnesthespaceofdirectionsinwhichitispossibletomovewhileremainingonthemanifold.Thisone-dimensionalmanifoldhasasingletangentline.Weindicateanexampletangentlineatonepoint,withanimageshowinghowthistangentdirectionappearsinimagespace.Graypixelsindicatepixelsthatdonotchangeaswemovealongthetangentline,whitepixelsindicatepixelsthatbrighten,andblackpixelsindicatepixelsthatdarken.517 CHAPTER14.AUTOENCODERS x0x1x2x00.02.04.06.08.10.rx()IdentityOptimalreconstruction Figure14.7:Iftheautoencoderlearnsareconstructionfunctionthatisinvarianttosmallperturbationsnearthedatapoints,itcapturesthemanifoldstructureofthedata.Herethemanifoldstructureisacollectionof-dimensionalmanifolds.Thedasheddiagonal0lineindicatestheidentityfunctiontargetforreconstruction.Theoptimalreconstructionfunctioncrossestheidentityfunctionwhereverthereisadatapoint.Thehorizontalarrowsatthebottomoftheplotindicatether(x)−xreconstructiondirectionvectoratthebaseofthearrow,ininputspace,alwayspointingtowardsthenearest“manifold”(asingledatapoint,inthe1-Dcase).Thedenoisingautoencoderexplicitlytriestomakethederivativeofthereconstructionfunctionr(x)smallaroundthedatapoints.Thecontractiveautoencoderdoesthesamefortheencoder.Althoughthederivativeofr(x)isaskedtobesmallaroundthedatapoints,itcanbelargebetweenthedatapoints.Thespacebetweenthedatapointscorrespondstotheregionbetweenthemanifolds,wherethereconstructionfunctionmusthavealargederivativeinordertomapcorruptedpointsbackontothemanifold.themanifold.Sucharepresentationforaparticularexampleisalsocalleditsembedding.Itistypicallygivenbyalow-dimensionalvector,withlessdimensionsthanthe“ambient”spaceofwhichthemanifoldisalow-dimensionalsubset.Somealgorithms(non-parametricmanifoldlearningalgorithms,discussedbelow)directlylearnanembeddingforeachtrainingexample,whileotherslearnamoregeneralmapping,sometimescalledanencoder,orrepresentationfunction,thatmapsanypointintheambientspace(theinputspace)toitsembedding.Manifoldlearninghasmostlyfocusedonunsupervisedlearningproceduresthatattempttocapturethesemanifolds.Mostoftheinitialmachinelearningresearchonlearningnonlinearmanifoldshasfocusedonnon-parametricmethodsbasedonthenearest-neighborgraph.Thisgraphhasonenodepertrainingexampleandedgesconnectingnearneighborstoeachother.Thesemethods(Schölkopfetal.,;1998RoweisandSaul2000Tenenbaum2000Brand2003Belkin,;etal.,;,;518 CHAPTER14.AUTOENCODERS Figure14.8:Non-parametricmanifoldlearningproceduresbuildanearestneighborgraphinwhichnodesrepresenttrainingexamplesadirectededgesindicatenearestneighborrelationships. Variousprocedurescanthusobtainthetangentplaneassociatedwithaneighborhoodofthegraphaswellasacoordinatesystemthatassociateseachtrainingexamplewithareal-valuedvectorposition,orembedding.Itispossibletogeneralizesucharepresentationtonewexamplesbyaformofinterpolation.Solongasthenumberofexamplesislargeenoughtocoverthecurvatureandtwistsofthemanifold,theseapproachesworkwell.ImagesfromtheQMULMultiviewFaceDataset(,Gongetal.2000).andNiyogi2003DonohoandGrimes2003WeinbergerandSaul2004Hinton,;,;,;andRoweis2003vanderMaatenandHinton2008,;,)associateeachofnodeswithatangentplanethatspansthedirectionsofvariationsassociatedwiththediﬀerencevectorsbetweentheexampleanditsneighbors,asillustratedinﬁgure.14.8Aglobalcoordinatesystemcanthenbeobtainedthroughanoptimizationorsolvingalinearsystem.Figureillustrateshowamanifoldcanbetiledbya14.9largenumberoflocallylinearGaussian-likepatches(or“pancakes,”becausetheGaussiansareﬂatinthetangentdirections).However,thereisafundamentaldiﬃcultywithsuchlocalnon-parametricapproachestomanifoldlearning,raisedin():iftheBengioandMonperrus2005manifoldsarenotverysmooth(theyhavemanypeaksandtroughsandtwists),onemayneedaverylargenumberoftrainingexamplestocovereachoneof519 CHAPTER14.AUTOENCODERS Figure14.9:Ifthetangentplanes(seeﬁgure)ateachlocationareknown,thenthey14.6canbetiledtoformaglobalcoordinatesystemoradensityfunction.EachlocalpatchcanbethoughtofasalocalEuclideancoordinatesystemorasalocallyﬂatGaussian,or“pancake,”withaverysmallvarianceinthedirectionsorthogonaltothepancakeandaverylargevarianceinthedirectionsdeﬁningthecoordinatesystemonthepancake.AmixtureoftheseGaussiansprovidesanestimateddensityfunction,asinthemanifoldParzenwindowalgorithm(,)oritsnon-localneural-netbasedVincentandBengio2003variant(,).Bengioetal.2006cthesevariations,withnochancetogeneralizetounseenvariations.Indeed,thesemethodscanonlygeneralizetheshapeofthemanifoldbyinterpolatingbetweenneighboringexamples.Unfortunately,themanifoldsinvolvedinAIproblemscanhaveverycomplicatedstructurethatcanbediﬃculttocapturefromonlylocalinterpolation.Considerforexamplethemanifoldresultingfromtranslationshowninﬁgure.Ifwewatchjustonecoordinatewithintheinputvector,14.6xi,astheimageistranslated,wewillobservethatonecoordinateencountersapeakoratroughinitsvalueonceforeverypeakortroughinbrightnessintheimage. Inotherwords,thecomplexityofthepatternsofbrightnessinanunderlyingimagetemplatedrivesthecomplexityofthemanifoldsthataregeneratedbyperformingsimpleimagetransformations.Thismotivatestheuseofdistributedrepresentationsanddeeplearningforcapturingmanifoldstructure.520 CHAPTER14.AUTOENCODERS14.7ContractiveAutoencodersThecontractiveautoencoder(,,)introducesanexplicitregularizerRifaietal.2011abonthecodeh=f(x),encouragingthederivativesofftobeassmallaspossible:Ω() = hλ∂f()x∂x2F.(14.18)ThepenaltyΩ(h)isthesquaredFrobeniusnorm(sumofsquaredelements)oftheJacobianmatrixofpartialderivativesassociatedwiththeencoderfunction.Thereisaconnectionbetweenthedenoisingautoencoderandthecontractiveautoencoder:()showedthatinthelimitofsmallGaussianAlainandBengio2013input noise, the denoising reconstructionerroris equivalent toacontractivepenaltyonthereconstructionfunctionthatmapsxtor=g(f(x)).Inotherwords,denoisingautoencodersmakethereconstructionfunctionresistsmallbutﬁnite-sizedperturbationsoftheinput,whilecontractiveautoencodersmakethefeatureextractionfunctionresistinﬁnitesimalperturbationsoftheinput.WhenusingtheJacobian-basedcontractivepenaltytopretrainfeaturesf(x)forusewithaclassiﬁer,thebestclassiﬁcationaccuracyusuallyresultsfromapplyingthecontractivepenaltytof(x)ratherthantog(f(x)).Acontractivepenaltyonf(x)alsohascloseconnectionstoscorematching,asdiscussedinsection.14.5.1ThenamecontractivearisesfromthewaythattheCAEwarpsspace.Speciﬁ-cally,becausetheCAEistrainedtoresistperturbationsofitsinput,itisencouragedtomapaneighborhoodofinputpointstoasmallerneighborhoodofoutputpoints.Wecanthinkofthisascontractingtheinputneighborhoodtoasmalleroutputneighborhood.Toclarify,theCAEiscontractiveonlylocally—allperturbationsofatrainingpointxaremappedneartof(x).Globally,twodiﬀerentpointsxandxmaybemappedtof(x)andf(x)pointsthatarefartherapartthantheoriginalpoints.Itisplausiblethatfbeexpandingin-betweenorfarfromthedatamanifolds(seeforexamplewhathappensinthe1-Dtoyexampleofﬁgure).Whenthe14.7Ω(h)penaltyisappliedtosigmoidalunits,oneeasywaytoshrinktheJacobianistomakethesigmoidunitssaturatetoor.ThisencouragestheCAEtoencode01inputpointswithextremevaluesofthesigmoidthatmaybeinterpretedasabinarycode.ItalsoensuresthattheCAEwillspreaditscodevaluesthroughoutmostofthehypercubethatitssigmoidalhiddenunitscanspan.WecanthinkoftheJacobianmatrixJatapointxasapproximatingthenonlinearencoderf(x)asbeingalinearoperator.Thisallowsustousetheword“contractive”moreformally. Inthetheoryoflinearoperators,alinearoperator521 CHAPTER14.AUTOENCODERSissaidtobecontractiveifthenormofJxremainslessthanorequaltofor1allunit-normx.Inotherwords,Jiscontractiveifitshrinkstheunitsphere.WecanthinkoftheCAEaspenalizingtheFrobeniusnormofthelocallinearapproximationoff(x)ateverytrainingpointxinordertoencourageeachoftheselocallinearoperatortobecomeacontraction.Asdescribed insection, regularizedautoencoderslearnmanifoldsby14.6balancingtwoopposingforces.InthecaseoftheCAE,thesetwoforcesarereconstructionerrorandthecontractivepenaltyΩ(h).ReconstructionerroralonewouldencouragetheCAEtolearnanidentityfunction.ThecontractivepenaltyalonewouldencouragetheCAEtolearnfeaturesthatareconstantwithrespecttox.Thecompromisebetweenthesetwoforcesyieldsanautoencoderwhosederivatives∂f()x∂xaremostlytiny.Onlyasmallnumberofhiddenunits,correspondingtoasmallnumberofdirectionsintheinput,mayhavesigniﬁcantderivatives.ThegoaloftheCAEistolearnthemanifoldstructureofthedata.DirectionsxwithlargeJxrapidlychangeh,sothesearelikelytobedirectionswhichapproximatethetangentplanesofthemanifold.Experimentsby()Rifaietal.2011aand()showthattrainingtheCAEresultsinmostsingularvaluesRifaietal.2011bofJdroppingbelowinmagnitudeandthereforebecomingcontractive.However,1somesingularvaluesremainabove,becausethereconstructionerrorpenalty1encouragestheCAEtoencodethedirectionswiththemostlocalvariance.Thedirectionscorrespondingtothelargestsingularvaluesareinterpretedasthetangentdirectionsthatthecontractiveautoencoderhaslearned.Ideally,thesetangentdirectionsshouldcorrespondtorealvariationsinthedata.Forexample,aCAEappliedtoimagesshouldlearntangentvectorsthatshowhowtheimagechangesasobjectsintheimagegraduallychangepose,asshowninﬁgure.Visualizations14.6oftheexperimentallyobtainedsingularvectorsdoseemtocorrespondtomeaningfultransformationsoftheinputimage,asshowninﬁgure.14.10OnepracticalissuewiththeCAEregularizationcriterionisthatalthoughitischeaptocomputeinthecaseofasinglehiddenlayerautoencoder,itbecomesmuchmoreexpensiveinthecaseofdeeperautoencoders.ThestrategyfollowedbyRifai2011aetal.()istoseparatelytrainaseriesofsingle-layerautoencoders,eachtrainedtoreconstructthepreviousautoencoder’shiddenlayer.Thecompositionoftheseautoencodersthenformsadeepautoencoder.Becauseeachlayerwasseparatelytrainedtobelocallycontractive,thedeepautoencoderiscontractiveaswell.TheresultisnotthesameaswhatwouldbeobtainedbyjointlytrainingtheentirearchitecturewithapenaltyontheJacobianofthedeepmodel,butitcapturesmanyofthedesirablequalitativecharacteristics.Anotherpracticalissueisthatthecontractionpenaltycanobtainuselessresults522 CHAPTER14.AUTOENCODERSInputpointTangentvectorsLocalPCA(nosharingacrossregions)ContractiveautoencoderFigure14.10:IllustrationoftangentvectorsofthemanifoldestimatedbylocalPCAandbyacontractiveautoencoder.ThelocationonthemanifoldisdeﬁnedbytheinputimageofadogdrawnfromtheCIFAR-10dataset. ThetangentvectorsareestimatedbytheleadingsingularvectorsoftheJacobianmatrix∂h∂xoftheinput-to-codemapping.AlthoughbothlocalPCAandtheCAEcancapturelocaltangents,theCAEisabletoformmoreaccurateestimatesfromlimitedtrainingdatabecauseitexploitsparametersharingacrossdiﬀerentlocationsthatshareasubsetofactivehiddenunits. TheCAEtangentdirectionstypicallycorrespondtomovingorchangingpartsoftheobject(suchastheheadorlegs).Imagesreproducedwithpermissionfrom().Rifaietal.2011cifwedonotimposesomesortofscaleonthedecoder.Forexample,theencodercouldconsistofmultiplyingtheinputbyasmallconstantandthedecodercouldconsistofdividingthecodeby.Asapproaches,theencoderdrivesthe0contractivepenaltyΩ(h)toapproachwithouthavinglearnedanythingaboutthe0distribution.Meanwhile,thedecodermaintainsperfectreconstruction.InRifaietal.(),thisispreventedbytyingtheweightsof2011afandg.Bothfandgarestandardneuralnetworklayersconsistingofanaﬃnetransformationfollowedbyanelement-wisenonlinearity,soitisstraightforwardtosettheweightmatrixofgtobethetransposeoftheweightmatrixof.f14.8PredictiveSparseDecompositionPredictivesparsedecomposition(PSD)isamodelthatisahybridofsparsecodingandparametricautoencoders(Kavukcuoglu2008etal.,).Aparametricencoderistrainedtopredicttheoutputofiterativeinference.PSDhasbeenappliedtounsupervisedfeaturelearningforobjectrecognitioninimagesandvideo(Kavukcuoglu20092010Jarrett2009Farabet2011etal.,,;etal.,;etal.,),aswellasforaudio(,).ThemodelconsistsofanencoderHenaﬀetal.2011f(x)andadecoderg(h)thatarebothparametric.Duringtraining,hiscontrolledbythe523 CHAPTER14.AUTOENCODERSoptimizationalgorithm.Trainingproceedsbyminimizing||−||xg()h2+λ||h1+()γf||−hx||2.(14.19)Likeinsparsecoding,thetrainingalgorithmalternatesbetweenminimizationwithrespecttohandminimizationwithrespecttothemodelparameters.Minimizationwithrespecttohisfastbecausef(x)providesagoodinitialvalueofhandthecostfunctionconstrainshtoremainnearf(x)anyway.Simplegradientdescentcanobtainreasonablevaluesofinasfewastensteps.hThetrainingprocedureusedbyPSDisdiﬀerentfromﬁrsttrainingasparsecodingmodelandthentrainingf(x)topredictthevaluesofthesparsecodingfeatures.ThePSDtrainingprocedureregularizesthedecodertouseparametersforwhichcaninfergoodcodevalues.f()xPredictivesparsecodingisanexampleoflearnedapproximateinference.Insection,thistopicisdevelopedfurther.Thetoolspresentedinchapter19.519makeitclearthatPSDcanbeinterpretedastrainingadirectedsparsecodingprobabilisticmodelbymaximizingalowerboundonthelog-likelihoodofthemodel.InpracticalapplicationsofPSD,theiterativeoptimizationisonlyusedduringtraining.Theparametricencoderfisusedtocomputethelearnedfeatureswhenthemodelisdeployed.Evaluatingfiscomputationallyinexpensivecomparedtoinferringhviagradientdescent.Becausefisadiﬀerentiableparametricfunction,PSDmodelsmaybestackedandusedtoinitializeadeepnetworktobetrainedwithanothercriterion.14.9ApplicationsofAutoencodersAutoencodershavebeensuccessfullyappliedtodimensionalityreductionandinfor-mationretrievaltasks.Dimensionalityreductionwasoneoftheﬁrstapplicationsofrepresentationlearninganddeeplearning.Itwasoneoftheearlymotivationsforstudyingautoencoders.Forexample,HintonandSalakhutdinov2006()trainedastackofRBMsandthenusedtheirweightstoinitializeadeepautoencoderwithgraduallysmallerhiddenlayers,culminatinginabottleneckof30units.TheresultingcodeyieldedlessreconstructionerrorthanPCAinto30dimensionsandthelearnedrepresentationwasqualitativelyeasiertointerpretandrelatetotheunderlyingcategories,withthesecategoriesmanifestingaswell-separatedclusters.Lower-dimensionalrepresentationscanimproveperformanceonmanytasks,suchasclassiﬁcation.Modelsofsmallerspacesconsumelessmemoryandruntime.524 CHAPTER14.AUTOENCODERSManyformsofdimensionalityreductionplacesemanticallyrelatedexamplesneareachother,asobservedbySalakhutdinovandHinton2007bTorralba()andetal.().Thehintsprovidedbythemappingtothelower-dimensionalspaceaid2008generalization.Onetaskthatbeneﬁtsevenmorethanusualfromdimensionalityreductionisinformationretrieval,thetaskofﬁndingentriesinadatabasethatresembleaqueryentry. Thistaskderivestheusualbeneﬁtsfromdimensionalityreductionthatothertasksdo,butalsoderivestheadditionalbeneﬁtthatsearchcanbecomeextremelyeﬃcientincertainkindsoflowdimensionalspaces.Speciﬁcally, ifwetrainthedimensionalityreductionalgorithmtoproduceacodethatislow-dimensionaland,thenwecanstorealldatabaseentriesinahashtablebinarymappingbinarycodevectorstoentries.Thishashtableallowsustoperforminformationretrievalbyreturningalldatabaseentriesthathavethesamebinarycodeasthe query.Wecanalso search overslightlylesssimilar entries veryeﬃciently,justbyﬂippingindividualbitsfromtheencodingofthequery. Thisapproachtoinformationretrievalviadimensionalityreductionandbinarizationiscalledsemantichashing(SalakhutdinovandHinton2007b2009b,,),andhasbeenappliedtobothtextualinput(SalakhutdinovandHinton2007b2009b,,)andimages(Torralba2008Weiss2008KrizhevskyandHinton2011etal.,;etal.,;,).Toproducebinarycodesforsemantichashing,onetypicallyusesanencodingfunctionwithsigmoidsontheﬁnallayer.Thesigmoidunitsmustbetrainedtobesaturatedtonearly0ornearly1forallinputvalues.Onetrickthatcanaccomplishthisissimplytoinjectadditivenoisejustbeforethesigmoidnonlinearityduringtraining.Themagnitudeofthenoiseshouldincreaseovertime.Toﬁghtthatnoiseandpreserveasmuchinformationaspossible,thenetworkmustincreasethemagnitudeoftheinputstothesigmoidfunction,untilsaturationoccurs.Theideaoflearningahashingfunctionhasbeenfurtherexploredinseveraldirections,includingtheideaoftrainingtherepresentationssoastooptimizealossmoredirectlylinkedtothetaskofﬁndingnearbyexamplesinthehashtable(,).NorouziandFleet2011 525 Chapter15RepresentationLearningInthischapter,weﬁrstdiscusswhatitmeanstolearnrepresentationsandhowthenotionofrepresentationcanbeusefultodesigndeeparchitectures.Wediscusshowlearningalgorithmssharestatisticalstrengthacrossdiﬀerenttasks,includingusinginformationfromunsupervisedtaskstoperformsupervisedtasks.Sharedrepresentationsareusefultohandlemultiplemodalitiesordomains,ortotransferlearnedknowledgetotasksforwhichfewornoexamplesaregivenbutataskrepresentationexists.Finally,westepbackandargueaboutthereasonsforthesuccessofrepresentationlearning,startingwiththetheoreticaladvantagesofdistributedrepresentations(Hinton1986etal.,)anddeeprepresentationsandendingwiththemoregeneralideaofunderlyingassumptionsaboutthedatageneratingprocess,inparticularaboutunderlyingcausesoftheobserveddata.Manyinformationprocessingtaskscanbeveryeasyorverydiﬃcultdependingonhowtheinformationisrepresented.Thisisageneralprincipleapplicabletodailylife,computerscienceingeneral,andtomachinelearning.Forexample,itisstraightforwardforapersontodivide210by6usinglongdivision. ThetaskbecomesconsiderablylessstraightforwardifitisinsteadposedusingtheRomannumeralrepresentationofthenumbers.MostmodernpeopleaskedtodivideCCXbyVIwouldbeginbyconvertingthenumberstotheArabicnumeralrepresentation,permittinglongdivisionproceduresthatmakeuseoftheplacevaluesystem.Moreconcretely,wecanquantifytheasymptoticruntimeofvariousoperationsusingappropriateorinappropriaterepresentations.Forexample,insertinganumberintothecorrectpositioninasortedlistofnumbersisanO(n)operationifthelistisrepresentedasalinkedlist,butonlyO(logn)ifthelistisrepresentedasared-blacktree.Inthecontextofmachinelearning,whatmakesonerepresentationbetterthan526 CHAPTER15.REPRESENTATIONLEARNINGanother?Generallyspeaking,agoodrepresentationisonethatmakesasubsequentlearningtaskeasier.Thechoiceofrepresentationwillusuallydependonthechoiceofthesubsequentlearningtask.Wecanthinkoffeedforwardnetworkstrainedbysupervisedlearningasper-formingakindofrepresentationlearning.Speciﬁcally,thelastlayerofthenetworkistypicallyalinearclassiﬁer,suchasasoftmaxregressionclassiﬁer.Therestofthenetworklearnstoprovidearepresentationtothisclassiﬁer.Trainingwithasupervisedcriterionnaturallyleadstotherepresentationateveryhiddenlayer(butmoresonearthetophiddenlayer)takingonpropertiesthatmaketheclassiﬁcationtaskeasier.Forexample,classesthatwerenotlinearlyseparableintheinputfeaturesmaybecomelinearlyseparableinthelasthiddenlayer.Inprinciple,thelastlayercouldbeanotherkindofmodel,suchasanearestneighborclassiﬁer(SalakhutdinovandHinton2007a,).Thefeaturesinthepenultimatelayershouldlearndiﬀerentpropertiesdependingonthetypeofthelastlayer.Supervisedtrainingoffeedforwardnetworksdoesnotinvolveexplicitlyimposinganyconditiononthelearnedintermediatefeatures.Otherkindsofrepresentationlearningalgorithmsareoftenexplicitlydesignedtoshapetherepresentationinsomeparticularway.Forexample,supposewewanttolearnarepresentationthatmakesdensityestimationeasier.Distributionswithmoreindependencesareeasiertomodel,sowecoulddesignanobjectivefunctionthatencouragestheelementsoftherepresentationvectorhtobeindependent.Justlikesupervisednetworks,unsuperviseddeeplearningalgorithmshaveamaintrainingobjectivebutalsolearnarepresentationasasideeﬀect.Regardlessofhowarepresentationwasobtained,itcanbeusedforanothertask.Alternatively,multipletasks(somesupervised,someunsupervised)canbelearnedtogetherwithsomesharedinternalrepresentation.Mostrepresentationlearningproblemsfaceatradeoﬀbetweenpreservingasmuchinformationabouttheinputaspossibleandattainingniceproperties(suchasindependence).Representationlearningisparticularlyinterestingbecauseitprovidesonewaytoperformunsupervisedandsemi-supervisedlearning.Weoftenhaveverylargeamountsofunlabeledtrainingdataandrelativelylittlelabeledtrainingdata.Trainingwithsupervisedlearningtechniquesonthelabeledsubsetoftenresultsinsevereoverﬁtting.Semi-supervisedlearningoﬀersthechancetoresolvethisoverﬁttingproblembyalsolearningfromtheunlabeleddata.Speciﬁcally,wecanlearngoodrepresentationsfortheunlabeleddata,andthenusetheserepresentationstosolvethesupervisedlearningtask.Humansandanimalsareabletolearnfromveryfewlabeledexamples.Wedo527 CHAPTER15.REPRESENTATIONLEARNINGnotyetknowhowthisispossible.Manyfactorscouldexplainimprovedhumanperformance—forexample,thebrainmayuseverylargeensemblesofclassiﬁersorBayesianinferencetechniques.Onepopularhypothesisisthatthebrainisabletoleverageunsupervisedorsemi-supervisedlearning.Therearemanywaystoleverageunlabeleddata.Inthischapter,wefocusonthehypothesisthattheunlabeleddatacanbeusedtolearnagoodrepresentation.15.1GreedyLayer-WiseUnsupervisedPretrainingUnsupervisedlearningplayedakeyhistoricalroleintherevivalofdeepneuralnetworks,enablingresearchersfortheﬁrsttimetotrainadeepsupervisednetworkwithoutrequiringarchitecturalspecializationslikeconvolutionorrecurrence.Wecallthisprocedureunsupervisedpretraining,ormoreprecisely,greedylayer-wiseunsupervisedpretraining.Thisprocedureisacanonicalexampleofhowarepresentationlearnedforonetask(unsupervisedlearning,tryingtocapturetheshapeoftheinputdistribution)cansometimesbeusefulforanothertask(supervisedlearningwiththesameinputdomain).Greedylayer-wiseunsupervisedpretrainingreliesonasingle-layerrepresen-tationlearningalgorithmsuchasanRBM,asingle-layerautoencoder,asparsecodingmodel,oranothermodelthatlearnslatentrepresentations.Eachlayerispretrainedusingunsupervisedlearning,takingtheoutputofthepreviouslayerandproducingasoutputanewrepresentationofthedata,whosedistribution(oritsrelationtoothervariablessuchascategoriestopredict)ishopefullysimpler.Seealgorithmforaformaldescription.15.1Greedylayer-wisetrainingproceduresbasedonunsupervisedcriteriahavelongbeenusedtosidestepthediﬃcultyofjointlytrainingthelayersofadeepneuralnetforasupervisedtask.ThisapproachdatesbackatleastasfarastheNeocognitron(Fukushima1975,).Thedeeplearningrenaissanceof2006beganwiththediscoverythatthisgreedylearningprocedurecouldbeusedtoﬁndagoodinitializationforajointlearningprocedureoverallthelayers,andthatthisapproachcouldbeusedtosuccessfullytrainevenfullyconnectedarchitectures(Hinton2006Hintonetal.,;andSalakhutdinov2006Hinton2006Bengio2007Ranzato2007a,;,;etal.,;etal.,).Priortothisdiscovery,onlyconvolutionaldeepnetworksornetworkswhosedepthresultedfromrecurrencewereregardedasfeasibletotrain.Today,wenowknowthatgreedylayer-wisepretrainingisnotrequiredtotrainfullyconnecteddeeparchitectures,buttheunsupervisedpretrainingapproachwastheﬁrstmethodtosucceed.Greedylayer-wisepretrainingiscalledgreedybecauseitisagreedyalgo-528 CHAPTER15.REPRESENTATIONLEARNINGrithm,meaningthatitoptimizeseachpieceofthesolutionindependently,onepieceatatime,ratherthanjointlyoptimizingallpieces.Itiscalledlayer-wisebecausetheseindependentpiecesarethelayersofthenetwork.Speciﬁcally,greedylayer-wisepretrainingproceedsonelayeratatime,trainingthek-thlayerwhilekeepingthepreviousonesﬁxed.Inparticular,thelowerlayers(whicharetrainedﬁrst)arenotadaptedaftertheupperlayersareintroduced.Itiscalledunsuper-visedbecauseeachlayeristrainedwithanunsupervisedrepresentationlearningalgorithm.Howeveritisalsocalledpretraining,becauseitissupposedtobeonlyaﬁrststepbeforeajointtrainingalgorithmisappliedtoﬁne-tuneallthelayerstogether.Inthecontextofasupervisedlearningtask,itcanbeviewedasaregularizer(insomeexperiments,pretrainingdecreasestesterrorwithoutdecreasingtrainingerror)andaformofparameterinitialization.Itiscommontousetheword“pretraining”torefernotonlytothepretrainingstageitselfbuttotheentiretwophaseprotocolthatcombinesthepretrainingphaseandasupervisedlearningphase.Thesupervisedlearningphasemayinvolvetrainingasimpleclassiﬁerontopofthefeatureslearnedinthepretrainingphase,oritmayinvolvesupervisedﬁne-tuningoftheentirenetworklearnedinthepretrainingphase.Nomatterwhatkindofunsupervisedlearningalgorithmorwhatmodeltypeisemployed,inthevastmajorityofcases,theoveralltrainingschemeisnearlythesame.Whilethechoiceofunsupervisedlearningalgorithmwillobviouslyimpactthedetails,mostapplicationsofunsupervisedpretrainingfollowthisbasicprotocol.Greedylayer-wiseunsupervisedpretrainingcanalsobeusedasinitializationforotherunsupervisedlearningalgorithms,suchasdeepautoencoders(HintonandSalakhutdinov2006,)andprobabilisticmodelswithmanylayersoflatentvariables.Suchmodelsincludedeepbeliefnetworks(,)anddeepHintonetal.2006Boltzmannmachines(SalakhutdinovandHinton2009a,).Thesedeepgenerativemodelswillbedescribedinchapter.20Asdiscussedinsection, itisalsopossibletohavegreedylayer-wise8.7.4supervisedpretraining.Thisbuildsonthepremisethattrainingashallownetworkiseasierthantrainingadeepone,whichseemstohavebeenvalidatedinseveralcontexts(,).Erhanetal.201015.1.1WhenandWhyDoesUnsupervisedPretrainingWork?Onmanytasks,greedylayer-wiseunsupervisedpretrainingcanyieldsubstantialimprovementsintesterrorforclassiﬁcationtasks.Thisobservationwasresponsiblefortherenewedinterestedindeepneuralnetworksstartingin2006(Hintonetal.,529 CHAPTER15.REPRESENTATIONLEARNINGAlgorithm15.1Greedylayer-wiseunsupervisedpretrainingprotocol.Giventhefollowing: UnsupervisedfeaturelearningalgorithmL,whichtakesatrainingsetofexamplesandreturnsanencoderorfeaturefunctionf.TherawinputdataisX,withonerowperexampleandf(1)(X)istheoutputoftheﬁrststageencoderonX.Inthecasewhereﬁne-tuningisperformed,weusealearnerTwhichtakesaninitialfunctionf,inputexamplesX(andinthesupervisedﬁne-tuningcase,associatedtargetsY),andreturnsatunedfunction.Thenumberofstagesis.mf←Identityfunction˜XX= fordok,...,m= 1f()k= (L˜X)ff←()k◦f˜X←f()k(˜X)endforifﬁne-tuningthenff,,←T(XY)endifReturnf2006Bengio2007Ranzato2007a;etal.,;etal.,).Onmanyothertasks,however,unsupervisedpretrainingeitherdoesnotconferabeneﬁtorevencausesnoticeableharm.()studiedtheeﬀectofpretrainingonmachinelearningMaetal.2015modelsforchemicalactivitypredictionandfoundthat,onaverage,pretrainingwasslightlyharmful,butformanytaskswassigniﬁcantlyhelpful.Becauseunsupervisedpretrainingissometimeshelpfulbutoftenharmfulitisimportanttounderstandwhenandwhyitworksinordertodeterminewhetheritisapplicabletoaparticulartask.Attheoutset,itisimportanttoclarifythatmostofthisdiscussionisrestrictedtogreedyunsupervisedpretraininginparticular.Thereareother,completelydiﬀerentparadigmsforperformingsemi-supervisedlearningwithneuralnetworks,suchasvirtualadversarialtrainingdescribedinsection.Itisalsopossibleto7.13trainanautoencoderorgenerativemodelatthesametimeasthesupervisedmodel.Examplesofthissingle-stageapproachincludethediscriminativeRBM(LarochelleandBengio2008,)andtheladdernetwork(,),inwhichthetotalRasmusetal.2015objectiveisanexplicitsumofthetwoterms(oneusingthelabelsandoneonlyusingtheinput).Unsupervisedpretrainingcombinestwodiﬀerentideas.First,itmakesuseof530 CHAPTER15.REPRESENTATIONLEARNINGtheideathatthechoiceofinitialparametersforadeepneuralnetworkcanhaveasigniﬁcantregularizingeﬀectonthemodel(and,toalesserextent,thatitcanimproveoptimization).Second,itmakesuseofthemoregeneralideathatlearningabouttheinputdistributioncanhelptolearnaboutthemappingfrominputstooutputs.Bothoftheseideasinvolvemanycomplicatedinteractionsbetweenseveralpartsofthemachinelearningalgorithmthatarenotentirelyunderstood.Theﬁrstidea,thatthechoiceofinitialparametersforadeepneuralnetworkcanhaveastrongregularizingeﬀectonitsperformance,istheleastwellunderstood.Atthetimethatpretrainingbecamepopular,itwasunderstoodasinitializingthemodelinalocationthatwouldcauseittoapproachonelocalminimumratherthananother. Today,localminimaarenolongerconsideredtobeaseriousproblemforneuralnetworkoptimization.Wenowknowthatourstandardneuralnetworktrainingproceduresusuallydonotarriveatacriticalpointofanykind.Itremainspossiblethatpretraininginitializesthemodelinalocationthatwouldotherwisebeinaccessible—forexample,aregionthatissurroundedbyareaswherethecostfunctionvariessomuchfromoneexampletoanotherthatminibatchesgiveonlyaverynoisyestimateofthegradient,oraregionsurroundedbyareaswheretheHessianmatrixissopoorlyconditionedthatgradientdescentmethodsmustuseverysmallsteps.However,ourabilitytocharacterizeexactlywhataspectsofthepretrainedparametersareretainedduringthesupervisedtrainingstageislimited.Thisisonereasonthatmodernapproachestypicallyusesimultaneousunsupervisedlearningandsupervisedlearningratherthantwosequentialstages.Onemayalsoavoidstrugglingwiththesecomplicatedideasabouthowoptimizationinthesupervisedlearningstagepreservesinformationfromtheunsupervisedlearningstagebysimplyfreezingthe parametersfor thefeature extractorsand usingsupervisedlearningonlytoaddaclassiﬁerontopofthelearnedfeatures.Theotheridea,thatalearningalgorithmcanuseinformationlearnedintheunsupervisedphasetoperformbetterinthesupervisedlearningstage,isbetterunderstood.Thebasicideaisthatsomefeaturesthatareusefulfortheunsupervisedtaskmayalsobeusefulforthesupervisedlearningtask.Forexample,ifwetrainagenerativemodelofimagesofcarsandmotorcycles,itwillneedtoknowaboutwheels,andabouthowmanywheelsshouldbeinanimage.Ifwearefortunate,therepresentationofthewheelswilltakeonaformthatiseasyforthesupervisedlearnertoaccess.Thisisnotyetunderstoodatamathematical,theoreticallevel,soitisnotalwayspossibletopredictwhichtaskswillbeneﬁtfromunsupervisedlearninginthisway.Manyaspectsofthisapproacharehighlydependentonthespeciﬁcmodelsused.Forexample,ifwewishtoaddalinearclassiﬁeron531 CHAPTER15.REPRESENTATIONLEARNINGtopofpretrainedfeatures,thefeaturesmustmaketheunderlyingclasseslinearlyseparable.Thesepropertiesoftenoccurnaturallybutdonotalwaysdoso.Thisisanotherreasonthatsimultaneoussupervisedandunsupervisedlearningcanbepreferable—theconstraintsimposedbytheoutputlayerarenaturallyincludedfromthestart.Fromthepointofviewofunsupervisedpretrainingaslearningarepresentation,wecanexpectunsupervisedpretrainingtobemoreeﬀectivewhentheinitialrepresentationispoor. Onekeyexampleofthisistheuseofwordembeddings.Wordsrepresentedbyone-hotvectorsarenotveryinformativebecauseeverytwodistinctone-hotvectorsarethesamedistancefromeachother(squaredL2distanceof).Learnedwordembeddingsnaturallyencodesimilaritybetweenwordsbytheir2distancefromeachother.Becauseofthis,unsupervisedpretrainingisespeciallyusefulwhenprocessingwords.Itislessusefulwhenprocessingimages,perhapsbecauseimagesalreadylieinarichvectorspacewheredistancesprovidealowqualitysimilaritymetric.Fromthepointofviewofunsupervisedpretrainingasaregularizer,wecanexpectunsupervisedpretrainingtobemosthelpfulwhenthenumberoflabeledexamplesisverysmall.Becausethesourceofinformationaddedbyunsupervisedpretrainingistheunlabeleddata,wemayalsoexpectunsupervisedpretrainingtoperformbest whenthe number ofunlabeled examplesis very large.Theadvantageofsemi-supervisedlearningviaunsupervisedpretrainingwithmanyunlabeledexamplesandfewlabeledexampleswasmadeparticularlyclearin2011withunsupervisedpretrainingwinningtwointernationaltransferlearningcompetitions(,;,),insettingswheretheMesniletal.2011Goodfellowetal.2011numberoflabeledexamplesinthetargettaskwassmall(fromahandfultodozensofexamplesperclass).TheseeﬀectswerealsodocumentedincarefullycontrolledexperimentsbyPaine2014etal.().Otherfactorsarelikelytobeinvolved.Forexample,unsupervisedpretrainingislikelytobemostusefulwhenthefunctiontobelearnedisextremelycomplicated.Unsupervisedlearningdiﬀersfromregularizerslikeweightdecaybecauseitdoesnotbiasthelearnertowarddiscoveringasimplefunctionbutrathertowarddiscoveringfeaturefunctionsthatareusefulfortheunsupervisedlearningtask. Ifthetrueunderlyingfunctionsarecomplicatedandshapedbyregularitiesoftheinputdistribution,unsupervisedlearningcanbeamoreappropriateregularizer.Thesecaveatsaside,wenowanalyzesomesuccesscaseswhereunsupervisedpretrainingisknowntocauseanimprovement,andexplainwhatisknownaboutwhythisimprovementoccurs.Unsupervisedpretraininghasusuallybeenusedtoimproveclassiﬁers,andisusuallymostinterestingfromthepointofviewof532 CHAPTER15.REPRESENTATIONLEARNING  Figure15.1:Visualizationvianonlinearprojectionofthelearningtrajectoriesofdiﬀerentneuralnetworksinfunctionspace(notparameterspace,toavoidtheissueofmany-to-onemappingsfromparametervectorstofunctions),withdiﬀerentrandominitializationsandwithorwithoutunsupervisedpretraining.Eachpointcorrespondstoadiﬀerentneuralnetwork,ataparticulartimeduringitstrainingprocess.Thisﬁgureisadaptedwithpermissionfrom().Acoordinateinfunctionspaceisaninﬁnite-Erhanetal.2010dimensionalvectorassociatingeveryinputxwithanoutputy.()madeErhanetal.2010alinearprojectiontohigh-dimensionalspacebyconcatenatingtheyformanyspeciﬁcxpoints.Theythenmadeafurthernonlinearprojectionto2-DbyIsomap(Tenenbaumetal.,).Colorindicatestime.Allnetworksareinitializednearthecenteroftheplot2000(correspondingtotheregionoffunctionsthatproduceapproximatelyuniformdistributionsovertheclassyformostinputs).Overtime,learningmovesthefunctionoutward,topointsthatmakestrongpredictions.Trainingconsistentlyterminatesinoneregionwhenusingpretrainingandinanother,non-overlappingregionwhennotusingpretraining.Isomaptriestopreserveglobalrelativedistances(andhencevolumes)sothesmallregioncorrespondingtopretrainedmodelsmayindicatethatthepretraining-basedestimatorhasreducedvariance. 533 CHAPTER15.REPRESENTATIONLEARNINGreducingtestseterror.However,unsupervisedpretrainingcanhelptasksotherthanclassiﬁcation,andcanacttoimproveoptimizationratherthanbeingmerelyaregularizer.Forexample,itcanimprovebothtrainandtestreconstructionerrorfordeepautoencoders(HintonandSalakhutdinov2006,).Erhan2010etal.()performedmanyexperimentstoexplainseveralsuccessesofunsupervisedpretraining.Bothimprovementstotrainingerrorandimprovementstotesterrormaybeexplainedintermsofunsupervisedpretrainingtakingtheparametersintoaregionthatwouldotherwisebeinaccessible.Neuralnetworktrainingisnon-deterministic,andconvergestoadiﬀerentfunctioneverytimeitisrun. Trainingmayhaltatapointwherethegradientbecomessmall,apointwhereearlystoppingendstrainingtopreventoverﬁtting,oratapointwherethegradientislargebutitisdiﬃculttoﬁndadownhillstepduetoproblemssuchasstochasticityorpoorconditioningoftheHessian. Neuralnetworksthatreceiveunsupervisedpretrainingconsistentlyhaltinthesameregionoffunctionspace,whileneuralnetworkswithoutpretrainingconsistentlyhaltinanotherregion.Seeﬁgureforavisualizationofthisphenomenon.Theregionwherepretrained15.1networksarriveissmaller,suggestingthatpretrainingreducesthevarianceoftheestimationprocess,whichcaninturnreducetheriskofsevereover-ﬁtting.Inotherwords,unsupervisedpretraininginitializesneuralnetworkparametersintoaregionthattheydonotescape,andtheresultsfollowingthisinitializationaremoreconsistentandlesslikelytobeverybadthanwithoutthisinitialization.Erhan2010etal.()alsoprovidesomeanswersastopretrainingworkswhenbest—themeanandvarianceofthetesterrorweremostreducedbypretrainingfordeepernetworks.Keepinmindthattheseexperimentswereperformedbeforetheinventionandpopularizationofmoderntechniquesfortrainingverydeepnetworks(rectiﬁedlinearunits,dropoutandbatchnormalization)solessisknownabouttheeﬀectofunsupervisedpretraininginconjunctionwithcontemporaryapproaches.Animportantquestionishowunsupervisedpretrainingcanactasaregularizer.Onehypothesisisthatpretrainingencouragesthelearningalgorithmtodiscoverfeaturesthatrelatetotheunderlyingcausesthatgeneratetheobserveddata.Thisisanimportantideamotivatingmanyotheralgorithmsbesidesunsupervisedpretraining,andisdescribedfurtherinsection.15.3Comparedtootherformsofunsupervisedlearning,unsupervisedpretraininghasthedisadvantagethatitoperateswithtwoseparatetrainingphases. Manyregularizationstrategieshavetheadvantageofallowingtheusertocontrolthestrengthoftheregularizationbyadjustingthevalueofasinglehyperparameter.Unsupervisedpretrainingdoesnotoﬀeraclearwaytoadjustthethestrengthoftheregularization arisingfromtheunsupervised stage.Instead, thereare534 CHAPTER15.REPRESENTATIONLEARNINGverymanyhyperparameters,whoseeﬀectmaybemeasuredafterthefactbutisoftendiﬃculttopredictaheadoftime.Whenweperformunsupervisedandsupervisedlearningsimultaneously,insteadofusingthepretrainingstrategy,thereisasinglehyperparameter,usuallyacoeﬃcientattachedtotheunsupervisedcost, thatdetermineshowstronglytheunsupervisedobjectivewillregularizethesupervisedmodel.Onecanalwayspredictablyobtainlessregularizationbydecreasingthiscoeﬃcient.Inthecaseofunsupervisedpretraining,thereisnotawayofﬂexiblyadaptingthestrengthoftheregularization—eitherthesupervisedmodelisinitializedtopretrainedparameters,oritisnot.Anotherdisadvantageofhavingtwoseparatetrainingphasesisthateachphasehasitsownhyperparameters.Theperformanceofthesecondphaseusuallycannotbepredictedduringtheﬁrstphase,sothereisalongdelaybetweenproposinghyperparametersfortheﬁrstphaseandbeingabletoupdatethemusingfeedbackfromthesecondphase.Themostprincipledapproachistousevalidationseterrorinthesupervisedphaseinordertoselectthehyperparametersofthepretrainingphase,asdiscussedin().Inpractice,somehyperparameters,Larochelleetal.2009likethenumberofpretrainingiterations,aremoreconvenientlysetduringthepretrainingphase,usingearlystoppingontheunsupervisedobjective,whichisnotidealbutcomputationallymuchcheaperthanusingthesupervisedobjective.Today,unsupervisedpretraininghasbeenlargelyabandoned,exceptintheﬁeldofnaturallanguageprocessing,wherethenaturalrepresentationofwordsasone-hotvectorsconveysnosimilarityinformationandwhereverylargeunlabeledsetsareavailable.Inthatcase,theadvantageofpretrainingisthatonecanpretrainonceonahugeunlabeledset(forexamplewithacorpuscontainingbillionsofwords),learnagoodrepresentation(typicallyofwords,butalsoofsentences),andthenusethisrepresentationorﬁne-tuneitforasupervisedtaskforwhichthetrainingsetcontainssubstantiallyfewerexamples.ThisapproachwaspioneeredbybyCollobertandWeston2008bTurian2010Collobert(),etal.(),andetal.()andremainsincommonusetoday.2011aDeeplearningtechniquesbasedonsupervisedlearning,regularizedwithdropoutorbatchnormalization,areabletoachievehuman-levelperformanceonverymanytasks,butonlywithextremelylargelabeleddatasets.Thesesametechniquesout-performunsupervisedpretrainingonmedium-sizeddatasetssuchasCIFAR-10andMNIST,whichhaveroughly5,000labeledexamplesperclass.Onextremelysmalldatasets,suchasthealternativesplicingdataset,Bayesianmethodsoutperformmethodsbasedonunsupervisedpretraining(Srivastava2013,).Forthesereasons,thepopularityofunsupervisedpretraininghasdeclined.Nevertheless,unsupervisedpretrainingremainsanimportantmilestoneinthehistoryofdeeplearningresearch535 CHAPTER15.REPRESENTATIONLEARNINGandcontinuestoinﬂuencecontemporaryapproaches.Theideaofpretraininghasbeengeneralizedtosupervisedpretrainingdiscussedinsection,asavery8.7.4commonapproachfortransferlearning.Supervisedpretrainingfortransferlearningispopular(,;Oquabetal.2014Yosinski2014etal.,)forusewithconvolutionalnetworkspretrainedontheImageNetdataset.Practitionerspublishtheparametersofthesetrainednetworksforthispurpose,justlikepretrainedwordvectorsarepublishedfornaturallanguagetasks(,;Collobertetal.2011aMikolov2013aetal.,).15.2TransferLearningandDomainAdaptationTransferlearninganddomainadaptationrefertothesituationwherewhathasbeenlearnedinonesetting(i.e.,distributionP1)isexploitedtoimprovegeneralizationinanothersetting(saydistributionP2).Thisgeneralizestheideapresentedintheprevioussection,wherewetransferredrepresentationsbetweenanunsupervisedlearningtaskandasupervisedlearningtask.Intransferlearning,thelearnermustperformtwoormorediﬀerenttasks,butweassumethatmanyofthefactorsthatexplainthevariationsinP1arerelevanttothevariationsthatneedtobecapturedforlearningP2.Thisistypicallyunderstoodinasupervisedlearningcontext,wheretheinputisthesamebutthetargetmaybeofadiﬀerentnature.Forexample,wemaylearnaboutonesetofvisualcategories,suchascatsanddogs,intheﬁrstsetting,thenlearnaboutadiﬀerentsetofvisualcategories,suchasantsandwasps,inthesecondsetting.Ifthereissigniﬁcantlymoredataintheﬁrstsetting(sampledfromP1),thenthatmayhelptolearnrepresentationsthatareusefultoquicklygeneralizefromonlyveryfewexamplesdrawnfromP2.Manyvisualcategoriessharelow-levelnotionsofedgesandvisualshapes,theeﬀectsofgeometricchanges,changesinlighting,etc. Ingeneral,transferlearning,multi-tasklearning(section),anddomain7.7adaptationcanbeachievedviarepresentationlearningwhenthereexistfeaturesthatareusefulforthediﬀerentsettingsortasks,correspondingtounderlyingfactorsthatappearinmorethanonesetting.Thisisillustratedinﬁgure,with7.2sharedlowerlayersandtask-dependentupperlayers.However, sometimes, whatisshared amongthe diﬀerent tasksisnotthesemanticsoftheinputbutthesemanticsoftheoutput.Forexample,aspeechrecognitionsystemneedstoproducevalidsentencesattheoutputlayer,buttheearlierlayersneartheinputmayneedtorecognizeverydiﬀerentversionsofthesamephonemesorsub-phonemicvocalizationsdependingonwhichpersonisspeaking.Incaseslikethese,itmakesmoresensetosharetheupperlayers(neartheoutput)oftheneuralnetwork,andhaveatask-speciﬁcpreprocessing,as536 CHAPTER15.REPRESENTATIONLEARNINGillustratedinﬁgure.15.2 Selection switchh(1)h(1)h(2)h(2)h(3)h(3)y yh(shared)h(shared) x(1)x(1)x(2)x(2)x(3)x(3)Figure15.2: Examplearchitectureformulti-taskortransferlearningwhentheoutputvariablehasthesamesemanticsforalltaskswhiletheinputvariablehasadiﬀerentyx meaning(andpossiblyevenadiﬀerentdimension)foreachtask(or,forexample,eachuser),calledx(1),x(2)andx(3)forthreetasks.Thelowerlevels(uptotheselectionswitch)aretask-speciﬁc,whiletheupperlevelsareshared.Thelowerlevelslearntotranslatetheirtask-speciﬁcinputintoagenericsetoffeatures.Intherelatedcaseofdomainadaptation,thetask(andtheoptimalinput-to-outputmapping)remainsthesamebetweeneachsetting,buttheinputdistributionisslightlydiﬀerent.Forexample,considerthetaskofsentimentanalysis,whichconsistsofdeterminingwhetheracommentexpressespositiveornegativesentiment.Commentspostedonthewebcomefrommanycategories.Adomainadaptationscenariocanarisewhenasentimentpredictortrainedoncustomerreviewsofmediacontentsuchasbooks,videosandmusicislaterusedtoanalyzecommentsaboutconsumerelectronicssuchastelevisionsorsmartphones.Onecanimaginethatthereisanunderlyingfunctionthattellswhetheranystatementispositive,neutralornegative,butofcoursethevocabularyandstylemayvaryfromonedomaintoanother,makingitmorediﬃculttogeneralizeacrossdomains.Simpleunsupervisedpretraining(withdenoisingautoencoders)hasbeenfoundtobeverysuccessfulforsentimentanalysiswithdomainadaptation(,).Glorotetal.2011bArelatedproblemisthatofconceptdrift,whichwecanviewasaformoftransferlearningduetogradualchangesinthedatadistributionovertime.Bothconceptdriftandtransferlearningcanbeviewedasparticularformsof537 CHAPTER15.REPRESENTATIONLEARNINGmulti-tasklearning.Whilethephrase“multi-tasklearning” typicallyreferstosupervisedlearningtasks,themoregeneralnotionoftransferlearningisapplicabletounsupervisedlearningandreinforcementlearningaswell.Inallofthesecases,theobjectiveistotakeadvantageofdatafromtheﬁrstsettingtoextractinformationthatmaybeusefulwhenlearningorevenwhendirectlymakingpredictionsinthesecondsetting.Thecoreideaofrepresentationlearningisthatthesamerepresentationmaybeusefulinbothsettings.Usingthesamerepresentationinbothsettingsallowstherepresentationtobeneﬁtfromthetrainingdatathatisavailableforbothtasks.Asmentionedbefore,unsuperviseddeeplearningfortransferlearninghasfoundsuccessinsomemachinelearningcompetitions(,;Mesniletal.2011Goodfellowetal.,).Intheﬁrstofthesecompetitions,theexperimentalsetupisthe2011following.Eachparticipantisﬁrstgivenadatasetfromtheﬁrstsetting(fromdistributionP1),illustratingexamplesofsomesetofcategories.Theparticipantsmustusethistolearnagoodfeaturespace(mappingtherawinputtosomerepresentation),suchthatwhenweapplythislearnedtransformationtoinputsfromthetransfersetting(distributionP2),alinearclassiﬁercanbetrainedandgeneralizewellfromveryfewlabeledexamples.Oneofthemoststrikingresultsfoundinthiscompetitionisthatasanarchitecturemakesuseofdeeperanddeeperrepresentations(learnedinapurelyunsupervisedwayfromdatacollectedintheﬁrstsetting,P1),thelearningcurveonthenewcategoriesofthesecond(transfer)settingP2becomesmuchbetter.Fordeeprepresentations,fewerlabeledexamplesofthetransfertasksarenecessarytoachievetheapparentlyasymptoticgeneralizationperformance.Twoextremeformsoftransferlearningareone-shotlearningandzero-shotlearning,sometimesalsocalledzero-datalearning.Onlyonelabeledexampleofthetransfertaskisgivenforone-shotlearning,whilenolabeledexamplesaregivenatallforthezero-shotlearningtask.One-shotlearning(Fei-Fei2006etal.,)ispossiblebecausetherepresentationlearnstocleanlyseparatetheunderlyingclassesduringtheﬁrststage.Duringthetransferlearningstage,onlyonelabeledexampleisneededtoinferthelabelofmanypossibletestexamplesthatallclusteraroundthesamepointinrepresentationspace.Thisworkstotheextentthatthefactorsofvariationcorrespondingtotheseinvarianceshavebeencleanlyseparatedfromtheotherfactors,inthelearnedrepresentationspace,andwehavesomehowlearnedwhichfactorsdoanddonotmatterwhendiscriminatingobjectsofcertaincategories.Asanexampleofazero-shotlearningsetting,considertheproblemofhavingalearnerreadalargecollectionoftextandthensolveobjectrecognitionproblems.538 CHAPTER15.REPRESENTATIONLEARNINGItmaybepossibletorecognizeaspeciﬁcobjectclassevenwithouthavingseenanimageofthatobject,ifthetextdescribestheobjectwellenough. Forexample,havingreadthatacathasfourlegsandpointyears,thelearnermightbeabletoguessthatanimageisacat,withouthavingseenacatbefore.Zero-datalearning(Larochelle2008Palatuccietal.,)andzero-shotlearning(etal.,;2009Socher2013betal.,)areonlypossiblebecauseadditionalinformationhasbeenexploitedduringtraining.Wecanthinkofthezero-datalearningscenarioasincludingthreerandomvariables:thetraditionalinputsx,thetraditionaloutputsortargetsy,andanadditionalrandomvariabledescribingthetask,T.Themodelistrainedtoestimatetheconditionaldistributionp(yx|,T)whereTisadescriptionofthetaskwewishthemodeltoperform. Inourexampleofrecognizingcatsafterhavingreadaboutcats,theoutputisabinaryvariableywithy= 1indicating“yes”andy= 0indicating“no.”ThetaskvariableTthenrepresentsquestionstobeansweredsuchas“Isthereacatinthisimage?”IfwehaveatrainingsetcontainingunsupervisedexamplesofobjectsthatliveinthesamespaceasT,wemaybeabletoinferthemeaningofunseeninstancesofT.Inourexampleofrecognizingcatswithouthavingseenanimageofthecat,itisimportantthatwehavehadunlabeledtextdatacontainingsentencessuchas“catshavefourlegs”or“catshavepointyears.”Zero-shotlearningrequiresTtoberepresentedinawaythatallowssomesortofgeneralization.Forexample,Tcannotbejustaone-hotcodeindicatinganobjectcategory.()provideinsteadadistributedrepresentationSocheretal.2013bofobjectcategoriesbyusingalearnedwordembeddingforthewordassociatedwitheachcategory.Asimilarphenomenonhappensinmachinetranslation(Klementiev2012etal.,;Mikolov2013bGouws2014etal.,;etal.,):wehavewordsinonelanguage,andtherelationshipsbetweenwordscanbelearnedfromunilingualcorpora;ontheotherhand,wehavetranslatedsentenceswhichrelatewordsinonelanguagewithwordsintheother.EventhoughwemaynothavelabeledexamplestranslatingwordAinlanguageXtowordBinlanguageY,wecangeneralizeandguessatranslationforwordAbecausewehavelearnedadistributedrepresentationforwordsinlanguageX,adistributedrepresentationforwordsinlanguageY,andcreatedalink(possiblytwo-way)relatingthetwospaces,viatrainingexamplesconsistingofmatchedpairsofsentencesinbothlanguages.Thistransferwillbemostsuccessfulifallthreeingredients(thetworepresentationsandtherelationsbetweenthem)arelearnedjointly.Zero-shotlearningisaparticularformoftransferlearning.Thesameprincipleexplainshowonecanperformmulti-modallearning,capturingarepresentation539 CHAPTER15.REPRESENTATIONLEARNINGhx=fx()x xtestytesthy=fy()y y−space Relationship between embedded points within one of the domainsMaps between representation spaces fxfyx−space ()pairsinthetrainingsetxy,fx:encoderfunctionforxfy:encoderfunctionforyFigure15.3:Transferlearningbetweentwodomainsxandyenableszero-shotlearning.Labeledorunlabeledexamplesofxallowonetolearnarepresentationfunctionfxandsimilarlywithexamplesofytolearnfy.Eachapplicationofthefxandfyfunctionsappearsasanupwardarrow,withthestyleofthearrowsindicatingwhichfunctionisapplied.Distanceinhxspaceprovidesasimilaritymetricbetweenanypairofpointsinxspacethatmaybemoremeaningfulthandistanceinxspace.Likewise,distanceinhyspaceprovidesasimilaritymetricbetweenanypairofpointsinyspace.Bothofthesesimilarityfunctionsareindicatedwithdottedbidirectionalarrows.Labeledexamples(dashedhorizontallines)arepairs(xy,)whichallowonetolearnaone-wayortwo-waymap(solidbidirectionalarrow)betweentherepresentationsfx(x)andtherepresentationsfy(y)andanchortheserepresentationstoeachother.Zero-datalearningisthenenabledasfollows.Onecanassociateanimagextesttoawordytest,evenifnoimageofthatwordwaseverpresented,simplybecauseword-representationsfy(ytest)andimage-representationsfx(xtest)canberelatedtoeachotherviathemapsbetweenrepresentationspaces.Itworksbecause,althoughthatimageandthatwordwereneverpaired,theirrespectivefeaturevectorsfx(xtest)andfy(ytest)havebeenrelatedtoeachother.FigureinspiredfromsuggestionbyHrantKhachatrian.540 CHAPTER15.REPRESENTATIONLEARNINGinonemodality,arepresentationintheother,andtherelationship(ingeneralajointdistribution)betweenpairs(xy,)consistingofoneobservationxinonemodalityandanotherobservationyintheothermodality(SrivastavaandSalakhutdinov,2012).Bylearningallthreesetsofparameters(fromxtoitsrepresentation,fromytoitsrepresentation,andtherelationshipbetweenthetworepresentations),conceptsinonerepresentationareanchoredintheother,andvice-versa,allowingonetomeaningfully generalizeto newpairs.Theprocedureis illustratedinﬁgure.15.315.3Semi-SupervisedDisentanglingofCausalFactorsAnimportantquestionaboutrepresentationlearningis“whatmakesonerepre-sentationbetterthananother?”Onehypothesisisthatanidealrepresentationisoneinwhichthefeatureswithintherepresentationcorrespondtotheunder-lyingcausesoftheobserveddata,withseparatefeaturesordirectionsinfeaturespacecorrespondingtodiﬀerentcauses,sothattherepresentationdisentanglesthecausesfromoneanother.Thishypothesismotivatesapproachesinwhichweﬁrstseekagoodrepresentationforp(x). Sucharepresentationmayalsobeagoodrepresentationforcomputingp(yx|)ifyisamongthemostsalientcausesofx. Thisideahasguidedalargeamountofdeeplearningresearchsinceatleastthe1990s(BeckerandHinton1992HintonandSejnowski1999,;,),inmoredetail.Forotherargumentsaboutwhensemi-supervisedlearningcanoutperformpuresupervisedlearning,wereferthereadertosection1.2of().Chapelleetal.2006Inotherapproachestorepresentationlearning,wehaveoftenbeenconcernedwitharepresentationthatiseasytomodel—forexample,onewhoseentriesaresparse,orindependentfromeachother.Arepresentationthatcleanlyseparatestheunderlyingcausalfactorsmaynotnecessarilybeonethatiseasytomodel.However,afurtherpartofthehypothesismotivatingsemi-supervisedlearningviaunsupervisedrepresentationlearningisthatformanyAItasks,thesetwopropertiescoincide: onceweareabletoobtaintheunderlyingexplanationsforwhatweobserve,itgenerallybecomeseasytoisolateindividualattributesfromtheothers.Speciﬁcally,ifarepresentationhrepresentsmanyoftheunderlyingcausesoftheobservedx,andtheoutputsyareamongthemostsalientcauses,thenitiseasytopredictfrom.yhFirst,letusseehowsemi-supervisedlearningcanfailbecauseunsupervisedlearningofp(x)isofnohelptolearnp(yx|).Considerforexamplethecasewherep(x)isuniformlydistributedandwewanttolearnf(x) =E[y|x].Clearly,observingatrainingsetofvaluesalonegivesusnoinformationabout.xp()y x|541 CHAPTER15.REPRESENTATIONLEARNING xpx()y=1y=2y=3 Figure15.4:Exampleofadensityoverxthatisamixtureoverthree components.Thecomponentidentityisanunderlyingexplanatoryfactor,y.Becausethemixturecomponents(e.g., naturalobjectclassesinimagedata)arestatisticallysalient,justmodelingp(x)inanunsupervisedwaywithnolabeledexamplealreadyrevealsthefactory.Next,letusseeasimpleexampleofhowsemi-supervisedlearningcansucceed.Considerthesituationwherexarisesfromamixture,withonemixturecomponentpervalueofy,asillustratedinﬁgure. Ifthemixturecomponentsarewell-15.4separated,thenmodelingp(x)revealspreciselywhereeachcomponentis,andasinglelabeledexampleofeachclasswillthenbeenoughtoperfectlylearnp(yx|).Butmoregenerally,whatcouldmakeandbetiedtogether?p()y x|p()xIfyiscloselyassociatedwithoneofthecausalfactorsofx,thenp(x)andp(yx|)will bestronglytied, andunsupervisedrepresentationlearningthattriestodisentangletheunderlyingfactorsofvariationislikelytobeusefulasasemi-supervisedlearningstrategy.Considertheassumptionthatyisoneofthecausalfactorsofx,andlethrepresentallthosefactors.Thetruegenerativeprocesscanbeconceivedasstructuredaccordingtothisdirectedgraphicalmodel,withastheparentof:h xp,pp.(hx) = ()xh|()h(15.1)Asaconsequence,thedatahasmarginalprobabilityp() = xEhp.()xh|(15.2)Fromthisstraightforwardobservation,weconcludethatthebestpossiblemodelofx(fromageneralizationpointofview)istheonethatuncoverstheabove“true”542 CHAPTER15.REPRESENTATIONLEARNINGstructure,withhasalatentvariablethatexplainstheobservedvariationsinx.The“ideal”representationlearningdiscussedaboveshouldthusrecovertheselatentfactors.Ifyisoneofthese(orcloselyrelatedtooneofthem),thenitwillbeveryeasytolearntopredictyfromsucharepresentation.WealsoseethattheconditionaldistributionofygivenxistiedbyBayes’ruletothecomponentsintheaboveequation:p() =yx|pp()xy|()yp()x.(15.3)Thusthemarginalp(x) isintimatelytiedtotheconditionalp(yx|) andknowledgeofthestructureoftheformershouldbehelpfultolearnthelatter.Therefore,insituationsrespectingtheseassumptions,semi-supervisedlearningshouldimproveperformance.Animportantresearchproblemregardsthefactthatmostobservationsareformedbyanextremelylargenumberofunderlyingcauses.Supposey=hi,buttheunsupervisedlearnerdoesnotknowwhichhi.Thebruteforcesolutionisforanunsupervisedlearnertolearnarepresentationthatcapturesthereasonablyallsalientgenerativefactorshjanddisentanglesthemfromeachother,thusmakingiteasytopredictfrom,regardlessofwhichhyhiisassociatedwith.yInpractice,thebruteforcesolutionisnotfeasiblebecauseitisnotpossibletocaptureallormostofthefactorsofvariationthatinﬂuenceanobservation.Forexample,inavisualscene,shouldtherepresentationalwaysencodeallofthesmallestobjectsinthebackground?Itisawell-documentedpsychologicalphenomenonthathumanbeingsfailtoperceivechangesintheirenvironmentthatarenotimmediatelyrelevanttothetasktheyareperforming—see,e.g.,SimonsandLevin1998().Animportantresearchfrontierinsemi-supervisedlearningisdeterminingtoencodeineachsituation.Currently,twoofthemainstrategieswhatfordealingwithalargenumberofunderlyingcausesaretouseasupervisedlearningsignalatthesametimeastheunsupervisedlearningsignalsothatthemodelwillchoosetocapturethemostrelevantfactorsofvariation,ortousemuchlargerrepresentationsifusingpurelyunsupervisedlearning.Anemergingstrategyforunsupervisedlearningistomodifythedeﬁnitionofwhichunderlyingcausesaremostsalient.Historically,autoencodersandgenerativemodelshavebeentrainedtooptimizeaﬁxedcriterion,oftensimilartomeansquarederror.Theseﬁxedcriteriadeterminewhichcausesareconsideredsalient.Forexample,meansquarederrorappliedtothepixelsofanimageimplicitlyspeciﬁesthatanunderlyingcauseisonlysalientifitsigniﬁcantlychangesthebrightnessofalargenumberofpixels.Thiscanbeproblematicifthetaskwewishtosolveinvolvesinteractingwithsmallobjects.Seeﬁgureforanexample15.5543 CHAPTER15.REPRESENTATIONLEARNINGInputReconstruction Figure15.5:Anautoencodertrainedwithmeansquarederrorforaroboticstaskhasfailedtoreconstructapingpongball.Theexistenceofthepingpongballandallofitsspatialcoordinatesareimportantunderlyingcausalfactorsthatgeneratetheimageandarerelevanttotheroboticstask. Unfortunately,theautoencoderhaslimitedcapacity,andthetrainingwithmeansquarederrordidnotidentifythepingpongballasbeingsalientenoughtoencode.ImagesgraciouslyprovidedbyChelseaFinn.ofaroboticstaskinwhichanautoencoderhasfailedtolearntoencodeasmallpingpongball.Thissamerobotiscapableofsuccessfullyinteractingwithlargerobjects,suchasbaseballs,whicharemoresalientaccordingtomeansquarederror.Otherdeﬁnitionsofsaliencearepossible.Forexample,ifagroupofpixelsfollowahighlyrecognizablepattern,evenifthatpatterndoesnotinvolveextremebrightnessordarkness,thenthatpatterncouldbeconsideredextremelysalient.Onewaytoimplementsuchadeﬁnitionofsalienceistousearecentlydevelopedapproachcalledgenerativeadversarialnetworks(,).Goodfellowetal.2014cInthisapproach,agenerativemodelistrainedtofoolafeedforwardclassiﬁer.Thefeedforwardclassiﬁerattemptstorecognizeallsamplesfromthegenerativemodelasbeingfake,andallsamplesfromthetrainingsetasbeingreal.Inthisframework,anystructuredpatternthatthefeedforwardnetworkcanrecognizeishighlysalient.Thegenerativeadversarialnetworkwillbedescribedinmoredetailinsection.Forthepurposesofthepresentdiscussion,itissuﬃcientto20.10.4understandthattheylearnhowtodeterminewhatissalient.()Lotteretal.2015showedthatmodelstrainedtogenerateimagesofhumanheadswilloftenneglecttogeneratetheearswhentrainedwithmeansquarederror,butwillsuccessfullygeneratetheearswhentrainedwiththeadversarialframework.Becausetheearsarenotextremelybrightordarkcomparedtothesurroundingskin,theyarenotespeciallysalientaccordingtomeansquarederrorloss,buttheirhighly544 CHAPTER15.REPRESENTATIONLEARNINGGroundTruthMSEAdversarial Figure15.6:Predictivegenerativenetworksprovideanexampleoftheimportanceoflearningwhichfeaturesaresalient. Inthisexample,thepredictivegenerativenetworkhasbeentrainedtopredicttheappearanceofa3-Dmodelofahumanheadataspeciﬁcviewingangle.(Left)Groundtruth.Thisisthecorrectimage,thatthenetworkshouldemit.Imageproducedbyapredictivegenerativenetworktrainedwithmean(Center)squarederroralone.Becausetheearsdonotcauseanextremediﬀerenceinbrightnesscomparedtotheneighboringskin,theywerenotsuﬃcientlysalientforthemodeltolearntorepresentthem.(Right)Imageproducedbyamodeltrainedwithacombinationofmeansquarederrorandadversarialloss. Usingthislearnedcostfunction,theearsaresalientbecausetheyfollowapredictablepattern.Learningwhichunderlyingcausesareimportantandrelevantenoughtomodelisanimportantactiveareaofresearch.Figuresgraciouslyprovidedby().Lotteretal.2015recognizableshapeandconsistentpositionmeansthatafeedforwardnetworkcaneasilylearntodetectthem,makingthemhighlysalientunderthegenerativeadversarialframework.Seeﬁgureforexampleimages.Generativeadversarial15.6networksareonlyonesteptowarddeterminingwhichfactorsshouldberepresented.Weexpectthatfutureresearchwilldiscoverbetterwaysofdeterminingwhichfactorstorepresent,anddevelopmechanismsforrepresentingdiﬀerentfactorsdependingonthetask.Abeneﬁtoflearningtheunderlyingcausalfactors,aspointedoutbySchölkopfetal.(),isthatifthetruegenerativeprocesshas2012xasaneﬀectandyasacause,thenmodelingp(x y|)isrobusttochangesinp(y). Ifthecause-eﬀectrelationshipwasreversed,thiswouldnotbetrue,sincebyBayes’rule,p(x y|)wouldbesensitivetochangesinp(y).Veryoften,whenweconsiderchangesindistributionduetodiﬀerentdomains,temporalnon-stationarity,orchangesinthenatureofthetask,thecausalmechanismsremaininvariant(thelawsoftheuniverseareconstant)whilethemarginaldistributionovertheunderlyingcausescanchange.Hence,bettergeneralizationandrobustnesstoallkindsofchangescan545 CHAPTER15.REPRESENTATIONLEARNINGbeexpectedvialearningagenerativemodelthatattemptstorecoverthecausalfactorsand.h p()xh|15.4DistributedRepresentationDistributedrepresentationsofconcepts—representationscomposedofmanyele-mentsthatcanbesetseparatelyfromeachother—areoneofthemostimportanttoolsforrepresentationlearning.Distributedrepresentationsarepowerfulbecausetheycanusenfeatureswithkvaluestodescribekndiﬀerentconcepts.Aswehaveseenthroughoutthisbook,bothneuralnetworkswithmultiplehiddenunitsandprobabilisticmodelswithmultiplelatentvariablesmakeuseofthestrategyofdistributedrepresentation. Wenowintroduceanadditionalobservation. Manydeeplearningalgorithmsaremotivatedbytheassumptionthatthehiddenunitscanlearntorepresenttheunderlyingcausalfactorsthatexplainthedata,asdiscussedinsection.Distributedrepresentationsarenaturalforthisapproach,15.3becauseeachdirectioninrepresentationspacecancorrespondtothevalueofadiﬀerentunderlyingconﬁgurationvariable.Anexampleofadistributedrepresentationisavectorofnbinaryfeatures,whichcantake2nconﬁgurations,eachpotentiallycorrespondingtoadiﬀerentregionininputspace,asillustratedinﬁgure.Thiscanbecomparedwith15.7asymbolicrepresentation,wheretheinputisassociatedwithasinglesymbolorcategory.Iftherearensymbolsinthedictionary,onecanimaginenfeaturedetectors,eachcorrespondingtothedetectionofthepresenceoftheassociatedcategory.Inthatcaseonlyndiﬀerentconﬁgurationsoftherepresentationspacearepossible,carvingndiﬀerentregionsininputspace,asillustratedinﬁgure.15.8Suchasymbolicrepresentationisalsocalledaone-hotrepresentation,sinceitcanbecapturedbyabinaryvectorwithnbitsthataremutuallyexclusive(onlyoneofthemcanbeactive).Asymbolicrepresentationisaspeciﬁcexampleofthebroaderclassofnon-distributedrepresentations,whicharerepresentationsthatmaycontainmanyentriesbutwithoutsigniﬁcantmeaningfulseparatecontrolovereachentry.Examplesoflearningalgorithms basedonnon-distributedrepresentationsinclude:•Clusteringmethods,includingthek-meansalgorithm:eachinputpointisassignedtoexactlyonecluster.•k-nearestneighborsalgorithms:oneorafewtemplatesorprototypeexamplesareassociatedwithagiveninput.Inthecaseofk>1,therearemultiple546 CHAPTER15.REPRESENTATIONLEARNING h1h2h3 h= [1,,11]h= [0,,11]h= [1,,01]h= [1,,10]h= [0,,10]h= [0,,01]h= [1,,00] Figure15.7:Illustrationofhowalearningalgorithmbasedonadistributedrepresentationbreaksuptheinputspaceintoregions.Inthisexample,therearethreebinaryfeaturesh1,h2,andh3. Eachfeatureisdeﬁnedbythresholdingtheoutputofalearned,lineartransformation.EachfeaturedividesR2intotwohalf-planes.Leth+ibethesetofinputpointsforwhichhi=1andh−ibethesetofinputpointsforwhichhi=0.Inthisillustration,eachlinerepresentsthedecisionboundaryforonehi,withthecorrespondingarrowpointingtotheh+isideoftheboundary.Therepresentationasawholetakesonauniquevalueateachpossibleintersectionofthesehalf-planes.Forexample,therepresentationvalue[1,1,1]correspondstotheregionh+1∩h+2∩h+3.Comparethistothenon-distributedrepresentationsinﬁgure.Inthegeneralcaseof15.8dinputdimensions,adistributedrepresentationdividesRdbyintersectinghalf-spacesratherthanhalf-planes.ThedistributedrepresentationwithnfeaturesassignsuniquecodestoO(nd)diﬀerentregions,whilethenearestneighboralgorithmwithnexamplesassignsuniquecodestoonlynregions.Thedistributedrepresentationisthusabletodistinguishexponentiallymanymoreregionsthanthenon-distributedone.Keepinmindthatnotallhvaluesarefeasible(thereisnoh=0inthisexample)andthatalinearclassiﬁerontopofthedistributedrepresentationisnotabletoassigndiﬀerentclassidentitiestoeveryneighboringregion;evenadeeplinear-thresholdnetworkhasaVCdimensionofonlyO(wwlog)wherewisthenumberofweights(,).ThecombinationofapowerfulrepresentationSontag1998layerandaweakclassiﬁerlayercanbeastrongregularizer;aclassiﬁertryingtolearntheconceptof“person”versus“notaperson”doesnotneedtoassignadiﬀerentclasstoaninputrepresentedas“womanwithglasses”thanitassignstoaninputrepresentedas“manwithoutglasses.”Thiscapacityconstraintencourageseachclassiﬁertofocusonfewhiandencouragestolearntorepresenttheclassesinalinearlyseparableway.h547 CHAPTER15.REPRESENTATIONLEARNINGvaluesdescribingeachinput,buttheycannotbecontrolledseparatelyfromeachother,sothisdoesnotqualifyasatruedistributedrepresentation.•Decisiontrees:onlyoneleaf(andthenodesonthepathfromroottoleaf)isactivatedwhenaninputisgiven.•Gaussianmixturesandmixturesofexperts:thetemplates(clustercenters)orexpertsarenowassociatedwithadegreeofactivation.Aswiththek-nearestneighborsalgorithm,eachinputisrepresentedwithmultiplevalues,butthosevaluescannotreadilybecontrolledseparatelyfromeachother.•KernelmachineswithaGaussiankernel(orothersimilarlylocalkernel):althoughthedegreeofactivationofeach“supportvector”ortemplateexampleisnowcontinuous-valued,thesameissuearisesaswithGaussianmixtures.•Languageortranslationmodelsbasedonn-grams.Thesetofcontexts(sequencesofsymbols)ispartitionedaccordingtoatreestructureofsuﬃxes.Aleafmaycorrespondtothelasttwowordsbeingw1andw2,forexample.Separateparametersareestimatedforeachleafofthetree(withsomesharingbeingpossible).Forsomeofthesenon-distributedalgorithms,theoutputisnotconstantbypartsbutinsteadinterpolatesbetweenneighboringregions.Therelationshipbetweenthenumberofparameters(orexamples)andthenumberofregionstheycandeﬁneremainslinear.Animportantrelatedconceptthatdistinguishesadistributedrepresentationfromasymboliconeisthatgeneralizationarisesduetosharedattributesbetweendiﬀerentconcepts.Aspuresymbols,“cat”and“dog”areasfarfromeachotherasanyothertwosymbols.However,ifoneassociatesthemwithameaningfuldistributedrepresentation,thenmanyofthethingsthatcanbesaidaboutcatscangeneralizetodogsandvice-versa.Forexample,ourdistributedrepresentationmaycontainentriessuchas“has_fur”or“number_of_legs”thathavethesamevaluefortheembeddingofboth“cat”and“dog.”Neurallanguagemodelsthatoperateondistributedrepresentationsofwordsgeneralizemuchbetterthanothermodelsthatoperatedirectlyonone-hotrepresentationsofwords,asdiscussedinsection.Distributedrepresentationsinducearich12.4similarityspace,inwhichsemanticallycloseconcepts(orinputs)arecloseindistance,apropertythatisabsentfrompurelysymbolicrepresentations.Whenandwhycantherebeastatisticaladvantagefromusingadistributedrepresentationaspartofalearningalgorithm? Distributedrepresentationscan548 CHAPTER15.REPRESENTATIONLEARNING Figure15.8:Illustrationofhowthenearestneighboralgorithmbreaksuptheinputspaceintodiﬀerentregions.Thenearestneighboralgorithmprovidesanexampleofalearningalgorithmbasedonanon-distributedrepresentation.Diﬀerentnon-distributedalgorithmsmayhavediﬀerentgeometry, but theytypicallybreaktheinput spaceintoregions,withaseparatesetofparametersforeachregion.Theadvantageofanon-distributedapproachisthat,givenenoughparameters,itcanﬁtthetrainingsetwithoutsolvingadiﬃcultoptimizationalgorithm,becauseitisstraightforwardtochooseadiﬀerentoutputindependentlyforeachregion.Thedisadvantageisthatsuchnon-distributedmodelsgeneralizeonlylocallyviathesmoothnessprior,makingitdiﬃculttolearnacomplicatedfunctionwithmorepeaksandtroughsthantheavailablenumberofexamples.Contrastthiswithadistributedrepresentation,ﬁgure.15.7 549 CHAPTER15.REPRESENTATIONLEARNINGhaveastatisticaladvantagewhenanapparentlycomplicatedstructurecanbecompactlyrepresentedusingasmallnumberofparameters.Sometraditionalnon-distributedlearningalgorithmsgeneralizeonlyduetothesmoothnessassumption,whichstatesthatifuv≈,thenthetargetfunctionftobelearnedhasthepropertythatf(u)≈f(v),ingeneral.Therearemanywaysofformalizingsuchanassumption,buttheendresultisthatifwehaveanexample(x,y)forwhichweknowthatf(x)≈y,thenwechooseanestimatorˆfthatapproximatelysatisﬁestheseconstraintswhilechangingaslittleaspossiblewhenwemovetoanearbyinputx+.Thisassumptionisclearlyveryuseful,butitsuﬀersfromthecurseofdimensionality: inordertolearnatargetfunctionthatincreasesanddecreasesmanytimesinmanydiﬀerentregions,1wemayneedanumberofexamplesthatisatleastaslargeasthenumberofdistinguishableregions.Onecanthinkofeachoftheseregionsasacategoryorsymbol:byhavingaseparatedegreeoffreedomforeachsymbol(orregion),wecanlearnanarbitrarydecodermappingfromsymboltovalue. However,thisdoesnotallowustogeneralizetonewsymbolsfornewregions.Ifwearelucky,theremaybesomeregularityinthetargetfunction,besidesbeingsmooth.Forexample,aconvolutionalnetworkwithmax-poolingcanrecognizeanobjectregardlessofitslocationintheimage,eventhoughspatialtranslationoftheobjectmaynotcorrespondtosmoothtransformationsintheinputspace.Letusexamineaspecialcaseofadistributedrepresentationlearningalgorithm,thatextractsbinaryfeaturesbythresholdinglinearfunctionsoftheinput.EachbinaryfeatureinthisrepresentationdividesRdintoapairofhalf-spaces, asillustratedinﬁgure.Theexponentiallylargenumberofintersectionsof15.7nofthecorrespondinghalf-spacesdetermineshowmanyregionsthisdistributedrepresentationlearnercandistinguish.HowmanyregionsaregeneratedbyanarrangementofnhyperplanesinRd?Byapplyingageneralresultconcerningtheintersectionofhyperplanes(,),onecanshow(Zaslavsky1975Pascanu2014betal.,)thatthenumberofregionsthisbinaryfeaturerepresentationcandistinguishisdj=0nj= (Ond).(15.4)Therefore,weseeagrowththatisexponentialintheinputsizeandpolynomialinthenumberofhiddenunits.1Potentially,wemaywanttolearnafunctionwhosebehaviorisdistinctinexponentiallymanyregions:inad-dimensionalspacewithatleast2diﬀerentvaluestodistinguishperdimension,wemightwanttodiﬀerinf2ddiﬀerentregions,requiringO(2d)trainingexamples.550 CHAPTER15.REPRESENTATIONLEARNINGThisprovidesageometricargumenttoexplainthegeneralizationpowerofdistributedrepresentation:withO(nd)parameters(fornlinear-thresholdfeaturesinRd)wecandistinctlyrepresentO(nd) regionsininputspace.Ifinsteadwemadenoassumptionatallaboutthedata,andusedarepresentationwithoneuniquesymbolforeachregion,andseparateparametersforeachsymboltorecognizeitscorrespondingportionofRd,thenspecifyingO(nd)regionswouldrequireO(nd)examples.Moregenerally,theargumentinfavorofthedistributedrepresentationcouldbeextendedtothecasewhereinsteadofusinglinearthresholdunitsweusenonlinear,possiblycontinuous,featureextractorsforeachoftheattributesinthedistributedrepresentation.Theargumentinthiscaseisthatifaparametrictransformationwithkparameterscanlearnaboutrregionsininputspace,withkr,andifobtainingsucharepresentationwasusefultothetaskofinterest,thenwecouldpotentiallygeneralizemuchbetterinthiswaythaninanon-distributedsettingwherewewouldneedO(r)examplestoobtainthesamefeaturesandassociatedpartitioningoftheinputspaceintorregions.Usingfewerparameterstorepresentthemodelmeansthatwehavefewerparameterstoﬁt,andthusrequirefarfewertrainingexamplestogeneralizewell.Afurtherpartoftheargumentforwhymodelsbasedondistributedrepresen-tationsgeneralizewellisthattheircapacityremainslimiteddespitebeingabletodistinctlyencodesomanydiﬀerentregions.Forexample,theVCdimensionofaneuralnetworkoflinearthresholdunitsisonlyO(wwlog),wherewisthenumberofweights(Sontag1998,).Thislimitationarisesbecause,whilewecanassignverymanyuniquecodestorepresentationspace,wecannotuseabsolutelyallofthecodespace,norcanwelearnarbitraryfunctionsmappingfromtherepresentationspacehtotheoutputyusingalinearclassiﬁer.Theuseofadistributedrepresentationcombinedwithalinearclassiﬁerthusexpressesapriorbeliefthattheclassestoberecognizedarelinearlyseparableasafunctionoftheunderlyingcausalfactorscapturedbyh. Wewilltypicallywanttolearncategoriessuchasthesetofallimagesofallgreenobjectsorthesetofallimagesofcars,butnotcategoriesthatrequirenonlinear,XORlogic.Forexample,wetypicallydonotwanttopartitionthedataintothesetofallredcarsandgreentrucksasoneclassandthesetofallgreencarsandredtrucksasanotherclass.Theideasdiscussedsofarhavebeenabstract,buttheymaybeexperimentallyvalidated.()ﬁndthathiddenunitsinadeepconvolutionalnetworkZhouetal.2015trainedontheImageNetandPlacesbenchmarkdatasetslearnfeaturesthatareveryofteninterpretable,correspondingtoalabelthathumanswouldnaturallyassign.Inpracticeitiscertainlynotalwaysthecasethathiddenunitslearnsomethingthathasasimplelinguisticname,butitisinterestingtoseethisemergenearthetoplevelsofthebestcomputervisiondeepnetworks.Whatsuchfeatureshavein551 CHAPTER15.REPRESENTATIONLEARNING-+=Figure15.9:Agenerativemodelhaslearnedadistributedrepresentationthatdisentanglestheconceptofgenderfromtheconceptofwearingglasses. Ifwebeginwiththerepre-sentationoftheconceptofamanwithglasses,thensubtractthevectorrepresentingtheconceptofamanwithoutglasses,andﬁnallyaddthevectorrepresentingtheconceptofawomanwithoutglasses,weobtainthevectorrepresentingtheconceptofawomanwithglasses.Thegenerativemodelcorrectlydecodesalloftheserepresentationvectorstoimagesthatmayberecognizedasbelongingtothecorrectclass.Imagesreproducedwithpermissionfrom().Radfordetal.2015commonisthatonecouldimaginelearningabouteachofthemwithouthavingtoseealltheconﬁgurationsofalltheothers.()demonstratedthatRadfordetal.2015agenerativemodelcanlearnarepresentationofimagesoffaces,withseparatedirectionsinrepresentationspacecapturingdiﬀerentunderlyingfactorsofvariation.Figuredemonstratesthatonedirectioninrepresentationspacecorresponds15.9towhetherthepersonismaleorfemale,whileanothercorrespondstowhetherthepersoniswearingglasses.Thesefeatureswerediscoveredautomatically,notﬁxedapriori.Thereisnoneedtohavelabelsforthehiddenunitclassiﬁers:gradientdescentonanobjectivefunctionofinterestnaturallylearnssemanticallyinterestingfeatures,solongasthetaskrequiressuchfeatures.Wecanlearnaboutthedistinctionbetweenmaleandfemale,oraboutthepresenceorabsenceofglasses,withouthavingtocharacterizealloftheconﬁgurationsofthen−1otherfeaturesbyexamplescoveringallofthesecombinationsofvalues. Thisformofstatisticalseparabilityiswhatallowsonetogeneralizetonewconﬁgurationsofaperson’sfeaturesthathaveneverbeenseenduringtraining. 552 CHAPTER15.REPRESENTATIONLEARNING15.5ExponentialGainsfromDepthWehaveseeninsectionthatmultilayerperceptronsareuniversalapproxima-6.4.1tors,andthatsomefunctionscanberepresentedbyexponentiallysmallerdeepnetworkscomparedtoshallownetworks.Thisdecreaseinmodelsizeleadstoimprovedstatisticaleﬃciency.Inthissection,wedescribehowsimilarresultsapplymoregenerallytootherkindsofmodelswithdistributedhiddenrepresentations.Insection,wesawanexampleofagenerativemodelthatlearnedabout15.4theexplanatoryfactorsunderlyingimagesoffaces,includingtheperson’sgenderandwhethertheyarewearingglasses.Thegenerativemodelthataccomplishedthistaskwasbasedonadeepneuralnetwork.Itwouldnotbereasonabletoexpectashallownetwork,suchasalinearnetwork,tolearnthecomplicatedrelationshipbetweentheseabstractexplanatoryfactorsandthepixelsintheimage. InthisandotherAItasks,thefactorsthatcanbechosenalmostindependentlyfromeachotheryetstillcorrespondtomeaningfulinputsaremorelikelytobeveryhigh-levelandrelatedinhighlynonlinearwaystotheinput.Wearguethatthisdemandsdeepdistributedrepresentations,wherethehigherlevelfeatures(seenasfunctionsoftheinput)orfactors(seenasgenerativecauses)areobtainedthroughthecompositionofmanynonlinearities.Ithasbeenproveninmanydiﬀerentsettingsthatorganizingcomputationthroughthecompositionofmanynonlinearitiesandahierarchyofreusedfeaturescangiveanexponentialboosttostatisticaleﬃciency,ontopoftheexponentialboostgivenbyusingadistributedrepresentation.Manykindsofnetworks(e.g.,withsaturatingnonlinearities,Booleangates,sum/products,orRBFunits)withasinglehiddenlayercanbeshowntobeuniversalapproximators.Amodelfamilythatisauniversalapproximatorcanapproximatealargeclassoffunctions(includingallcontinuousfunctions)uptoanynon-zerotolerancelevel,givenenoughhiddenunits. However,therequirednumberofhiddenunitsmaybeverylarge.Theoreticalresultsconcerningtheexpressivepowerofdeeparchitecturesstatethattherearefamiliesoffunctionsthatcanberepresentedeﬃcientlybyanarchitectureofdepthk,butwouldrequireanexponentialnumberofhiddenunits(withrespecttotheinputsize)withinsuﬃcientdepth(depth2ordepth).k−1Insection,wesawthatdeterministicfeedforwardnetworksareuniversal6.4.1approximatorsoffunctions.Manystructuredprobabilisticmodelswithasinglehiddenlayeroflatentvariables,includingrestrictedBoltzmannmachinesanddeepbeliefnetworks,areuniversalapproximatorsofprobabilitydistributions(LeRouxandBengio20082010MontúfarandAy2011Montúfar2014Krause,,;,;,;etal.,2013).553 CHAPTER15.REPRESENTATIONLEARNINGInsection,wesawthatasuﬃcientlydeepfeedforwardnetworkcanhave6.4.1anexponentialadvantageoveranetworkthatistooshallow.Suchresultscanalsobeobtainedforothermodelssuchasprobabilisticmodels.Onesuchprobabilisticmodelisthesum-productnetworkorSPN(PoonandDomingos2011,).Thesemodelsusepolynomialcircuitstocomputetheprobabilitydistributionoverasetofrandomvariables.()showedthatthereexistDelalleauandBengio2011probabilitydistributionsforwhichaminimumdepthofSPNisrequiredtoavoidneedinganexponentiallylargemodel.Later,()MartensandMedabalimi2014showedthattherearesigniﬁcantdiﬀerencesbetweeneverytwoﬁnitedepthsofSPN,andthatsomeoftheconstraintsusedtomakeSPNstractablemaylimittheirrepresentationalpower.Anotherinterestingdevelopmentisasetoftheoreticalresultsfortheexpressivepoweroffamiliesofdeepcircuitsrelatedtoconvolutionalnets,highlightinganexponentialadvantageforthedeepcircuitevenwhentheshallowcircuitisallowedtoonlyapproximatethefunctioncomputedbythedeepcircuit(,Cohenetal.2015).Bycomparison,previoustheoreticalworkmadeclaimsregardingonlythecasewheretheshallowcircuitmustexactlyreplicateparticularfunctions.15.6ProvidingCluestoDiscoverUnderlyingCausesToclosethischapter,wecomebacktooneofouroriginalquestions:whatmakesonerepresentationbetterthananother?Oneanswer,ﬁrstintroducedinsection,is15.3thatanidealrepresentationisonethatdisentanglestheunderlyingcausalfactorsofvariationthatgeneratedthedata,especiallythosefactorsthatarerelevanttoourapplications.Moststrategiesforrepresentationlearningarebasedonintroducingcluesthathelpthelearningtoﬁndtheseunderlyingfactorsofvariations.Thecluescanhelpthelearnerseparatetheseobservedfactorsfromtheothers.Supervisedlearningprovidesaverystrongclue:alabely,presentedwitheachx,thatusuallyspeciﬁesthevalueofatleastoneofthefactorsofvariationdirectly.Moregenerally,tomakeuseofabundantunlabeleddata,representationlearningmakesuseofother,lessdirect,hintsabouttheunderlyingfactors.Thesehintstaketheformofimplicitpriorbeliefsthatwe,thedesignersofthelearningalgorithm,imposeinordertoguidethelearner.Resultssuchasthenofreelunchtheoremshowthatregularizationstrategiesarenecessarytoobtaingoodgeneralization.Whileitisimpossibletoﬁndauniversallysuperiorregularizationstrategy,onegoalofdeeplearningistoﬁndasetoffairlygenericregularizationstrategiesthatareapplicabletoawidevarietyofAItasks,similartothetasksthatpeopleandanimalsareabletosolve.554 CHAPTER15.REPRESENTATIONLEARNINGWeprovideherealistofthesegenericregularizationstrategies.Thelistisclearlynotexhaustive,butgivessomeconcreteexamplesofwaysthatlearningalgorithmscanbeencouragedtodiscoverfeaturesthatcorrespondtounderlyingfactors.Thislistwasintroducedinsection3.1of()andhasBengioetal.2013dbeenpartiallyexpandedhere.•Smoothness:Thisistheassumptionthatf(x+d)≈f(x)forunitdandsmall.Thisassumptionallowsthelearnertogeneralizefromtrainingexamplestonearbypointsininputspace.Manymachinelearningalgorithmsleveragethisidea,butitisinsuﬃcienttoovercomethecurseofdimensionality.•Linearity:Manylearningalgorithmsassumethatrelationshipsbetweensomevariablesarelinear.Thisallowsthealgorithmtomakepredictionsevenveryfarfromtheobserveddata,butcansometimesleadtooverlyextremepredictions.Mostsimplemachinelearningalgorithmsthatdonotmakethesmoothnessassumptioninsteadmakethelinearityassumption.Theseareinfactdiﬀerentassumptions—linearfunctionswithlargeweightsappliedtohigh-dimensionalspacesmaynotbeverysmooth.SeeGoodfellowetal.()forafurtherdiscussionofthelimitationsofthelinearityassumption.2014b•Multipleexplanatoryfactors:Manyrepresentationlearningalgorithmsaremotivatedbytheassumptionthatthedataisgeneratedbymultipleunderlyingexplanatoryfactors,andthatmosttaskscanbesolvedeasilygiventhestateofeachofthesefactors.Sectiondescribeshowthisviewmotivatessemi-15.3supervisedlearningviarepresentationlearning.Learningthestructureofp(x)requireslearningsomeofthesamefeaturesthatareusefulformodelingp(y|x)becausebothrefertothesameunderlyingexplanatoryfactors.Section15.4describeshowthisviewmotivatestheuseofdistributedrepresentations,withseparatedirectionsinrepresentationspacecorrespondingtoseparatefactorsofvariation.•Causalfactors:themodelisconstructedinsuchawaythatittreatsthefactorsofvariationdescribedbythelearnedrepresentationhasthecausesoftheobserveddatax,andnotvice-versa.Asdiscussedinsection,this15.3isadvantageousforsemi-supervisedlearningandmakesthelearnedmodelmorerobustwhenthedistributionovertheunderlyingcauseschangesorwhenweusethemodelforanewtask.•Depthahierarchical organization ofexplanatory factors, or :High-level,abstractconceptscanbedeﬁnedintermsofsimpleconcepts,formingahierarchy.From another pointof view, the use ofa deeparchitecture555 CHAPTER15.REPRESENTATIONLEARNINGexpressesourbeliefthatthetaskshouldbeaccomplishedviaamulti-stepprogram, witheachstep referringbacktothe outputoftheprocessingaccomplishedviaprevioussteps.•Sharedfactors across tasks:In thecontextwherewehavemanytasks,correspondingtodiﬀerentyivariablessharingthesameinputxorwhereeachtaskisassociatedwithasubsetorafunctionf()i(x)ofaglobalinputx,theassumptionisthateachyiisassociatedwithadiﬀerentsubsetfromacommonpoolofrelevantfactorsh.Becausethesesubsetsoverlap,learningalltheP(yi|x)viaasharedintermediaterepresentationP(h x|)allowssharingofstatisticalstrengthbetweenthetasks.•Manifolds:Probabilitymassconcentrates,andtheregionsinwhichitcon-centratesarelocallyconnectedandoccupyatinyvolume.Inthecontinuouscase,theseregionscanbeapproximatedbylow-dimensionalmanifoldswithamuchsmallerdimensionalitythantheoriginalspacewherethedatalives.Manymachinelearningalgorithmsbehavesensiblyonlyonthismanifold(,).Somemachinelearningalgorithms,especiallyGoodfellowetal.2014bautoencoders,attempttoexplicitlylearnthestructureofthemanifold.•Naturalclustering:Manymachinelearningalgorithmsassumethateachconnectedmanifoldintheinputspacemaybeassignedtoasingleclass.Thedatamaylieonmanydisconnectedmanifolds,buttheclassremainsconstantwithineachoneofthese. Thisassumptionmotivatesavarietyoflearningalgorithms,includingtangentpropagation,doublebackprop,themanifoldtangentclassiﬁerandadversarialtraining.•Temporalandspatialcoherence:Slowfeatureanalysisandrelatedalgorithmsmaketheassumptionthatthemostimportantexplanatoryfactorschangeslowlyovertime,oratleastthatitiseasiertopredictthetrueunderlyingexplanatoryfactorsthantopredictrawobservationssuchaspixelvalues.Seesectionforfurtherdescriptionofthisapproach.13.3•Sparsity:Mostfeaturesshouldpresumablynotberelevanttodescribingmostinputs—thereisnoneedtouseafeaturethatdetectselephanttrunkswhenrepresentinganimageofacat.Itisthereforereasonabletoimposeapriorthatanyfeaturethatcanbeinterpretedas“present”or“absent”shouldbeabsentmostofthetime.•SimplicityofFactorDependencies:Ingoodhigh-levelrepresentations,thefactorsarerelatedtoeachotherthroughsimpledependencies.Thesimplest556 CHAPTER15.REPRESENTATIONLEARNINGpossibleismarginalindependence,P(h) =iP(hi),butlineardependenciesorthosecapturedbyashallowautoencoderarealsoreasonableassumptions.Thiscanbeseeninmanylawsofphysics,andisassumedwhenpluggingalinearpredictororafactorizedpriorontopofalearnedrepresentation.Theconceptofrepresentationlearningtiestogetherallofthemanyformsofdeeplearning.Feedforwardandrecurrentnetworks,autoencodersanddeepprobabilisticmodelsalllearnandexploitrepresentations.Learning thebestpossiblerepresentationremainsanexcitingavenueofresearch. 557 Chapter16StructuredProbabilisticModelsforDeepLearningDeeplearningdrawsuponmanymodelingformalismsthatresearcherscanusetoguidetheirdesigneﬀortsanddescribetheiralgorithms.Oneoftheseformalismsistheideaofstructuredprobabilisticmodels.Wehavealreadydiscussedstructuredprobabilisticmodelsbrieﬂyinsection.Thatbriefpresentationwas3.14suﬃcienttounderstandhowtousestructuredprobabilisticmodelsasalanguagetodescribesomeofthealgorithmsinpart.Now,inpart,structuredprobabilisticIIIIImodelsareakeyingredientofmanyofthemostimportantresearchtopicsindeeplearning.Inordertopreparetodiscusstheseresearchideas,thischapterdescribesstructuredprobabilisticmodelsinmuchgreaterdetail.Thischapterisintendedtobeself-contained;thereaderdoesnotneedtoreviewtheearlierintroductionbeforecontinuingwiththischapter.Astructuredprobabilisticmodelisawayofdescribingaprobabilitydistribution,usingagraphtodescribewhichrandomvariablesintheprobabilitydistributioninteractwitheachotherdirectly.Hereweuse“graph”inthegraphtheorysense—asetofverticesconnectedtooneanotherbyasetofedges.Becausethestructureofthemodelisdeﬁnedbyagraph,thesemodelsareoftenalsoreferredtoasgraphicalmodels.Thegraphicalmodelsresearchcommunityislargeandhasdevelopedmanydiﬀerentmodels,trainingalgorithms,andinferencealgorithms.Inthischapter,weprovidebasicbackgroundonsomeofthemostcentralideasofgraphicalmodels,withanemphasisontheconceptsthathaveprovenmostusefultothedeeplearningresearchcommunity.Ifyoualreadyhaveastrongbackgroundingraphicalmodels,youmaywishtoskipmostofthischapter.However,evenagraphicalmodelexpert558 CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNINGmaybeneﬁtfromreadingtheﬁnalsectionofthischapter,section,inwhichwe16.7highlightsomeoftheuniquewaysthatgraphicalmodelsareusedfordeeplearningalgorithms.Deeplearningpractitionerstendtouseverydiﬀerentmodelstructures,learningalgorithmsandinferenceproceduresthanarecommonlyusedbytherestofthegraphicalmodelsresearchcommunity.Inthischapter,weidentifythesediﬀerencesinpreferencesandexplainthereasonsforthem.Inthischapterweﬁrstdescribethechallengesofbuildinglarge-scaleproba-bilisticmodels. Next,wedescribehowtouseagraphtodescribethestructureofaprobabilitydistribution.Whilethisapproachallowsustoovercomemanychallenges,itisnotwithoutitsowncomplications.Oneofthemajordiﬃcultiesingraphicalmodelingisunderstandingwhichvariablesneedtobeabletointeractdirectly,i.e.,whichgraphstructuresaremostsuitableforagivenproblem. Weoutlinetwoapproachestoresolvingthisdiﬃcultybylearningaboutthedependen-ciesinsection.Finally,weclosewithadiscussionoftheuniqueemphasisthat16.5deeplearningpractitionersplaceonspeciﬁcapproachestographicalmodelinginsection.16.716.1TheChallengeofUnstructuredModelingThegoalofdeeplearningistoscalemachinelearningtothekindsofchallengesneededtosolveartiﬁcialintelligence.Thismeansbeingabletounderstandhigh-dimensionaldatawithrichstructure.Forexample,wewouldlikeAIalgorithmstobeabletounderstandnaturalimages,1audiowaveformsrepresentingspeech,anddocumentscontainingmultiplewordsandpunctuationcharacters.Classiﬁcationalgorithmscantakeaninputfromsucharichhigh-dimensionaldistributionandsummarizeitwithacategoricallabel—whatobjectisinaphoto,whatwordisspokeninarecording,whattopicadocumentisabout.Theprocessofclassiﬁcationdiscardsmostoftheinformationintheinputandproducesasingleoutput(oraprobabilitydistributionovervaluesofthatsingleoutput).Theclassiﬁerisalsooftenabletoignoremanypartsoftheinput.Forexample,whenrecognizinganobjectinaphoto,itisusuallypossibletoignorethebackgroundofthephoto.Itispossibletoaskprobabilisticmodelstodomanyothertasks.Thesetasksareoftenmoreexpensivethanclassiﬁcation.Someofthemrequireproducingmultipleoutputvalues.Mostrequireacompleteunderstandingoftheentirestructureof1Anaturalimageisanimagethatmightbecapturedbyacamerainareasonablyordinaryenvironment,asopposedtoasyntheticallyrenderedimage,ascreenshotofawebpage,etc.559 CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNINGtheinput,withnooptiontoignoresectionsofit.Thesetasksincludethefollowing:•Densityestimation:givenaninputx,themachinelearningsystemreturnsanestimateofthetruedensityp(x)underthedatageneratingdistribution.Thisrequiresonlyasingleoutput,butitdoesrequireacompleteunderstand-ingoftheentireinput.Ifevenoneelementofthevectorisunusual,thesystemmustassignitalowprobability.•Denoising:givenadamagedorincorrectlyobservedinput˜x,themachinelearningsystemreturnsanestimateoftheoriginalorcorrectx.Forexample,themachinelearningsystemmightbeaskedtoremovedustorscratchesfromanoldphotograph.Thisrequiresmultipleoutputs(everyelementoftheestimatedcleanexamplex)andanunderstandingoftheentireinput(sinceevenonedamagedareawillstillrevealtheﬁnalestimateasbeingdamaged).•Missingvalueimputation:giventheobservationsofsomeelementsofx,themodelisaskedtoreturnestimatesoforaprobabilitydistributionoversomeoralloftheunobservedelementsofx.Thisrequiresmultipleoutputs.Becausethemodelcouldbeaskedtorestoreanyoftheelementsofx,itmustunderstandtheentireinput.•Sampling:themodelgeneratesnewsamplesfromthedistributionp(x).Applicationsincludespeechsynthesis,i.e.producingnewwaveformsthatsoundlikenaturalhumanspeech.Thisrequiresmultipleoutputvaluesandagoodmodeloftheentireinput.Ifthesampleshaveevenoneelementdrawnfromthewrongdistribution,thenthesamplingprocessiswrong.Foranexampleofasamplingtaskusingsmallnaturalimages,seeﬁgure.16.1Modelingarichdistributionoverthousandsormillionsofrandomvariablesisachallengingtask,bothcomputationallyandstatistically.Supposeweonlywantedtomodelbinaryvariables.Thisisthesimplestpossiblecase,andyetalreadyitseemsoverwhelming.Forasmall,32×322pixelcolor(RGB)image,thereare3072possiblebinaryimagesofthisform.Thisnumberisover10800timeslargerthantheestimatednumberofatomsintheuniverse.Ingeneral,ifwewishtomodeladistributionoverarandomvectorxcontainingndiscretevariablescapableoftakingonkvalueseach,thenthenaiveapproachofrepresentingP(x)bystoringalookuptablewithoneprobabilityvalueperpossibleoutcomerequiresknparameters!Thisisnotfeasibleforseveralreasons:560 CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING Figure16.1:Probabilisticmodelingofnaturalimages.(Top)Example32×32pixelcolorimagesfromtheCIFAR-10dataset(,).SamplesKrizhevskyandHinton2009(Bottom)drawnfromastructuredprobabilisticmodeltrainedonthisdataset.EachsampleappearsatthesamepositioninthegridasthetrainingexamplethatisclosesttoitinEuclideanspace.Thiscomparisonallowsustoseethatthemodelistrulysynthesizingnewimages,ratherthanmemorizingthetrainingdata.Contrastofbothsetsofimageshasbeenadjustedfordisplay.Figurereproducedwithpermissionfrom().Courvilleetal.2011561 CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING•Memory:thecostofstoringtherepresentation:Forallbutverysmallvaluesofnandk,representingthedistributionasatablewillrequiretoomanyvaluestostore.•Statisticaleﬃciency:Asthenumberofparametersinamodelincreases,sodoestheamountoftrainingdataneededtochoosethevaluesofthoseparametersusingastatisticalestimator.Becausethetable-basedmodelhasanastronomicalnumberofparameters,itwillrequireanastronomicallylargetrainingsettoﬁtaccurately.Anysuchmodelwilloverﬁtthetrainingsetverybadlyunlessadditionalassumptionsaremadelinkingthediﬀerententriesinthetable(forexample,likeinback-oﬀorsmoothedn-grammodels,section).12.4.1•Runtime: thecostofinference: SupposewewanttoperformaninferencetaskwhereweuseourmodelofthejointdistributionP(x)tocomputesomeotherdistribution,suchasthemarginaldistributionP(x1)ortheconditionaldistributionP(x2|x1).Computingthesedistributionswillrequiresummingacrosstheentiretable,sotheruntimeoftheseoperationsisashighastheintractablememorycostofstoringthemodel.•Runtime:thecostofsampling:Likewise,supposewewanttodrawasamplefromthemodel.Thenaivewaytodothisistosamplesomevalueu∼U(0,1),theniteratethroughthetable,addinguptheprobabilityvaluesuntiltheyexceeduandreturntheoutcomecorrespondingtothatpositioninthetable.Thisrequiresreadingthroughthewholetableintheworstcase,soithasthesameexponentialcostastheotheroperations.Theproblemwiththetable-basedapproachisthatweareexplicitlymodelingeverypossiblekindofinteractionbetweeneverypossiblesubsetofvariables.Theprobabilitydistributionsweencounterinrealtasksaremuchsimplerthanthis.Usually,mostvariablesinﬂuenceeachotheronlyindirectly.Forexample,considermodelingtheﬁnishingtimesofateaminarelayrace.Supposetheteamconsistsofthreerunners:Alice,BobandCarol.Atthestartoftherace,Alicecarriesabatonandbeginsrunningaroundatrack.Aftercompletingherlaparoundthetrack,shehandsthebatontoBob.BobthenrunshisownlapandhandsthebatontoCarol,whorunstheﬁnallap.Wecanmodeleachoftheirﬁnishingtimesasacontinuousrandomvariable.Alice’sﬁnishingtimedoesnotdependonanyoneelse’s,sinceshegoesﬁrst.Bob’sﬁnishingtimedependsonAlice’s,becauseBobdoesnothavetheopportunitytostarthislapuntilAlicehascompletedhers. IfAliceﬁnishesfaster,Bobwillﬁnishfaster,allelsebeing562 CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNINGequal.Finally,Carol’sﬁnishingtimedependsonbothherteammates.IfAliceisslow,Bobwillprobablyﬁnishlatetoo.Asaconsequence,Carolwillhavequitealatestartingtimeandthusislikelytohavealateﬁnishingtimeaswell.However,Carol’sﬁnishingtimedependsonlyindirectlyonAlice’sﬁnishingtimeviaBob’s.IfwealreadyknowBob’sﬁnishingtime,wewillnotbeabletoestimateCarol’sﬁnishingtimebetterbyﬁndingoutwhatAlice’sﬁnishingtimewas.Thismeanswecanmodeltherelayraceusingonlytwointeractions:Alice’seﬀectonBobandBob’seﬀectonCarol.Wecanomitthethird,indirectinteractionbetweenAliceandCarolfromourmodel.Structuredprobabilisticmodelsprovideaformalframeworkformodelingonlydirectinteractionsbetweenrandomvariables.Thisallowsthemodelstohavesigniﬁcantlyfewerparametersandthereforebeestimatedreliablyfromlessdata.Thesesmallermodelsalsohavedramaticallyreducedcomputationalcostintermsofstoringthemodel,performinginferenceinthemodel,anddrawingsamplesfromthemodel.16.2UsingGraphstoDescribeModelStructureStructuredprobabilisticmodelsusegraphs(inthegraphtheorysenseof“nodes”or“vertices”connectedbyedges)torepresentinteractionsbetweenrandomvariables.Eachnoderepresentsarandomvariable.Eachedgerepresentsadirectinteraction.Thesedirectinteractionsimplyother,indirectinteractions,butonlythedirectinteractionsneedtobeexplicitlymodeled.Thereismore thanone wayto describe theinteractionsin aprobabilitydistributionusingagraph.Inthefollowingsectionswedescribesomeofthemostpopularandusefulapproaches.Graphicalmodelscanbelargelydividedintotwocategories:modelsbasedondirectedacyclicgraphs,andmodelsbasedonundirectedgraphs.16.2.1DirectedModelsOnekindofstructuredprobabilisticmodelisthedirectedgraphicalmodel,otherwiseknownasthebeliefnetworkBayesiannetworkor2(Pearl1985,).Directedgraphicalmodelsarecalled“directed”becausetheiredgesaredirected,2JudeaPearlsuggestedusingtheterm“Bayesiannetwork”whenonewishesto“emphasizethejudgmental”natureofthevaluescomputedbythenetwork,i.e.tohighlightthattheyusuallyrepresentdegreesofbeliefratherthanfrequenciesofevents.563 CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNINGt0t0t1t1t2t2AliceBobCarolFigure16.2:Adirectedgraphicalmodeldepictingtherelayraceexample.Alice’sﬁnishingtimet0inﬂuencesBob’sﬁnishingtimet1,becauseBobdoesnotgettostartrunninguntilAliceﬁnishes.Likewise,CarolonlygetstostartrunningafterBobﬁnishes,soBob’sﬁnishingtimet1directlyinﬂuencesCarol’sﬁnishingtimet2.thatis,theypointfromonevertextoanother.Thisdirectionisrepresentedinthedrawingwithanarrow.Thedirectionofthearrowindicateswhichvariable’sprobabilitydistributionisdeﬁnedintermsoftheother’s.Drawinganarrowfromatobmeansthatwedeﬁnetheprobabilitydistributionoverbviaaconditionaldistribution,withaasoneofthevariablesontherightsideoftheconditioningbar.Inotherwords,thedistributionoverbdependsonthevalueofa.Continuingwiththerelayraceexamplefromsection,supposewename16.1Alice’sﬁnishingtimet0,Bob’sﬁnishingtimet1,andCarol’sﬁnishingtimet2.Aswesawearlier,ourestimateoft1dependsont0.Ourestimateoft2dependsdirectlyont1butonlyindirectlyont0.Wecandrawthisrelationshipinadirectedgraphicalmodel,illustratedinﬁgure.16.2Formally,adirectedgraphicalmodeldeﬁnedonvariablesxisdeﬁnedbyadirectedacyclicgraphGwhoseverticesaretherandomvariablesinthemodel,andasetoflocalconditionalprobabilitydistributionsp(xi|PaG(xi)) wherePaG(xi)givestheparentsofxiinG.Theprobabilitydistributionoverxisgivenbyp() = Πxip(xi|PaG(xi)).(16.1)Inourrelayraceexample,thismeansthat,usingthegraphdrawninﬁgure,16.2p(t0,t1,t2) = (pt0)(pt1|t0)(pt2|t1).(16.2)Thisisourﬁrsttimeseeingastructuredprobabilisticmodelinaction.Wecanexaminethecostofusingit,inordertoobservehowstructuredmodelinghasmanyadvantagesrelativetounstructuredmodeling.Supposewerepresentedtimebydiscretizingtimerangingfromminute0tominute10into6secondchunks.Thiswouldmaket0,t1andt2eachbeadiscretevariablewith100possiblevalues.Ifweattemptedtorepresentp(t0,t1,t2)withatable,itwouldneedtostore999,999values(100valuesoft0×100valuesoft1×100valuesoft2,minus1,sincetheprobabilityofoneoftheconﬁgurationsismade564 CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNINGredundantbytheconstraintthatthesumoftheprobabilitiesbe1).Ifinstead,weonlymakeatableforeachoftheconditionalprobabilitydistributions,thenthedistributionovert0requires99values,thetabledeﬁningt1givent0requires9900values,andsodoesthetabledeﬁningt2givent1.Thiscomestoatotalof19,899values.Thismeansthatusingthedirectedgraphicalmodelreducedournumberofparametersbyafactorofmorethan50!Ingeneral,tomodelndiscretevariableseachhavingkvalues,thecostofthesingletableapproachscaleslikeO(kn),aswehaveobservedbefore.Nowsupposewebuildadirectedgraphicalmodeloverthesevariables. Ifmisthemaximumnumberofvariablesappearing(oneithersideoftheconditioningbar)inasingleconditionalprobabilitydistribution,thenthecostofthetablesforthedirectedmodelscaleslikeO(km).Aslongaswecandesignamodelsuchthatm<<n,wegetverydramaticsavings.Inotherwords,solongaseachvariablehasfewparentsinthegraph,thedistributioncanberepresentedwithveryfewparameters. Somerestrictionsonthegraphstructure,suchasrequiringittobeatree,canalsoguaranteethatoperationslikecomputingmarginalorconditionaldistributionsoversubsetsofvariablesareeﬃcient.Itisimportanttorealizewhatkindsofinformationcanandcannotbeencodedinthegraph.Thegraphencodesonlysimplifyingassumptionsaboutwhichvariablesareconditionallyindependentfromeachother.Itisalsopossibletomakeotherkindsofsimplifyingassumptions. Forexample,supposeweassumeBobalwaysrunsthesameregardlessofhowAliceperformed.(Inreality,Alice’sperformanceprobablyinﬂuencesBob’sperformance—dependingonBob’spersonality,ifAlicerunsespeciallyfastinagivenrace,thismightencourageBobtopushhardandmatchherexceptionalperformance,oritmightmakehimoverconﬁdentandlazy).ThentheonlyeﬀectAlicehasonBob’sﬁnishingtimeisthatwemustaddAlice’sﬁnishingtimetothetotalamountoftimewethinkBobneedstorun.ThisobservationallowsustodeﬁneamodelwithO(k)parametersinsteadofO(k2).However,notethatt0andt1arestilldirectlydependentwiththisassumption,becauset1representstheabsolutetimeatwhichBobﬁnishes,notthetotaltimehehimselfspendsrunning.Thismeansourgraphmuststillcontainanarrowfromt0tot1.TheassumptionthatBob’spersonalrunningtimeisindependentfromallotherfactorscannotbeencodedinagraphovert0,t1,andt2.Instead,weencodethisinformationinthedeﬁnitionoftheconditionaldistributionitself.Theconditionaldistributionisnolongerakk×−1elementtableindexedbyt0andt1butisnowaslightlymorecomplicatedformulausingonlyk−1parameters.Thedirectedgraphicalmodelsyntaxdoesnotplaceanyconstraintonhowwedeﬁne565 CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNINGourconditionaldistributions.Itonlydeﬁneswhichvariablestheyareallowedtotakeinasarguments.16.2.2UndirectedModelsDirectedgraphicalmodelsgiveusonelanguagefordescribingstructuredprobabilis-ticmodels.Anotherpopularlanguageisthatofundirectedmodels,otherwiseknownasMarkovrandomﬁelds(MRFs)orMarkovnetworks(Kinder-mann1980,).Astheirnameimplies,undirectedmodelsusegraphswhoseedgesareundirected.Directedmodelsaremostnaturallyapplicabletosituationswherethereisaclearreasontodraweacharrowinoneparticulardirection.Oftenthesearesituationswhereweunderstandthecausalityandthecausalityonlyﬂowsinonedirection.Onesuchsituationistherelayraceexample.Earlierrunnersaﬀecttheﬁnishingtimesoflaterrunners;laterrunnersdonotaﬀecttheﬁnishingtimesofearlierrunners.Notallsituationswemightwanttomodelhavesuchacleardirectiontotheirinteractions.Whentheinteractionsseemtohavenointrinsicdirection,ortooperateinbothdirections,itmaybemoreappropriatetouseanundirectedmodel.Asanexampleofsuchasituation,supposewewanttomodeladistributionoverthreebinaryvariables:whetherornotyouaresick,whetherornotyourcoworkerissick,andwhetherornotyourroommateissick.Asintherelayraceexample,wecanmakesimplifyingassumptionsaboutthekindsofinteractionsthattakeplace.Assumingthatyourcoworkerandyourroommatedonotknoweachother,itisveryunlikelythatoneofthemwillgivetheotheraninfectionsuchasacolddirectly.Thiseventcanbeseenassorarethatitisacceptablenottomodelit.However,itisreasonablylikelythateitherofthemcouldgiveyouacold,andthatyoucouldpassitontotheother.Wecanmodeltheindirecttransmissionofacoldfromyourcoworkertoyourroommatebymodelingthetransmissionofthecoldfromyourcoworkertoyouandthetransmissionofthecoldfromyoutoyourroommate.Inthiscase,itisjustaseasyforyoutocauseyourroommatetogetsickasitisforyourroommatetomakeyousick,sothereisnotaclean,uni-directionalnarrativeonwhichtobasethemodel.Thismotivatesusinganundirectedmodel.Aswithdirectedmodels,iftwonodesinanundirectedmodelareconnectedbyanedge,thentherandomvariablescorrespondingtothosenodesinteractwitheachotherdirectly.Unlikedirectedmodels,theedgeinanundirectedmodelhasnoarrow,andisnotassociatedwithaconditionalprobabilitydistribution.566 CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNINGhrhrhyhyhchcFigure16.3:Anundirectedgraphrepresentinghowyourroommate’shealthhr,yourhealthhy,andyourworkcolleague’shealthhcaﬀecteachother.Youandyourroommatemightinfecteachotherwithacold,andyouandyourworkcolleaguemightdothesame,butassumingthatyourroommateandyourcolleaguedonotknoweachother,theycanonlyinfecteachotherindirectlyviayou.Wedenotetherandomvariablerepresentingyourhealthashy,therandomvariablerepresentingyourroommate’shealthashr,andtherandomvariablerepresentingyourcolleague’shealthashc.Seeﬁgureforadrawingofthe16.3graphrepresentingthisscenario.Formally,anundirectedgraphicalmodelisastructuredprobabilisticmodeldeﬁnedonanundirectedgraphG.ForeachcliqueCinthegraph,3afactorφ(C)(alsocalledacliquepotential) measurestheaﬃnityofthevariablesinthatcliqueforbeingineachoftheirpossiblejointstates.Thefactorsareconstrainedtobenon-negative.Togethertheydeﬁneanunnormalizedprobabilitydistribution˜p() = ΠxC∈Gφ.()C(16.3)Theunnormalizedprobabilitydistributioniseﬃcienttoworkwithsolongasallthecliquesaresmall.Itencodestheideathatstateswithhigheraﬃnityaremorelikely.However,unlikeinaBayesiannetwork,thereislittlestructuretothedeﬁnitionofthecliques,sothereisnothingtoguaranteethatmultiplyingthemtogetherwillyieldavalidprobabilitydistribution.Seeﬁgureforanexample16.4ofreadingfactorizationinformationfromanundirectedgraph.Ourexampleofthecoldspreadingbetweenyou,yourroommate,andyourcolleaguecontainstwocliques.Onecliquecontainshyandhc.Thefactorforthiscliquecanbedeﬁnedbyatable,andmighthavevaluesresemblingthese:hy= 0hy= 1hc= 021hc= 11103Acliqueofthegraphisasubsetofnodesthatareallconnectedtoeachotherbyanedgeofthegraph.567 CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNINGAstateof1indicatesgoodhealth,whileastateof0indicatespoorhealth(havingbeen infectedwith acold).Both ofyou areusuallyhealthy, sothecorrespondingstatehasthehighestaﬃnity.Thestatewhereonlyoneofyouissickhasthelowestaﬃnity,becausethisisararestate.Thestatewherebothofyouaresick(becauseoneofyouhasinfectedtheother)isahigheraﬃnitystate,thoughstillnotascommonasthestatewherebotharehealthy.Tocompletethemodel,wewouldneedtoalsodeﬁneasimilarfactorforthecliquecontaininghyandhr.16.2.3ThePartitionFunctionWhiletheunnormalizedprobabilitydistributionisguaranteedtobenon-negativeeverywhere,itisnotguaranteedtosumorintegrateto1.Toobtainavalidprobabilitydistribution,wemustusethecorrespondingnormalizedprobabilitydistribution:4p() =x1Z˜p()x(16.4)whereZisthevalue thatresultsintheprobability distributionsummingorintegratingto1:Z=˜pd.()xx(16.5)YoucanthinkofZasaconstantwhentheφfunctionsareheldconstant.Notethatiftheφfunctionshaveparameters,thenZisafunctionofthoseparameters.ItiscommonintheliteraturetowriteZwithitsargumentsomittedtosavespace.ThenormalizingconstantZisknownasthepartitionfunction,atermborrowedfromstatisticalphysics.SinceZisanintegralorsumoverallpossiblejointassignmentsofthestatexitisoftenintractabletocompute. Inordertobeabletoobtainthenormalizedprobabilitydistributionofanundirectedmodel, themodelstructureandthedeﬁnitionsoftheφfunctionsmustbeconducivetocomputingZeﬃciently.Inthecontextofdeeplearning,Zisusuallyintractable. DuetotheintractabilityofcomputingZexactly,wemustresorttoapproximations.Suchapproximatealgorithmsarethetopicofchapter.18OneimportantconsiderationtokeepinmindwhendesigningundirectedmodelsisthatitispossibletospecifythefactorsinsuchawaythatZdoesnotexist.Thishappensifsomeofthevariablesinthemodelarecontinuousandtheintegral4AdistributiondeﬁnedbynormalizingaproductofcliquepotentialsisalsocalledaGibbsdistribution.568 CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNINGof˜povertheirdomaindiverges.Forexample,supposewewanttomodelasinglescalarvariablexwithasinglecliquepotential∈Rφxx() = 2.Inthiscase,Z=x2dx.(16.6)Sincethisintegraldiverges,thereisnoprobabilitydistributioncorrespondingtothischoiceofφ(x). Sometimesthechoiceofsomeparameteroftheφfunctionsdetermineswhetherthe probabilitydistribution isdeﬁned.For example, forφ(x;β) =exp−βx2,theβparameterdetermineswhetherZexists.PositiveβresultsinaGaussiandistributionoverxbutallothervaluesofβmakeφimpossibletonormalize.Onekeydiﬀerencebetweendirectedmodelingandundirectedmodelingisthatdirectedmodelsaredeﬁneddirectlyintermsofprobabilitydistributionsfromthestart,whileundirectedmodelsaredeﬁnedmorelooselybyφfunctionsthatarethenconvertedintoprobabilitydistributions.Thischangestheintuitionsonemustdevelopinordertoworkwiththesemodels.Onekeyideatokeepinmindwhileworkingwithundirectedmodelsisthatthedomainofeachofthevariableshasdramaticeﬀectonthekindofprobabilitydistributionthatagivensetofφfunctionscorrespondsto.Forexample,considerann-dimensionalvector-valuedrandomvariablexandanundirectedmodelparametrizedbyavectorofbiasesb.Supposewehaveonecliqueforeachelementofx,φ()i(xi) =exp(bixi).Whatkindofprobabilitydistributiondoesthisresultin?Theansweristhatwedonothaveenoughinformation,becausewehavenotyetspeciﬁedthedomainofx.Ifx ∈Rn,thentheintegraldeﬁningZdivergesandnoprobabilitydistributionexists.Ifx∈{0,1}n,thenp(x)factorizesintonindependentdistributions,withp(xi= 1) =sigmoid(bi).Ifthedomainofxisthesetofelementarybasisvectors({[1,0,...,0],[0,1,...,0],...,[0,0,...,1]})thenp(x)=softmax(b),soalargevalueofbiactuallyreducesp(xj=1)forj=i. Often,itispossibletoleveragetheeﬀectofacarefullychosendomainofavariableinordertoobtaincomplicatedbehaviorfromarelativelysimplesetofφfunctions.Wewillexploreapracticalapplicationofthisidealater,insection.20.616.2.4Energy-BasedModelsManyinterestingtheoreticalresultsaboutundirectedmodelsdependontheas-sumptionthat∀x,˜p(x)>0.Aconvenientwaytoenforcethisconditionistousean(EBM)whereenergy-basedmodel˜pE() = exp(x−())x(16.7)569 CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNINGabcdefFigure16.4:Thisgraphimpliesthatp(abcdef,,,,,)canbewrittenas1Zφab,(ab,)φbc,(bc,)φad,(ad,)φbe,(be,)φef,(ef,)foranappropriatechoiceoftheφfunc-tions.andE(x)isknownastheenergyfunction.Becauseexp(z)ispositiveforallz,thisguaranteesthatnoenergyfunctionwillresultinaprobabilityofzeroforanystatex.Beingcompletely freeto choose theenergyfunction makeslearningsimpler.Ifwelearnedthecliquepotentialsdirectly,wewouldneedtouseconstrainedoptimizationtoarbitrarilyimposesomespeciﬁcminimalprobabilityvalue.Bylearningtheenergyfunction,wecanuseunconstrainedoptimization.5Theprobabilitiesinanenergy-basedmodelcanapproacharbitrarilyclosetozerobutneverreachit.Anydistributionoftheformgivenbyequationisanexampleofa16.7Boltz-mann distribution.For this reason, manyenergy-based models are calledBoltzmannmachines(Fahlman1983Ackley1985Hintonetal.,;etal.,;etal.,1984HintonandSejnowski1986;,).Thereisnoacceptedguidelineforwhentocallamodelanenergy-basedmodelandwhentocallitaBoltzmannmachine.ThetermBoltzmannmachinewasﬁrstintroducedtodescribeamodelwithexclusivelybinaryvariables,buttodaymanymodelssuchasthemean-covariancerestrictedBoltzmannmachineincorporatereal-valuedvariablesaswell.WhileBoltzmannmachineswereoriginallydeﬁnedtoencompassbothmodelswithandwithoutla-tentvariables,thetermBoltzmannmachineistodaymostoftenusedtodesignatemodelswithlatentvariables,whileBoltzmannmachineswithoutlatentvariablesaremoreoftencalledMarkovrandomﬁeldsorlog-linearmodels.Cliquesinanundirectedgraphcorrespondtofactorsoftheunnormalizedprobabilityfunction.Becauseexp(a)exp(b) =exp(a+b),thismeansthatdiﬀerentcliquesintheundirectedgraphcorrespondtothediﬀerenttermsoftheenergyfunction.Inotherwords,anenergy-basedmodelisjustaspecialkindofMarkovnetwork:theexponentiationmakeseachtermintheenergyfunctioncorrespondtoafactorforadiﬀerentclique.Seeﬁgureforanexampleofhowtoreadthe16.55Forsomemodels,wemaystillneedtouseconstrainedoptimizationtomakesureexists.Z570 CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNINGabcdefFigure 16.5:Thisgraph impliesthatE(abcdef,,,,,)can be writtenasEab,(ab,)+Ebc,(bc,)+Ead,(ad,)+Ebe,(be,)+Eef,(ef,)foranappropriatechoiceoftheper-cliqueenergyfunctions.Notethatwecanobtaintheφfunctionsinﬁgurebysettingeach16.4φtotheexponentialofthecorrespondingnegativeenergy,e.g.,φab,(ab,) =exp(())−Eab,.formoftheenergyfunctionfromanundirectedgraphstructure.Onecanviewanenergy-basedmodelwithmultipletermsinitsenergyfunctionasbeingaproductofexperts(Hinton1999,).Eachtermintheenergyfunctioncorrespondstoanotherfactorintheprobabilitydistribution.Eachtermoftheenergyfunctioncanbethoughtofasan“expert”thatdetermineswhetheraparticularsoftconstraintissatisﬁed.Eachexpertmayenforceonlyoneconstraintthatconcernsonlyalow-dimensionalprojectionoftherandomvariables,butwhencombinedbymultiplicationofprobabilities,theexpertstogetherenforceacomplicatedhigh-dimensionalconstraint.Onepartofthedeﬁnitionofanenergy-basedmodelservesnofunctionalpurposefromamachinelearningpointofview:the−signinequation.This16.7−signcouldbeincorporatedintothedeﬁnitionofE.FormanychoicesofthefunctionE,thelearningalgorithmisfreetodeterminethesignoftheenergyanyway.The−signispresentprimarilytopreservecompatibilitybetweenthemachinelearningliteratureandthephysicsliterature.Manyadvancesinprobabilisticmodelingwereoriginallydevelopedbystatisticalphysicists,forwhomEreferstoactual,physicalenergyanddoesnothavearbitrarysign. Terminologysuchas“energy”and“partitionfunction”remainsassociatedwiththesetechniques,eventhoughtheirmathematicalapplicabilityisbroaderthanthephysicscontextinwhichtheyweredeveloped.Somemachinelearningresearchers(e.g.,(),whoSmolensky1986referredtonegativeenergyasharmony)havechosentoemitthenegation,butthisisnotthestandardconvention.Manyalgorithmsthatoperateonprobabilisticmodelsdonotneedtocomputepmodel(x)butonlylog ˜pmodel(x).Forenergy-basedmodelswithlatentvariablesh,thesealgorithmsaresometimesphrasedintermsofthenegativeofthisquantity,571 CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNINGasbasb(a)(b)Figure16.6:(a)Thepathbetweenrandomvariableaandrandomvariablebthroughsisactive,becausesisnotobserved.Thismeansthataandbarenotseparated.(b)Heresisshadedin,toindicatethatitisobserved.Becausetheonlypathbetweenaandbisthroughs,andthatpathisinactive,wecanconcludethataandbareseparatedgivens.calledthe:freeenergyF−() = xloghexp(())−Exh,.(16.8)Inthisbook,weusuallypreferthemoregenerallog ˜pmodel()xformulation.16.2.5SeparationandD-SeparationTheedgesinagraphicalmodeltelluswhichvariablesdirectlyinteract.Weoftenneedtoknowwhichvariablesindirectlyinteract.Someoftheseindirectinteractionscanbeenabledordisabledbyobservingothervariables.Moreformally,wewouldliketoknowwhichsubsetsofvariablesareconditionallyindependentfromeachother,giventhevaluesofothersubsetsofvariables.Identifyingtheconditionalindependencesinagraphisverysimpleinthecaseofundirectedmodels.Inthiscase,conditionalindependenceimpliedbythegraphiscalledseparation.WesaythatasetofvariablesAisseparatedfromanothersetofvariablesBgivenathirdsetofvariablesSifthegraphstructureimpliesthatAisindependentfromBgivenS.Iftwovariablesaandbareconnectedbyapathinvolvingonlyunobservedvariables,thenthosevariablesarenotseparated.Ifnopathexistsbetweenthem,orallpathscontainanobservedvariable,thentheyareseparated.Werefertopathsinvolvingonlyunobservedvariablesas“active”andpathsincludinganobservedvariableas“inactive.”Whenwedrawagraph,wecanindicateobservedvariablesbyshadingthemin.Seeﬁgureforadepictionofhowactiveandinactivepathsinanundirected16.6modellookwhendrawninthisway.Seeﬁgureforanexampleofreading16.7separationfromanundirectedgraph.Similar conceptsapply todirected models,except that inthe context ofdirectedmodels,theseconceptsarereferredtoasd-separation.The“d”standsfor“dependence.” D-separationfordirectedgraphsisdeﬁnedthesameasseparation572 CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNINGabcdFigure16.7:Anexampleofreadingseparationpropertiesfromanundirectedgraph.Herebisshadedtoindicatethatitisobserved.Becauseobservingbblockstheonlypathfromatoc,wesaythataandcareseparatedfromeachothergivenb.Theobservationofbalsoblocksonepathbetweenaandd,butthereisasecond,activepathbetweenthem.Therefore,aanddarenotseparatedgivenb.forundirectedgraphs:WesaythatasetofvariablesAisd-separatedfromanothersetofvariablesBgivenathirdsetofvariablesSifthegraphstructureimpliesthatisindependentfromgiven.ABSAswithundirectedmodels,wecanexaminetheindependencesimpliedbythegraphbylookingatwhatactivepathsexistinthegraph.Asbefore,twovariablesaredependentifthereisanactivepathbetweenthem,andd-separatedifnosuchpathexists.Indirectednets,determiningwhetherapathisactiveissomewhatmorecomplicated.Seeﬁgureforaguidetoidentifyingactivepathsina16.8directedmodel.Seeﬁgureforanexampleofreadingsomepropertiesfroma16.9graph.Itisimportanttorememberthatseparationandd-separationtellusonlyaboutthoseconditionalindependencesthatareimpliedbythegraph.Thereisnorequirementthatthegraphimplyallindependencesthatarepresent.Inparticular,itisalwayslegitimatetousethecompletegraph(thegraphwithallpossibleedges)torepresentanydistribution.Infact,somedistributionscontainindependencesthatarenotpossibletorepresentwithexistinggraphicalnotation.Context-speciﬁcindependencesareindependencesthatarepresentdependentonthevalueofsomevariablesinthenetwork. Forexample,consideramodelofthreebinaryvariables:a,bandc.Supposethatwhenais0,bandcareindependent,butwhenais1,bisdeterministicallyequaltoc. Encodingthebehaviorwhena= 1requiresanedgeconnectingbandc.Thegraphthenfailstoindicatethatbandcareindependentwhena.= 0Ingeneral,agraphwillneverimplythatanindependenceexistswhenitdoesnot.However,agraphmayfailtoencodeanindependence.573 CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNINGasbasbasbasbasb c(a)(b) (c)(d)Figure16.8:Allofthekindsofactivepathsoflengthtwothatcanexistbetweenrandomvariablesaandb.Anypathwitharrowsproceedingdirectlyfrom(a)atoborviceversa.Thiskindofpathbecomesblockedifsisobserved. Wehavealreadyseenthiskindofpathintherelayraceexample.(b)aandbareconnectedbyacommoncauses.Forexample,supposesisavariableindicatingwhetherornotthereisahurricaneandaandbmeasurethewindspeedattwodiﬀerentnearbyweathermonitoringoutposts.Ifweobserveveryhighwindsatstationa,wemightexpecttoalsoseehighwindsatb.Thiskindofpathcanbeblockedbyobservings.Ifwealreadyknowthereisahurricane,weexpecttoseehighwindsatb,regardlessofwhatisobservedata.Alowerthanexpectedwindata(forahurricane)wouldnotchangeourexpectationofwindsatb(knowingthereisahurricane).However,ifsisnotobserved,thenaandbaredependent,i.e.,thepathisactive.(c)aandbarebothparentsofs.ThisiscalledaV-structureorthecollidercase. TheV-structurecausesaandbtoberelatedbytheexplainingawayeﬀect.Inthiscase,thepathisactuallyactivewhensisobserved.Forexample,supposesisavariableindicatingthatyourcolleagueisnotatwork. Thevariablearepresentsherbeingsick,whilebrepresentsherbeingonvacation. Ifyouobservethatsheisnotatwork,youcanpresumesheisprobablysickoronvacation,butitisnotespeciallylikelythatbothhavehappenedatthesametime.Ifyouﬁndoutthatsheisonvacation,thisfactissuﬃcienttoherabsence.Youcaninferthatsheisprobablynotalsoexplainsick.Theexplainingawayeﬀecthappensevenifanydescendantof(d)sisobserved!Forexample,supposethatcisavariablerepresentingwhetheryouhavereceivedareportfromyourcolleague.Ifyounoticethatyouhavenotreceivedthereport,thisincreasesyourestimateoftheprobabilitythatsheisnotatworktoday,whichinturnmakesitmorelikelythatsheiseithersickoronvacation.TheonlywaytoblockapaththroughaV-structureistoobservenoneofthedescendantsofthesharedchild.574 CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING abcdeFigure16.9:Fromthisgraph,wecanreadoutseverald-separationproperties.Examplesinclude:•aandbared-separatedgiventheemptyset.•aandeared-separatedgivenc.•dandeared-separatedgivenc.Wecanalsoseethatsomevariablesarenolongerd-separatedwhenweobservesomevariables:•aandbarenotd-separatedgivenc.•aandbarenotd-separatedgivend. 575 CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING16.2.6ConvertingbetweenUndirectedandDirectedGraphsWeoftenrefertoaspeciﬁcmachinelearningmodelasbeingundirectedordirected.Forexample,wetypicallyrefertoRBMsasundirectedandsparsecodingasdirected.Thischoiceofwordingcanbesomewhatmisleading,becausenoprobabilisticmodelisinherentlydirectedorundirected.Instead,somemodelsaremosteasilydescribedusingadirectedgraph,ormosteasilydescribedusinganundirectedgraph.Directedmodelsandundirectedmodelsbothhavetheiradvantagesanddisad-vantages.Neitherapproachisclearlysuperioranduniversallypreferred.Instead,weshouldchoosewhichlanguagetouseforeachtask.Thischoicewillpartiallydependonwhichprobabilitydistributionwewishtodescribe.Wemaychoosetouseeitherdirectedmodelingorundirectedmodelingbasedonwhichapproachcancapturethemostindependencesintheprobabilitydistributionorwhichapproachusesthefewestedgestodescribethedistribution.Thereareotherfactorsthatcanaﬀectthedecisionofwhichlanguagetouse.Evenwhileworkingwithasingleprobabilitydistribution,wemaysometimesswitchbetweendiﬀerentmodelinglanguages.Sometimesadiﬀerentlanguagebecomesmoreappropriateifweobserveacertainsubsetofvariables,orifwewishtoperformadiﬀerentcomputationaltask.Forexample,thedirectedmodeldescriptionoftenprovidesastraightforwardapproachtoeﬃcientlydrawsamplesfromthemodel(describedinsection)16.3whiletheundirectedmodelformulationisoftenusefulforderivingapproximateinferenceprocedures(aswewillseeinchapter,wheretheroleofundirected19modelsishighlightedinequation).19.56Everyprobabilitydistributioncanberepresentedbyeitheradirectedmodelorbyanundirectedmodel.Intheworstcase,onecanalwaysrepresentanydistributionbyusinga“completegraph.”Inthecaseofadirectedmodel,thecompletegraphisanydirectedacyclicgraphwhereweimposesomeorderingontherandomvariables,andeachvariablehasallothervariablesthatprecedeitintheorderingasitsancestorsinthegraph.Foranundirectedmodel,thecompletegraphissimplyagraphcontainingasinglecliqueencompassingallofthevariables.Seeﬁgureforanexample.16.10Ofcourse,theutilityofagraphicalmodelisthatthegraphimpliesthatsomevariablesdonotinteractdirectly.Thecompletegraphisnotveryusefulbecauseitdoesnotimplyanyindependences.Whenwerepresentaprobabilitydistributionwithagraph,wewanttochooseagraphthatimpliesasmanyindependencesaspossible,withoutimplyinganyindependencesthatdonotactuallyexist.Fromthispointofview,somedistributionscanberepresentedmoreeﬃciently576 CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING Figure16.10:Examplesofcompletegraphs,whichcandescribeanyprobabilitydistribution.Hereweshowexampleswithfourrandomvariables.(Left)Thecompleteundirectedgraph.Intheundirectedcase,thecompletegraphisunique.Acompletedirectedgraph.(Right)Inthedirectedcase,thereisnotauniquecompletegraph.Wechooseanorderingofthevariablesanddrawanarcfromeachvariabletoeveryvariablethatcomesafteritintheordering.Therearethusafactorialnumberofcompletegraphsforeverysetofrandomvariables.Inthisexampleweorderthevariablesfromlefttoright,toptobottom.usingdirectedmodels,whileotherdistributionscanberepresentedmoreeﬃcientlyusing undirectedmodels.In other words,directed models canencode someindependencesthatundirectedmodelscannotencode,andviceversa.Directedmodelsareabletouseonespeciﬁckindofsubstructurethatundirectedmodelscannotrepresentperfectly.Thissubstructureiscalledanimmorality.Thestructureoccurswhentworandomvariablesaandbarebothparentsofathirdrandomvariablec,andthereisnoedgedirectlyconnectingaandbineitherdirection.(Thename“immorality”mayseemstrange;itwascoinedinthegraphicalmodelsliteratureasajokeaboutunmarriedparents.)ToconvertadirectedmodelwithgraphDintoanundirectedmodel,weneedtocreateanewgraphU. Foreverypairofvariablesxandy,weaddanundirectededgeconnectingxandytoUifthereisadirectededge(ineitherdirection)connectingxandyinDorifxandyarebothparentsinDofathirdvariablez.TheresultingUisknownasamoralizedgraph.Seeﬁgureforexamplesofconvertingdirectedmodelsto16.11undirectedmodelsviamoralization.Likewise,undirectedmodelscanincludesubstructuresthatnodirectedmodelcanrepresentperfectly.Speciﬁcally,adirectedgraphcannotcapturealloftheDconditionalindependencesimpliedbyanundirectedgraphUifUcontainsaloopoflengthgreaterthanthree,unlessthatloopalsocontainsachord.Aloopisasequenceofvariablesconnectedbyundirectededges,withthelastvariableinthesequenceconnectedbacktotheﬁrstvariableinthesequence. Achordisaconnectionbetweenanytwonon-consecutivevariablesinthesequencedeﬁningaloop.IfUhasloopsoflengthfourorgreateranddoesnothavechordsfortheseloops,wemustaddthechordsbeforewecanconvertittoadirectedmodel.Adding577 CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING h1h1h2h2h3h3 v1v1v2v2v3v3ab ca cb h1h1h2h2h3h3 v1v1v2v2v3v3ab ca cbFigure16.11: Examplesofconvertingdirectedmodels(toprow)toundirectedmodels(bottomrow)byconstructingmoralizedgraphs.(Left)Thissimplechaincanbeconvertedtoamoralizedgraphmerelybyreplacingitsdirectededgeswithundirectededges.Theresultingundirectedmodelimpliesexactlythesamesetofindependencesandconditionalindependences.Thisgraphisthesimplestdirectedmodelthatcannotbeconverted(Center)toanundirectedmodelwithoutlosingsomeindependences.Thisgraphconsistsentirelyofasingleimmorality.Becauseaandbareparentsofc,theyareconnectedbyanactivepathwhencisobserved.Tocapturethisdependence,theundirectedmodelmustincludeacliqueencompassingallthreevariables.Thiscliquefailstoencodethefactthatab⊥.(Right)Ingeneral,moralizationmayaddmanyedgestothegraph,thuslosingmanyimpliedindependences.Forexample,thissparsecodinggraphrequiresaddingmoralizingedgesbetweeneverypairofhiddenunits,thusintroducingaquadraticnumberofnewdirectdependences.578 CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNINGabdcabdcabdcFigure16.12:Convertinganundirectedmodeltoadirectedmodel.(Left)Thisundirectedmodelcannotbeconverteddirectedtoadirectedmodelbecauseithasaloopoflengthfourwithnochords.Speciﬁcally,theundirectedmodelencodestwodiﬀerentindependencesthatnodirectedmodelcancapturesimultaneously:acbd⊥|{,}andbdac⊥|{,}.To(Center)converttheundirectedmodeltoadirectedmodel,wemusttriangulatethegraph,byensuringthatallloopsofgreaterthanlengththreehaveachord.Todoso,wecaneitheraddanedgeconnectingaandcorwecanaddanedgeconnectingbandd.Inthisexample,wechoosetoaddtheedgeconnectingaandc.Toﬁnishtheconversion(Right)process,wemustassignadirectiontoeachedge.Whendoingso,wemustnotcreateanydirectedcycles.Onewaytoavoiddirectedcyclesistoimposeanorderingoverthenodes,andalwayspointeachedgefromthenodethatcomesearlierintheorderingtothenodethatcomeslaterintheordering.Inthisexample,weusethevariablenamestoimposealphabeticalorder.thesechordsdiscardssomeoftheindependenceinformationthatwasencodedinU.ThegraphformedbyaddingchordstoUisknownasachordalortriangulatedgraph,becausealltheloopscannowbedescribedintermsofsmaller,triangularloops.TobuildadirectedgraphDfromthechordalgraph,weneedtoalsoassigndirectionstotheedges.Whendoingso,wemustnotcreateadirectedcycleinD,ortheresultdoesnotdeﬁneavaliddirectedprobabilisticmodel.OnewaytoassigndirectionstotheedgesinDistoimposeanorderingontherandomvariables,thenpointeachedgefromthenodethatcomesearlierintheorderingtothenodethatcomeslaterintheordering.Seeﬁgureforademonstration.16.1216.2.7FactorGraphsFactorgraphsareanotherwayofdrawingundirectedmodelsthatresolveanambiguityinthegraphicalrepresentationofstandardundirectedmodelsyntax.Inanundirectedmodel,thescopeofeveryφfunctionmustbeaofsomecliquesubsetinthegraph.Ambiguityarisesbecauseitisnotclearifeachcliqueactuallyhasacorrespondingfactorwhosescopeencompassestheentireclique—forexample,acliquecontainingthreenodesmaycorrespondtoafactoroverallthreenodes,ormaycorrespondtothreefactorsthateachcontainonlyapairofthenodes.579 CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNINGFactorgraphsresolvethisambiguitybyexplicitlyrepresentingthescopeofeachφfunction.Speciﬁcally,afactorgraphisagraphicalrepresentationofanundirectedmodelthatconsistsofabipartiteundirectedgraph.Someofthenodesaredrawnascircles.Thesenodescorrespondtorandomvariablesasinastandardundirectedmodel. Therestofthenodesaredrawnassquares. Thesenodescorrespondtothefactorsφoftheunnormalizedprobabilitydistribution.Variablesandfactorsmaybeconnectedwithundirectededges.Avariableandafactorareconnectedinthegraphifandonlyifthevariableisoneoftheargumentstothefactorintheunnormalizedprobabilitydistribution.Nofactormaybeconnectedtoanotherfactorinthegraph,norcanavariablebeconnectedtoavariable.Seeﬁgure16.13foranexampleofhowfactorgraphscanresolveambiguityintheinterpretationofundirectednetworks.abcabcf1f1abcf1f1f2f2f3f3Figure16.13:Anexampleofhowafactorgraphcanresolveambiguityintheinterpretationofundirectednetworks.(Left)Anundirectednetworkwithacliqueinvolvingthreevariables:a,bandc.Afactorgraphcorrespondingtothesameundirectedmodel.This(Center)factorgraphhasonefactoroverallthreevariables. Anothervalidfactorgraph(Right)forthesameundirectedmodel.Thisfactorgraphhasthreefactors,eachoveronlytwovariables.Representation,inference,andlearningareallasymptoticallycheaperinthisfactorgraphthaninthefactorgraphdepictedinthecenter,eventhoughbothrequirethesameundirectedgraphtorepresent.16.3SamplingfromGraphicalModelsGraphicalmodelsalsofacilitatethetaskofdrawingsamplesfromamodel.Oneadvantageofdirectedgraphicalmodelsisthatasimpleandeﬃcientproce-durecalledancestralsamplingcanproduceasamplefromthejointdistributionrepresentedbythemodel.Thebasicideaistosortthevariablesxiinthegraphintoatopologicalordering,sothatforalliandj,jisgreaterthaniifxiisaparentofxj.Thevariables580 CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNINGcanthenbesampledinthisorder.Inotherwords,weﬁrstsamplex1∼P(x1),thensampleP(x2|PaG(x2)),andsoon,untilﬁnallywesampleP(xn|PaG(xn)).Solongaseachconditionaldistributionp(xi|PaG(xi))iseasytosamplefrom,thenthewholemodeliseasytosamplefrom.Thetopologicalsortingoperationguaranteesthatwecanreadtheconditionaldistributionsinequationand16.1samplefromtheminorder.Withoutthetopologicalsorting,wemightattempttosampleavariablebeforeitsparentsareavailable.Forsomegraphs,morethanonetopologicalorderingispossible.Ancestralsamplingmaybeusedwithanyofthesetopologicalorderings.Ancestralsamplingisgenerallyveryfast(assumingsamplingfromeachcondi-tionaliseasy)andconvenient.Onedrawbacktoancestralsamplingisthatitonlyappliestodirectedgraphicalmodels.Anotherdrawbackisthatitdoesnotsupporteveryconditionalsamplingoperation.Whenwewishtosamplefromasubsetofthevariablesinadirectedgraphicalmodel,givensomeothervariables,weoftenrequirethatallthecondition-ingvariablescomeearlierthanthevariablestobesampledintheorderedgraph.Inthiscase,wecansamplefromthelocalconditionalprobabilitydistributionsspeciﬁedbythemodeldistribution.Otherwise,theconditionaldistributionsweneedtosamplefromaretheposteriordistributionsgiventheobservedvariables.Theseposteriordistributionsareusuallynotexplicitlyspeciﬁedandparametrizedinthemodel.Inferringtheseposteriordistributionscanbecostly.Inmodelswherethisisthecase,ancestralsamplingisnolongereﬃcient.Unfortunately,ancestralsamplingisapplicableonlytodirectedmodels.Wecansamplefromundirectedmodelsbyconvertingthemtodirectedmodels,butthisoftenrequiressolvingintractableinferenceproblems(todeterminethemarginaldistributionovertherootnodesofthenewdirectedgraph)orrequiresintroducingsomanyedgesthattheresultingdirectedmodelbecomesintractable.Samplingfromanundirectedmodelwithoutﬁrstconvertingittoadirectedmodelseemstorequireresolvingcyclicaldependencies.Everyvariableinteractswitheveryothervariable,sothereisnoclearbeginningpointforthesamplingprocess.Unfortunately,drawingsamplesfromanundirectedgraphicalmodelisanexpensive,multi-passprocess.TheconceptuallysimplestapproachisGibbssampling.Supposewehaveagraphicalmodeloverann-dimensionalvectorofrandomvariablesx.Weiterativelyvisiteachvariablexianddrawasampleconditionedonalloftheothervariables,fromp(xi|x−i).Duetotheseparationpropertiesofthegraphicalmodel,wecanequivalentlyconditionononlytheneighborsofxi.Unfortunately,afterwehavemadeonepassthroughthegraphicalmodelandsampledallnvariables,westilldonothaveafairsamplefromp(x).Instead,wemustrepeatthe581 CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNINGprocessandresampleallnvariablesusingtheupdatedvaluesoftheirneighbors.Asymptotically,aftermanyrepetitions,thisprocessconvergestosamplingfromthecorrectdistribution.Itcanbediﬃculttodeterminewhenthesampleshavereachedasuﬃcientlyaccurateapproximationofthedesireddistribution.Samplingtechniquesforundirectedmodelsareanadvancedtopic,coveredinmoredetailinchapter.1716.4AdvantagesofStructuredModelingTheprimaryadvantageofusingstructuredprobabilisticmodelsisthattheyallowustodramaticallyreducethecostofrepresentingprobabilitydistributionsaswellaslearningandinference.Samplingisalsoacceleratedinthecaseofdirectedmodels,whilethesituationcanbecomplicatedwithundirectedmodels.Theprimarymechanismthatallowsalloftheseoperationstouselessruntimeandmemoryischoosingtonotmodelcertaininteractions.Graphicalmodelsconveyinformationbyleavingedgesout.Anywherethereisnotanedge,themodelspeciﬁestheassumptionthatwedonotneedtomodeladirectinteraction.Alessquantiﬁablebeneﬁtofusingstructuredprobabilisticmodelsisthattheyallowustoexplicitlyseparaterepresentationofknowledgefromlearningofknowledgeorinferencegivenexistingknowledge.Thismakesourmodelseasiertodevelopanddebug.Wecandesign,analyze,andevaluatelearningalgorithmsandinferencealgorithmsthatareapplicabletobroadclassesofgraphs.Independently,wecandesignmodelsthatcapturetherelationshipswebelieveareimportantinourdata.WecanthencombinethesediﬀerentalgorithmsandstructuresandobtainaCartesianproductofdiﬀerentpossibilities.Itwouldbemuchmorediﬃculttodesignend-to-endalgorithmsforeverypossiblesituation.16.5LearningaboutDependenciesAgoodgenerativemodelneedstoaccuratelycapturethedistributionovertheobservedor“visible” variablesv.Oftenthediﬀerentelementsofvarehighlydependentoneachother.Inthecontextofdeeplearning,theapproachmostcommonlyusedtomodelthesedependenciesistointroduceseverallatentor“hidden”variables,h.Themodelcanthencapturedependenciesbetweenanypairofvariablesviandvjindirectly,viadirectdependenciesbetweenviandh,anddirectdependenciesbetweenandvhj.Agoodmodelofvwhichdidnotcontainanylatentvariableswouldneedto582 CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNINGhaveverylargenumbersofparentspernodeinaBayesiannetworkorverylargecliquesinaMarkovnetwork.Justrepresentingthesehigherorderinteractionsiscostly—bothinacomputationalsense,becausethenumberofparametersthatmustbestoredinmemoryscalesexponentiallywiththenumberofmembersinaclique,butalsoinastatisticalsense,becausethisexponentialnumberofparametersrequiresawealthofdatatoestimateaccurately.Whenthemodelisintendedtocapturedependenciesbetweenvisiblevariableswithdirectconnections,itisusuallyinfeasibletoconnectallvariables,sothegraphmustbedesignedtoconnectthosevariablesthataretightlycoupledandomitedgesbetweenothervariables.AnentireﬁeldofmachinelearningcalledstructurelearningisdevotedtothisproblemForagoodreferenceonstructurelearning,see(KollerandFriedman2009,).Moststructurelearningtechniquesareaformofgreedysearch.Astructureisproposed,amodelwiththatstructureistrained,thengivenascore.Thescorerewardshightrainingsetaccuracyandpenalizesmodelcomplexity.Candidatestructureswithasmallnumberofedgesaddedorremovedarethenproposedasthenextstepofthesearch.Thesearchproceedstoanewstructurethatisexpectedtoincreasethescore.Usinglatentvariablesinsteadofadaptivestructureavoidstheneedtoperformdiscretesearchesandmultipleroundsoftraining.Aﬁxedstructureovervisibleandhiddenvariablescanusedirectinteractionsbetweenvisibleandhiddenunitstoimposeindirectinteractionsbetweenvisibleunits.Usingsimpleparameterlearningtechniqueswecanlearnamodelwithaﬁxedstructurethatimputestherightstructureonthemarginal.p()vLatentvariableshaveadvantagesbeyondtheirroleineﬃcientlycapturingp(v).Thenewvariableshalsoprovideanalternativerepresentationforv.Forexample,asdiscussedinsection,themixtureofGaussiansmodellearnsalatentvariable3.9.6thatcorrespondstowhichcategoryofexamplestheinputwasdrawnfrom.ThismeansthatthelatentvariableinamixtureofGaussiansmodelcanbeusedtodoclassiﬁcation. Inchapterwesawhowsimpleprobabilisticmodelslikesparse14codinglearnlatentvariablesthatcanbeusedasinputfeaturesforaclassiﬁer,orascoordinatesalongamanifold.Othermodelscanbeusedinthissameway,butdeepermodelsandmodelswithdiﬀerentkindsofinteractionscancreateevenricherdescriptionsoftheinput.Manyapproachesaccomplishfeaturelearningbylearninglatentvariables.Often,givensomemodelofvandh,experimentalobservationsshowthatE[hv|]orargmaxhp(hv,)isagoodfeaturemappingforv.583 CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING16.6InferenceandApproximateInferenceOneofthemainwayswecanuseaprobabilisticmodelistoaskquestionsabouthowvariablesarerelatedtoeachother.Givenasetofmedicaltests,wecanaskwhatdiseaseapatientmighthave.Inalatentvariablemodel,wemightwanttoextractfeaturesE[hv|]describingtheobservedvariablesv.Sometimesweneedtosolvesuchproblemsinordertoperformothertasks.Weoftentrainourmodelsusingtheprincipleofmaximumlikelihood.Becauselog()= pvEhh∼p(|v)[log()log()]phv,−phv|,(16.9)weoftenwanttocomputep(h|v)inordertoimplementalearningrule.Alloftheseareexamplesofinferenceproblemsinwhichwemustpredictthevalueofsomevariablesgivenothervariables,orpredicttheprobabilitydistributionoversomevariablesgiventhevalueofothervariables.Unfortunately,formostinterestingdeepmodels,theseinferenceproblemsareintractable,evenwhenweuseastructuredgraphicalmodeltosimplifythem.Thegraphstructureallowsustorepresentcomplicated,high-dimensionaldistributionswithareasonablenumberofparameters,butthegraphsusedfordeeplearningareusuallynotrestrictiveenoughtoalsoalloweﬃcientinference.Itisstraightforwardtoseethatcomputingthemarginalprobabilityofageneralgraphicalmodelis#Phard.Thecomplexityclass#PisageneralizationofthecomplexityclassNP.ProblemsinNPrequiredeterminingonlywhetheraproblemhasasolutionandﬁndingasolutionifoneexists.Problemsin#Prequirecountingthenumberofsolutions.Toconstructaworst-casegraphicalmodel,imaginethatwedeﬁneagraphicalmodeloverthebinaryvariablesina3-SATproblem. Wecanimposeauniformdistributionoverthesevariables.Wecanthenaddonebinarylatentvariableperclausethatindicateswhethereachclauseissatisﬁed.Wecanthenaddanotherlatentvariableindicatingwhetheralloftheclausesaresatisﬁed.Thiscanbedonewithoutmakingalargeclique,bybuildingareductiontreeoflatentvariables,witheachnodeinthetreereportingwhethertwoothervariablesaresatisﬁed.Theleavesofthistreearethevariablesforeachclause.Therootofthetreereportswhethertheentireproblemissatisﬁed. Duetotheuniformdistributionovertheliterals,themarginaldistributionovertherootofthereductiontreespeciﬁeswhatfractionofassignmentssatisfytheproblem.Whilethisisacontrivedworst-caseexample,NPhardgraphscommonlyariseinpracticalreal-worldscenarios.Thismotivatestheuseofapproximateinference.In thecontextof deeplearning,thisusuallyreferstovariationalinference,inwhichweapproximatethe584 CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNINGtruedistributionp(h|v)byseekinganapproximatedistributionq(hv|)thatisasclosetothetrueoneaspossible.Thisandothertechniquesaredescribedindepthinchapter.1916.7TheDeepLearningApproachtoStructuredProb-abilisticModelsDeeplearningpractitionersgenerallyusethesamebasiccomputationaltoolsasothermachinelearningpractitionerswhoworkwithstructuredprobabilisticmodels.However,inthecontextofdeeplearning,weusuallymakediﬀerentdesigndecisionsabouthowtocombinethesetools,resultinginoverallalgorithmsandmodelsthathaveaverydiﬀerentﬂavorfrommoretraditionalgraphicalmodels.Deeplearningdoesnotalwaysinvolveespeciallydeepgraphicalmodels.Inthecontextofgraphicalmodels,wecandeﬁnethedepthofamodelintermsofthegraphicalmodelgraphratherthanthecomputationalgraph.Wecanthinkofalatentvariablehiasbeingatdepthjiftheshortestpathfromhitoanobservedvariableisjsteps.Weusuallydescribethedepthofthemodelasbeingthegreatestdepthofanysuchhi.Thiskindofdepthisdiﬀerentfromthedepthinducedbythecomputationalgraph.Manygenerativemodelsusedfordeeplearninghavenolatentvariablesoronlyonelayeroflatentvariables,butusedeepcomputationalgraphstodeﬁnetheconditionaldistributionswithinamodel.Deeplearningessentiallyalwaysmakesuseoftheideaofdistributedrepresen-tations.Evenshallowmodelsusedfordeeplearningpurposes(suchaspretrainingshallowmodelsthatwilllaterbecomposedtoformdeepones)nearlyalwayshaveasingle,largelayeroflatentvariables.Deeplearningmodelstypicallyhavemorelatentvariablesthanobservedvariables.Complicatednonlinearinteractionsbetweenvariablesareaccomplishedviaindirectconnectionsthatﬂowthroughmultiplelatentvariables.Bycontrast,traditionalgraphicalmodelsusuallycontainmostlyvariablesthatareatleastoccasionallyobserved,evenifmanyofthevariablesaremissingatrandomfromsometrainingexamples.Traditionalmodelsmostlyusehigher-ordertermsandstructurelearningtocapturecomplicatednonlinearinteractionsbetweenvariables.Iftherearelatentvariables,theyareusuallyfewinnumber.Thewaythatlatentvariablesaredesignedalsodiﬀersindeeplearning.Thedeeplearningpractitionertypicallydoesnotintendforthelatentvariablestotakeonanyspeciﬁcsemanticsaheadoftime—thetrainingalgorithmisfreetoinventtheconceptsitneedstomodelaparticulardataset.Thelatentvariablesare585 CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNINGusuallynotveryeasyforahumantointerpretafterthefact,thoughvisualizationtechniquesmayallowsomeroughcharacterizationofwhattheyrepresent.Whenlatentvariablesareusedinthecontextoftraditionalgraphicalmodels,theyareoftendesignedwithsomespeciﬁcsemanticsinmind—thetopicofadocument,theintelligenceofastudent,thediseasecausingapatient’ssymptoms,etc.Thesemodelsareoftenmuchmoreinterpretablebyhumanpractitionersandoftenhavemoretheoreticalguarantees,yetarelessabletoscaletocomplexproblemsandarenotreusableinasmanydiﬀerentcontextsasdeepmodels.Anotherobviousdiﬀerenceisthekindofconnectivitytypicallyusedinthedeeplearningapproach.Deepgraphicalmodelstypicallyhavelargegroupsofunitsthatareallconnectedtoothergroupsofunits,sothattheinteractionsbetweentwogroupsmaybedescribedbyasinglematrix.Traditionalgraphicalmodelshaveveryfewconnectionsandthechoiceofconnectionsforeachvariablemaybeindividuallydesigned.Thedesignofthemodelstructureistightlylinkedwiththechoiceofinferencealgorithm.Traditionalapproachestographicalmodelstypicallyaimtomaintainthetractabilityofexactinference.Whenthisconstraintistoolimiting,apopularapproximateinferencealgorithmisanalgorithmcalledloopybeliefpropagation.Bothoftheseapproachesoftenworkwellwithverysparselyconnectedgraphs.Bycomparison,modelsusedindeeplearningtendtoconnecteachvisibleunitvitoverymanyhiddenunitshj,sothathcanprovideadistributedrepresentationofvi(andprobablyseveralotherobservedvariablestoo).Distributedrepresentationshavemanyadvantages,butfromthepointofviewofgraphicalmodelsandcomputationalcomplexity,distributedrepresentationshavethedisadvantageofusuallyyieldinggraphsthatarenotsparseenoughforthetraditionaltechniquesofexactinferenceandloopybeliefpropagationtoberelevant.Asaconsequence,oneofthemoststrikingdiﬀerencesbetweenthelargergraphicalmodelscommunityandthedeepgraphicalmodelscommunityisthatloopybeliefpropagationisalmostneverusedfordeeplearning.MostdeepmodelsareinsteaddesignedtomakeGibbssamplingorvariationalinferencealgorithmseﬃcient.Anotherconsiderationisthatdeeplearningmodelscontainaverylargenumberoflatentvariables,makingeﬃcientnumericalcodeessential.Thisprovidesanadditionalmotivation,besidesthechoiceofhigh-levelinferencealgorithm,forgroupingtheunitsintolayerswithamatrixdescribingtheinteractionbetweentwolayers.Thisallowstheindividualstepsofthealgorithmtobeimplementedwitheﬃcientmatrixproductoperations,orsparselyconnectedgeneralizations,likeblockdiagonalmatrixproductsorconvolutions.Finally,thedeeplearningapproachtographicalmodelingischaracterizedbyamarkedtoleranceoftheunknown.Ratherthansimplifyingthemodeluntilallquantitieswemightwantcanbecomputedexactly,weincreasethepowerof586 CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNINGthemodeluntilitisjustbarelypossibletotrainoruse.Weoftenusemodelswhosemarginaldistributionscannotbecomputed,andaresatisﬁedsimplytodrawapproximatesamplesfromthesemodels.Weoftentrainmodelswithanintractableobjectivefunctionthatwecannotevenapproximateinareasonableamountoftime,butwearestillabletoapproximatelytrainthemodelifwecaneﬃcientlyobtainanestimateofthegradientofsuchafunction.Thedeeplearningapproachisoftentoﬁgureoutwhattheminimumamountofinformationweabsolutelyneedis,andthentoﬁgureouthowtogetareasonableapproximationofthatinformationasquicklyaspossible.16.7.1Example:TheRestrictedBoltzmannMachineTherestrictedBoltzmannmachine(RBM)(,)orSmolensky1986harmoniumisthequintessentialexampleofhowgraphicalmodelsareusedfordeeplearning.TheRBMisnotitselfadeepmodel.Instead,ithasasinglelayeroflatentvariablesthatmaybeusedtolearnarepresentationfortheinput.Inchapter,wewill20seehowRBMscanbeusedtobuildmanydeepermodels.Here,weshowhowtheRBMexempliﬁesmanyofthepracticesusedinawidevarietyofdeepgraphicalmodels: itsunitsareorganizedintolargegroupscalledlayers,theconnectivitybetweenlayersisdescribedbyamatrix,theconnectivityisrelativelydense,themodelisdesignedtoalloweﬃcientGibbssampling,andtheemphasisofthemodeldesignisonfreeingthetrainingalgorithmtolearnlatentvariableswhosesemanticswerenotspeciﬁedbythedesigner.Later,insection,wewillrevisittheRBM20.2inmoredetail.ThecanonicalRBMisanenergy-basedmodelwithbinaryvisibleandhiddenunits.ItsenergyfunctionisE,(vhb) = −vc−hv−Wh,(16.10)whereb,c,andWareunconstrained,real-valued,learnableparameters.Wecanseethatthemodelisdividedintotwogroupsofunits:vandh,andtheinteractionbetweenthemisdescribedbyamatrixW.Themodelisdepictedgraphicallyinﬁgure.Asthisﬁguremakesclear,animportantaspectofthismodelis16.14thattherearenodirectinteractionsbetweenanytwovisibleunitsorbetweenanytwohiddenunits(hencethe“restricted,”ageneralBoltzmannmachinemayhavearbitraryconnections).TherestrictionsontheRBMstructureyieldthenicepropertiesp() = Πhv|ip(hi|v)(16.11)587 CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNINGh1h1h2h2h3h3v1v1v2v2v3v3h4h4Figure16.14:AnRBMdrawnasaMarkovnetwork.andp() = Πvh|ip(vi|h).(16.12)Theindividualconditionalsaresimpletocomputeaswell.ForthebinaryRBMweobtain:P(hi= 1 ) = |vσvW:,i+bi,(16.13)P(hi= 0 ) = 1|v−σvW:,i+bi.(16.14)TogetherthesepropertiesallowforeﬃcientblockGibbssampling,whichalter-natesbetweensamplingallofhsimultaneouslyandsamplingallofvsimultane-ously.SamplesgeneratedbyGibbssamplingfromanRBMmodelareshowninﬁgure.16.15Sincetheenergyfunctionitselfisjustalinearfunctionoftheparameters,itiseasytotakeitsderivatives.Forexample,∂∂Wi,jE,(vh) = −vihj.(16.15)Thesetwoproperties—eﬃcientGibbssamplingandeﬃcientderivatives—maketrainingconvenient.Inchapter,wewillseethatundirectedmodelsmaybe18trainedbycomputingsuchderivativesappliedtosamplesfromthemodel.Trainingthemodelinducesarepresentationhofthedatav.WecanoftenuseEhh∼p(|v)[]hasasetoffeaturestodescribe.vOverall,theRBMdemonstratesthetypicaldeeplearningapproachtograph-icalmodels: representationlearningaccomplishedvialayersoflatentvariables,combinedwitheﬃcientinteractionsbetweenlayersparametrizedbymatrices.Thelanguageofgraphicalmodelsprovidesanelegant,ﬂexibleandclearlanguagefordescribingprobabilisticmodels.Inthechaptersahead,weusethislanguage,amongotherperspectives,todescribeawidevarietyofdeepprobabilisticmodels.588 CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING Figure16.15:SamplesfromatrainedRBM,anditsweights.Imagereproducedwithpermissionfrom().LISA2008(Left)SamplesfromamodeltrainedonMNIST,drawnusingGibbssampling.EachcolumnisaseparateGibbssamplingprocess.Eachrowrepresentstheoutputofanother1,000stepsofGibbssampling.Successivesamplesarehighlycorrelatedwithoneanother.Thecorrespondingweightvectors.Compare(Right)thistothesamplesandweightsofalinearfactormodel,showninﬁgure.Thesamples13.2herearemuchbetterbecausetheRBMpriorp(h)isnotconstrainedtobefactorial.TheRBMcanlearnwhichfeaturesshouldappeartogetherwhensampling.Ontheotherhand,theRBMposteriorisfactorial,whilethesparsecodingposteriorisnot,p()hv|p()hv|sothesparsecodingmodelmaybebetterforfeatureextraction.Othermodelsareabletohavebothanon-factorialandanon-factorial.p()hp()hv| 589 Chapter17MonteCarloMethodsRandomizedalgorithmsfallintotworoughcategories:LasVegasalgorithmsandMonteCarloalgorithms.LasVegasalgorithmsalwaysreturnpreciselythecorrectanswer(orreportthattheyfailed).Thesealgorithmsconsumearandomamountofresources,usuallymemoryortime.Incontrast,MonteCarloalgorithmsreturnanswerswitharandomamountoferror.Theamountoferrorcantypicallybereducedbyexpendingmoreresources(usuallyrunningtimeandmemory).Foranyﬁxedcomputationalbudget,aMonteCarloalgorithmcanprovideanapproximateanswer.Manyproblemsinmachinelearningaresodiﬃcultthatwecanneverexpecttoobtainpreciseanswerstothem.ThisexcludesprecisedeterministicalgorithmsandLasVegasalgorithms.Instead,wemustusedeterministicapproximatealgorithmsorMonteCarloapproximations.Bothapproachesareubiquitousinmachinelearning.Inthischapter,wefocusonMonteCarlomethods.17.1SamplingandMonteCarloMethodsManyimportanttechnologiesusedtoaccomplishmachinelearninggoalsarebasedondrawingsamplesfromsomeprobabilitydistributionandusingthesesamplestoformaMonteCarloestimateofsomedesiredquantity.17.1.1WhySampling?Therearemanyreasonsthatwemaywishtodrawsamplesfromaprobabilitydistribution.Samplingprovidesaﬂexiblewaytoapproximatemanysumsand590 CHAPTER17.MONTECARLOMETHODSintegralsatreducedcost.Sometimesweusethistoprovideasigniﬁcantspeeduptoacostlybuttractablesum,asinthecasewhenwesubsamplethefulltrainingcostwithminibatches.Inothercases,ourlearningalgorithmrequiresustoapproximateanintractablesumorintegral,suchasthegradientofthelogpartitionfunctionofanundirectedmodel.Inmanyothercases,samplingisactuallyourgoal,inthesensethatwewanttotrainamodelthatcansamplefromthetrainingdistribution.17.1.2BasicsofMonteCarloSamplingWhenasumoranintegralcannotbecomputedexactly(forexamplethesumhasanexponentialnumberoftermsandnoexactsimpliﬁcationisknown)itisoftenpossibletoapproximateitusingMonteCarlosampling.Theideaistoviewthesumorintegralasifitwasanexpectationundersomedistributionandtoapproximatetheexpectationbyacorrespondingaverage.Lets=xpfE()x() = xp[()]fx(17.1)ors=pfdE()x()xx= p[()]fx(17.2)bethesumorintegraltoestimate,rewrittenasanexpectation,withtheconstraintthatpisaprobabilitydistribution(forthesum)oraprobabilitydensity(fortheintegral)overrandomvariable.xWecanapproximatesbydrawingnsamplesx(1),...,x()nfrompandthenformingtheempiricalaverageˆsn=1nni=1f(x()i).(17.3)Thisapproximationisjustiﬁedbyafewdiﬀerentproperties.Theﬁrsttrivialobservationisthattheestimatorˆsisunbiased,sinceE[ˆsn] =1nni=1E[(fx()i)] =1nni=1ss.= (17.4)Butinaddition,thelawoflargenumbersstatesthatifthesamplesx()iarei.i.d.,thentheaverageconvergesalmostsurelytotheexpectedvalue:limn→∞ˆsn= s,(17.5)591 CHAPTER17.MONTECARLOMETHODSprovidedthatthevarianceoftheindividualterms,Var[f(x()i)],isbounded.Toseethismoreclearly,considerthevarianceofˆsnasnincreases.ThevarianceVar[ˆsn]decreasesandconvergesto0,solongasVar[(fx()i)] <∞:Var[ˆsn] =1n2ni=1Var[()]fx(17.6)=Var[()]fxn.(17.7)ThisconvenientresultalsotellsushowtoestimatetheuncertaintyinaMonteCarloaverageorequivalentlytheamountofexpectederroroftheMonteCarloapproximation.Wecomputeboththeempiricalaverageofthef(x()i)andtheirempiricalvariance,1andthendividetheestimatedvariancebythenumberofsamplesntoobtainanestimatorofVar[ˆsn]. Thecentrallimittheoremtellsusthatthedistributionoftheaverage,ˆsn,convergestoanormaldistributionwithmeansandvarianceVar[()]fxn.Thisallowsustoestimateconﬁdenceintervalsaroundtheestimateˆsn,usingthecumulativedistributionofthenormaldensity.However,allthisreliesonourabilitytoeasilysamplefromthebasedistributionp(x),butdoingsoisnotalwayspossible.Whenitisnotfeasibletosamplefromp,analternativeistouseimportancesampling,presentedinsection.A17.2moregeneralapproachistoformasequenceofestimatorsthatconvergetowardsthedistributionofinterest.ThatistheapproachofMonteCarloMarkovchains(section).17.317.2ImportanceSamplingAnimportantstepinthedecompositionoftheintegrand(orsummand)usedbytheMonteCarlomethodinequationisdecidingwhichpartoftheintegrandshould17.2playtheroletheprobabilityp(x)andwhichpartoftheintegrandshouldplaytheroleofthequantityf(x) whoseexpectedvalue(underthatprobabilitydistribution)istobeestimated.Thereisnouniquedecompositionbecausep(x)f(x)canalwaysberewrittenaspfq()x() = x()xpf()x()xq()x,(17.8)wherewenowsamplefromqandaveragepfq.Inmanycases,wewishtocomputeanexpectationforagivenpandanf,andthefactthattheproblemisspeciﬁed1Theunbiasedestimatorofthevarianceisoftenpreferred,inwhichthesumofsquareddiﬀerencesisdividedbyinsteadof.n−1n592 CHAPTER17.MONTECARLOMETHODSfromthestartasanexpectationsuggeststhatthispandfwouldbeanaturalchoiceofdecomposition.However,theoriginalspeciﬁcationoftheproblemmaynotbethetheoptimalchoiceintermsofthenumberofsamplesrequiredtoobtainagivenlevelofaccuracy. Fortunately,theformoftheoptimalchoiceq∗canbederivedeasily.Theoptimalq∗correspondstowhatiscalledoptimalimportancesampling.Becauseoftheidentityshowninequation,anyMonteCarloestimator17.8ˆsp=1nni,=1x()i∼pf(x()i)(17.9)canbetransformedintoanimportancesamplingestimatorˆsq=1nni,=1x()i∼qp(x()i)(fx()i)q(x()i).(17.10)Weseereadilythattheexpectedvalueoftheestimatordoesnotdependon:qEq[ˆsq] = Eq[ˆsp] = s.(17.11)However,thevarianceofanimportancesamplingestimatorcanbegreatlysensitivetothechoiceof.ThevarianceisgivenbyqVar[ˆsq] = Var[pf()x()xq()x]/n.(17.12)Theminimumvarianceoccurswhenisqq∗() =xpf()x|()x|Z,(17.13)whereZisthenormalizationconstant,chosensothatq∗(x)sumsorintegratesto1asappropriate.Betterimportancesamplingdistributionsputmoreweightwheretheintegrandislarger.Infact,whenf(x)doesnotchangesign,Var[ˆsq∗]=0,meaningthatwhentheoptimaldistributionisused.asinglesampleissuﬃcientOfcourse,thisisonlybecausethecomputationofq∗hasessentiallysolvedtheoriginalproblem,soitisusuallynotpracticaltousethisapproachofdrawingasinglesamplefromtheoptimaldistribution.Anychoiceofsamplingdistributionqisvalid(inthesenseofyieldingthecorrectexpectedvalue)andq∗istheoptimalone(inthesenseofyieldingminimumvariance).Samplingfromq∗isusuallyinfeasible,butotherchoicesofqcanbefeasiblewhilestillreducingthevariancesomewhat.593 CHAPTER17.MONTECARLOMETHODSAnotherapproachistousebiasedimportancesampling,whichhastheadvantageofnotrequiringnormalizedporq.Inthecaseofdiscretevariables,thebiasedimportancesamplingestimatorisgivenbyˆsBIS=ni=1p(x()i)q(x()i)f(x()i)ni=1p(x()i)q(x()i)(17.14)=ni=1p(x()i)˜q(x()i)f(x()i)ni=1p(x()i)˜q(x()i)(17.15)=ni=1˜p(x()i)˜q(x()i)f(x()i)ni=1˜p(x()i)˜q(x()i),(17.16)where˜pand˜qaretheunnormalizedformsofpandqandthex()iarethesamplesfromq.ThisestimatorisbiasedbecauseE[ˆsBIS]=s,exceptasymptoticallywhenn→∞andthedenominatorofequationconvergesto1.Hencethisestimator17.14iscalledasymptoticallyunbiased.AlthoughagoodchoiceofqcangreatlyimprovetheeﬃciencyofMonteCarloestimation,apoorchoiceofqcanmaketheeﬃciencymuchworse.Goingbacktoequation,weseethatiftherearesamplesof17.12qforwhichpf()x|()x|q()xislarge,thenthevarianceoftheestimatorcangetverylarge.Thismayhappenwhenq(x)istinywhileneitherp(x)norf(x)aresmallenoughtocancelit.Theqdistributionisusuallychosentobeaverysimpledistributionsothatitiseasytosamplefrom.Whenxishigh-dimensional,thissimplicityinqcausesittomatchporpf||poorly.Whenq(x()i)p(x()i)|f(x()i)|,importancesamplingcollectsuselesssamples(summingtinynumbersorzeros).Ontheotherhand,whenq(x()i)p(x()i)|f(x()i)|,whichwillhappenmorerarely,theratiocanbehuge.Becausetheselattereventsarerare,theymaynotshowupinatypicalsample,yieldingtypicalunderestimationofs,compensatedrarelybygrossoverestimation.Suchverylargeorverysmallnumbersaretypicalwhenxishighdimensional,becauseinhighdimensionthedynamicrangeofjointprobabilitiescanbeverylarge.Inspiteofthisdanger,importancesamplinganditsvariantshavebeenfoundveryusefulinmanymachinelearningalgorithms,includingdeeplearningalgorithms.Forexample,seetheuseofimportancesamplingtoacceleratetraininginneurallanguagemodelswithalargevocabulary(section)orotherneuralnets12.4.3.3withalargenumberofoutputs.Seealsohowimportancesamplinghasbeenusedtoestimateapartitionfunction(thenormalizationconstantofaprobability594 CHAPTER17.MONTECARLOMETHODSdistribution)insection,andtoestimatethelog-likelihoodindeepdirected18.7modelssuchasthevariationalautoencoder,insection.Importancesampling20.10.3mayalsobeusedtoimprovetheestimateofthegradientofthecostfunctionusedtotrainmodelparameterswithstochasticgradientdescent,particularlyformodelssuchasclassiﬁerswheremostofthetotalvalueofthecostfunctioncomesfromasmallnumberofmisclassiﬁedexamples.Samplingmorediﬃcultexamplesmorefrequentlycanreducethevarianceofthegradientinsuchcases(,).Hinton200617.3MarkovChainMonteCarloMethodsInmanycases,wewishtouseaMonteCarlotechniquebutthereisnotractablemethodfordrawingexactsamplesfromthedistributionpmodel(x)orfromagood(lowvariance)importancesamplingdistributionq(x).Inthecontextofdeeplearning,thismostoftenhappenswhenpmodel(x)isrepresentedbyanundirectedmodel.Inthesecases,weintroduceamathematicaltoolcalledaMarkovchaintoapproximatelysamplefrompmodel(x).ThefamilyofalgorithmsthatuseMarkovchainstoperformMonteCarloestimatesiscalledMarkovchainMonteCarlomethods(MCMC).MarkovchainMonteCarlomethodsformachinelearningaredescribedatgreaterlengthinKollerandFriedman2009(). Themoststandard,genericguaranteesforMCMCtechniquesareonlyapplicablewhenthemodeldoesnotassignzeroprobabilitytoanystate.Therefore,itismostconvenientto present these techniques assampling froman energy-basedmodel (EBM)p(x)∝−exp(E())xasdescribedinsection.IntheEBMformulation,every16.2.4stateisguaranteedtohavenon-zeroprobability.MCMCmethodsareinfactmorebroadlyapplicableandcanbeusedwithmanyprobabilitydistributionsthatcontainzeroprobabilitystates.However,thetheoreticalguaranteesconcerningthebehaviorofMCMCmethodsmustbeprovenonacase-by-casebasisfordiﬀerentfamiliesofsuchdistributions.Inthecontextofdeeplearning,itismostcommontorelyonthemostgeneraltheoreticalguaranteesthatnaturallyapplytoallenergy-basedmodels.Tounderstandwhydrawingsamplesfromanenergy-basedmodelisdiﬃcult,consideranEBMoverjusttwovariables,deﬁningadistributionab.Inorderp(,)tosamplea,wemustdrawafromp(ab|),andinordertosampleb,wemustdrawitfromp(ba|).Itseemstobeanintractablechicken-and-eggproblem.Directedmodelsavoidthisbecausetheirgraphisdirectedandacyclic.Toperformancestralsamplingonesimplysampleseachofthevariablesintopologicalorder,conditioningoneachvariable’sparents,whichareguaranteedtohavealreadybeensampled(section).Ancestralsamplingdeﬁnesaneﬃcient,single-passmethod16.3595 CHAPTER17.MONTECARLOMETHODSofobtainingasample.InanEBM,wecanavoidthischickenandeggproblembysamplingusingaMarkovchain.ThecoreideaofaMarkovchainistohaveastatexthatbeginsasanarbitraryvalue.Overtime,werandomlyupdatexrepeatedly.Eventuallyxbecomes(verynearly)afairsamplefromp(x).Formally,aMarkovchainisdeﬁnedbyarandomstatexandatransitiondistributionT(x|x)specifyingtheprobabilitythatarandomupdatewillgotostatexifitstartsinstatex.RunningtheMarkovchainmeansrepeatedlyupdatingthestatextoavaluexsampledfromT(x|x).TogainsometheoreticalunderstandingofhowMCMCmethodswork,itisusefultoreparametrizetheproblem.First,werestrictourattentiontothecasewheretherandomvariablexhascountablymanystates.Wecanthenrepresentthestateasjustapositiveintegerx. Diﬀerentintegervaluesofxmapbacktodiﬀerentstatesintheoriginalproblem.xConsiderwhathappenswhenweruninﬁnitelymanyMarkovchainsinparallel.AllofthestatesofthediﬀerentMarkovchainsaredrawnfromsomedistributionq()t(x),wheretindicatesthenumberoftimestepsthathaveelapsed.Atthebeginning,q(0)issomedistributionthatweusedtoarbitrarilyinitializexforeachMarkovchain.Later,q()tisinﬂuencedbyalloftheMarkovchainstepsthathaverunsofar.Ourgoalisforq()t()xtoconvergeto.px()Becausewehavereparametrizedtheproblemintermsofpositiveintegerx,wecandescribetheprobabilitydistributionusingavector,withqvqiv(= x) = i.(17.17)ConsiderwhathappenswhenweupdateasingleMarkovchain’sstatextoanewstatex.Theprobabilityofasinglestatelandinginstatexisgivenbyq(+1)t(x) =xq()t()(xTx|x.)(17.18)Usingourintegerparametrization,wecanrepresenttheeﬀectofthetransitionoperatorusingamatrix.WedeﬁnesothatTAAAi,j= (Tx= = )i|xj.(17.19)Usingthisdeﬁnition,wecannowrewriteequation.Ratherthanwritingitin17.18termsofqandTtounderstandhowasinglestateisupdated,wemaynowusevandAtodescribehowtheentiredistributionoverallthediﬀerentMarkovchains(runninginparallel)shiftsasweapplyanupdate:v()t= Av(1)t−.(17.20)596 CHAPTER17.MONTECARLOMETHODSApplyingtheMarkovchainupdaterepeatedlycorrespondstomultiplyingbythematrixArepeatedly.Inotherwords,wecanthinkoftheprocessasexponentiatingthematrix:Av()t= Atv(0).(17.21)ThematrixAhasspecialstructurebecauseeachofitscolumnsrepresentsaprobabilitydistribution.Suchmatricesarecalledstochasticmatrices.Ifthereisanon-zeroprobabilityoftransitioningfromanystatextoanyotherstatexforsomepowert,thenthePerron-Frobeniustheorem(,;Perron1907Frobenius1908,)guaranteesthatthelargesteigenvalueisrealandequalto.Overtime,wecan1seethatalloftheeigenvaluesareexponentiated:v()t=VλVdiag()−1tv(0)= ()VdiagλtV−1v(0).(17.22)Thisprocesscausesalloftheeigenvaluesthatarenotequaltotodecaytozero.1Undersomeadditionalmildconditions,Aisguaranteedtohaveonlyoneeigenvectorwitheigenvalue.Theprocessthusconvergestoa1stationarydistribution,sometimesalsocalledthe.Atconvergence,equilibriumdistributionv= = Avv,(17.23)andthissameconditionholdsforeveryadditionalstep.Thisisaneigenvectorequation.Tobeastationarypoint,vmustbeaneigenvectorwithcorrespondingeigenvalue.Thisconditionguaranteesthatoncewehavereachedthestationary1distribution,repeatedapplicationsofthetransitionsamplingproceduredonotchangetheoverthestatesofallthevariousMarkovchains(althoughdistributiontransitionoperatordoeschangeeachindividualstate,ofcourse).IfwehavechosenTcorrectly,thenthestationarydistributionqwillbeequaltothedistributionpwewishtosamplefrom.WewilldescribehowtochooseTshortly,insection.17.4MostpropertiesofMarkovChainswithcountablestatescanbegeneralizedtocontinuousvariables.Inthissituation,someauthorscalltheMarkovChainaHarrischainbutweusethetermMarkovChaintodescribebothconditions.Ingeneral,aMarkovchainwithtransitionoperatorTwillconverge,undermildconditions,toaﬁxedpointdescribedbytheequationq(x) = Ex∼qT(x|x),(17.24)whichinthediscretecaseisjustrewritingequation.When17.23xisdiscrete,theexpectationcorrespondstoasum,andwhenxiscontinuous,theexpectationcorrespondstoanintegral.597 CHAPTER17.MONTECARLOMETHODSRegardlessofwhetherthestateiscontinuousordiscrete,allMarkovchainmethodsconsistofrepeatedlyapplyingstochasticupdatesuntileventuallythestatebeginstoyieldsamplesfromtheequilibriumdistribution.RunningtheMarkovchainuntilitreachesitsequilibriumdistributioniscalled“burningin”theMarkovchain.Afterthechainhasreachedequilibrium,asequenceofinﬁnitelymanysamplesmaybedrawnfromfromtheequilibriumdistribution. Theyareidenticallydistributedbutanytwosuccessivesampleswillbehighlycorrelatedwitheachother.Aﬁnitesequenceofsamplesmaythusnotbeveryrepresentativeoftheequilibriumdistribution.Onewaytomitigatethisproblemistoreturnonlyeverynsuccessivesamples, sothatourestimateofthestatisticsoftheequilibriumdistributionisnotasbiasedbythecorrelationbetweenanMCMCsampleandthenextseveralsamples.Markovchainsarethusexpensivetousebecauseofthetimerequiredtoburnintotheequilibriumdistributionandthetimerequiredtotransitionfromonesampletoanotherreasonablydecorrelatedsampleafterreachingequilibrium.Ifonedesirestrulyindependentsamples,onecanrunmultipleMarkovchainsinparallel.Thisapproachusesextraparallelcomputationtoeliminatelatency.ThestrategyofusingonlyasingleMarkovchaintogenerateallsamplesandthestrategyofusingoneMarkovchainforeachdesiredsamplearetwoextremes;deeplearningpractitionersusuallyuseanumberofchainsthatissimilartothenumberofexamplesinaminibatchandthendrawasmanysamplesasareneededfromthisﬁxedsetofMarkovchains.AcommonlyusednumberofMarkovchainsis100.AnotherdiﬃcultyisthatwedonotknowinadvancehowmanystepstheMarkovchainmustrunbeforereachingitsequilibriumdistribution.Thislengthoftimeiscalledthemixingtime.ItisalsoverydiﬃculttotestwhetheraMarkovchainhasreachedequilibrium.Wedonothaveapreciseenoughtheoryforguidingusinansweringthisquestion.Theorytellsusthatthechainwillconverge,butnotmuchmore.IfweanalyzetheMarkovchainfromthepointofviewofamatrixAactingonavectorofprobabilitiesv,thenweknowthatthechainmixeswhenAthaseﬀectivelylostalloftheeigenvaluesfromAbesidestheuniqueeigenvalueof.1Thismeansthatthemagnitudeofthesecondlargesteigenvaluewilldeterminethemixingtime.However,inpractice,wecannotactuallyrepresentourMarkovchainintermsofamatrix.Thenumberofstatesthatourprobabilisticmodelcanvisitisexponentiallylargeinthenumberofvariables,soitisinfeasibletorepresentv,A,ortheeigenvaluesofA. Duetotheseandotherobstacles,weusuallydonotknowwhetheraMarkovchainhasmixed.Instead,wesimplyruntheMarkovchainforanamountoftimethatweroughlyestimatetobesuﬃcient,anduseheuristicmethodstodeterminewhetherthechainhasmixed.Theseheuristicmethodsincludemanuallyinspectingsamplesormeasuringcorrelationsbetween598 CHAPTER17.MONTECARLOMETHODSsuccessivesamples.17.4GibbsSamplingSofarwehavedescribedhowtodrawsamplesfromadistributionq(x)byrepeatedlyupdatingxx←∼T(x|x).However,wehavenotdescribedhowtoensurethatq(x)isausefuldistribution.Twobasicapproachesareconsideredinthisbook.TheﬁrstoneistoderiveTfromagivenlearnedpmodel,describedbelowwiththecaseofsamplingfromEBMs.ThesecondoneistodirectlyparametrizeTandlearnit,sothatitsstationarydistributionimplicitlydeﬁnesthepmodelofinterest.Examplesofthissecondapproacharediscussedinsectionsand.20.1220.13Inthecontextofdeeplearning,wecommonlyuseMarkovchainstodrawsamplesfromanenergy-basedmodeldeﬁningadistributionpmodel(x).Inthiscase,wewanttheq(x)fortheMarkovchaintobepmodel(x).Toobtainthedesiredq()x,wemustchooseanappropriateT(x|x).AconceptuallysimpleandeﬀectiveapproachtobuildingaMarkovchainthatsamplesfrompmodel(x)istouseGibbssampling,inwhichsamplingfromT(x|x)isaccomplishedbyselectingonevariablexiandsamplingitfrompmodelconditionedonitsneighborsintheundirectedgraphGdeﬁningthestructureoftheenergy-basedmodel.Itisalsopossibletosampleseveralvariablesatthesametimesolongastheyareconditionallyindependentgivenalloftheirneighbors.AsshownintheRBMexampleinsection,allofthehiddenunitsofan16.7.1RBMmaybesampledsimultaneouslybecausetheyareconditionallyindependentfromeachothergivenallofthevisibleunits.Likewise,allofthevisibleunitsmaybesampledsimultaneouslybecausetheyareconditionallyindependentfromeachothergivenallofthehiddenunits.GibbssamplingapproachesthatupdatemanyvariablessimultaneouslyinthiswayarecalledblockGibbssampling.AlternateapproachestodesigningMarkovchainstosamplefrompmodelarepossible.Forexample,theMetropolis-Hastingsalgorithmiswidelyusedinotherdisciplines.Inthecontextofthedeeplearningapproachtoundirectedmodeling,itisraretouseanyapproachotherthanGibbssampling.Improvedsamplingtechniquesareonepossibleresearchfrontier.17.5TheChallengeofMixingbetweenSeparatedModesTheprimarydiﬃcultyinvolvedwithMCMCmethodsisthattheyhaveatendencytomixpoorly.Ideally,successivesamplesfromaMarkovchaindesignedtosample599 CHAPTER17.MONTECARLOMETHODSfromp(x)wouldbecompletelyindependentfromeachotherandwouldvisitmanydiﬀerentregionsinxspaceproportionaltotheirprobability.Instead,especiallyinhighdimensionalcases,MCMCsamplesbecomeverycorrelated.Werefertosuchbehaviorasslowmixingorevenfailuretomix.MCMCmethodswithslowmixingcanbeseenasinadvertentlyperformingsomethingresemblingnoisygradientdescentontheenergyfunction,orequivalentlynoisyhillclimbingontheprobability,withrespecttothestateofthechain(therandomvariablesbeingsampled). Thechaintendstotakesmallsteps(inthespaceofthestateoftheMarkovchain),fromaconﬁgurationx(1)t−toaconﬁgurationx()t,withtheenergyE(x()t)generallylowerorapproximatelyequaltotheenergyE(x(1)t−),withapreferenceformovesthatyieldlowerenergyconﬁgurations.Whenstartingfromaratherimprobableconﬁguration(higherenergythanthetypicalonesfromp(x)),thechaintendstograduallyreducetheenergyofthestateandonlyoccasionallymovetoanothermode.Oncethechainhasfoundaregionoflowenergy(forexample,ifthevariablesarepixelsinanimage,aregionoflowenergymightbeaconnectedmanifoldofimagesofthesameobject),whichwecallamode,thechainwilltendtowalkaroundthatmode(followingakindofrandomwalk).Onceinawhileitwillstepoutofthatmodeandgenerallyreturntoitor(ifitﬁndsanescaperoute)movetowardsanothermode.Theproblemisthatsuccessfulescaperoutesarerareformanyinterestingdistributions,sotheMarkovchainwillcontinuetosamplethesamemodelongerthanitshould.ThisisveryclearwhenweconsidertheGibbssamplingalgorithm(section).17.4Inthiscontext,considertheprobabilityofgoingfromonemodetoanearbymodewithinagivennumberofsteps.Whatwilldeterminethatprobabilityistheshapeofthe“energybarrier” betweenthesemodes.Transitionsbetweentwomodesthatareseparatedbyahighenergybarrier(aregionoflowprobability)areexponentiallylesslikely(intermsoftheheightoftheenergybarrier).Thisisillustratedinﬁgure.Theproblemariseswhentherearemultiplemodeswith17.1highprobabilitythatareseparatedbyregionsoflowprobability,especiallywheneachGibbssamplingstepmustupdateonlyasmallsubsetofvariableswhosevaluesarelargelydeterminedbytheothervariables.Asasimpleexample,consideranenergy-basedmodelovertwovariablesaandb,whicharebothbinarywithasign,takingonvalues−1 1and.IfE(ab,) =−wabforsomelargepositivenumberw,thenthemodelexpressesastrongbeliefthataandbhavethesamesign.ConsiderupdatingbusingaGibbssamplingstepwitha= 1. TheconditionaldistributionoverbisgivenbyP(b= 1|a= 1)=σ(w).Ifwislarge,thesigmoidsaturates,andtheprobabilityofalsoassigningbtobe1iscloseto1.Likewise,ifa=−1,theprobabilityofassigningbtobe−1iscloseto1.AccordingtoPmodel(ab,),bothsignsofbothvariablesareequallylikely.600 CHAPTER17.MONTECARLOMETHODS Figure17.1:PathsfollowedbyGibbssamplingforthreedistributions,withtheMarkovchaininitializedatthemodeinbothcases.(Left)Amultivariatenormaldistributionwithtwoindependentvariables.Gibbssamplingmixeswellbecausethevariablesareindependent.Amultivariatenormaldistributionwithhighlycorrelatedvariables.(Center)ThecorrelationbetweenvariablesmakesitdiﬃcultfortheMarkovchaintomix.Becausetheupdateforeachvariablemustbeconditionedontheothervariable,thecorrelationreducestherateatwhichtheMarkovchaincanmoveawayfromthestartingpoint.(Right)AmixtureofGaussianswithwidelyseparatedmodesthatarenotaxis-aligned.Gibbssamplingmixesveryslowlybecauseitisdiﬃculttochangemodeswhilealteringonlyonevariableatatime.AccordingtoPmodel(ab|),bothvariablesshouldhavethesamesign.ThismeansthatGibbssamplingwillonlyveryrarelyﬂipthesignsofthesevariables.Inmorepracticalscenarios,thechallengeisevengreaterbecausewecarenotonlyaboutmakingtransitionsbetweentwomodesbutmoregenerallybetweenallthemanymodesthatarealmodelmightcontain.Ifseveralsuchtransitionsarediﬃcultbecauseofthediﬃcultyofmixingbetweenmodes,thenitbecomesveryexpensivetoobtainareliablesetofsamplescoveringmostofthemodes,andconvergenceofthechaintoitsstationarydistributionisveryslow.Sometimesthisproblemcanberesolvedbyﬁndinggroupsofhighlydependentunitsandupdatingallofthemsimultaneouslyinablock. Unfortunately,whenthedependenciesarecomplicated,itcanbecomputationallyintractabletodrawasamplefromthegroup.Afterall,theproblemthattheMarkovchainwasoriginallyintroducedtosolveisthisproblemofsamplingfromalargegroupofvariables.Inthecontextofmodelswithlatentvariables,whichdeﬁneajointdistributionpmodel(xh,),weoftendrawsamplesofxbyalternatingbetweensamplingfrompmodel(xh|)andsamplingfrompmodel(hx|).Fromthepointofviewofmixing601 CHAPTER17.MONTECARLOMETHODS Figure17.2:Anillustrationoftheslowmixingproblemindeepprobabilisticmodels.Eachpanelshouldbereadlefttoright,toptobottom.(Left)ConsecutivesamplesfromGibbssamplingappliedtoadeepBoltzmannmachinetrainedontheMNISTdataset.Consecutivesamplesaresimilartoeachother.BecausetheGibbssamplingisperformedinadeepgraphicalmodel,thissimilarityisbasedmoreonsemanticratherthanrawvisualfeatures,butitisstilldiﬃcultfortheGibbschaintotransitionfromonemodeofthedistributiontoanother,forexamplebychangingthedigitidentity.Consecutive(Right)ancestralsamplesfromagenerativeadversarialnetwork.Becauseancestralsamplinggenerateseachsampleindependentlyfromtheothers,thereisnomixingproblem.rapidly,wewouldlikepmodel(hx|)tohaveveryhighentropy.However,fromthepointofviewoflearningausefulrepresentationofh,wewouldlikehtoencodeenoughinformationaboutxtoreconstructitwell,whichimpliesthathandxshouldhaveveryhighmutualinformation.Thesetwogoalsareatoddswitheachother.Weoftenlearngenerativemodelsthatverypreciselyencodexintohbutarenotabletomixverywell.ThissituationarisesfrequentlywithBoltzmannmachines—thesharperthedistributionaBoltzmannmachinelearns,theharderitisforaMarkovchainsamplingfromthemodeldistributiontomixwell.Thisproblemisillustratedinﬁgure.17.2AllthiscouldmakeMCMCmethodslessusefulwhenthedistributionofinteresthasamanifoldstructurewithaseparatemanifoldforeachclass:thedistributionisconcentratedaroundmanymodesandthesemodesareseparatedbyvastregionsofhighenergy.ThistypeofdistributioniswhatweexpectinmanyclassiﬁcationproblemsandwouldmakeMCMCmethodsconvergeveryslowlybecauseofpoormixingbetweenmodes.602 CHAPTER17.MONTECARLOMETHODS17.5.1TemperingtoMixbetweenModesWhenadistributionhassharppeaksofhighprobabilitysurroundedbyregionsoflowprobability,itisdiﬃculttomixbetweenthediﬀerentmodesofthedistribution.Severaltechniquesforfastermixingarebasedonconstructingalternativeversionsofthetargetdistributioninwhichthepeaksarenotashighandthesurroundingvalleysarenotaslow.Energy-basedmodelsprovideaparticularlysimplewaytodoso.Sofar,wehavedescribedanenergy-basedmodelasdeﬁningaprobabilitydistributionpE.() exp(x∝−())x(17.25)Energy-basedmodelsmaybeaugmentedwithanextraparameterβcontrollinghowsharplypeakedthedistributionis:pβ() exp(())x∝−βEx.(17.26)Theβparameterisoftendescribedasbeingthereciprocalofthetemperature,reﬂectingtheoriginofenergy-basedmodelsinstatisticalphysics.Whenthetemperaturefallstozeroandrisestoinﬁnity,theenergy-basedmodelbecomesβdeterministic.Whenthetemperaturerisestoinﬁnityandβfallstozero,thedistribution(fordiscrete)becomesuniform.xTypically,amodelistrainedtobeevaluatedatβ= 1.However,wecanmakeuseofothertemperatures,particularlythosewhereβ<1.Temperingisageneralstrategyofmixingbetweenmodesofp1rapidlybydrawingsampleswith.β<1Markovchainsbasedontemperedtransitions(,)temporarilyNeal1994samplefromhigher-temperaturedistributionsinordertomixtodiﬀerentmodes,thenresumesamplingfromtheunittemperaturedistribution.ThesetechniqueshavebeenappliedtomodelssuchasRBMs (Salakhutdinov2010,).Anotherapproachistouseparalleltempering(,),inwhichtheMarkovchainIba2001simulatesmanydiﬀerentstatesinparallel,atdiﬀerenttemperatures.Thehighesttemperaturestatesmixslowly,whilethelowesttemperaturestates,attemperature1,provideaccuratesamplesfromthemodel.Thetransitionoperatorincludesstochasticallyswappingstatesbetweentwodiﬀerenttemperaturelevels,sothatasuﬃcientlyhigh-probabilitysamplefromahigh-temperatureslotcanjumpintoalowertemperatureslot.ThisapproachhasalsobeenappliedtoRBMs(Desjardinsetal.etal.,;2010Cho,). Althoughtemperingisapromisingapproach,at2010thispointithasnotallowedresearcherstomakeastrongadvanceinsolvingthechallengeofsamplingfromcomplexEBMs.Onepossiblereasonisthattherearecriticaltemperaturesaroundwhichthetemperaturetransitionmustbeveryslow(asthetemperatureisgraduallyreduced)inorderfortemperingtobeeﬀective.603 CHAPTER17.MONTECARLOMETHODS17.5.2DepthMayHelpMixingWhendrawingsamplesfromalatentvariablemodelp(hx,),wehaveseenthatifp(hx|)encodesxtoowell,thensamplingfromp(xh|)willnotchangexverymuchandmixingwillbepoor.Onewaytoresolvethisproblemistomakehbeadeeprepresentation,thatencodesintoinsuchawaythataMarkovchaininxhthespaceofhcanmixmoreeasily.Manyrepresentationlearningalgorithms,suchasautoencodersandRBMs,tendtoyieldamarginaldistributionoverhthatismoreuniformandmoreunimodalthantheoriginaldatadistributionoverx.Itcanbearguedthatthisarisesfromtryingtominimizereconstructionerrorwhileusingalloftheavailablerepresentationspace,becauseminimizingreconstructionerroroverthetrainingexampleswillbebetterachievedwhendiﬀerenttrainingexamplesareeasilydistinguishablefromeachotherinh-space,andthuswellseparated.Bengio2013aetal.()observedthatdeeperstacksofregularizedautoencodersorRBMsyieldmarginaldistributionsinthetop-levelh-spacethatappearedmorespreadoutandmoreuniform,withlessofagapbetweentheregionscorrespondingtodiﬀerentmodes(categories,intheexperiments).TraininganRBMinthathigher-levelspaceallowedGibbssamplingtomixfasterbetweenmodes.Itremainshoweverunclearhowtoexploitthisobservationtohelpbettertrainandsamplefromdeepgenerativemodels.Despitethediﬃcultyofmixing,MonteCarlotechniquesareusefulandareoftenthebesttoolavailable.Indeed,theyaretheprimarytoolusedtoconfronttheintractablepartitionfunctionofundirectedmodels,discussednext. 604 Chapter18ConfrontingthePartitionFunctionInsectionwesawthatmanyprobabilisticmodels(commonlyknownasundi-16.2.2rectedgraphicalmodels)aredeﬁnedbyanunnormalizedprobabilitydistribution˜p(x;θ).Wemustnormalize˜pbydividingbyapartitionfunctionZ(θ)inordertoobtainavalidprobabilitydistribution:p(;) =xθ1Z()θ˜p.(;)xθ(18.1)Thepartitionfunctionisanintegral(forcontinuousvariables)orsum(fordiscretevariables)overtheunnormalizedprobabilityofallstates:˜pd()xx(18.2)orx˜p.()x(18.3)Thisoperationisintractableformanyinterestingmodels.Aswewillseeinchapter,severaldeeplearningmodelsaredesignedto20haveatractablenormalizingconstant,oraredesignedtobeusedinwaysthatdonotinvolvecomputingp(x)atall. However,othermodelsdirectlyconfrontthechallengeofintractablepartitionfunctions.Inthischapter,wedescribetechniquesusedfortrainingandevaluatingmodelsthathaveintractablepartitionfunctions.605 CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION18.1TheLog-LikelihoodGradientWhat makes learning undirectedmodels bymaximumlikelihood particularlydiﬃcultisthatthepartitionfunctiondependsontheparameters.Thegradientofthelog-likelihoodwithrespecttotheparametershasatermcorrespondingtothegradientofthepartitionfunction:∇θlog(;) = pxθ∇θlog ˜p(;)xθ−∇θlog()Zθ.(18.4)Thisisawell-knowndecompositionintothepositivephaseandnegativephaseoflearning.Formostundirectedmodelsofinterest,thenegativephaseisdiﬃcult.Modelswithnolatentvariablesorwithfewinteractionsbetweenlatentvariablestypicallyhaveatractablepositivephase.ThequintessentialexampleofamodelwithastraightforwardpositivephaseanddiﬃcultnegativephaseistheRBM,whichhashiddenunitsthatareconditionallyindependentfromeachothergiventhevisibleunits.Thecasewherethepositivephaseisdiﬃcult,withcomplicatedinteractionsbetweenlatentvariables,isprimarilycoveredinchapter.Thischapterfocuses19onthediﬃcultiesofthenegativephase.Letuslookmorecloselyatthegradientof:logZ∇θlogZ(18.5)=∇θZZ(18.6)=∇θx˜p()xZ(18.7)=x∇θ˜p()xZ.(18.8)Formodelsthatguaranteep(x)>0forallx,wecansubstituteexp(log ˜p())xfor˜p()x:x∇θexp(log ˜p())xZ(18.9)=xexp(log ˜p())x∇θlog ˜p()xZ(18.10)=x˜p()x∇θlog ˜p()xZ(18.11)=xp()x∇θlog ˜p()x(18.12)606 CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION= Exx∼p()∇θlog ˜p.()x(18.13)Thisderivationmadeuseofsummationoverdiscretex,butasimilarresultappliesusingintegrationovercontinuousx.Inthecontinuousversionofthederivation,weuseLeibniz’srulefordiﬀerentiationundertheintegralsigntoobtaintheidentity∇θ˜pd()xx=∇θ˜pd.()xx(18.14)Thisidentityisapplicableonlyundercertainregularityconditionson˜pand∇θ˜p(x).Inmeasuretheoreticterms,theconditionsare:(i)Theunnormalizeddistribution˜pmustbeaLebesgue-integrablefunctionofxforeveryvalueofθ;(ii)Thegradient∇θ˜p(x)mustexistforallθandalmostallx;(iii)TheremustexistanintegrablefunctionR(x)thatbounds∇θ˜p(x)inthesensethatmaxi|∂∂θi˜p(x)|≤R(x)forallθandalmostallx.Fortunately,mostmachinelearningmodelsofinteresthavetheseproperties.Thisidentity∇θlog= ZExx∼p()∇θlog ˜p()x(18.15)isthebasisforavarietyofMonteCarlomethodsforapproximatelymaximizingthelikelihoodofmodelswithintractablepartitionfunctions.TheMonteCarloapproachtolearningundirectedmodelsprovidesanintuitiveframeworkinwhichwecanthinkofboththepositivephaseandthenegativephase.Inthepositivephase,weincreaselog ˜p(x)forxdrawnfromthedata.Inthenegativephase,wedecreasethepartitionfunctionbydecreasinglog ˜p(x) drawnfromthemodeldistribution.Inthedeeplearningliterature,itiscommontoparametrizelog ˜pintermsofanenergyfunction(equation).Inthiscase,wecaninterpretthepositive16.7phaseaspushingdownontheenergyoftrainingexamplesandthenegativephaseaspushingupontheenergyofsamplesdrawnfromthemodel,asillustratedinﬁgure.18.118.2StochasticMaximumLikelihoodandContrastiveDivergenceThenaivewayofimplementingequationistocomputeitbyburningin18.15asetofMarkovchainsfromarandominitializationeverytimethegradientisneeded.Whenlearningisperformedusingstochasticgradientdescent,thismeansthechainsmustbeburnedinoncepergradientstep.Thisapproachleadstothe607 CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTIONtrainingprocedurepresentedinalgorithm.Thehighcostofburninginthe18.1Markovchainsintheinnerloopmakesthisprocedurecomputationallyinfeasible,butthisprocedureisthestartingpointthatothermorepracticalalgorithmsaimtoapproximate.Algorithm18.1AnaiveMCMCalgorithmformaximizingthelog-likelihoodwithanintractablepartitionfunctionusinggradientascent.Set,thestepsize,toasmallpositivenumber.Setk,thenumberofGibbssteps,highenoughtoallowburnin.Perhaps100totrainanRBMonasmallimagepatch.whilenotconvergeddoSampleaminibatchofexamplesm{x(1),...,x()m}fromthetrainingset.g←1mmi=1∇θlog ˜p(x()i;)θ.Initializeasetofmsamples{˜x(1),...,˜x()m}torandomvalues(e.g.,fromauniformornormaldistribution,orpossiblyadistributionwithmarginalsmatchedtothemodel’smarginals).fordoik= 1tofordojm= 1to˜x()j←gibbs_update(˜x()j).endforendforgg←−1mmi=1∇θlog ˜p(˜x()i;)θ.θθ←+.gendwhileWecanviewtheMCMCapproachtomaximumlikelihoodastryingtoachievebalancebetweentwoforces,onepushinguponthemodeldistributionwherethedataoccurs,andanotherpushingdownonthemodeldistributionwherethemodelsamplesoccur.Figureillustratesthisprocess.Thetwoforcescorrespondto18.1maximizinglog ˜pandminimizinglogZ.Severalapproximationstothenegativephasearepossible.Eachoftheseapproximationscanbeunderstoodasmakingthenegativephasecomputationallycheaperbutalsomakingitpushdowninthewronglocations.Becausethenegativephaseinvolvesdrawingsamplesfromthemodel’sdistri-bution,wecanthinkofitasﬁndingpointsthatthemodelbelievesinstrongly.Becausethenegativephaseactstoreducetheprobabilityofthosepoints,theyaregenerallyconsideredtorepresentthemodel’sincorrectbeliefsabouttheworld.Theyarefrequentlyreferredtointheliteratureas“hallucinations”or“fantasyparticles.”Infact,thenegativephasehasbeenproposedasapossibleexplanation608 CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION xp(x)Thepositivephasepmodel()xpdata()x xp(x)Thenegativephasepmodel()xpdata()x Figure18.1:Theviewofalgorithmashavinga“positivephase”and“negativephase.”18.1(Left)Inthepositivephase,wesamplepointsfromthedatadistribution,andpushupontheirunnormalizedprobability.Thismeanspointsthatarelikelyinthedatagetpusheduponmore.(Right)Inthenegativephase,wesamplepointsfromthemodeldistribution,andpushdownontheirunnormalizedprobability.Thiscounteractsthepositivephase’stendencytojustaddalargeconstanttotheunnormalizedprobabilityeverywhere.Whenthedatadistributionandthemodeldistributionareequal,thepositivephasehasthesamechancetopushupatapointasthenegativephasehastopushdown.Whenthisoccurs,thereisnolongeranygradient(inexpectation)andtrainingmustterminate.fordreaminginhumansandotheranimals(CrickandMitchison1983,),theideabeingthatthebrainmaintainsaprobabilisticmodeloftheworldandfollowsthegradientoflog ˜pwhileexperiencingrealeventswhileawakeandfollowsthenegativegradientoflog ˜ptominimizelogZwhilesleepingandexperiencingeventssampledfromthecurrentmodel.Thisviewexplainsmuchofthelanguageusedtodescribealgorithmswithapositiveandnegativephase,butithasnotbeenproventobecorrectwithneuroscientiﬁcexperiments.Inmachinelearningmodels,itisusuallynecessarytousethepositiveandnegativephasesimultaneously,ratherthaninseparatetimeperiodsofwakefulnessandREMsleep. Aswewillseeinsection,othermachinelearningalgorithmsdrawsamplesfromthemodel19.5distributionforotherpurposesandsuchalgorithmscouldalsoprovideanaccountforthefunctionofdreamsleep.Giventhisunderstandingoftheroleofthepositiveandnegativephaseoflearning,wecanattempttodesignalessexpensivealternativetoalgorithm.18.1ThemaincostofthenaiveMCMCalgorithmisthecostofburningintheMarkovchainsfromarandominitializationateachstep. AnaturalsolutionistoinitializetheMarkovchainsfromadistributionthatisveryclosetothemodeldistribution,609 CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTIONsothattheburninoperationdoesnottakeasmanysteps.Thecontrastivedivergence(CD,orCD-ktoindicateCDwithkGibbssteps)algorithminitializestheMarkovchainateachstepwithsamplesfromthedatadistribution(Hinton20002010,,).Thisapproachispresentedasalgorithm.18.2Obtainingsamplesfromthedatadistributionisfree,becausetheyarealreadyavailableinthedataset.Initially,thedatadistributionisnotclosetothemodeldistribution,sothenegativephaseisnotveryaccurate.Fortunately,thepositivephasecanstillaccuratelyincreasethemodel’sprobabilityofthedata.Afterthepositivephasehashadsometimetoact,themodeldistributionisclosertothedatadistribution,andthenegativephasestartstobecomeaccurate.Algorithm18.2Thecontrastivedivergencealgorithm,usinggradientascentastheoptimizationprocedure.Set,thestepsize,toasmallpositivenumber.Setk,thenumberofGibbssteps,highenoughtoallowaMarkovchainsamplingfromp(x;θ)tomixwheninitializedfrompdata.Perhaps1-20totrainanRBMonasmallimagepatch.whilenotconvergeddoSampleaminibatchofexamplesm{x(1),...,x()m}fromthetrainingset.g←1mmi=1∇θlog ˜p(x()i;)θ.fordoim= 1to˜x()i←x()i.endforfordoik= 1tofordojm= 1to˜x()j←gibbs_update(˜x()j).endforendforgg←−1mmi=1∇θlog ˜p(˜x()i;)θ.θθ←+.gendwhileOfcourse,CDisstillanapproximationtothecorrectnegativephase.ThemainwaythatCDqualitativelyfailstoimplementthecorrectnegativephaseisthatitfailstosuppressregionsofhighprobabilitythatarefarfromactualtrainingexamples.Theseregionsthathavehighprobabilityunderthemodelbutlowprobabilityunderthedatageneratingdistributionarecalledspuriousmodes.Figureillustrateswhythishappens.Essentially,itisbecausemodesinthe18.2modeldistributionthatarefarfromthedatadistributionwillnotbevisitedby610 CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION xp(x)pmodel()xpdata()x Figure18.2: Anillustrationofhowthenegativephaseofcontrastivedivergence(algo-rithm)canfailtosuppressspuriousmodes.Aspuriousmodeisamodethatis18.2presentinthemodeldistributionbutabsentinthedatadistribution.BecausecontrastivedivergenceinitializesitsMarkovchainsfromdatapointsandrunstheMarkovchainforonlyafewsteps,itisunlikelytovisitmodesinthemodelthatarefarfromthedatapoints.Thismeansthatwhensamplingfromthemodel,wewillsometimesgetsamplesthatdonotresemblethedata.Italsomeansthatduetowastingsomeofitsprobabilitymassonthesemodes,themodelwillstruggletoplacehighprobabilitymassonthecorrectmodes.Forthepurposeofvisualization,thisﬁgureusesasomewhatsimpliﬁedconceptofdistance—thespuriousmodeisfarfromthecorrectmodealongthenumberlineinR.ThiscorrespondstoaMarkovchainbasedonmakinglocalmoveswithasinglexvariableinR.Formostdeepprobabilisticmodels,theMarkovchainsarebasedonGibbssamplingandcanmakenon-localmovesofindividualvariablesbutcannotmoveallofthevariablessimultaneously.Fortheseproblems,itisusuallybettertoconsidertheeditdistancebetweenmodes,ratherthantheEuclideandistance.However,editdistanceinahighdimensionalspaceisdiﬃculttodepictina2-Dplot.Markovchainsinitializedattrainingpoints,unlessisverylarge.kCarreira-PerpiñanandHinton2005()showed experimentallythatthe CDestimatorisbiasedforRBMsandfullyvisibleBoltzmannmachines,inthatitconvergestodiﬀerentpointsthanthemaximumlikelihoodestimator.Theyarguethatbecausethebiasissmall,CDcouldbeusedasaninexpensivewaytoinitializeamodelthatcouldlaterbeﬁne-tunedviamoreexpensiveMCMCmethods.BengioandDelalleau2009()showedthatCDcanbeinterpretedasdiscardingthesmallesttermsofthecorrectMCMCupdategradient,whichexplainsthebias.CDisusefulfortrainingshallowmodelslikeRBMs.ThesecaninturnbestackedtoinitializedeepermodelslikeDBNsorDBMs. However,CDdoesnotprovidemuchhelpfortrainingdeepermodelsdirectly.Thisisbecauseitisdiﬃcult611 CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTIONtoobtainsamplesofthehiddenunitsgivensamplesofthevisibleunits.Sincethehiddenunitsarenotincludedinthedata,initializingfromtrainingpointscannotsolvetheproblem.Evenifweinitializethevisibleunitsfromthedata,wewillstillneedtoburninaMarkovchainsamplingfromthedistributionoverthehiddenunitsconditionedonthosevisiblesamples.TheCDalgorithmcanbethoughtofaspenalizingthemodelforhavingaMarkovchainthatchangestheinputrapidlywhentheinputcomesfromthedata.ThismeanstrainingwithCDsomewhatresemblesautoencodertraining.EventhoughCDismorebiasedthansomeoftheothertrainingmethods,itcanbeusefulforpretrainingshallowmodelsthatwilllaterbestacked.Thisisbecausetheearliestmodelsinthestackareencouragedtocopymoreinformationuptotheirlatentvariables,therebymakingitavailabletothelatermodels.Thisshouldbethoughtofmoreofasanoften-exploitablesideeﬀectofCDtrainingratherthanaprincipleddesignadvantage.SutskeverandTieleman2010()showedthattheCDupdatedirectionisnotthegradientofanyfunction.ThisallowsforsituationswhereCDcouldcycleforever,butinpracticethisisnotaseriousproblem.AdiﬀerentstrategythatresolvesmanyoftheproblemswithCDistoinitial-izetheMarkovchainsateachgradientstepwiththeirstatesfromthepreviousgradientstep.Thisapproachwasﬁrstdiscoveredunderthenamestochasticmax-imumlikelihood(SML)intheappliedmathematicsandstatisticscommunity(Younes1998,)andlaterindependentlyrediscoveredunderthenamepersistentcontrastivedivergence(PCD,orPCD-ktoindicatetheuseofkGibbsstepsperupdate)inthedeeplearningcommunity(,).Seealgorithm.Tieleman200818.3Thebasicideaofthisapproachisthat,solongasthestepstakenbythestochasticgradientalgorithmaresmall,thenthemodelfromthepreviousstepwillbesimilartothemodelfromthecurrentstep.Itfollowsthatthesamplesfromthepreviousmodel’sdistributionwillbeveryclosetobeingfairsamplesfromthecurrentmodel’sdistribution,soaMarkovchaininitializedwiththesesampleswillnotrequiremuchtimetomix.BecauseeachMarkovchainiscontinuallyupdatedthroughoutthelearningprocess,ratherthanrestartedateachgradientstep,thechainsarefreetowanderfarenoughtoﬁndallofthemodel’smodes.SMListhusconsiderablymoreresistanttoformingmodelswithspuriousmodesthanCDis.Moreover,becauseitispossibletostorethestateofallofthesampledvariables,whethervisibleorlatent,SMLprovidesaninitializationpointforboththehiddenandvisibleunits.CDisonlyabletoprovideaninitializationforthevisibleunits,andthereforerequiresburn-infordeepmodels.SMLisabletotraindeepmodelseﬃciently.612 CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTIONMarlin2010etal.()comparedSMLtomanyoftheothercriteriapresentedinthischapter.TheyfoundthatSMLresultsinthebesttestsetlog-likelihoodforanRBM,andthatiftheRBM’shiddenunitsareusedasfeaturesforanSVMclassiﬁer,SMLresultsinthebestclassiﬁcationaccuracy.SMLisvulnerabletobecominginaccurateifthestochasticgradientalgorithmcanmovethemodelfasterthantheMarkovchaincanmixbetweensteps.Thiscanhappenifkistoosmalloristoolarge.Thepermissiblerangeofvaluesisunfortunatelyhighlyproblem-dependent.Thereisnoknownwaytotestformallywhetherthechainissuccessfullymixingbetweensteps.Subjectively,ifthelearningrateistoohighforthenumberofGibbssteps,thehumanoperatorwillbeabletoobservethatthereismuchmorevarianceinthenegativephasesamplesacrossgradientstepsratherthanacrossdiﬀerentMarkovchains.Forexample,amodeltrainedonMNISTmightsampleexclusively7sononestep.Thelearningprocesswillthenpushdownstronglyonthemodecorrespondingto7s,andthemodelmightsampleexclusively9sonthenextstep.Algorithm18.3Thestochasticmaximumlikelihood/persistentcontrastivedivergencealgorithmusinggradientascentastheoptimizationprocedure.Set,thestepsize,toasmallpositivenumber.Setk,thenumberofGibbssteps,highenoughtoallowaMarkovchainsamplingfromp(x;θ+g)toburnin,startingfromsamplesfromp(x;θ).Perhaps1forRBMonasmallimagepatch,or5-50foramorecomplicatedmodellikeaDBM.Initializeasetofmsamples{˜x(1),...,˜x()m}torandomvalues(e.g.,fromauniformornormaldistribution,orpossiblyadistributionwithmarginalsmatchedtothemodel’smarginals).whilenotconvergeddoSampleaminibatchofexamplesm{x(1),...,x()m}fromthetrainingset.g←1mmi=1∇θlog ˜p(x()i;)θ.fordoik= 1tofordojm= 1to˜x()j←gibbs_update(˜x()j).endforendforgg←−1mmi=1∇θlog ˜p(˜x()i;)θ.θθ←+.gendwhileCaremustbetakenwhenevaluatingthesamplesfromamodeltrainedwithSML.ItisnecessarytodrawthesamplesstartingfromafreshMarkovchain613 CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTIONinitializedfromarandomstartingpointafterthemodelisdonetraining.Thesamplespresentinthepersistentnegativechainsusedfortraininghavebeeninﬂuencedbyseveralrecentversionsofthemodel,andthuscanmakethemodelappeartohavegreatercapacitythanitactuallydoes.BerglundandRaiko2013()performedexperimentstoexaminethebiasandvarianceintheestimateofthegradientprovidedbyCDandSML.CDprovestohavelowervariancethantheestimatorbasedonexactsampling.SMLhashighervariance.ThecauseofCD’slowvarianceisitsuseofthesametrainingpointsinboththepositiveandnegativephase.Ifthenegativephaseisinitializedfromdiﬀerenttrainingpoints,thevariancerisesabovethatoftheestimatorbasedonexactsampling.AllofthesemethodsbasedonusingMCMCtodrawsamplesfromthemodelcaninprinciplebeusedwithalmostanyvariantofMCMC.ThismeansthattechniquessuchasSMLcanbeimprovedbyusinganyoftheenhancedMCMCtechniquesdescribedinchapter,suchasparalleltempering(,17Desjardinsetal.2010Cho2010;etal.,).OneapproachtoacceleratingmixingduringlearningreliesnotonchangingtheMonteCarlosamplingtechnologybutratheronchangingtheparametrizationofthemodelandthecostfunction.FastPCDorFPCD(,TielemanandHinton2009)involvesreplacingtheparametersθofatraditionalmodelwithanexpressionθθ= ()slow+θ()fast.(18.16)Therearenowtwiceasmanyparametersasbefore,andtheyareaddedtogetherelement-wisetoprovidetheparametersusedbytheoriginalmodeldeﬁnition.Thefastcopyoftheparametersistrainedwithamuchlargerlearningrate,allowingittoadaptrapidlyinresponsetothenegativephaseoflearningandpushtheMarkovchaintonewterritory.ThisforcestheMarkovchaintomixrapidly,thoughthiseﬀectonlyoccursduringlearningwhilethefastweightsarefreetochange.Typicallyonealsoappliessigniﬁcantweightdecaytothefastweights,encouragingthemtoconvergetosmallvalues,afteronlytransientlytakingonlargevalueslongenoughtoencouragetheMarkovchaintochangemodes.OnekeybeneﬁttotheMCMC-basedmethodsdescribedinthissectionisthattheyprovideanestimateofthegradientoflogZ,andthuswecanessentiallydecomposetheproblemintothelog ˜pcontributionandthelogZcontribution.Wecanthenuseanyothermethodtotacklelog ˜p(x),andjustaddournegativephasegradientontotheothermethod’sgradient.Inparticular,thismeansthatourpositivephasecanmakeuseofmethodsthatprovideonlyalowerboundon˜p.MostoftheothermethodsofdealingwithlogZpresentedinthischapterare614 CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTIONincompatiblewithbound-basedpositivephasemethods.18.3PseudolikelihoodMonteCarloapproximationstothepartitionfunctionanditsgradientdirectlyconfrontthepartitionfunction.Otherapproachessidesteptheissue,bytrainingthemodelwithoutcomputingthepartitionfunction.Mostoftheseapproachesarebasedontheobservationthatitiseasytocomputeratiosofprobabilitiesinanundirectedprobabilisticmodel.Thisisbecausethepartitionfunctionappearsinboththenumeratorandthedenominatoroftheratioandcancelsout:p()xp()y=1Z˜p()x1Z˜p()y=˜p()x˜p()y.(18.17)Thepseudolikelihoodisbasedontheobservationthatconditionalprobabilitiestakethisratio-basedform,andthuscanbecomputedwithoutknowledgeofthepartitionfunction.Supposethatwepartitionxintoa,bandc,whereacontainsthevariableswewanttoﬁndtheconditionaldistributionover,bcontainsthevariableswewanttoconditionon,andccontainsthevariablesthatarenotpartofourquery.p() =ab|p,(ab)p()b=p,(ab)ac,p,,(abc)=˜p,(ab)ac,˜p,,(abc).(18.18)Thisquantityrequiresmarginalizingouta,whichcanbeaveryeﬃcientoperationprovidedthataandcdonotcontainverymanyvariables.Intheextremecase,acanbeasinglevariableandccanbeempty,makingthisoperationrequireonlyasmanyevaluationsof˜pastherearevaluesofasinglerandomvariable.Unfortunately,inordertocomputethelog-likelihood,weneedtomarginalizeoutlargesetsofvariables.Iftherearenvariablestotal,wemustmarginalizeasetofsize.Bythechainruleofprobability,n−1log() = log(pxpx1)+log(px2|x1)++(···pxn|x1:1n−).(18.19)Inthiscase,wehavemadeamaximallysmall,butccanbeaslargeasx2:n.Whatifwesimplymovecintobtoreducethecomputationalcost?Thisyieldsthepseudolikelihood(,)objectivefunction,basedonpredictingthevalueBesag1975offeaturexigivenalloftheotherfeaturesx−i:ni=1log(pxi|x−i).(18.20)615 CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTIONIfeachrandomvariablehaskdiﬀerentvalues,thisrequiresonlykn×evaluationsof˜ptocompute,asopposedtotheknevaluationsneededtocomputethepartitionfunction.Thismaylooklikeanunprincipledhack,butitcanbeproventhatestimationbymaximizingthepseudolikelihoodisasymptoticallyconsistent(,).Mase1995Ofcourse,inthecaseofdatasetsthatdonotapproachthelargesamplelimit,pseudolikelihoodmaydisplaydiﬀerentbehaviorfromthemaximumlikelihoodestimator.Itispossibletotradecomputationalcomplexityfordeviationfrommaximumlikelihoodbehaviorbyusingthegeneralizedpseudolikelihoodestimator(HuangandOgata2002,).ThegeneralizedpseudolikelihoodestimatorusesmdiﬀerentsetsS()i,i= 1,...,mofindicesofvariablesthatappeartogetherontheleftsideoftheconditioningbar.Intheextremecaseofm= 1andS(1)=1,...,nthegeneralizedpseudolikelihoodrecoversthelog-likelihood. Intheextremecaseofm=nandS()i={}i,thegeneralizedpseudolikelihoodrecoversthepseudolikelihood.Thegeneralizedpseudolikelihoodobjectivefunctionisgivenbymi=1log(pxS()i|x−S()i).(18.21)Theperformanceofpseudolikelihood-basedapproachesdependslargelyonhowthemodelwillbeused.Pseudolikelihoodtendstoperformpoorlyontasksthatrequireagoodmodelofthefulljointp(x),suchasdensityestimationandsampling.However,itcanperformbetterthanmaximumlikelihoodfortasksthatrequireonlytheconditionaldistributionsusedduringtraining,suchasﬁllinginsmallamountsofmissingvalues.GeneralizedpseudolikelihoodtechniquesareespeciallypowerfulifthedatahasregularstructurethatallowstheSindexsetstobedesignedtocapturethemostimportantcorrelationswhileleavingoutgroupsofvariablesthatonlyhavenegligiblecorrelation.Forexample,innaturalimages,pixelsthatarewidelyseparatedinspacealsohaveweakcorrelation,sothegeneralizedpseudolikelihoodcanbeappliedwitheachsetbeingasmall,spatiallylocalizedwindow.SOneweaknessofthepseudolikelihoodestimatoristhatitcannotbeusedwithotherapproximationsthatprovideonlyalowerboundon˜p(x),suchasvariationalinference,whichwillbecoveredinchapter.Thisisbecause19˜pappearsinthedenominator.Alowerboundonthedenominatorprovidesonlyanupperboundontheexpressionasawhole,andthereisnobeneﬁttomaximizinganupperbound.ThismakesitdiﬃculttoapplypseudolikelihoodapproachestodeepmodelssuchasdeepBoltzmannmachines,sincevariationalmethodsareoneofthedominantapproachestoapproximatelymarginalizingoutthemanylayersofhiddenvariables616 CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTIONthatinteractwitheachother. However,pseudolikelihoodisstillusefulfordeeplearning,becauseitcanbeusedtotrainsinglelayermodels,ordeepmodelsusingapproximateinferencemethodsthatarenotbasedonlowerbounds.PseudolikelihoodhasamuchgreatercostpergradientstepthanSML,duetoitsexplicitcomputationofalloftheconditionals.However,generalizedpseudo-likelihoodandsimilarcriteriacanstillperformwellifonlyonerandomlyselectedconditionaliscomputedperexample(Goodfellow2013betal.,),therebybringingthecomputationalcostdowntomatchthatofSML.ThoughthepseudolikelihoodestimatordoesnotexplicitlyminimizelogZ,itcanstillbethoughtofashavingsomethingresemblinganegativephase.Thedenominatorsofeachconditionaldistributionresultinthelearningalgorithmsuppressingtheprobabilityofallstatesthathaveonlyonevariablediﬀeringfromatrainingexample.SeeMarlinanddeFreitas2011()foratheoreticalanalysisoftheasymptoticeﬃciencyofpseudolikelihood.18.4ScoreMatchingandRatioMatchingScorematching(,)providesanotherconsistentmeansoftrainingaHyvärinen2005modelwithoutestimatingZoritsderivatives.Thenamescorematchingcomesfromterminologyinwhichthederivativesofalogdensitywithrespecttoitsargument,∇xlogp(x),arecalleditsscore.Thestrategyusedbyscorematchingistominimizetheexpectedsquareddiﬀerencebetweenthederivativesofthemodel’slogdensitywithrespecttotheinputandthederivativesofthedata’slogdensitywithrespecttotheinput:L,(xθ) =12||∇xlogpmodel(;)xθ−∇xlogpdata()x||22(18.22)J() =θ12Epdata()xL,(xθ)(18.23)θ∗= minθJ()θ(18.24)ThisobjectivefunctionavoidsthediﬃcultiesassociatedwithdiﬀerentiatingthepartitionfunctionZbecauseZisnotafunctionofxandtherefore∇xZ= 0.Initially,scorematchingappearstohaveanewdiﬃculty: computingthescoreofthedatadistributionrequiresknowledgeofthetruedistributiongeneratingthetrainingdata,pdata.Fortunately,minimizingtheexpectedvalueofisL,(xθ)617 CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTIONequivalenttominimizingtheexpectedvalueof˜L,(xθ) =nj=1∂2∂x2jlogpmodel(;)+xθ12∂∂xjlogpmodel(;)xθ2(18.25)whereisthedimensionalityof.nxBecausescorematchingrequirestakingderivativeswithrespecttox,itisnotapplicabletomodelsofdiscretedata.However,thelatentvariablesinthemodelmaybediscrete.Likethepseudolikelihood,scorematchingonlyworkswhenweareabletoevaluatelog ˜p(x)anditsderivativesdirectly.Itisnotcompatiblewithmethodsthatonlyprovidealowerboundonlog ˜p(x),becausescorematchingrequiresthederivativesandsecondderivativesoflog ˜p(x)andalowerboundconveysnoinformationaboutitsderivatives.Thismeansthatscorematchingcannotbeappliedtoestimatingmodelswithcomplicatedinteractionsbetweenthehiddenunits,suchassparsecodingmodelsordeepBoltzmannmachines.Whilescorematchingcanbeusedtopretraintheﬁrsthiddenlayerofalargermodel,ithasnotbeenappliedasapretrainingstrategyforthedeeperlayersofalargermodel.Thisisprobablybecausethehiddenlayersofsuchmodelsusuallycontainsomediscretevariables.Whilescorematchingdoesnotexplicitlyhaveanegativephase,itcanbeviewedasaversionofcontrastivedivergenceusingaspeciﬁckindofMarkovchain(,).TheMarkovchaininthiscaseisnotGibbssampling,butHyvärinen2007aratheradiﬀerentapproachthatmakeslocalmovesguidedbythegradient.ScorematchingisequivalenttoCDwiththistypeofMarkovchainwhenthesizeofthelocalmovesapproacheszero.Lyu2009()generalizedscorematchingtothediscretecase(butmadeanerrorintheirderivationthatwascorrectedby()).Marlinetal.2010Marlinetal.()foundthat2010generalizedscorematching(GSM)doesnotworkinhighdimensionaldiscretespaceswheretheobservedprobabilityofmanyeventsis0.Amoresuccessfulapproachtoextendingthebasicideasofscorematchingtodiscretedataisratiomatching(,).RatiomatchingappliesHyvärinen2007bspeciﬁcallytobinarydata.Ratiomatchingconsistsofminimizingtheaverageoverexamplesofthefollowingobjectivefunction:L()RM() =xθ,nj=111+pmodel(;)xθpmodel(());)fx,jθ2,(18.26)618 CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTIONwherereturnswiththebitatpositionﬂipped.Ratiomatchingavoidsf,j(x)x jthepartitionfunctionusingthesametrickasthepseudolikelihoodestimator:inaratiooftwoprobabilities,thepartitionfunctioncancelsout.()Marlinetal.2010foundthatratiomatchingoutperformsSML,pseudolikelihoodandGSMintermsoftheabilityofmodelstrainedwithratiomatchingtodenoisetestsetimages.Likethepseudolikelihoodestimator,ratiomatchingrequiresnevaluationsof˜pperdatapoint,makingitscomputationalcostperupdateroughlyntimeshigherthanthatofSML.Aswiththepseudolikelihoodestimator,ratiomatchingcanbethoughtofaspushingdownonallfantasystatesthathaveonlyonevariablediﬀerentfromatrainingexample.Sinceratiomatchingappliesspeciﬁcallytobinarydata,thismeansthatitactsonallfantasystateswithinHammingdistance1ofthedata.Ratiomatchingcanalsobeusefulasthebasisfordealingwithhigh-dimensionalsparsedata,suchaswordcountvectors.ThiskindofdataposesachallengeforMCMC-basedmethodsbecausethedataisextremelyexpensivetorepresentindenseformat,yettheMCMCsamplerdoesnotyieldsparsevaluesuntilthemodelhaslearnedtorepresentthesparsityinthedatadistribution.DauphinandBengio()overcamethisissuebydesigninganunbiasedstochasticapproximationto2013ratiomatching.Theapproximationevaluatesonlyarandomlyselectedsubsetofthetermsoftheobjective,anddoesnotrequirethemodeltogeneratecompletefantasysamples.SeeMarlinanddeFreitas2011()foratheoreticalanalysisoftheasymptoticeﬃciencyofratiomatching.18.5DenoisingScoreMatchingInsomecaseswemaywishtoregularizescorematching,byﬁttingadistributionpsmoothed() =xpdata()()yqxy|dy(18.27)ratherthanthetruepdata.Thedistributionq(xy|) isacorruptionprocess,usuallyonethatformsbyaddingasmallamountofnoiseto.xyDenoisingscorematchingisespeciallyusefulbecauseinpracticeweusuallydonothaveaccesstothetruepdatabutratheronlyanempiricaldistributiondeﬁnedbysamplesfromit.Anyconsistentestimatorwill,givenenoughcapacity,makepmodelintoasetofDiracdistributionscenteredonthetrainingpoints.Smoothingbyqhelpstoreducethisproblem,atthelossoftheasymptoticconsistencyproperty619 CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTIONdescribedinsection.()introducedaprocedurefor5.4.5KingmaandLeCun2010performingregularizedscorematchingwiththesmoothingdistributionqbeingnormallydistributednoise.Recallfromsectionthatseveralautoencodertrainingalgorithmsare14.5.1equivalenttoscorematchingordenoisingscorematching.Theseautoencodertrainingalgorithmsare thereforea wayof overcomingthe partitionfunctionproblem.18.6Noise-ContrastiveEstimationMosttechniquesforestimatingmodelswithintractablepartitionfunctionsdonotprovideanestimateofthepartitionfunction.SMLandCDestimateonlythegradientofthelogpartitionfunction,ratherthanthepartitionfunctionitself.Scorematchingandpseudolikelihoodavoidcomputingquantitiesrelatedtothepartitionfunctionaltogether.Noise-contrastive estimation (NCE)(Gutmann and Hyvarinen2010, )takesadiﬀerentstrategy.Inthisapproach,theprobabilitydistributionestimatedbythemodelisrepresentedexplicitlyaslogpmodel() = log ˜xpmodel(;)+xθc,(18.28)wherecisexplicitlyintroducedasanapproximationof−logZ(θ).Ratherthanestimatingonlyθ,thenoisecontrastiveestimationproceduretreatscasjustanotherparameterandestimatesθandcsimultaneously,usingthesamealgorithmforboth.Theresultinglogpmodel(x)thusmaynotcorrespondexactlytoavalidprobabilitydistribution,butwillbecomecloserandclosertobeingvalidastheestimateofimproves.c1Suchanapproachwouldnotbepossibleusingmaximumlikelihoodasthecriterionfortheestimator.Themaximumlikelihoodcriterionwouldchoosetosetccarbitrarilyhigh,ratherthansettingtocreateavalidprobabilitydistribution.NCEworksbyreducingtheunsupervisedlearningproblemofestimatingp(x)tothatoflearningaprobabilisticbinaryclassiﬁerinwhichoneofthecategoriescorrespondstothedatageneratedbythemodel.Thissupervisedlearningproblemisconstructedinsuchawaythatmaximumlikelihoodestimationinthissupervised1NCEisalsoapplicabletoproblemswithatractablepartitionfunction,wherethereisnoneedtointroducetheextraparameterc.However,ithasgeneratedthemostinterestasameansofestimatingmodelswithdiﬃcultpartitionfunctions.620 CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTIONlearningproblemdeﬁnesanasymptoticallyconsistentestimatoroftheoriginalproblem.Speciﬁcally,weintroduceaseconddistribution,thenoisedistributionpnoise(x).Thenoisedistributionshouldbetractabletoevaluateandtosamplefrom. Wecannowconstructamodeloverbothxandanew,binaryclassvariabley.Inthenewjointmodel,wespecifythatpjoint(= 1) =y12,(18.29)pjoint(= 1) = x|ypmodel()x,(18.30)andpjoint(= 0) = x|ypnoise()x.(18.31)Inotherwords,yisaswitchvariablethatdetermineswhetherwewillgeneratexfromthemodelorfromthenoisedistribution.Wecanconstructasimilarjointmodeloftrainingdata.Inthiscase,theswitchvariabledetermineswhetherwedrawxfromthedataorfromthenoisedistribution.Formally,ptrain(y=1)=12,ptrain(x|y=1)=pdata(x), andptrain(= 0) = x|ypnoise()x.Wecannowjustusestandardmaximumlikelihoodlearningonthesupervisedlearningproblemofﬁttingpjointtoptrain:θ,c= argmaxθ,cEx,py∼trainlogpjoint()y|x.(18.32)Thedistributionpjointisessentiallyalogisticregressionmodelappliedtothediﬀerenceinlogprobabilitiesofthemodelandthenoisedistribution:pjoint(= 1 ) =y|xpmodel()xpmodel()+xpnoise()x(18.33)=11+pnoise()xpmodel()x(18.34)=11+explogpnoise()xpmodel()x(18.35)= σ−logpnoise()xpmodel()x(18.36)= (logσpmodel()logx−pnoise())x.(18.37)621 CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTIONNCEisthussimpletoapplysolongaslog ˜pmodeliseasytoback-propagatethrough,and,asspeciﬁedabove,pnoiseiseasytoevaluate(inordertoevaluatepjoint)andsamplefrom(inordertogeneratethetrainingdata).NCEismostsuccessfulwhenappliedtoproblemswithfewrandomvariables,butcanworkwellevenifthoserandomvariablescantakeonahighnumberofvalues.Forexample,ithasbeensuccessfullyappliedtomodelingtheconditionaldistributionoverawordgiventhecontextoftheword(MnihandKavukcuoglu,2013).Thoughthewordmaybedrawnfromalargevocabulary,thereisonlyoneword.WhenNCEisappliedtoproblemswithmanyrandomvariables,itbecomeslesseﬃcient.Thelogisticregressionclassiﬁercanrejectanoisesamplebyidentifyinganyonevariablewhosevalueisunlikely.Thismeansthatlearningslowsdowngreatlyafterpmodelhaslearnedthebasicmarginalstatistics.Imaginelearningamodelofimagesoffaces,usingunstructuredGaussiannoiseaspnoise.Ifpmodellearnsabouteyes,itcanrejectalmostallunstructurednoisesampleswithouthavinglearnedanythingaboutotherfacialfeatures,suchasmouths.Theconstraintthatpnoisemustbeeasytoevaluateandeasytosamplefromcanbeoverlyrestrictive.Whenpnoiseissimple,mostsamplesarelikelytobetooobviouslydistinctfromthedatatoforcepmodeltoimprovenoticeably.Likescorematchingandpseudolikelihood,NCEdoesnotworkifonlyalowerboundon˜pisavailable.Suchalowerboundcouldbeusedtoconstructalowerboundonpjoint(y= 1|x),butitcanonlybeusedtoconstructanupperboundonpjoint(y= 0|x),whichappearsinhalfthetermsoftheNCEobjective.Likewise,alowerboundonpnoiseisnotuseful,becauseitprovidesonlyanupperboundonpjoint(= 1 )y|x.Whenthemodeldistributioniscopiedtodeﬁneanewnoisedistributionbeforeeachgradientstep,NCEdeﬁnesaprocedurecalledself-contrastiveestimation,whose expected gradientis equivalentto the expected gradientofmaximumlikelihood(,).ThespecialcaseofNCEwherethenoisesamplesGoodfellow2014are thosegenerated by themodel suggests thatmaximumlikelihood can beinterpretedasaprocedurethatforcesamodeltoconstantlylearntodistinguishrealityfromitsownevolvingbeliefs,whilenoisecontrastiveestimationachievessomereducedcomputationalcostbyonlyforcingthemodeltodistinguishrealityfromaﬁxedbaseline(thenoisemodel).Usingthesupervisedtaskofclassifyingbetweentrainingsamplesandgeneratedsamples(withthemodelenergyfunctionusedindeﬁningtheclassiﬁer)toprovideagradientonthemodelwasintroducedearlierinvariousforms(Wellingetal.,2003bBengio2009;,).622 CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTIONNoise contrastiveestimation is basedon the ideathat agood generativemodelshould be abletodistinguish datafromnoise.Aclosely relatedideaisthat agood generativemodelshould beabletogenerate samplesthatnoclassiﬁercandistinguishfromdata.Thisideayieldsgenerativeadversarialnetworks(section).20.10.418.7EstimatingthePartitionFunctionWhilemuchofthischapterisdedicatedtodescribingmethodsthatavoidneedingtocomputetheintractablepartitionfunctionZ(θ)associatedwithanundirectedgraphicalmodel,inthissectionwediscussseveralmethodsfordirectlyestimatingthepartitionfunction.Estimatingthepartitionfunctioncanbeimportantbecausewerequireitifwewishtocomputethenormalizedlikelihoodofdata.Thisisoftenimportantinevaluatingthemodel,monitoringtrainingperformance,andcomparingmodelstoeachother.Forexample,imaginewehavetwomodels:modelMAdeﬁningaprobabil-itydistributionpA(x;θA)=1ZA˜pA(x;θA)andmodelMBdeﬁningaprobabilitydistributionpB(x;θB)=1ZB˜pB(x;θB).Acommonwaytocomparethemodelsistoevaluateandcomparethelikelihoodthatbothmodelsassigntoani.i.d.testdataset.Supposethetestsetconsistsofmexamples{x(1),...,x()m}.IfipA(x()i;θA) >ipB(x()i;θB)orequivalentlyifilogpA(x()i;θA)−ilogpB(x()i;θB) 0>,(18.38)thenwesaythatMAisabettermodelthanMB(or,atleast,itisabettermodelofthetestset),inthesensethatithasabettertestlog-likelihood.Unfortunately,testingwhetherthisconditionholdsrequiresknowledgeofthepartitionfunction.Unfortunately,equationseemstorequireevaluatingthelogprobabilitythat18.38themodelassignstoeachpoint,whichinturnrequiresevaluatingthepartitionfunction.Wecansimplifythesituationslightlybyre-arrangingequation18.38intoaformwhereweneedtoknowonlytheratioofthetwomodel’spartitionfunctions:ilogpA(x()i;θA)−ilogpB(x()i;θB) =ilog˜pA(x()i;θA)˜pB(x()i;θB)−mlogZ(θA)Z(θB).(18.39)623 CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTIONWecanthusdeterminewhetherMAisabettermodelthanMBwithoutknowingthepartitionfunctionofeithermodelbutonlytheirratio.Aswewillseeshortly,wecanestimatethisratiousingimportancesampling,providedthatthetwomodelsaresimilar.If,however,wewantedtocomputetheactualprobabilityofthetestdataundereitherMAorMB,wewouldneedtocomputetheactualvalueofthepartitionfunctions.Thatsaid,ifweknewtheratiooftwopartitionfunctions,r=Z(θB)Z(θA),andweknewtheactualvalueofjustoneofthetwo,sayZ(θA),wecouldcomputethevalueoftheother:Z(θB) = (rZθA) =Z(θB)Z(θA)Z(θA).(18.40)Asimplewaytoestimatethe partitionfunctionistouse aMonteCarlomethodsuchassimpleimportancesampling.Wepresenttheapproachintermsofcontinuousvariablesusingintegrals,butitcanbereadilyappliedtodiscretevariablesbyreplacingtheintegralswithsummation.Weuseaproposaldistributionp0(x)=1Z0˜p0(x)whichsupportstractablesamplingandtractableevaluationofboththepartitionfunctionZ0andtheunnormalizeddistribution˜p0()x.Z1=˜p1()xdx(18.41)=p0()xp0()x˜p1()xdx(18.42)= Z0p0()x˜p1()x˜p0()xdx(18.43)ˆZ1=Z0KKk=1˜p1(x()k)˜p0(x()k)st:..x()k∼p0(18.44)Inthelastline,wemakeaMonteCarloestimator,ˆZ1,oftheintegralusingsamplesdrawnfromp0(x)andthenweighteachsamplewiththeratiooftheunnormalized˜p1andtheproposalp0.Weseealsothatthisapproachallowsustoestimatetheratiobetweenthepartitionfunctionsas1KKk=1˜p1(x()k)˜p0(x()k)st:..x()k∼p0.(18.45)Thisvaluecanthenbeuseddirectlytocomparetwomodelsasdescribedinequation.18.39624 CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTIONIfthedistributionp0isclosetop1,equationcanbeaneﬀectivewayof18.44estimatingthepartitionfunction(Minka2005,).Unfortunately,mostofthetimep1isbothcomplicated(usuallymultimodal)anddeﬁnedoverahighdimensionalspace.Itisdiﬃculttoﬁndatractablep0thatissimpleenoughtoevaluatewhilestillbeingcloseenoughtop1toresultinahighqualityapproximation.Ifp0andp1arenotclose,mostsamplesfromp0willhavelowprobabilityunderp1andthereforemake(relatively)negligiblecontributiontothesuminequation.18.44Havingfewsamples withsigniﬁcantweightsinthis sumwillresultinanestimatorthatisofpoorqualityduetohighvariance. ThiscanbeunderstoodquantitativelythroughanestimateofthevarianceofourestimateˆZ1:ˆVarˆZ1=Z0K2Kk=1˜p1(x()k)˜p0(x()k)−ˆZ12.(18.46)Thisquantityislargestwhenthereissigniﬁcantdeviationinthevaluesoftheimportanceweights˜p1(x()k)˜p0(x()k).Wenowturntotworelatedstrategiesdevelopedtocopewiththechalleng-ingtaskofestimatingpartitionfunctionsforcomplexdistributionsoverhigh-dimensionalspaces: annealedimportancesamplingandbridgesampling.Bothstartwiththesimpleimportancesamplingstrategyintroducedaboveandbothattempttoovercometheproblemoftheproposalp0beingtoofarfromp1byintroducingintermediatedistributionsthatattempttobetweenbridgethegapp0andp1.18.7.1AnnealedImportanceSamplingInsituationswhereDKL(p0p1)islarge(i.e.,wherethereislittleoverlapbetweenp0andp1),astrategycalledannealedimportancesampling(AIS)attemptstobridgethegapbyintroducingintermediatedistributions(,;,Jarzynski1997Neal2001).Considerasequenceofdistributionspη0,...,pηn,with0 =η0<η1<<···ηn−1<ηn= 1sothattheﬁrstandlastdistributionsinthesequencearep0andp1respectively.Thisapproachallowsustoestimatethepartitionfunctionofamultimodaldistributiondeﬁnedoverahigh-dimensionalspace(suchasthedistributiondeﬁnedbyatrainedRBM).Webeginwithasimplermodelwithaknownpartitionfunction(suchasanRBMwithzeroesforweights)andestimatetheratiobetweenthetwomodel’spartitionfunctions. Theestimateofthisratioisbasedontheestimateoftheratiosofasequenceofmanysimilardistributions,suchasthesequenceofRBMswithweightsinterpolatingbetweenzeroandthelearnedweights.625 CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTIONWecannowwritetheratioZ1Z0asZ1Z0=Z1Z0Zη1Zη1···Zηn−1Zηn−1(18.47)=Zη1Z0Zη2Zη1···Zηn−1Zηn−2Z1Zηn−1(18.48)=n−1j=0Zηj+1Zηj(18.49)Providedthedistributionspηjandpηj+1,forall0≤≤−jn1,aresuﬃcientlyclose,wecanreliablyestimateeachofthefactorsZηj+1ZηjusingsimpleimportancesamplingandthenusethesetoobtainanestimateofZ1Z0.Wheredotheseintermediatedistributionscomefrom?Justastheoriginalproposaldistributionp0isadesignchoice,soisthesequenceofdistributionspη1...pηn−1.Thatis,itcanbespeciﬁcallyconstructedtosuittheproblemdomain.Onegeneral-purposeandpopularchoicefortheintermediatedistributionsistousetheweightedgeometricaverageofthetargetdistributionp1andthestartingproposaldistribution(forwhichthepartitionfunctionisknown)p0:pηj∝pηj1p1−ηj0(18.50)Inordertosamplefromtheseintermediatedistributions,wedeﬁneaseriesofMarkovchaintransitionfunctionsTηj(x|x) thatdeﬁnetheconditionalprobabilitydistributionoftransitioningtoxgivenwearecurrentlyatx.ThetransitionoperatorTηj(x|x)isdeﬁnedtoleavepηj()xinvariant:pηj() =xpηj(x)Tηj(xx|)dx(18.51)ThesetransitionsmaybeconstructedasanyMarkovchainMonteCarlomethod(e.g.,Metropolis-Hastings,Gibbs),includingmethodsinvolvingmultiplepassesthroughalloftherandomvariablesorotherkindsofiterations.TheAISsamplingstrategyisthentogeneratesamplesfromp0andthenusethetransitionoperatorstosequentiallygeneratesamplesfromtheintermediatedistributionsuntilwearriveatsamplesfromthetargetdistributionp1:•fork...K= 1–Samplex()kη1∼p0()x626 CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION–Samplex()kη2∼Tη1(x()kη2|x()kη1)–...–Samplex()kηn−1∼Tηn−2(x()kηn−1|x()kηn−2)–Samplex()kηn∼Tηn−1(x()kηn|x()kηn−1)•endForsamplek,wecanderivetheimportanceweightbychainingtogethertheimportanceweightsforthejumpsbetweentheintermediatedistributionsgiveninequation:18.49w()k=˜pη1(x()kη1)˜p0(x()kη1)˜pη2(x()kη2)˜pη1(x()kη2)...˜p1(x()k1)˜pηn−1(x()kηn).(18.52)Toavoidnumericalissuessuchasoverﬂow,itisprobablybesttocomputelogw()kbyaddingandsubtractinglogprobabilities,ratherthancomputingw()kbymultiplyinganddividingprobabilities.Withthesamplingprocedurethusdeﬁnedandtheimportanceweightsgiveninequation,theestimateoftheratioofpartitionfunctionsisgivenby:18.52Z1Z0≈1KKk=1w()k(18.53)Inordertoverifythatthisproceduredeﬁnesavalidimportancesamplingscheme,wecanshow(,)thattheAISprocedurecorrespondstosimpleNeal2001importancesamplingonanextendedstatespacewithpointssampledovertheproductspace[xη1,...,xηn−1,x1].Todothis,wedeﬁnethedistributionovertheextendedspaceas:˜p(xη1,...,xηn−1,x1)(18.54)=˜p1(x1)˜Tηn−1(xηn−1|x1)˜Tηn−2(xηn−2|xηn−1)...˜Tη1(xη1|xη2),(18.55)where˜TaisthereverseofthetransitionoperatordeﬁnedbyTa(viaanapplicationofBayes’rule):˜Ta(x|x) =pa(x)pa()xTa(xx|) =˜pa(x)˜pa()xTa(xx|).(18.56)Pluggingtheaboveintotheexpressionforthejointdistributionontheextendedstatespacegiveninequation,weget:18.55˜p(xη1,...,xηn−1,x1)(18.57)627 CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION=˜p1(x1)˜pηn−1(xηn−1)˜pηn−1(x1)Tηn−1(x1|xηn−1)n−2i=1˜pηi(xηi)˜pηi(xηi+1)Tηi(xηi+1|xηi)(18.58)=˜p1(x1)˜pηn−1(x1)Tηn−1(x1|xηn−1)˜pη1(xη1)n−2i=1˜pηi+1(xηi+1)˜pηi(xηi+1)Tηi(xηi+1|xηi).(18.59)Wenowhavemeansofgeneratingsamplesfromthejointproposaldistributionqovertheextendedsampleviaasamplingschemegivenabove,withthejointdistributiongivenby:q(xη1,...,xηn−1,x1) = p0(xη1)Tη1(xη2|xη1)...Tηn−1(x1|xηn−1).(18.60)Wehaveajointdistributionontheextendedspacegivenbyequation.Taking18.59q(xη1,...,xηn−1,x1)astheproposaldistributionontheextendedstatespacefromwhichwewilldrawsamples,itremainstodeterminetheimportanceweights:w()k=˜p(xη1,...,xηn−1,x1)q(xη1,...,xηn−1,x1)=˜p1(x()k1)˜pηn−1(x()kηn−1)...˜pη2(x()kη2)˜p1(x()kη1)˜pη1(x()kη1)˜p0(x()k0).(18.61)TheseweightsarethesameasproposedforAIS.ThuswecaninterpretAISassimpleimportancesamplingappliedtoanextendedstateanditsvalidityfollowsimmediatelyfromthevalidityofimportancesampling.Annealedimportancesampling(AIS)wasﬁrstdiscoveredby()Jarzynski1997andthenagain,independently,by().ItiscurrentlythemostcommonNeal2001wayofestimatingthepartitionfunctionforundirectedprobabilisticmodels.Thereasonsforthismayhavemoretodowiththepublicationofaninﬂuentialpaper(SalakhutdinovandMurray2008,)describingitsapplicationtoestimatingthepartitionfunctionofrestrictedBoltzmannmachinesanddeepbeliefnetworksthanwithanyinherentadvantagethemethodhasovertheothermethoddescribedbelow.AdiscussionofthepropertiesoftheAISestimator(e.g..itsvarianceandeﬃciency)canbefoundin().Neal200118.7.2BridgeSamplingBridgesampling()isanothermethodthat,likeAIS,addressestheBennett1976shortcomingsofimportancesampling.Ratherthanchainingtogetheraseriesof628 CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTIONintermediatedistributions,bridgesamplingreliesonasingledistributionp∗,knownasthebridge,tointerpolatebetweenadistributionwithknownpartitionfunction,p0,andadistributionp1forwhichwearetryingtoestimatethepartitionfunctionZ1.BridgesamplingestimatestheratioZ1/Z0astheratiooftheexpectedimpor-tanceweightsbetween˜p0and˜p∗andbetween˜p1and˜p∗:Z1Z0≈Kk=1˜p∗(x()k0)˜p0(x()k0)Kk=1˜p∗(x()k1)˜p1(x()k1)(18.62)Ifthebridgedistributionp∗ischosencarefullytohavealargeoverlapofsupportwithbothp0andp1,thenbridgesamplingcanallowthedistancebetweentwodistributions(ormoreformally,DKL(p0p1))tobemuchlargerthanwithstandardimportancesampling.Itcanbeshownthattheoptimalbridgingdistributionisgivenbyp()opt∗(x)∝˜p0()˜xp1()xr˜p0()+˜xp1()xwherer=Z1/Z0.Atﬁrst,thisappearstobeanunworkablesolutionasitwouldseemtorequiretheveryquantitywearetryingtoestimate,Z1/Z0.However,itispossibletostartwithacoarseestimateofrandusetheresultingbridgedistributiontoreﬁneourestimateiteratively(,).Thatis,weNeal2005iterativelyre-estimatetheratioanduseeachiterationtoupdatethevalueof.rLinkedimportancesamplingBothAISandbridgesamplinghavetheirad-vantages.IfDKL(p0p1)isnottoolarge(becausep0andp1aresuﬃcientlyclose)bridgesamplingcanbeamoreeﬀectivemeansofestimatingtheratioofpartitionfunctionsthanAIS.If,however,thetwodistributionsaretoofarapartforasingledistributionp∗tobridgethegapthenonecanatleastuseAISwithpotentiallymanyintermediatedistributionstospanthedistancebetweenp0andp1.Neal()showedhowhislinkedimportancesamplingmethodleveragedthepowerof2005thebridgesamplingstrategytobridgetheintermediatedistributionsusedinAIStosigniﬁcantlyimprovetheoverallpartitionfunctionestimates.EstimatingthepartitionfunctionwhiletrainingWhileAIShasbecomeacceptedasthestandardmethodforestimatingthepartitionfunctionformanyundirectedmodels, itissuﬃcientlycomputationallyintensivethatitremainsinfeasibletouseduringtraining.However,alternativestrategiesthathavebeenexploredtomaintainanestimateofthepartitionfunctionthroughouttrainingUsingacombinationofbridgesampling,short-chainAISandparalleltempering,Desjardins2011etal.()devisedaschemetotrackthepartitionfunctionofan629 CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTIONRBMthroughoutthetrainingprocess.ThestrategyisbasedonthemaintenanceofindependentestimatesofthepartitionfunctionsoftheRBMateverytemperatureoperatingintheparalleltemperingscheme.Theauthorscombinedbridgesamplingestimatesoftheratiosofpartitionfunctionsofneighboringchains(i.e.fromparalleltempering)withAISestimatesacrosstimetocomeupwithalowvarianceestimateofthepartitionfunctionsateveryiterationoflearning.Thetoolsdescribedinthischapterprovidemanydiﬀerentwaysofovercomingtheproblemofintractablepartitionfunctions,buttherecanbeseveralotherdiﬃcultiesinvolvedintrainingandusinggenerativemodels.Foremostamongtheseistheproblemofintractableinference,whichweconfrontnext. 630 Chapter19ApproximateInferenceManyprobabilisticmodelsarediﬃculttotrainbecauseitisdiﬃculttoperforminferenceinthem.Inthecontextofdeeplearning,weusuallyhaveasetofvisiblevariablesvandasetoflatentvariablesh.Thechallengeofinferenceusuallyreferstothediﬃcultproblemofcomputingp(hv|)ortakingexpectationswithrespecttoit.Suchoperationsareoftennecessaryfortaskslikemaximumlikelihoodlearning.Manysimplegraphicalmodelswithonlyonehiddenlayer,suchasrestrictedBoltzmannmachinesandprobabilisticPCA,aredeﬁnedinawaythatmakesinferenceoperationslikecomputingp(hv|),ortakingexpectationswithrespecttoit,simple.Unfortunately,mostgraphicalmodelswithmultiplelayersofhiddenvariableshaveintractableposteriordistributions.Exactinferencerequiresanexponentialamountoftimeinthesemodels.Evensomemodelswithonlyasinglelayer,suchassparsecoding,havethisproblem.Inthischapter,weintroduceseveralofthetechniquesforconfrontingtheseintractableinferenceproblems.Later,inchapter,wewilldescribehowtouse20thesetechniquestotrainprobabilisticmodelsthatwouldotherwisebeintractable,suchasdeepbeliefnetworksanddeepBoltzmannmachines.Intractableinferenceproblemsindeeplearningusuallyarisefrominteractionsbetweenlatentvariablesinastructuredgraphicalmodel.Seeﬁgureforsome19.1examples.Theseinteractionsmaybeduetodirectinteractionsinundirectedmodelsor“explainingaway”interactionsbetweenmutualancestorsofthesamevisibleunitindirectedmodels.631 CHAPTER19.APPROXIMATEINFERENCE Figure19.1:Intractableinferenceproblemsindeeplearningareusuallytheresultofinteractionsbetweenlatentvariablesinastructuredgraphicalmodel.Thesecanbeduetoedgesdirectlyconnectingonelatentvariabletoanother,orduetolongerpathsthatareactivatedwhenthechildofaV-structureisobserved.(Left)Asemi-restrictedBoltzmannmachine(,)withconnectionsbetweenhiddenOsinderoandHinton2008units.Thesedirectconnectionsbetweenlatentvariablesmaketheposteriordistributionintractableduetolargecliquesoflatentvariables.AdeepBoltzmannmachine,(Center)organizedintolayersofvariableswithoutintra-layerconnections,stillhasanintractableposteriordistributionduetotheconnectionsbetweenlayers.Thisdirectedmodel(Right)hasinteractionsbetweenlatentvariableswhenthevisiblevariablesareobserved,becauseeverytwolatentvariablesareco-parents.Someprobabilisticmodelsareabletoprovidetractableinferenceoverthelatentvariablesdespitehavingoneofthegraphstructuresdepictedabove.Thisispossibleiftheconditionalprobabilitydistributionsarechosentointroduceadditionalindependencesbeyondthosedescribedbythegraph.Forexample,probabilisticPCAhasthegraphstructureshownintheright,yetstillhassimpleinferenceduetospecialpropertiesofthespeciﬁcconditionaldistributionsituses(linear-Gaussianconditionalswithmutuallyorthogonalbasisvectors). 632 CHAPTER19.APPROXIMATEINFERENCE19.1InferenceasOptimizationManyapproachestoconfrontingtheproblemofdiﬃcultinferencemakeuseoftheobservationthatexactinferencecanbedescribedasanoptimizationproblem.Approximateinferencealgorithmsmaythenbederivedbyapproximatingtheunderlyingoptimizationproblem.Toconstructtheoptimizationproblem,assumewehaveaprobabilisticmodelconsistingofobservedvariablesvandlatentvariablesh.Wewouldliketocomputethelogprobabilityoftheobserveddata,logp(v;θ).Sometimesitistoodiﬃculttocomputelogp(v;θ)ifitiscostlytomarginalizeouth.Instead,wecancomputealowerboundL(vθ,,q)onlogp(v;θ).Thisboundiscalledtheevidencelowerbound(ELBO).Anothercommonlyusednameforthislowerboundisthenegativevariationalfreeenergy.Speciﬁcally,theevidencelowerboundisdeﬁnedtobeL−() = log(;)vθ,,qpvθDKL(()(;))qhv|phv|θ(19.1)whereisanarbitraryprobabilitydistributionover.qhBecausethediﬀerencebetweenlogp(v)andL(vθ,,q)isgivenbytheKLdivergenceandbecausetheKLdivergenceisalwaysnon-negative,wecanseethatLalwayshasatmostthesamevalueasthedesiredlogprobability.Thetwoareequalifandonlyifisthesamedistributionas.qp()hv|Surprisingly,Lcanbeconsiderablyeasiertocomputeforsomedistributionsq.SimplealgebrashowsthatwecanrearrangeLintoamuchmoreconvenientform:L−() =log(;)vθ,,qpvθDKL(()(;))qhv|phv|θ(19.2)=log(;)pvθ−Eh∼qlogq()hv|p()hv|(19.3)=log(;)pvθ−Eh∼qlogq()hv|p,(hvθ;)p(;)vθ(19.4)=log(;)pvθ−Eh∼q[log()log(;)+log(;)]qhv|−phv,θpvθ(19.5)=−Eh∼q[log()log(;)]qhv|−phv,θ.(19.6)Thisyieldsthemorecanonicaldeﬁnitionoftheevidencelowerbound,L() = vθ,,qEh∼q[log()]+()phv,Hq.(19.7)Foranappropriatechoiceofq,Listractabletocompute.Foranychoiceofq,Lprovidesalowerboundonthelikelihood.Forq(hv|)thatarebetter633 CHAPTER19.APPROXIMATEINFERENCEapproximationsofp(hv|),thelowerboundLwillbetighter,inotherwords,closertologp(v). Whenq(hv|)=p(hv|),theapproximationisperfect,andL() = log(;)vθ,,qpvθ.WecanthusthinkofinferenceastheprocedureforﬁndingtheqthatmaximizesL.ExactinferencemaximizesLperfectlybysearchingoverafamilyoffunctionsqthatincludesp(hv|).Throughoutthischapter,wewillshowhowtoderivediﬀerentformsofapproximateinferencebyusingapproximateoptimizationtoﬁndq.Wecanmaketheoptimizationprocedurelessexpensivebutapproximatebyrestrictingthefamilyofdistributionsqtheoptimizationisallowedtosearchoverorbyusinganimperfectoptimizationprocedurethatmaynotcompletelymaximizebutmerelyincreaseitbyasigniﬁcantamount.LNomatterwhatchoiceofqweuse,Lisalowerbound.Wecangettighterorlooserboundsthatarecheaperormoreexpensivetocomputedependingonhowwechoosetoapproachthisoptimizationproblem. Wecanobtainapoorlymatchedqbutreducethecomputationalcostbyusinganimperfectoptimizationprocedure,orbyusingaperfectoptimizationprocedureoverarestrictedfamilyofqdistributions.19.2ExpectationMaximizationTheﬁrstalgorithmweintroducebasedonmaximizingalowerboundListheexpectationmaximization(EM)algorithm,apopulartrainingalgorithmformodelswithlatentvariables.WedescribehereaviewontheEMalgorithmdevelopedby().UnlikemostoftheotheralgorithmsweNealandHinton1999describeinthischapter,EMisnotanapproachtoapproximateinference,butratheranapproachtolearningwithanapproximateposterior.TheEMalgorithmconsistsofalternatingbetweentwostepsuntilconvergence:•TheE-step(Expectationstep):Letθ(0)denotethevalueoftheparametersatthebeginningofthestep.Setq(h()i|v)=p(h()i|v()i;θ(0))forallindicesiofthetrainingexamplesv()iwewanttotrainon(bothbatchandminibatchvariantsarevalid).Bythiswemeanqisdeﬁnedintermsofthecurrentparametervalueofθ(0);ifwevaryθthenp(hv|;θ)willchangebutqp()hv|willremainequalto(;hv|θ(0)).•The(Maximizationstep):CompletelyorpartiallymaximizeM-stepiL(v()i,,qθ)(19.8)634 CHAPTER19.APPROXIMATEINFERENCEwithrespecttousingyouroptimizationalgorithmofchoice.θThiscanbeviewedasacoordinateascentalgorithmtomaximizeL.Ononestep,wemaximizeLwithrespecttoq,andontheother,wemaximizeLwithrespectto.θStochasticgradientascentonlatentvariablemodelscanbeseenasaspecialcaseoftheEMalgorithmwheretheMstepconsistsoftakingasinglegradientstep.OthervariantsoftheEMalgorithmcanmakemuchlargersteps.Forsomemodelfamilies,theMstepcanevenbeperformedanalytically,jumpingallthewaytotheoptimalsolutionforgiventhecurrent.θqEventhoughtheE-stepinvolvesexactinference,wecanthinkoftheEMalgorithmasusingapproximateinferenceinsomesense.Speciﬁcally,theM-stepassumesthatthesamevalueofqcanbeusedforallvaluesofθ.ThiswillintroduceagapbetweenLandthetruelogp(v)astheM-stepmovesfurtherandfurtherawayfromthevalueθ(0)usedintheE-step.Fortunately,theE-stepreducesthegaptozeroagainasweentertheloopforthenexttime.TheEMalgorithmcontainsafewdiﬀerentinsights.First,thereisthebasicstructureofthelearningprocess,inwhichweupdatethemodelparameterstoimprovethelikelihoodofacompleteddataset,whereallmissingvariableshavetheirvaluesprovidedbyanestimateoftheposteriordistribution.ThisparticularinsightisnotuniquetotheEMalgorithm.Forexample,usinggradientdescenttomaximizethelog-likelihoodalsohasthissameproperty;thelog-likelihoodgradientcomputationsrequiretakingexpectationswithrespecttotheposteriordistributionoverthehiddenunits. AnotherkeyinsightintheEMalgorithmisthatwecancontinuetouseonevalueofqevenafterwehavemovedtoadiﬀerentvalueofθ.ThisparticularinsightisusedthroughoutclassicalmachinelearningtoderivelargeM-stepupdates.Inthecontextofdeeplearning,mostmodelsaretoocomplextoadmitatractablesolutionforanoptimallargeM-stepupdate,sothissecondinsightwhichismoreuniquetotheEMalgorithmisrarelyused.19.3MAPInferenceandSparseCodingWeusuallyusetheterminferencetorefertocomputingtheprobabilitydistributionoveronesetofvariablesgivenanother.Whentrainingprobabilisticmodelswithlatentvariables,weareusuallyinterestedincomputingp(hv|).Analternativeformofinferenceistocomputethesinglemostlikelyvalueofthemissingvariables,ratherthantoinfertheentiredistributionovertheirpossiblevalues.Inthecontext635 CHAPTER19.APPROXIMATEINFERENCEoflatentvariablemodels,thismeanscomputingh∗= argmaxhp.()hv|(19.9)Thisisknownasmaximumaposterioriinference,abbreviatedMAPinference.MAPinferenceisusuallynotthoughtofasapproximateinference—itdoescomputetheexactmostlikelyvalueofh∗.However,ifwewishtodevelopalearningprocessbasedonmaximizingL(vh,,q),thenitishelpfultothinkofMAPinferenceasaprocedurethatprovidesavalueofq.Inthissense,wecanthinkofMAPinferenceasapproximateinference,becauseitdoesnotprovidetheoptimalq.Recallfromsectionthatexactinferenceconsistsofmaximizing19.1L() = vθ,,qEh∼q[log()]+()phv,Hq(19.10)withrespecttoqoveranunrestrictedfamilyofprobabilitydistributions,usinganexactoptimizationalgorithm.WecanderiveMAPinferenceasaformofapproximateinferencebyrestrictingthefamilyofdistributionsqmaybedrawnfrom.Speciﬁcally,werequiretotakeonaDiracdistribution:qqδ.() = hv|()hµ−(19.11)Thismeansthatwecannowcontrolqentirelyviaµ.DroppingtermsofLthatdonotvarywith,weareleftwiththeoptimizationproblemµµ∗= argmaxµlog(= )phµv,,(19.12)whichisequivalenttotheMAPinferenceproblemh∗= argmaxhp.()hv|(19.13)WecanthusjustifyalearningproceduresimilartoEM,inwhichwealternatebetweenperformingMAPinferencetoinferh∗andthenupdateθtoincreaselogp(h∗,v).AswithEM,thisisaformofcoordinateascentonL,wherewealternatebetweenusing inferenceto optimizeLwithrespect toqandusingparameterupdatestooptimizeLwithrespecttoθ.TheprocedureasawholecanbejustiﬁedbythefactthatLisalowerboundonlogp(v).InthecaseofMAPinference,thisjustiﬁcationisrathervacuous,becausetheboundisinﬁnitelyloose,duetotheDiracdistribution’sdiﬀerentialentropyofnegativeinﬁnity.However,addingnoisetowouldmaketheboundmeaningfulagain.µ636 CHAPTER19.APPROXIMATEINFERENCEMAPinferenceiscommonlyusedindeeplearningasbothafeatureextractorandalearningmechanism.Itisprimarilyusedforsparsecodingmodels.Recallfromsectionthatsparsecodingisalinearfactormodelthatimposes13.4asparsity-inducingprioronitshiddenunits.AcommonchoiceisafactorialLaplaceprior,withph(i) =λ2e−|λhi|.(19.14)Thevisibleunitsarethengeneratedbyperformingalineartransformationandaddingnoise:p,β() = (;+xh|NvWhb−1I).(19.15)Computingorevenrepresentingp(hv|)isdiﬃcult.Everypairofvariableshiandhjarebothparentsofv.Thismeansthatwhenvisobserved,thegraphicalmodelcontainsanactivepathconnectinghiandhj.Allofthehiddenunitsthusparticipateinonemassivecliqueinp(hv|).IfthemodelwereGaussianthentheseinteractionscouldbemodeledeﬃcientlyviathecovariancematrix,butthesparsepriormakestheseinteractionsnon-Gaussian.Becausep(hv|)isintractable,soisthecomputationofthelog-likelihoodanditsgradient.Wethuscannotuseexactmaximumlikelihoodlearning.Instead,weuseMAPinferenceandlearntheparametersbymaximizingtheELBOdeﬁnedbytheDiracdistributionaroundtheMAPestimateof.hIfweconcatenateallofthehvectorsinthetrainingsetintoamatrixH,andconcatenateallofthevectorsintoamatrix,thenthesparsecodinglearningvVprocessconsistsofminimizingJ,(HW) =i,j|Hi,j|+i,jVHW−2i,j.(19.16)MostapplicationsofsparsecodingalsoinvolveweightdecayoraconstraintonthenormsofthecolumnsofW,inordertopreventthepathologicalsolutionwithextremelysmallandlarge.HWWecanminimizeJbyalternatingbetweenminimizationwithrespecttoHandminimizationwithrespecttoW.Bothsub-problemsareconvex.Infact,theminimizationwithrespecttoWisjustalinearregressionproblem.However,minimizationofJwithrespecttobothargumentsisusuallynotaconvexproblem.MinimizationwithrespecttoHrequiresspecializedalgorithmssuchasthefeature-signsearchalgorithm(,).Leeetal.2007637 CHAPTER19.APPROXIMATEINFERENCE19.4VariationalInferenceandLearningWe have seen how the evidence lower boundL(vθ,,q)is a lower bound onlogp(v;θ),howinferencecanbeviewedasmaximizingLwithrespecttoq,andhowlearningcanbeviewedasmaximizingLwithrespecttoθ.WehaveseenthattheEMalgorithmallowsustomakelargelearningstepswithaﬁxedqandthatlearningalgorithmsbasedonMAPinferenceallowustolearnusingapointestimateofp(hv|)ratherthaninferringtheentiredistribution.Nowwedevelopthemoregeneralapproachtovariationallearning.ThecoreideabehindvariationallearningisthatwecanmaximizeLoverarestrictedfamilyofdistributionsq.ThisfamilyshouldbechosensothatitiseasytocomputeEqlogp(hv,). Atypicalwaytodothisistointroduceassumptionsabouthowfactorizes.qAcommonapproachtovariationallearningistoimposetherestrictionthatqisafactorialdistribution:q() =hv|iqh(i|v).(19.17)Thisiscalledthemeanﬁeldapproach.Moregenerally,wecanimposeanygraphi-calmodelstructurewechooseonq,toﬂexiblydeterminehowmanyinteractionswewantourapproximationtocapture.Thisfullygeneralgraphicalmodelapproachiscalledstructuredvariationalinference(,).SaulandJordan1996Thebeautyofthevariationalapproachisthatwedonotneedtospecifyaspeciﬁcparametricformforq.Wespecifyhowitshouldfactorize,butthentheoptimizationproblemdeterminestheoptimalprobabilitydistributionwithinthosefactorizationconstraints.Fordiscretelatentvariables,thisjustmeansthatweusetraditionaloptimizationtechniquestooptimizeaﬁnitenumberofvariablesdescribingtheqdistribution.Forcontinuouslatentvariables,thismeansthatweuseabranchofmathematicscalledcalculusofvariationstoperformoptimizationoveraspaceoffunctions,andactuallydeterminewhichfunctionshouldbeusedtorepresentq.Calculusof variations istheorigin ofthenames “variationallearning”and“variationalinference,”thoughthesenamesapplyevenwhenthelatentvariablesarediscreteandcalculusofvariationsisnotneeded.Inthecaseofcontinuouslatentvariables,calculusofvariationsisapowerfultechniquethatremovesmuchoftheresponsibilityfromthehumandesignerofthemodel,whonowmustspecifyonlyhowqfactorizes,ratherthanneedingtoguesshowtodesignaspeciﬁcthatcanaccuratelyapproximatetheposterior.qBecauseL(vθ,,q)isdeﬁnedtobelogp(v;θ)−DKL(q(hv|)p(hv|;θ)),wecanthinkofmaximizingLwithrespecttoqasminimizingDKL(q(hv|)p(hv|)).638 CHAPTER19.APPROXIMATEINFERENCEInthissense,weareﬁttingqtop. However,wearedoingsowiththeoppositedirectionoftheKLdivergencethanweareusedtousingforﬁttinganapproximation.Whenweusemaximumlikelihoodlearningtoﬁtamodeltodata,weminimizeDKL(pdatapmodel).Asillustratedinﬁgure,thismeansthatmaximumlikelihood3.6encouragesthemodeltohavehighprobabilityeverywherethatthedatahashighprobability, whileouroptimization-basedinferenceprocedureencouragesqtohavelowprobabilityeverywherethetrueposteriorhaslowprobability.BothdirectionsoftheKLdivergencecanhavedesirableandundesirableproperties.Thechoiceofwhichtousedependsonwhichpropertiesarethehighestpriorityforeachapplication.Inthecaseoftheinferenceoptimizationproblem,wechoosetouseDKL(q(hv|)p(hv|))forcomputationalreasons.Speciﬁcally,computingDKL(q(hv|)p(hv|))involvesevaluatingexpectationswithrespecttoq,sobydesigningqtobesimple,wecansimplifytherequiredexpectations.TheoppositedirectionoftheKLdivergencewouldrequirecomputingexpectationswithrespecttothetrueposterior.Becausetheformofthetrueposteriorisdeterminedbythechoiceofmodel,wecannotdesignareduced-costapproachtocomputingDKL(()())phv|qhv|exactly.19.4.1DiscreteLatentVariablesVariationalinferencewithdiscretelatentvariablesisrelativelystraightforward.Wedeﬁneadistributionq,typicallyonewhereeachfactorofqisjustdeﬁnedbyalookuptableoverdiscretestates. Inthesimplestcase,hisbinaryandwemakethemeanﬁeldassumptionthatfactorizesovereachindividualqhi.Inthiscasewecanparametrizeqwithavectorˆhwhoseentriesareprobabilities.Thenqh(i= 1 ) =|vˆhi.Afterdetermininghowtorepresentq,wesimplyoptimizeitsparameters.Inthecaseofdiscretelatentvariables,thisisjustastandardoptimizationproblem.Inprincipletheselectionofqcouldbedonewithanyoptimizationalgorithm,suchasgradientdescent.Becausethisoptimizationmustoccurintheinnerloopofalearningalgorithm,itmustbeveryfast.Toachievethisspeed,wetypicallyusespecialoptimizationalgorithmsthataredesignedtosolvecomparativelysmallandsimpleproblemsinveryfewiterations.Apopularchoiceistoiterateﬁxedpointequations,inotherwords,tosolve∂∂ˆhiL= 0(19.18)forˆhi.Werepeatedlyupdatediﬀerentelementsofˆhuntilwesatisfyaconvergence639 CHAPTER19.APPROXIMATEINFERENCEcriterion.Tomakethismoreconcrete,weshowhowtoapplyvariationalinferencetothebinarysparsecodingmodel(wepresentherethemodeldevelopedbyHennigesetal.()butdemonstratetraditional,genericmeanﬁeldappliedtothemodel,2010whiletheyintroduceaspecializedalgorithm).Thisderivationgoesintoconsiderablemathematicaldetailandisintendedforthereaderwhowishestofullyresolveanyambiguityinthehigh-levelconceptualdescriptionofvariationalinferenceandlearningwehavepresentedsofar.Readerswhodonotplantoderiveorimplementvariationallearningalgorithmsmaysafelyskiptothenextsectionwithoutmissinganynewhigh-levelconcepts.Readerswhoproceedwiththebinarysparsecodingexampleareencouragedtoreviewthelistofusefulpropertiesoffunctionsthatcommonlyariseinprobabilisticmodelsinsection.Weusetheseproperties3.10liberallythroughoutthefollowingderivationswithouthighlightingexactlywhereweuseeachone.Inthebinarysparsecodingmodel,theinputv∈RnisgeneratedfromthemodelbyaddingGaussiannoisetothesumofmdiﬀerentcomponentswhichcaneachbepresentorabsent.Eachcomponentisswitchedonoroﬀbythecorrespondinghiddenunitinh∈{}01,m:ph(i= 1) = (σbi)(19.19)p,() = (;vh|NvWhβ−1)(19.20)wherebisalearnablesetofbiases,Wisalearnableweightmatrix,andβisalearnable,diagonalprecisionmatrix.Trainingthismodelwithmaximumlikelihoodrequirestakingthederivativewithrespecttotheparameters.Considerthederivativewithrespecttooneofthebiases:∂∂bilog()pv(19.21)=∂∂bip()vp()v(19.22)=∂∂bihp,(hv)p()v(19.23)=∂∂bihpp()h()vh|p()v(19.24)640 CHAPTER19.APPROXIMATEINFERENCEh1h1h2h2h3h3v1v1v2v2v3v3h4h4h1h1h2h2h3h3h4h4Figure19.2:Thegraphstructureofabinarysparsecodingmodelwithfourhiddenunits.(Left)Thegraphstructureofp(hv,).Notethattheedgesaredirected,andthateverytwohiddenunitsareco-parentsofeveryvisibleunit.Thegraphstructureof(Right)p(hv|).Inordertoaccountfortheactivepathsbetweenco-parents,theposteriordistributionneedsanedgebetweenallofthehiddenunits.=hp()vh|∂∂bip()hp()v(19.25)=hp()hv|∂∂bip()hp()h(19.26)=Eh∼|p(hv)∂∂bilog()ph.(19.27)Thisrequirescomputingexpectationswithrespecttop(hv|).Unfortunately,p(hv|)isacomplicateddistribution.Seeﬁgureforthegraphstructureof19.2p(hv,)andp(hv|).Theposteriordistributioncorrespondstothecompletegraphoverthehiddenunits,sovariableeliminationalgorithmsdonothelpustocomputetherequiredexpectationsanyfasterthanbruteforce.Wecanresolvethisdiﬃcultybyusingvariationalinferenceandvariationallearninginstead.Wecanmakeameanﬁeldapproximation:q() =hv|iqh(i|v).(19.28)Thelatentvariablesofthebinarysparsecodingmodelarebinary,sotorepresentafactorialqwesimplyneedtomodelmBernoullidistributionsq(hi|v).AnaturalwaytorepresentthemeansoftheBernoullidistributionsiswithavectorˆhofprobabilities,withq(hi=1|v)=ˆhi.Weimposearestrictionthatˆhiisneverequalto0orto1,inordertoavoiderrorswhencomputing,forexample,logˆhi.Wewillseethatthevariationalinferenceequationsneverassignorto01ˆhi641 CHAPTER19.APPROXIMATEINFERENCEanalytically.However,inasoftwareimplementation,machineroundingerrorcouldresultinorvalues.Insoftware,wemaywishtoimplementbinarysparse01codingusinganunrestrictedvectorofvariationalparameterszandobtainˆhviatherelationˆh=σ(z).Wecanthussafelycomputelogˆhionacomputerbyusingtheidentitylog(σzi) = (−ζ−zi)relatingthesigmoidandthesoftplus.Tobeginourderivationofvariationallearninginthebinarysparsecodingmodel,weshowthattheuseofthismeanﬁeldapproximationmakeslearningtractable.TheevidencelowerboundisgivenbyL()vθ,,q(19.29)=Eh∼q[log()]+()phv,Hq(19.30)=Eh∼q[log()+log()log()]phpvh|−qhv|(19.31)=Eh∼qmi=1log(phi)+ni=1log(pvi|−h)mi=1log(qhi|v)(19.32)=mi=1ˆhi(log(σbi)log−ˆhi)+(1−ˆhi)(log(σ−bi)log(1−−ˆhi))(19.33)+Eh∼qni=1logβi2πexp−βi2(vi−Wi,:h)2(19.34)=mi=1ˆhi(log(σbi)log−ˆhi)+(1−ˆhi)(log(σ−bi)log(1−−ˆhi))(19.35)+12ni=1logβi2π−βiv2i−2viWi,:ˆh+jW2i,jˆhj+kj=Wi,jWi,kˆhjˆhk.(19.36)Whiletheseequationsaresomewhatunappealingaesthetically,theyshowthatLcanbeexpressedinasmallnumberofsimplearithmeticoperations.TheevidencelowerboundListhereforetractable.WecanuseLasareplacementfortheintractablelog-likelihood.Inprinciple,wecouldsimplyrungradientascentonbothvandhandthiswouldmakeaperfectlyacceptablecombinedinferenceandtrainingalgorithm.Usually,however,wedonotdothis,fortworeasons.First,thiswouldrequirestoringˆhforeachv.Wetypicallypreferalgorithmsthatdonotrequireper-examplememory.Itisdiﬃculttoscalelearningalgorithmstobillionsofexamplesifwemustrememberadynamicallyupdatedvectorassociatedwitheachexample.642 CHAPTER19.APPROXIMATEINFERENCESecond,wewouldliketobeabletoextractthefeaturesˆhveryquickly,inordertorecognizethecontentofv. Inarealisticdeployedsetting,wewouldneedtobeabletocomputeˆhinrealtime.Forboththesereasons,wetypicallydonotusegradientdescenttocomputethemeanﬁeldparametersˆh.Instead,werapidlyestimatethemwithﬁxedpointequations.Theideabehindﬁxedpointequationsisthatweareseekingalocalmaximumwithrespecttoˆh, where∇hL(vθ,,ˆh)=0.Wecannoteﬃcientlysolvethisequationwithrespecttoallofˆhsimultaneously.However,wecansolveforasinglevariable:∂∂ˆhiL(vθ,,ˆh) = 0.(19.37)Wecantheniterativelyapplythesolutiontotheequationfori=1,...,m,andrepeatthecycleuntilwesatisfyaconvergecriterion.CommonconvergencecriteriaincludestoppingwhenafullcycleofupdatesdoesnotimproveLbymorethansometoleranceamount,orwhenthecycledoesnotchangeˆhbymorethansomeamount.Iteratingmeanﬁeldﬁxedpointequationsisageneraltechniquethatcanprovidefastvariationalinferenceinabroadvarietyofmodels.Tomakethismoreconcrete,weshowhowtoderivetheupdatesforthebinarysparsecodingmodelinparticular.First,wemustwriteanexpressionforthederivativeswithrespecttoˆhi.Todoso,wesubstituteequationintotheleftsideofequation:19.3619.37∂∂ˆhiL(vθ,,ˆh)(19.38)=∂∂ˆhimj=1ˆhj(log(σbj)log−ˆhj)+(1−ˆhj)(log(σ−bj)log(1−−ˆhj))(19.39)+12nj=1logβj2π−βjv2j−2vjWj,:ˆh+kW2j,kˆhk+lk=Wj,kWj,lˆhkˆhl(19.40)=log(σbi)log−ˆhi−−1+log(1ˆhi)+1log(−σ−bi)(19.41)+nj=1βjvjWj,i−12W2j,i−ki=Wj,kWj,iˆhk(19.42)643 CHAPTER19.APPROXIMATEINFERENCE=bi−logˆhi+log(1−ˆhi)+vβW:,i−12W:,iβW:,i−ji=W:,jβW:,iˆhj.(19.43)Toapplytheﬁxedpointupdateinferencerule,wesolvefortheˆhithatsetsequationto0:19.43ˆhi= σbi+vβW:,i−12W:,iβW:,i−ji=W:,jβW:,iˆhj.(19.44)Atthispoint,wecanseethatthereisacloseconnectionbetweenrecurrentneuralnetworksandinferenceingraphicalmodels.Speciﬁcally,themeanﬁeldﬁxedpointequationsdeﬁnedarecurrentneuralnetwork.Thetaskofthisnetworkistoperforminference.Wehavedescribedhowtoderivethisnetworkfromamodeldescription,butitisalsopossibletotraintheinferencenetworkdirectly.Severalideasbasedonthisthemearedescribedinchapter.20Inthecaseofbinarysparsecoding,wecanseethattherecurrentnetworkconnectionspeciﬁedbyequationconsistsofrepeatedlyupdatingthehidden19.44unitsbasedonthechangingvaluesoftheneighboringhiddenunits.TheinputalwayssendsaﬁxedmessageofvβWtothehiddenunits,butthehiddenunitsconstantlyupdatethemessagetheysendtoeachother.Speciﬁcally,twounitsˆhiandˆhjinhibiteachotherwhentheirweightvectorsarealigned.Thisisaformofcompetition—betweentwohiddenunitsthatbothexplaintheinput,onlytheonethatexplainstheinputbestwillbeallowedtoremainactive.Thiscompetitionisthemeanﬁeldapproximation’sattempttocapturetheexplainingawayinteractionsinthebinarysparsecodingposterior.Theexplainingawayeﬀectactuallyshouldcauseamulti-modalposterior,sothatifwedrawsamplesfromtheposterior,somesampleswillhaveoneunitactive,othersampleswillhavetheotherunitactive,butveryfewsampleshavebothactive.Unfortunately,explainingawayinteractionscannotbemodeledbythefactorialqusedformeanﬁeld,sothemeanﬁeldapproximationisforcedtochooseonemodetomodel.Thisisaninstanceofthebehaviorillustratedinﬁgure.3.6Wecanrewriteequationintoanequivalentformthatrevealssomefurther19.44insights:ˆhi= σbi+v−ji=W:,jˆhjβW:,i−12W:,iβW:,i.(19.45)Inthisreformulation,weseetheinputateachstepasconsistingofv−ji=W:,jˆhjratherthanv.Wecanthusthinkofunitiasattemptingtoencodetheresidual644 CHAPTER19.APPROXIMATEINFERENCEerrorinvgiventhecodeoftheotherunits.Wecanthusthinkofsparsecodingasaniterativeautoencoder,thatrepeatedlyencodesanddecodesitsinput,attemptingtoﬁxmistakesinthereconstructionaftereachiteration.Inthisexample,wehavederivedanupdaterulethatupdatesasingleunitatatime.Itwouldbeadvantageoustobeabletoupdatemoreunitssimultaneously.Somegraphicalmodels,suchasdeepBoltzmannmachines,arestructuredinsuchawaythatwecansolveformanyentriesofˆhsimultaneously.Unfortunately,binarysparsecodingdoesnotadmitsuchblockupdates.Instead,wecanuseaheuristictechniquecalleddampingtoperformblockupdates.Inthedampingapproach,wesolvefortheindividuallyoptimalvaluesofeveryelementofˆh,thenmoveallofthevaluesinasmallstepinthatdirection.ThisapproachisnolongerguaranteedtoincreaseLateachstep,butworkswellinpracticeformanymodels.SeeKollerandFriedman2009()formoreinformationaboutchoosingthedegreeofsynchronyanddampingstrategiesinmessagepassingalgorithms.19.4.2CalculusofVariationsBeforecontinuingwithourpresentationofvariationallearning,wemustbrieﬂyintroduceanimportantsetofmathematicaltoolsusedinvariationallearning:calculusofvariations.ManymachinelearningtechniquesarebasedonminimizingafunctionJ(θ)byﬁndingtheinputvectorθ∈Rnforwhichittakesonitsminimalvalue.Thiscanbeaccomplishedwithmultivariatecalculusandlinearalgebra,bysolvingforthecriticalpointswhere∇θJ(θ) =0.Insomecases,weactuallywanttosolveforafunctionf(x),suchaswhenwewanttoﬁndtheprobabilitydensityfunctionoversomerandomvariable.Thisiswhatcalculusofvariationsenablesustodo.Afunction ofa functionfisknown asafunctionalJ[f].Muchas wecantakepartialderivativesofafunctionwithrespecttoelementsofitsvector-valuedargument,wecantakefunctionalderivatives,alsoknownasvariationalderivatives,ofafunctionalJ[f]withrespecttoindividualvaluesofthefunctionf(x)atanyspeciﬁcvalueofx.ThefunctionalderivativeofthefunctionalJwithrespecttothevalueofthefunctionatpointisdenotedfxδδfx()J.Acompleteformaldevelopmentoffunctionalderivativesisbeyondthescopeofthisbook.Forourpurposes,itissuﬃcienttostatethatfordiﬀerentiablefunctionsfgy,()xanddiﬀerentiablefunctions(x)withcontinuousderivatives,thatδδf()xgf,d(()xx)x=∂∂ygf,.(()xx)(19.46)645 CHAPTER19.APPROXIMATEINFERENCETogainsomeintuitionforthisidentity,onecanthinkoff(x)asbeingavectorwithuncountablymanyelements,indexedbyarealvectorx.Inthis(somewhatincompleteview),theidentityprovidingthefunctionalderivativesisthesameaswewouldobtainforavectorθ∈Rnindexedbypositiveintegers:∂∂θijgθ(j,j) =∂∂θigθ(i,i.)(19.47)ManyresultsinothermachinelearningpublicationsarepresentedusingthemoregeneralEuler-Lagrangeequationwhichallowsgtodependonthederivativesoffaswellasthevalueoff,butwedonotneedthisfullygeneralformfortheresultspresentedinthisbook.Tooptimizeafunctionwithrespecttoavector,wetakethegradientofthefunctionwithrespecttothevectorandsolveforthepointwhereeveryelementofthegradientisequaltozero.Likewise,wecanoptimizeafunctionalbysolvingforthefunctionwherethefunctionalderivativeateverypointisequaltozero.Asanexampleofhowthisprocessworks,considertheproblemofﬁndingtheprobabilitydistributionfunctionoverx∈Rthathasmaximaldiﬀerentialentropy.Recallthattheentropyofaprobabilitydistributionisdeﬁnedaspx()Hp[] = −Exlog()px.(19.48)Forcontinuousvalues,theexpectationisanintegral:Hp[] = −pxpxdx.()log()(19.49)WecannotsimplymaximizeH[p] withrespecttothefunctionp(x),becausetheresultmightnotbeaprobabilitydistribution.Instead,weneedtouseLagrangemultipliers toadd aconstraint thatp(x)integratesto 1.Also,theentropyincreaseswithoutboundasthevarianceincreases.Thismakesthequestionofwhichdistributionhasthegreatestentropyuninteresting.Instead,weaskwhichdistributionhasmaximalentropyforﬁxedvarianceσ2.Finally,theproblemisunderdeterminedbecausethedistributioncanbeshiftedarbitrarilywithoutchangingtheentropy.Toimposeauniquesolution,weaddaconstraintthatthemeanofthedistributionbeµ. TheLagrangianfunctionalforthisoptimizationproblemisL[] = pλ1pxdx()−1+λ2([])+Ex−µλ3E[()xµ−2]−σ2+[]Hp(19.50)646 CHAPTER19.APPROXIMATEINFERENCE=λ1pxλ()+2pxxλ()+3pxxµ()(−)2−pxpx()log()dxλ−1−µλ2−σ2λ3.(19.51)TominimizetheLagrangianwithrespecttop,wesetthefunctionalderivativesequalto0:∀x,δδpx()L= λ1+λ2xλ+3()xµ−2−−1log() = 0px.(19.52)Thisconditionnowtellsusthefunctionalformofp(x).Byalgebraicallyre-arrangingtheequation,weobtainpx() = expλ1+λ2xλ+3()xµ−2−1.(19.53)Weneverassumeddirectlythatp(x)wouldtakethisfunctionalform;weobtainedtheexpressionitselfbyanalyticallyminimizingafunctional.Toﬁnishtheminimizationproblem,wemustchoosetheλvaluestoensurethatallofourconstraintsaresatisﬁed.Wearefreetochooseanyλvalues,becausethegradientoftheLagrangianwithrespecttotheλvariablesiszerosolongastheconstraintsaresatisﬁed.Tosatisfyalloftheconstraints,wemaysetλ1=1−logσ√2π,λ2= 0,andλ3= −12σ2toobtainpxxµ,σ() = (N;2).(19.54)Thisisonereasonforusingthenormaldistributionwhenwedonotknowthetruedistribution.Becausethenormaldistributionhasthemaximumentropy,weimposetheleastpossibleamountofstructurebymakingthisassumption.WhileexaminingthecriticalpointsoftheLagrangianfunctionalfortheentropy,wefoundonlyonecriticalpoint,correspondingtomaximizingtheentropyforﬁxedvariance.Whatabouttheprobabilitydistributionfunctionthatminimizestheentropy?Whydidwenotﬁndasecondcriticalpointcorrespondingtotheminimum?Thereasonisthatthereisnospeciﬁcfunctionthatachievesminimalentropy.Asfunctionsplacemoreprobabilitydensityonthetwopointsx=µ+σandx=µσ−,andplacelessprobabilitydensityonallothervaluesofx,theyloseentropywhilemaintainingthedesiredvariance.However,anyfunctionplacingexactlyzeromassonallbuttwopointsdoesnotintegratetoone,andisnotavalidprobabilitydistribution.Therethusisnosingleminimalentropyprobabilitydistributionfunction,muchasthereisnosingleminimalpositiverealnumber.Instead,wecansaythatthereisasequenceofprobabilitydistributionsconvergingtowardputtingmassonlyonthesetwopoints.Thisdegeneratescenariomaybe647 CHAPTER19.APPROXIMATEINFERENCEdescribedasamixtureofDiracdistributions.BecauseDiracdistributionsarenotdescribedbyasingleprobabilitydistributionfunction,noDiracormixtureofDiracdistributioncorrespondstoasinglespeciﬁcpointinfunctionspace.Thesedistributionsarethusinvisibletoourmethodofsolvingforaspeciﬁcpointwherethefunctionalderivativesarezero.Thisisalimitationofthemethod.DistributionssuchastheDiracmustbefoundbyothermethods,suchasguessingthesolutionandthenprovingthatitiscorrect.19.4.3ContinuousLatentVariablesWhenourgraphicalmodelcontainscontinuouslatentvariables, wemaystillperformvariationalinferenceandlearningbymaximizingL.However,wemustnowusecalculusofvariationswhenmaximizingwithrespectto.Lq()hv|Inmostcases,practitionersneednotsolveanycalculusofvariationsproblemsthemselves.Instead,thereisageneralequationforthemeanﬁeldﬁxedpointupdates.Ifwemakethemeanﬁeldapproximationq() =hv|iqh(i|v),(19.55)andﬁxq(hj|v)forallj=i,thentheoptimalq(hi|v)maybeobtainedbynormalizingtheunnormalizeddistribution˜qh(i|v) = expEh−i∼q(h−i|v)log ˜p,(vh)(19.56)solongaspdoesnotassignprobabilitytoanyjointconﬁgurationofvariables.0Carryingouttheexpectationinsidetheequationwillyieldthecorrectfunctionalformofq(hi|v).Itisonlynecessarytoderivefunctionalformsofqdirectlyusingcalculusofvariationsifonewishestodevelopanewformofvariationallearning;equationyieldsthemeanﬁeldapproximationforanyprobabilisticmodel.19.56Equationisaﬁxedpointequation,designedtobeiterativelyappliedfor19.56eachvalueofirepeatedlyuntilconvergence.However,italsotellsusmorethanthat.Ittellsusthefunctionalformthattheoptimalsolutionwilltake,whetherwearrivetherebyﬁxedpointequationsornot.Thismeanswecantakethefunctionalformfromthatequationbutregardsomeofthevaluesthatappearinitasparameters,thatwecanoptimizewithanyoptimizationalgorithmwelike.Asanexample,consideraverysimpleprobabilisticmodel,withlatentvariablesh∈R2andjustonevisiblevariable,v.Supposethatp(h)=N(h;0,I)andp(v|h)=N(v;wh;1).Wecouldactuallysimplifythismodelbyintegratingouth;theresultisjustaGaussiandistributionoverv. Themodelitselfisnot648 CHAPTER19.APPROXIMATEINFERENCEinteresting;wehaveconstructeditonlytoprovideasimpledemonstrationofhowcalculusofvariationsmaybeappliedtoprobabilisticmodeling.Thetrueposteriorisgiven,uptoanormalizingconstant,byp()hv|(19.57)∝p,(hv)(19.58)=(ph1)(ph2)()pvh|(19.59)∝exp−12h21+h22+(vh−1w1−h2w2)2(19.60)=exp−12h21+h22+v2+h21w21+h22w22−2vh1w1−2vh2w2+2h1w1h2w2.(19.61)Duetothepresenceofthetermsmultiplyingh1andh2together,wecanseethatthetrueposteriordoesnotfactorizeoverh1andh2.Applyingequation,weﬁndthat19.56˜qh(1|v)(19.62)=expEh2∼q(h2|v)log ˜p,(vh)(19.63)=exp−12Eh2∼q(h2|v)h21+h22+v2+h21w21+h22w22(19.64)−2vh1w1−2vh2w2+2h1w1h2w2].(19.65)Fromthis,wecanseethatthereareeﬀectivelyonlytwovaluesweneedtoobtainfromq(h2|v):Eh2∼|q(hv)[h2]andEh2∼|q(hv)[h22]. Writingtheseash2andh22,weobtain˜qh(1|v) = exp−12h21+h22+v2+h21w21+h22w22(19.66)−2vh1w1−2vh2w2+2h1w1h2w2].(19.67)Fromthis,wecanseethat˜qhasthefunctionalformofaGaussian.Wecanthusconcludeq(hv|)=N(h;µβ,−1)whereµanddiagonalβarevariationalparametersthatwecanoptimizeusinganytechniquewechoose.ItisimportanttorecallthatwedidnoteverassumethatqwouldbeGaussian;itsGaussianformwasderivedautomaticallybyusingcalculusofvariationstomaximizeqwith649 CHAPTER19.APPROXIMATEINFERENCErespecttoL.Usingthesameapproachonadiﬀerentmodelcouldyieldadiﬀerentfunctionalformof.qThiswasofcourse,justasmallcaseconstructedfordemonstrationpurposes.Forexamplesofrealapplicationsofvariationallearningwithcontinuousvariablesinthecontextofdeeplearning,see().Goodfellowetal.2013d19.4.4InteractionsbetweenLearningandInferenceUsingapproximateinferenceaspartofalearningalgorithmaﬀectsthelearningprocess,andthisinturnaﬀectstheaccuracyoftheinferencealgorithm.Speciﬁcally,thetrainingalgorithmtendstoadaptthemodelinawaythatmakestheapproximatingassumptionsunderlyingtheapproximateinferencealgorithmbecomemoretrue.Whentrainingtheparameters,variationallearningincreasesEh∼qlog()pvh,.(19.68)Foraspeciﬁcv,thisincreasesp(hv|)forvaluesofhthathavehighprobabilityunderq(hv|)anddecreasesp(hv|)forvaluesofhthathavelowprobabilityunder.q()hv|Thisbehaviorcausesourapproximatingassumptionstobecomeself-fulﬁllingprophecies.Ifwetrainthemodelwithaunimodalapproximateposterior,wewillobtainamodelwithatrueposteriorthatisfarclosertounimodalthanwewouldhaveobtainedbytrainingthemodelwithexactinference.Computingthetrueamountofharmimposedonamodelbyavariationalapproximationisthusverydiﬃcult.Thereexistseveralmethodsforestimatinglogp(v).Weoftenestimatelogp(v;θ)aftertrainingthemodel,andﬁndthatthegapwithL(vθ,,q)issmall.Fromthis,wecanconcludethatourvariationalapproximationisaccurateforthespeciﬁcvalueofθthatweobtainedfromthelearningprocess.Weshouldnotconcludethatourvariationalapproximationisaccurateingeneralorthatthevariationalapproximationdidlittleharmtothelearningprocess.Tomeasurethetrueamountofharminducedbythevariationalapproximation,wewouldneedtoknowθ∗=maxθlogp(v;θ). ItispossibleforL(vθ,,q)≈logp(v;θ)andlogp(v;θ)logp(v;θ∗)toholdsimultaneously.IfmaxqL(vθ,∗,q)logp(v;θ∗),becauseθ∗inducestoocomplicatedofaposteriordistributionforourqfamilytocapture, thenthelearningprocesswillneverapproachθ∗.Suchaproblemisverydiﬃculttodetect,becausewecanonlyknowforsurethatithappenedifwehaveasuperiorlearningalgorithmthatcanﬁndθ∗forcomparison.650 CHAPTER19.APPROXIMATEINFERENCE19.5LearnedApproximateInferenceWehaveseenthatinferencecanbethoughtofasanoptimizationprocedurethatincreasesthevalueofafunctionL.Explicitlyperformingoptimizationviaiterativeproceduressuchasﬁxedpointequationsorgradient-basedoptimizationisoftenveryexpensiveandtime-consuming.Manyapproachestoinferenceavoidthisexpensebylearningtoperformapproximateinference. Speciﬁcally,wecanthinkoftheoptimizationprocessasafunctionfthatmapsaninputvtoanapproximatedistributionq∗=argmaxqL(v,q).Oncewethinkofthemulti-stepiterativeoptimizationprocessasjustbeingafunction,wecanapproximateitwithaneuralnetworkthatimplementsanapproximationˆf(;)vθ.19.5.1Wake-SleepOneofthemaindiﬃcultieswithtrainingamodeltoinferhfromvisthatwedonothaveasupervisedtrainingsetwithwhichtotrainthemodel.Givenav,wedonotknowtheappropriateh.Themappingfromvtohdependsonthechoiceofmodelfamily,andevolvesthroughoutthelearningprocessasθchanges.Thewake-sleepalgorithm(Hinton1995bFrey1996etal.,;etal.,)resolvesthisproblembydrawingsamplesofbothhandvfromthemodeldistribution. Forexample,inadirectedmodel,thiscanbedonecheaplybyperformingancestralsamplingbeginningathandendingatv.Theinferencenetworkcanthenbetrainedtoperformthereversemapping: predictingwhichhcausedthepresentv.Themaindrawbacktothisapproachisthatwewillonlybeabletotraintheinferencenetworkonvaluesofvthathavehighprobabilityunderthemodel.Earlyinlearning,themodeldistributionwillnotresemblethedatadistribution,sotheinferencenetworkwillnothaveanopportunitytolearnonsamplesthatresembledata.Insectionwesawthatonepossibleexplanationfortheroleofdreamsleep18.2inhumanbeingsandanimalsisthatdreamscouldprovidethenegativephasesamplesthatMonteCarlotrainingalgorithmsusetoapproximatethenegativegradientofthelogpartitionfunctionofundirectedmodels.Anotherpossibleexplanationforbiologicaldreamingisthatitisprovidingsamplesfromp(hv,)whichcanbeusedtotrainaninferencenetworktopredicthgivenv.Insomesenses,thisexplanationismoresatisfyingthanthepartitionfunctionexplanation.MonteCarloalgorithmsgenerallydonotperformwelliftheyarerunusingonlythepositivephaseofthegradientforseveralstepsthenwithonlythenegativephaseofthegradientforseveralsteps.Humanbeingsandanimalsareusuallyawakeforseveralconsecutivehoursthenasleepforseveralconsecutivehours.Itis651 CHAPTER19.APPROXIMATEINFERENCEnotreadilyapparenthowthisschedulecouldsupportMonteCarlotrainingofanundirectedmodel.LearningalgorithmsbasedonmaximizingLcanberunwithprolongedperiodsofimprovingqandprolongedperiodsofimprovingθ,however.Iftheroleofbiologicaldreamingistotrainnetworksforpredictingq,thenthisexplainshowanimalsareabletoremainawakeforseveralhours(thelongertheyareawake,thegreaterthegapbetweenLandlogp(v),butLwillremainalowerbound)andtoremainasleepforseveralhours(thegenerativemodelitselfisnotmodiﬁedduringsleep)withoutdamagingtheirinternalmodels.Ofcourse,theseideasarepurelyspeculative,andthereisnohardevidencetosuggestthatdreamingaccomplisheseitherofthesegoals.Dreamingmayalsoservereinforcementlearningratherthanprobabilisticmodeling,bysamplingsyntheticexperiencesfromtheanimal’stransitionmodel,onwhichtotraintheanimal’spolicy.Orsleepmayservesomeotherpurposenotyetanticipatedbythemachinelearningcommunity.19.5.2OtherFormsofLearnedInferenceThisstrategyoflearnedapproximateinferencehasalsobeenappliedtoothermodels.SalakhutdinovandLarochelle2010()showedthatasinglepassinalearnedinferencenetworkcouldyieldfasterinferencethaniteratingthemeanﬁeldﬁxedpointequationsinaDBM.Thetrainingprocedureisbasedonrunningtheinferencenetwork,thenapplyingonestepofmeanﬁeldtoimproveitsestimates,andtrainingtheinferencenetworktooutputthisreﬁnedestimateinsteadofitsoriginalestimate.Wehavealreadyseeninsectionthatthepredictivesparsedecomposition14.8modeltrainsashallowencodernetworktopredictasparsecodefortheinput.Thiscanbeseenasahybridbetweenanautoencoderandsparsecoding.Itispossibletodeviseprobabilisticsemanticsforthemodel,underwhichtheencodermaybeviewedasperforminglearnedapproximateMAPinference.Duetoitsshallowencoder,PSDisnotabletoimplementthekindofcompetitionbetweenunitsthatwehaveseeninmeanﬁeldinference.However,thatproblemcanberemediedbytrainingadeepencodertoperformlearnedapproximateinference,asintheISTAtechnique(,).GregorandLeCun2010bLearned approximate inference hasrecently become one of the dominantapproachestogenerativemodeling,intheformofthevariationalautoencoder(,;,).Inthiselegantapproach,thereisnoneedtoKingma2013Rezendeetal.2014constructexplicittargetsfortheinferencenetwork.Instead,theinferencenetworkissimplyusedtodeﬁneL,andthentheparametersoftheinferencenetworkareadaptedtoincrease.Thismodelisdescribedindepthlater,insection.L20.10.3652 CHAPTER19.APPROXIMATEINFERENCEUsingapproximateinference,itispossibletotrainanduseawidevarietyofmodels.Manyofthesemodelsaredescribedinthenextchapter. 653 Chapter20DeepGenerativeModelsInthischapter,wepresentseveralofthespeciﬁckindsofgenerativemodelsthatcanbebuiltandtrainedusingthetechniquespresentedinchapters–.Allof1619thesemodelsrepresentprobabilitydistributionsovermultiplevariablesinsomeway.Someallowtheprobabilitydistributionfunctiontobeevaluatedexplicitly.Othersdonotallowtheevaluationoftheprobabilitydistributionfunction,butsupportoperationsthatimplicitlyrequireknowledgeofit,suchasdrawingsamplesfromthedistribution.Someofthesemodelsarestructuredprobabilisticmodelsdescribedintermsofgraphsandfactors,usingthelanguageofgraphicalmodelspresentedinchapter. Otherscannoteasilybedescribedintermsoffactors,16butrepresentprobabilitydistributionsnonetheless.20.1BoltzmannMachinesBoltzmannmachineswereoriginallyintroducedasageneral“connectionist”ap-proachtolearningarbitraryprobabilitydistributionsoverbinaryvectors(Fahlmanetal.,;1983Ackley1985Hinton1984HintonandSejnowski1986etal.,;etal.,;,).VariantsoftheBoltzmannmachinethatincludeotherkindsofvariableshavelongagosurpassedthepopularityoftheoriginal.InthissectionwebrieﬂyintroducethebinaryBoltzmannmachineanddiscusstheissuesthatcomeupwhentryingtotrainandperforminferenceinthemodel.WedeﬁnetheBoltzmannmachineoverad-dimensionalbinaryrandomvectorx ∈{0,1}d. TheBoltzmannmachineisanenergy-basedmodel(section),16.2.4654 CHAPTER20.DEEPGENERATIVEMODELSmeaningwedeﬁnethejointprobabilitydistributionusinganenergyfunction:P() =xexp(())−ExZ,(20.1)whereE(x)istheenergyfunctionandZisthepartitionfunctionthatensuresthatxP() = 1x.TheenergyfunctionoftheBoltzmannmachineisgivenbyE() = x−xUxb−x,(20.2)whereUisthe“weight”matrixofmodelparametersandbisthevectorofbiasparameters.InthegeneralsettingoftheBoltzmannmachine,wearegivenasetoftrainingexamples,eachofwhicharen-dimensional.Equationdescribesthejoint20.1probabilitydistributionovertheobservedvariables.Whilethisscenarioiscertainlyviable,itdoeslimitthekindsofinteractionsbetweentheobservedvariablestothosedescribedbytheweightmatrix.Speciﬁcally,itmeansthattheprobabilityofoneunitbeingonisgivenbyalinearmodel(logisticregression)fromthevaluesoftheotherunits.TheBoltzmannmachinebecomesmorepowerfulwhennotallthevariablesareobserved.Inthiscase,thelatentvariables,canactsimilarlytohiddenunitsinamulti-layerperceptronandmodelhigher-orderinteractionsamongthevisibleunits.JustastheadditionofhiddenunitstoconvertlogisticregressionintoanMLPresultsintheMLPbeingauniversalapproximatoroffunctions,aBoltzmannmachinewithhiddenunitsisnolongerlimitedtomodelinglinearrelationshipsbetweenvariables.Instead,theBoltzmannmachinebecomesauniversalapproximatorofprobabilitymassfunctionsoverdiscretevariables(,).LeRouxandBengio2008Formally,wedecomposetheunitsxintotwosubsets:thevisibleunitsvandthelatent(orhidden)units.TheenergyfunctionbecomeshE,(vhv) = −Rvv−Whh−Shb−vc−h.(20.3)BoltzmannMachineLearningLearningalgorithmsforBoltzmannmachinesareusuallybasedonmaximumlikelihood.AllBoltzmannmachineshaveanintractablepartitionfunction,sothemaximumlikelihoodgradientmustbeap-proximatedusingthetechniquesdescribedinchapter.18OneinterestingpropertyofBoltzmannmachineswhentrainedwithlearningrulesbasedonmaximumlikelihoodisthattheupdateforaparticularweightconnectingtwounitsdependsonlythestatisticsofthosetwounits,collectedunderdiﬀerentdistributions:Pmodel(v)andˆPdata(v)Pmodel(hv|).Therestofthe655 CHAPTER20.DEEPGENERATIVEMODELSnetworkparticipatesinshapingthosestatistics,buttheweightcanbeupdatedwithoutknowinganythingabouttherestofthenetworkorhowthosestatisticswereproduced.Thismeansthatthelearningruleis“local,”whichmakesBoltzmannmachinelearningsomewhatbiologicallyplausible. ItisconceivablethatifeachneuronwerearandomvariableinaBoltzmannmachine,thentheaxonsanddendritesconnectingtworandomvariablescouldlearnonlybyobservingtheﬁringpatternofthecellsthattheyactuallyphysicallytouch.Inparticular,inthepositivephase,twounitsthatfrequentlyactivatetogetherhavetheirconnectionstrengthened.ThisisanexampleofaHebbianlearningrule(,)oftenHebb1949summarizedwiththemnemonic“ﬁretogether,wiretogether.” Hebbianlearningrulesareamongtheoldesthypothesizedexplanationsforlearninginbiologicalsystemsandremainrelevanttoday(,).Giudiceetal.2009Otherlearningalgorithmsthatusemoreinformationthanlocalstatisticsseemtorequireustohypothesizetheexistenceofmoremachinerythanthis.Forexample,forthebraintoimplementback-propagationinamultilayerperceptron,itseemsnecessaryforthebraintomaintainasecondarycommunicationnetworkfortransmittinggradientinformationbackwardsthroughthenetwork.Proposalsforbiologicallyplausibleimplementations(andapproximations)ofback-propagationhavebeenmade(,;,)butremaintobevalidated,andHinton2007aBengio2015Bengio2015()linksback-propagationofgradientstoinferenceinenergy-basedmodelssimilartotheBoltzmannmachine(butwithcontinuouslatentvariables).ThenegativephaseofBoltzmannmachinelearningissomewhathardertoexplainfromabiologicalpointofview.Asarguedinsection,dreamsleep18.2maybeaformofnegativephasesampling.Thisideaismorespeculativethough.20.2RestrictedBoltzmannMachinesInventedunderthenameharmonium(,),restrictedBoltzmannSmolensky1986machinesaresomeofthemostcommonbuildingblocksofdeepprobabilisticmodels.WehavebrieﬂydescribedRBMspreviously,insection.Herewereviewthe16.7.1previousinformationandgointomoredetail.RBMsareundirectedprobabilisticgraphicalmodelscontainingalayerofobservablevariablesandasinglelayeroflatentvariables.RBMsmaybestacked(oneontopoftheother)toformdeepermodels.Seeﬁgureforsomeexamples.Inparticular,ﬁgureashowsthe20.120.1graphstructureoftheRBMitself.Itisabipartitegraph,withnoconnectionspermittedbetweenanyvariablesintheobservedlayerorbetweenanyunitsinthelatentlayer.656 CHAPTER20.DEEPGENERATIVEMODELS h1h1h2h2h3h3v1v1v2v2v3v3h4h4h(1)1h(1)1h(1)2h(1)2h(1)3h(1)3v1v1v2v2v3v3h(2)1h(2)1h(2)2h(2)2h(2)3h(2)3h(1)4h(1)4(a)(b)h(1)1h(1)1h(1)2h(1)2h(1)3h(1)3v1v1v2v2v3v3h(2)1h(2)1h(2)2h(2)2h(2)3h(2)3h(1)4h(1)4(c)Figure20.1:ExamplesofmodelsthatmaybebuiltwithrestrictedBoltzmannmachines.(a)TherestrictedBoltzmannmachineitselfisanundirectedgraphicalmodelbasedonabipartitegraph,withvisibleunitsinonepartofthegraphandhiddenunitsintheotherpart.Therearenoconnectionsamongthevisibleunits,noranyconnectionsamongthehiddenunits. TypicallyeveryvisibleunitisconnectedtoeveryhiddenunitbutitispossibletoconstructsparselyconnectedRBMssuchasconvolutionalRBMs.A(b)deepbeliefnetworkisahybridgraphicalmodelinvolvingbothdirectedandundirectedconnections.LikeanRBM,ithasnointralayerconnections.However,aDBNhasmultiplehiddenlayers,andthusthereareconnectionsbetweenhiddenunitsthatareinseparatelayers.AllofthelocalconditionalprobabilitydistributionsneededbythedeepbeliefnetworkarecopieddirectlyfromthelocalconditionalprobabilitydistributionsofitsconstituentRBMs.Alternatively,wecouldalsorepresentthedeepbeliefnetworkwithacompletelyundirectedgraph,butitwouldneedintralayerconnectionstocapturethedependenciesbetweenparents.AdeepBoltzmannmachineisanundirectedgraphical(c)modelwithseverallayersoflatentvariables.LikeRBMsandDBNs,DBMslackintralayerconnections. DBMsarelesscloselytiedtoRBMsthanDBNsare. WheninitializingaDBMfromastackofRBMs,itisnecessarytomodifytheRBMparametersslightly.SomekindsofDBMsmaybetrainedwithoutﬁrsttrainingasetofRBMs.657 CHAPTER20.DEEPGENERATIVEMODELSWebeginwiththebinaryversionoftherestrictedBoltzmannmachine,butasweseelaterthereareextensionstoothertypesofvisibleandhiddenunits.Moreformally,lettheobservedlayerconsistofasetofnvbinaryrandomvariableswhichwerefertocollectivelywiththevectorv.Werefertothelatentorhiddenlayerofnhbinaryrandomvariablesas.hLikethegeneralBoltzmannmachine,therestrictedBoltzmannmachineisanenergy-basedmodelwiththejointprobabilitydistributionspeciﬁedbyitsenergyfunction:P,(= vvh= ) =h1Zexp(())−Evh,.(20.4)TheenergyfunctionforanRBMisgivenbyE,(vhb) = −vc−hv−Wh,(20.5)andisthenormalizingconstantknownasthepartitionfunction:ZZ=vhexp(){−Evh,}.(20.6)ItisapparentfromthedeﬁnitionofthepartitionfunctionZthatthenaivemethodofcomputingZ(exhaustivelysummingoverallstates)couldbecomputationallyintractable,unlessacleverlydesignedalgorithmcouldexploitregularitiesintheprobabilitydistributiontocomputeZfaster.InthecaseofrestrictedBoltzmannmachines,()formallyprovedthatthepartitionfunctionLongandServedio2010Zisintractable.TheintractablepartitionfunctionZimpliesthatthenormalizedjointprobabilitydistributionisalsointractabletoevaluate.P()v20.2.1ConditionalDistributionsThoughP(v)isintractable,thebipartitegraphstructureoftheRBMhastheveryspecialpropertythatitsconditionaldistributionsP(hv|)andP(vh|)arefactorialandrelativelysimpletocomputeandtosamplefrom.Derivingtheconditionaldistributionsfromthejointdistributionisstraightfor-ward:P() =hv|P,(hv)P()v(20.7)=1P()v1Zexpbvc+hv+Wh(20.8)=1Zexpchv+Wh(20.9)658 CHAPTER20.DEEPGENERATIVEMODELS=1Zexpnhj=1cjhj+nhj=1vW:,jhj(20.10)=1Znhj=1expcjhj+vW:,jhj(20.11)Sinceweareconditioningonthevisibleunitsv,wecantreattheseasconstantwithrespecttothedistributionP(hv|).ThefactorialnatureoftheconditionalP(hv|)followsimmediatelyfromourabilitytowritethejointprobabilityoverthevectorhastheproductof(unnormalized)distributionsovertheindividualelements,hj.Itisnowasimplematterofnormalizingthedistributionsovertheindividualbinaryhj.Ph(j= 1 ) =|v˜Ph(j= 1 )|v˜Ph(j= 0 )+|v˜Ph(j= 1 )|v(20.12)=expcj+vW:,jexp0+exp{}{cj+vW:,j}(20.13)= σcj+vW:,j.(20.14)Wecannowexpressthefullconditionaloverthehiddenlayerasthefactorialdistribution:P() =hv|nhj=1σ(21)(+h−cWv)j.(20.15)Asimilarderivationwillshowthattheotherconditionofinteresttous,P(vh|),isalsoafactorialdistribution:P() =vh|nvi=1σ((21)(+))v−bWhi.(20.16)20.2.2TrainingRestrictedBoltzmannMachinesBecausetheRBMadmitseﬃcientevaluationanddiﬀerentiationof˜P(v)andeﬃcientMCMCsamplingintheformofblockGibbssampling,itcanreadilybetrainedwithanyofthetechniquesdescribedinchapterfortrainingmodels18thathaveintractablepartitionfunctions. ThisincludesCD,SML(PCD),ratiomatchingandsoon.Comparedtootherundirectedmodelsusedindeeplearning,theRBMisrelativelystraightforwardtotrainbecausewecancomputeP(h|v)659 CHAPTER20.DEEPGENERATIVEMODELSexactlyinclosedform.Someotherdeepmodels,suchasthedeepBoltzmannmachine,combineboththediﬃcultyofanintractablepartitionfunctionandthediﬃcultyofintractableinference.20.3DeepBeliefNetworksDeepbeliefnetworks(DBNs)wereoneoftheﬁrstnon-convolutionalmodelstosuccessfullyadmittrainingofdeeparchitectures(Hinton2006Hintonetal.,;,2007b).Theintroductionofdeepbeliefnetworksin2006beganthecurrentdeeplearningrenaissance.Priortotheintroductionofdeepbeliefnetworks,deepmodelswereconsideredtoodiﬃculttooptimize.Kernelmachineswithconvexobjectivefunctionsdominatedtheresearchlandscape.Deepbeliefnetworksdemonstratedthatdeeparchitecturescanbesuccessful,byoutperformingkernelizedsupportvectormachinesontheMNISTdataset(,).Today,deepbeliefHintonetal.2006networkshavemostlyfallenoutoffavorandarerarelyused,evencomparedtootherunsupervisedorgenerativelearningalgorithms,buttheyarestilldeservedlyrecognizedfortheirimportantroleindeeplearninghistory.Deepbeliefnetworksaregenerativemodelswithseverallayersoflatentvariables.Thelatentvariablesaretypicallybinary,whilethevisibleunitsmaybebinaryorreal.Therearenointralayerconnections.Usually,everyunitineachlayerisconnectedtoeveryunitineachneighboringlayer,thoughitispossibletoconstructmoresparselyconnectedDBNs.Theconnectionsbetweenthetoptwolayersareundirected.Theconnectionsbetweenallotherlayersaredirected,withthearrowspointedtowardthelayerthatisclosesttothedata.Seeﬁgurebforanexample.20.1ADBNwithlhiddenlayerscontainslweightmatrices:W(1),...,W()l.Italsocontainsl+1biasvectors:b(0),...,b()l,withb(0)providingthebiasesforthevisiblelayer.TheprobabilitydistributionrepresentedbytheDBNisgivenbyP(h()l,h(1)l−) exp∝b()lh()l+b(1)l−h(1)l−+h(1)l−W()lh()l,(20.17)Ph(()ki= 1 |h(+1)k) = σb()ki+W(+1)k:,ih(+1)k∀∀∈−i,k1,...,l2,(20.18)Pv(i= 1 |h(1)) = σb(0)i+W(1):,ih(1)∀i.(20.19)Inthecaseofreal-valuedvisibleunits,substitutev∼Nvb;(0)+W(1)h(1),β−1(20.20)660 CHAPTER20.DEEPGENERATIVEMODELSwithβdiagonalfortractability.Generalizationstootherexponentialfamilyvisibleunitsarestraightforward,atleastintheory.ADBNwithonlyonehiddenlayerisjustanRBM.TogenerateasamplefromaDBN,weﬁrstrunseveralstepsofGibbssamplingonthetoptwohiddenlayers.ThisstageisessentiallydrawingasamplefromtheRBMdeﬁnedbythetoptwohiddenlayers.Wecanthenuseasinglepassofancestralsamplingthroughtherestofthemodeltodrawasamplefromthevisibleunits.Deepbeliefnetworksincurmanyoftheproblemsassociatedwithbothdirectedmodelsandundirectedmodels.Inferenceinadeepbeliefnetworkisintractableduetotheexplainingawayeﬀectwithineachdirectedlayer,andduetotheinteractionbetweenthetwohiddenlayersthathaveundirectedconnections.Evaluatingormaximizingthestandardevidencelowerboundonthelog-likelihoodisalsointractable,becausetheevidencelowerboundtakestheexpectationofcliqueswhosesizeisequaltothenetworkwidth.Evaluatingormaximizingthelog-likelihoodrequiresnotjustconfrontingtheproblemofintractableinferencetomarginalizeoutthelatentvariables,butalsotheproblemofanintractablepartitionfunctionwithintheundirectedmodelofthetoptwolayers.Totrainadeepbeliefnetwork,onebeginsbytraininganRBMtomaximizeEv∼pdatalogp(v)usingcontrastivedivergenceorstochasticmaximumlikelihood.TheparametersoftheRBMthendeﬁnetheparametersoftheﬁrstlayeroftheDBN.Next,asecondRBMistrainedtoapproximatelymaximizeEv∼pdataEh(1)∼p(1)(h(1)|v)logp(2)(h(1))(20.21)wherep(1)istheprobabilitydistributionrepresentedbytheﬁrstRBMandp(2)istheprobabilitydistributionrepresentedbythesecondRBM.Inotherwords,thesecondRBMistrainedtomodelthedistributiondeﬁnedbysamplingthehiddenunitsoftheﬁrstRBM,whentheﬁrstRBMisdrivenbythedata. Thisprocedurecanberepeatedindeﬁnitely,toaddasmanylayerstotheDBNasdesired,witheachnewRBMmodelingthesamplesofthepreviousone.EachRBMdeﬁnesanotherlayeroftheDBN.Thisprocedurecanbejustiﬁedasincreasingavariationallowerboundonthelog-likelihoodofthedataundertheDBN(Hintonetal.,).2006Inmostapplications,noeﬀortismadetojointlytraintheDBNafterthegreedylayer-wiseprocedureiscomplete.However,itispossibletoperformgenerativeﬁne-tuningusingthewake-sleepalgorithm.661 CHAPTER20.DEEPGENERATIVEMODELSThetrainedDBNmaybeuseddirectlyasagenerativemodel,butmostoftheinterestinDBNsarosefromtheirabilitytoimproveclassiﬁcationmodels.WecantaketheweightsfromtheDBNandusethemtodeﬁneanMLP:h(1)= σb(1)+vW(1).(20.22)h()l= σb()li+h(1)l−W()l∀∈l2,...,m,(20.23)AfterinitializingthisMLPwiththeweightsandbiaseslearnedviagenerativetrainingoftheDBN,wemaytraintheMLPtoperformaclassiﬁcationtask.ThisadditionaltrainingoftheMLPisanexampleofdiscriminativeﬁne-tuning.ThisspeciﬁcchoiceofMLPissomewhatarbitrary,comparedtomanyoftheinferenceequationsinchapterthatarederivedfromﬁrstprinciples.ThisMLP19isaheuristicchoicethatseemstoworkwellinpracticeandisusedconsistentlyintheliterature.Manyapproximateinferencetechniquesaremotivatedbytheirabilitytoﬁndamaximallyvariationallowerboundonthelog-likelihoodtightundersomesetofconstraints.Onecanconstructavariationallowerboundonthelog-likelihoodusingthehiddenunitexpectationsdeﬁnedbytheDBN’sMLP,butthisistrueofprobabilitydistributionoverthehiddenunits,andthereisnoanyreasontobelievethatthisMLPprovidesaparticularlytightbound. Inparticular,theMLPignoresmanyimportantinteractionsintheDBNgraphicalmodel.TheMLPpropagatesinformationupwardfromthevisibleunitstothedeepesthiddenunits,butdoesnotpropagateanyinformationdownwardorsideways.TheDBNgraphicalmodelhasexplainingawayinteractionsbetweenallofthehiddenunitswithinthesamelayeraswellastop-downinteractionsbetweenlayers.Whilethelog-likelihoodofaDBNisintractable,itmaybeapproximatedwithAIS(SalakhutdinovandMurray2008,).Thispermitsevaluatingitsqualityasagenerativemodel.Theterm“deepbeliefnetwork”iscommonlyusedincorrectlytorefertoanykindofdeepneuralnetwork,evennetworkswithoutlatentvariablesemantics.Theterm“deepbeliefnetwork”shouldreferspeciﬁcallytomodelswithundirectedconnectionsinthedeepestlayeranddirectedconnectionspointingdownwardbetweenallotherpairsofconsecutivelayers.Theterm“deepbeliefnetwork”mayalsocausesomeconfusionbecausetheterm“beliefnetwork”issometimesusedtorefertopurelydirectedmodels,whiledeepbeliefnetworkscontainanundirectedlayer.DeepbeliefnetworksalsosharetheacronymDBNwithdynamicBayesiannetworks(DeanandKanazawa1989,),whichareBayesiannetworksforrepresentingMarkovchains.662 CHAPTER20.DEEPGENERATIVEMODELS h(1)1h(1)1h(1)2h(1)2h(1)3h(1)3v1v1v2v2v3v3h(2)1h(2)1h(2)2h(2)2h(2)3h(2)3h(1)4h(1)4Figure20.2:ThegraphicalmodelforadeepBoltzmannmachinewithonevisiblelayer(bottom)andtwohiddenlayers.Connectionsareonlybetweenunitsinneighboringlayers.Therearenointralayerlayerconnections.20.4DeepBoltzmannMachinesAdeepBoltzmannmachineorDBM(SalakhutdinovandHinton2009a,)isanotherkindofdeep,generativemodel. Unlikethedeepbeliefnetwork(DBN),itisanentirelyundirectedmodel.UnliketheRBM,theDBMhasseverallayersoflatentvariables(RBMshavejustone). ButliketheRBM,withineachlayer,eachofthevariablesaremutuallyindependent,conditionedonthevariablesintheneighboringlayers.Seeﬁgureforthegraphstructure.DeepBoltzmann20.2machineshavebeenappliedtoavarietyoftasksincludingdocumentmodeling(Srivastava2013etal.,).LikeRBMsandDBNs, DBMstypicallycontainonlybinaryunits—asweassumeforsimplicityofourpresentationofthemodel—butitisstraightforwardtoincludereal-valuedvisibleunits.ADBMisanenergy-basedmodel,meaningthatthethejointprobabilitydistributionoverthemodelvariablesisparametrizedbyanenergyfunctionE.InthecaseofadeepBoltzmannmachinewithonevisiblelayer,v,andthreehiddenlayers,h(1),h(2)andh(3),thejointprobabilityisgivenby:Pvh,(1),h(2),h(3)=1Z()θexp−E,(vh(1),h(2),h(3);)θ.(20.24)Tosimplifyourpresentation,weomitthebiasparametersbelow.TheDBMenergyfunctionisthendeﬁnedasfollows:E,(vh(1),h(2),h(3);) = θ−vW(1)h(1)−h(1)W(2)h(2)−h(2)W(3)h(3).(20.25)663 CHAPTER20.DEEPGENERATIVEMODELS h(1)1h(1)1h(1)2h(1)2h(1)3h(1)3v1v1v2v2h(2)1h(2)1h(2)2h(2)2h(2)3h(2)3h(3)1h(3)1h(3)2h(3)2 v1v2h(2)1h(2)1h(2)2h(2)2h(2)3h(2)3h(1)1h(1)1h(1)2h(1)2h(1)3h(1)3h(3)1h(3)1h(3)2h(3)2 Figure20.3:AdeepBoltzmannmachine,re-arrangedtorevealitsbipartitegraphstructure.IncomparisontotheRBMenergyfunction(equation),theDBMenergy20.5functionincludesconnectionsbetweenthehiddenunits(latentvariables)intheformoftheweightmatrices(W(2)andW(3)).Aswewillsee,theseconnectionshavesigniﬁcantconsequencesforboththemodelbehavioraswellashowwegoaboutperforminginferenceinthemodel.IncomparisontofullyconnectedBoltzmannmachines(witheveryunitcon-nectedtoeveryotherunit),theDBMoﬀerssomeadvantagesthataresimilartothoseoﬀeredbytheRBM.Speciﬁcally,asillustratedinﬁgure,theDBM20.3layerscanbeorganizedintoabipartitegraph,withoddlayersononesideandevenlayersontheother.Thisimmediatelyimpliesthatwhenweconditiononthevariablesintheevenlayer,thevariablesintheoddlayersbecomeconditionallyindependent.Ofcourse,whenweconditiononthevariablesintheoddlayers,thevariablesintheevenlayersalsobecomeconditionallyindependent.ThebipartitestructureoftheDBMmeansthatwecanapplythesameequa-tionswehavepreviouslyusedfortheconditionaldistributionsofanRBMtodeterminetheconditionaldistributionsinaDBM.Theunitswithinalayerareconditionallyindependentfromeachothergiventhevaluesoftheneighboringlayers,sothedistributionsoverbinaryvariablescanbefullydescribedbytheBernoulliparametersgivingtheprobabilityofeachunitbeingactive.Inourexamplewithtwohiddenlayers,theactivationprobabilitiesaregivenby:Pv(i= 1 |h(1)) = σW(1)i,:h(1),(20.26)664 CHAPTER20.DEEPGENERATIVEMODELSPh((1)i= 1 |vh,(2)) = σvW(1):,i+W(2)i,:h(2)(20.27)andPh((2)k= 1 |h(1)) = σh(1)W(2):,k.(20.28)ThebipartitestructuremakesGibbssamplinginadeepBoltzmannmachineeﬃcient. ThenaiveapproachtoGibbssamplingistoupdateonlyonevariableatatime.RBMsallowallofthevisibleunitstobeupdatedinoneblockandallofthehiddenunitstobeupdatedinasecondblock.OnemightnaivelyassumethataDBMwithllayersrequiresl+1updates,witheachiterationupdatingablockconsistingofonelayerofunits.Instead,itispossibletoupdatealloftheunitsinonlytwoiterations.Gibbssamplingcanbedividedintotwoblocksofupdates,oneincludingallevenlayers(includingthevisiblelayer)andtheotherincludingalloddlayers.DuetothebipartiteDBMconnectionpattern,giventheevenlayers,thedistributionovertheoddlayersisfactorialandthuscanbesampledsimultaneouslyandindependentlyasablock.Likewise,giventheoddlayers,theevenlayerscanbesampledsimultaneouslyandindependentlyasablock.Eﬃcientsamplingisespeciallyimportantfortrainingwiththestochasticmaximumlikelihoodalgorithm.20.4.1InterestingPropertiesDeepBoltzmannmachineshavemanyinterestingproperties.DBMsweredevelopedafterDBNs.ComparedtoDBNs,theposteriordistribu-tionP(hv|)issimplerforDBMs.Somewhatcounterintuitively,thesimplicityofthisposteriordistributionallowsricherapproximationsoftheposterior.InthecaseoftheDBN,weperformclassiﬁcationusingaheuristicallymotivatedapproximateinferenceprocedure,inwhichweguessthatareasonablevalueforthemeanﬁeldexpectationofthehiddenunitscanbeprovidedbyanupwardpassthroughthenetworkinanMLPthatusessigmoidactivationfunctionsandthesameweightsastheoriginalDBN.distributionAnyQ(h)maybeusedtoobtainavariationallowerboundonthelog-likelihood.Thisheuristicprocedurethereforeallowsustoobtainsuchabound.However,theboundisnotexplicitlyoptimizedinanyway,sotheboundmaybefarfromtight.Inparticular,theheuristicestimateofQignoresinteractionsbetweenhiddenunitswithinthesamelayeraswellasthetop-downfeedbackinﬂuenceofhiddenunitsindeeperlayersonhiddenunitsthatareclosertotheinput.BecausetheheuristicMLP-basedinferenceprocedureintheDBNisnotabletoaccountfortheseinteractions,theresultingQispresumablyfar665 CHAPTER20.DEEPGENERATIVEMODELSfromoptimal.InDBMs,allofthehiddenunitswithinalayerareconditionallyindependentgiventheotherlayers. Thislackofintralayerinteractionmakesitpossibletouseﬁxedpointequationstoactuallyoptimizethevariationallowerboundandﬁndthetrueoptimalmeanﬁeldexpectations(towithinsomenumericaltolerance).TheuseofpropermeanﬁeldallowstheapproximateinferenceprocedureforDBMstocapturetheinﬂuenceoftop-downfeedbackinteractions.ThismakesDBMsinterestingfromthepointofviewofneuroscience,becausethehumanbrainisknowntousemanytop-downfeedbackconnections.Becauseofthisproperty,DBMshavebeenusedascomputationalmodelsofrealneuroscientiﬁcphenomena(,;Seriesetal.2010Reichert2011etal.,).OneunfortunatepropertyofDBMsisthatsamplingfromthemisrelativelydiﬃcult.DBNsonlyneedtouseMCMCsamplingintheirtoppairoflayers.Theotherlayersareusedonlyattheendofthesamplingprocess,inoneeﬃcientancestralsamplingpass.TogenerateasamplefromaDBM,itisnecessarytouseMCMCacrossalllayers,witheverylayerofthemodelparticipatingineveryMarkovchaintransition.20.4.2DBMMeanFieldInferenceTheconditionaldistributionoveroneDBMlayergiventheneighboringlayersisfactorial.IntheexampleoftheDBMwithtwohiddenlayers,thesedistributionsareP(vh|(1)),P(h(1)|vh,(2))andP(h(2)|h(1)).Thedistributionoverallhiddenlayersgenerallydoesnotfactorizebecauseofinteractionsbetweenlayers.Intheexamplewithtwohiddenlayers,P(h(1),h(2)|v)doesnotfactorizedueduetotheinteractionweightsW(2)betweenh(1)andh(2)whichrenderthesevariablesmutuallydependent.AswasthecasewiththeDBN,wearelefttoseekoutmethodstoapproximatetheDBMposteriordistribution. However,unliketheDBN,theDBMposteriordistributionovertheirhiddenunits—whilecomplicated—iseasytoapproximatewithavariationalapproximation(asdiscussedinsection), speciﬁcallya19.4meanﬁeldapproximation.Themeanﬁeldapproximationisasimpleformofvariationalinference,wherewerestricttheapproximatingdistributiontofullyfactorialdistributions.InthecontextofDBMs,themeanﬁeldequationscapturethebidirectionalinteractionsbetweenlayers.InthissectionwederivetheiterativeapproximateinferenceprocedureoriginallyintroducedinSalakhutdinovandHinton().2009aInvariationalapproximationstoinference,weapproachthetaskofapproxi-666 CHAPTER20.DEEPGENERATIVEMODELSmatingaparticulartargetdistribution—inourcase,theposteriordistributionoverthehiddenunitsgiventhevisibleunits—bysomereasonablysimplefamilyofdis-tributions.Inthecaseofthemeanﬁeldapproximation,theapproximatingfamilyisthesetofdistributionswherethehiddenunitsareconditionallyindependent.Wenowdevelopthemeanﬁeldapproachfortheexamplewithtwohiddenlayers.LetQ(h(1),h(2)|v)betheapproximationofP(h(1),h(2)|v).ThemeanﬁeldassumptionimpliesthatQ(h(1),h(2)|v) =jQh((1)j|v)kQh((2)k|v).(20.29)ThemeanﬁeldapproximationattemptstoﬁndamemberofthisfamilyofdistributionsthatbestﬁtsthetrueposteriorP(h(1),h(2)|v). Importantly,theinferenceprocessmustberunagaintoﬁndadiﬀerentdistributionQeverytimeweuseanewvalueof.vOnecanconceiveofmanywaysofmeasuringhowwellQ(hv|)ﬁtsP(hv|).ThemeanﬁeldapproachistominimizeKL() =QPhQ(h(1),h(2)|v)logQ(h(1),h(2)|v)P(h(1),h(2)|v).(20.30)Ingeneral,wedonothavetoprovideaparametricformoftheapproximatingdistributionbeyondenforcingtheindependenceassumptions.Thevariationalapproximationprocedureisgenerallyabletorecoverafunctionalformoftheapproximatedistribution.However,inthecaseofameanﬁeldassumptiononbinaryhiddenunits(thecasewearedevelopinghere)thereisnolossofgeneralityresultingfromﬁxingaparametrizationofthemodelinadvance.WeparametrizeQasaproductofBernoullidistributions,thatisweassociatetheprobabilityofeachelementofh(1)withaparameter.Speciﬁcally,foreachj,ˆh(1)j=Q(h(1)j= 1|v),whereˆh(1)j∈[0,1]andforeachk,ˆh(2)k=Q(h(2)k= 1|v),whereˆh(2)k∈[01],.Thuswehavethefollowingapproximationtotheposterior:Q(h(1),h(2)|v) =jQh((1)j|v)kQh((2)k|v)(20.31)=j(ˆh(1)j)h(1)j(1−ˆh(1)j)(1−h(1)j)×k(ˆh(2)k)h(2)k(1−ˆh(2)k)(1−h(2)k).(20.32)Ofcourse,forDBMswithmorelayerstheapproximateposteriorparametrizationcanbeextendedintheobviousway,exploitingthebipartitestructureofthegraph667 CHAPTER20.DEEPGENERATIVEMODELStoupdatealloftheevenlayerssimultaneouslyandthentoupdatealloftheoddlayerssimultaneously,followingthesamescheduleasGibbssampling.NowthatwehavespeciﬁedourfamilyofapproximatingdistributionsQ,itremainstospecifyaprocedureforchoosingthememberofthisfamilythatbestﬁtsP.Themoststraightforwardwaytodothisistousethemeanﬁeldequationsspeciﬁedbyequation.Theseequationswerederivedbysolvingforwherethe19.56derivativesofthevariationallowerboundarezero.Theydescribeinanabstractmannerhowtooptimizethevariationallowerboundforanymodel,simplybytakingexpectationswithrespectto.QApplyingthesegeneralequations,weobtaintheupdaterules(again,ignoringbiasterms):ˆh(1)j= σiviW(1)i,j+kW(2)j,kˆh(2)k,j∀(20.33)ˆh(2)k= σjW(2)j,kˆh(1)j,k.∀(20.34)Ataﬁxedpointofthissystemofequations,wehavealocalmaximumofthevariationallowerboundL(Q).Thustheseﬁxedpointupdateequationsdeﬁneaniterativealgorithmwherewealternateupdatesofˆh(1)j(usingequation)and20.33updatesofˆh(2)k(usingequation).OnsmallproblemssuchasMNIST,asfew20.34asteniterationscanbesuﬃcienttoﬁndanapproximatepositivephasegradientforlearning,andﬁftyusuallysuﬃcetoobtainahighqualityrepresentationofasinglespeciﬁcexampletobeusedforhigh-accuracyclassiﬁcation.ExtendingapproximatevariationalinferencetodeeperDBMsisstraightforward.20.4.3DBMParameterLearningLearningintheDBMmustconfrontboththechallengeofanintractablepartitionfunction,usingthetechniquesfromchapter,andthechallengeofanintractable18posteriordistribution,usingthetechniquesfromchapter.19Asdescribedinsection,variationalinferenceallowstheconstructionof20.4.2adistributionQ(hv|)thatapproximatestheintractableP(hv|).LearningthenproceedsbymaximizingL(vθ,Q,),thevariationallowerboundontheintractablelog-likelihood,.log(;)Pvθ668 CHAPTER20.DEEPGENERATIVEMODELSForadeepBoltzmannmachinewithtwohiddenlayers,isgivenbyLL() =Q,θijviW(1)i,jˆh(1)j+jkˆh(1)jW(2)j,kˆh(2)k−Hlog()+Zθ()Q.(20.35)Thisexpressionstillcontainsthelogpartitionfunction,logZ(θ).BecauseadeepBoltzmannmachinecontainsrestrictedBoltzmannmachinesascomponents,thehardnessresultsforcomputingthepartitionfunctionandsamplingthatapplytorestrictedBoltzmannmachinesalsoapplytodeepBoltzmannmachines.ThismeansthatevaluatingtheprobabilitymassfunctionofaBoltzmannmachinerequiresapproximatemethodssuchasannealedimportancesampling.Likewise,trainingthemodelrequiresapproximationstothegradientofthelogpartitionfunction.Seechapterforageneraldescriptionofthesemethods.DBMsaretypicallytrained18usingstochasticmaximumlikelihood.Manyoftheothertechniquesdescribedinchapterarenotapplicable.Techniquessuchaspseudolikelihoodrequirethe18abilitytoevaluatetheunnormalizedprobabilities,ratherthanmerelyobtainavariationallowerboundonthem.ContrastivedivergenceisslowfordeepBoltzmannmachinesbecausetheydonotalloweﬃcientsamplingofthehiddenunitsgiventhevisibleunits—instead,contrastivedivergencewouldrequireburninginaMarkovchaineverytimeanewnegativephasesampleisneeded.Thenon-variationalversionofstochasticmaximumlikelihoodalgorithmwasdiscussedearlier,insection. Variationalstochasticmaximumlikelihoodas18.2appliedtotheDBMisgiveninalgorithm.Recallthatwedescribeasimpliﬁed20.1varientoftheDBMthatlacksbiasparameters;includingthemistrivial.20.4.4Layer-WisePretrainingUnfortunately,trainingaDBMusingstochasticmaximumlikelihood(asdescribedabove)fromarandominitializationusuallyresultsinfailure.Insomecases,themodelfailstolearntorepresentthedistributionadequately.Inothercases,theDBMmayrepresentthedistributionwell,butwithnohigherlikelihoodthancouldbeobtainedwithjustanRBM.ADBMwithverysmallweightsinallbuttheﬁrstlayerrepresentsapproximatelythesamedistributionasanRBM.Varioustechniquesthatpermitjointtraininghavebeendevelopedandaredescribedinsection.However,theoriginalandmostpopularmethodfor20.4.5overcomingthejointtrainingproblemofDBMsisgreedylayer-wisepretraining.Inthismethod,eachlayeroftheDBMistrainedinisolationasanRBM.Theﬁrstlayeristrainedtomodeltheinputdata.EachsubsequentRBMistrainedtomodelsamplesfromthepreviousRBM’sposteriordistribution. Afterallofthe669 CHAPTER20.DEEPGENERATIVEMODELSAlgorithm20.1ThevariationalstochasticmaximumlikelihoodalgorithmfortrainingaDBMwithtwohiddenlayers.Set,thestepsize,toasmallpositivenumberSetk,thenumberofGibbssteps,highenoughtoallowaMarkovchainofp(vh,(1),h(2);θ+∆θ)toburnin,startingfromsamplesfromp(vh,(1),h(2);θ).Initializethreematrices,˜V,˜H(1)and˜H(2)eachwithmrowssettorandomvalues(e.g.,fromBernoullidistributions,possiblywithmarginalsmatchedtothemodel’smarginals).whilenotconverged(learningloop)doSampleaminibatchofmexamplesfromthetrainingdataandarrangethemastherowsofadesignmatrix.VInitializematricesˆH(1)andˆH(2),possiblytothemodel’smarginals.whilenotconverged(meanﬁeldinferenceloop)doˆH(1)←σVW(1)+ˆH(2)W(2).ˆH(2)←σˆH(1)W(2).endwhile∆W(1)←1mVˆH(1)∆W(2)←1mˆH(1)ˆH(2)fordolk= 1to(Gibbssampling)Gibbsblock1:∀i,j,˜Vi,jsampledfromP(˜Vi,j= 1) = σW(1)j,:˜H(1)i,:.∀i,j,˜H(2)i,jsampledfromP(˜H(2)i,j= 1) = σ˜H(1)i,:W(2):,j.Gibbsblock2:∀i,j,˜H(1)i,jsampledfromP(˜H(1)i,j= 1) = σ˜Vi,:W(1):,j+˜H(2)i,:W(2)j,:.endfor∆W(1)←∆W(1)−1mV˜H(1)∆W(2)←∆W(2)−1m˜H(1)˜H(2)W(1)←W(1)+∆W(1)(thisisacartoonillustration,inpracticeuseamoreeﬀectivealgorithm,suchasmomentumwithadecayinglearningrate)W(2)←W(2)+∆W(2)endwhile670 CHAPTER20.DEEPGENERATIVEMODELSRBMshavebeentrainedinthisway,theycanbecombinedtoformaDBM.TheDBMmaythenbetrainedwithPCD.TypicallyPCDtrainingwillmakeonlyasmallchangeinthemodel’sparametersanditsperformanceasmeasuredbythelog-likelihooditassignstothedata,oritsabilitytoclassifyinputs.Seeﬁgure20.4foranillustrationofthetrainingprocedure.Thisgreedylayer-wisetrainingprocedureisnotjustcoordinateascent.Itbearssomepassingresemblancetocoordinateascentbecauseweoptimizeonesubsetoftheparametersateachstep.Thetwomethodsdiﬀerbecausethegreedylayer-wisetrainingprocedureusesadiﬀerentobjectivefunctionateachstep.Greedylayer-wisepretrainingofaDBMdiﬀersfromgreedylayer-wisepre-trainingofaDBN.TheparametersofeachindividualRBMmaybecopiedtothecorrespondingDBNdirectly.InthecaseoftheDBM,theRBMparametersmustbemodiﬁedbeforeinclusionintheDBM.AlayerinthemiddleofthestackofRBMsistrainedwithonlybottom-upinput,butafterthestackiscombinedtoformtheDBM,thelayerwillhavebothbottom-upandtop-downinput. Toaccountforthiseﬀect,SalakhutdinovandHinton2009a()advocatedividingtheweightsofallbutthetopandbottomRBMinhalfbeforeinsertingthemintotheDBM.Additionally,thebottomRBMmustbetrainedusingtwo“copies”ofeachvisibleunitandtheweightstiedtobeequalbetweenthetwocopies.Thismeansthattheweightsareeﬀectivelydoubledduringtheupwardpass.Similarly,thetopRBMshouldbetrainedwithtwocopiesofthetopmostlayer.ObtainingthestateoftheartresultswiththedeepBoltzmannmachinerequiresamodiﬁcationofthestandardSMLalgorithm,whichistouseasmallamountofmeanﬁeldduringthenegativephaseofthejointPCDtrainingstep(SalakhutdinovandHinton2009a,). Speciﬁcally,theexpectationoftheenergygradientshouldbecomputedwithrespecttothemeanﬁelddistributioninwhichalloftheunitsareindependentfromeachother.Theparametersofthismeanﬁelddistributionshouldbeobtainedbyrunningthemeanﬁeldﬁxedpointequationsforjustonestep.See()foracomparisonoftheperformanceofcenteredGoodfellowetal.2013bDBMswithandwithouttheuseofpartialmeanﬁeldinthenegativephase.20.4.5JointlyTrainingDeepBoltzmannMachinesClassicDBMsrequiregreedyunsupervisedpretraining,andtoperformclassiﬁcationwell,requireaseparateMLP-basedclassiﬁerontopofthehiddenfeaturestheyextract.Thishassomeundesirableproperties.ItishardtotrackperformanceduringtrainingbecausewecannotevaluatepropertiesofthefullDBMwhiletrainingtheﬁrstRBM.Thus,itishardtotellhowwellourhyperparameters671 CHAPTER20.DEEPGENERATIVEMODELS d)a)b) c) Figure20.4:ThedeepBoltzmannmachinetrainingprocedureusedtoclassifytheMNISTdataset(SalakhutdinovandHinton2009aSrivastava2014,;etal.,).TrainanRBM(a)byusingCDtoapproximatelymaximizelogP(v).TrainasecondRBMthatmodels(b)h(1)andtargetclassybyusingCD-ktoapproximatelymaximizelogP(h(1),y)whereh(1)isdrawnfromtheﬁrstRBM’sposteriorconditionedonthedata.Increasekfrom1to20duringlearning.CombinethetwoRBMsintoaDBM.Trainittoapproximately(c)maximizelogP(v,y)usingstochasticmaximumlikelihoodwithk= 5.Delete(d)yfromthemodel.Deﬁneanewsetoffeaturesh(1)andh(2)thatareobtainedbyrunningmeanﬁeldinferenceinthemodellackingy.UsethesefeaturesasinputtoanMLPwhosestructureisthesameasanadditionalpassofmeanﬁeld,withanadditionaloutputlayerfortheestimateofy.InitializetheMLP’sweightstobethesameastheDBM’sweights.TraintheMLPtoapproximatelymaximizelogP(y|v)usingstochasticgradientdescentanddropout.Figurereprintedfrom(,).Goodfellowetal.2013b672 CHAPTER20.DEEPGENERATIVEMODELSareworkinguntilquitelateinthetrainingprocess.SoftwareimplementationsofDBMsneedtohavemanydiﬀerentcomponentsforCDtrainingofindividualRBMs,PCDtrainingofthefullDBM,andtrainingbasedonback-propagationthroughtheMLP.Finally,theMLPontopoftheBoltzmannmachinelosesmanyoftheadvantagesoftheBoltzmannmachineprobabilisticmodel,suchasbeingabletoperforminferencewhensomeinputvaluesaremissing.TherearetwomainwaystoresolvethejointtrainingproblemofthedeepBoltzmannmachine.The ﬁrstisthecentereddeepBoltzmann machine(MontavonandMuller2012,),whichreparametrizesthemodelinordertomaketheHessianofthecostfunctionbetter-conditionedatthebeginningofthelearningprocess.Thisyieldsamodelthatcanbetrainedwithoutagreedylayer-wisepretrainingstage.Theresultingmodelobtainsexcellenttestsetlog-likelihoodandproduceshighqualitysamples.Unfortunately,itremainsunabletocompetewithappropriatelyregularizedMLPsasaclassiﬁer.ThesecondwaytojointlytrainadeepBoltzmannmachineistouseamulti-predictiondeepBoltzmannmachine(Goodfellow2013betal.,).Thismodelusesanalternativetrainingcriterionthatallowstheuseoftheback-propagationalgorithminordertoavoidtheproblemswithMCMCestimatesofthegradient.Unfortunately, thenewcriteriondoesnotleadtogoodlikelihoodorsamples,but,comparedtotheMCMCapproach,itdoesleadtosuperiorclassiﬁcationperformanceandabilitytoreasonwellaboutmissinginputs.ThecenteringtrickfortheBoltzmannmachineiseasiesttodescribeifwereturntothegeneralviewofaBoltzmannmachineasconsistingofasetofunitsxwithaweightmatrixUandbiasesb.Recallfromequationthatheenergy20.2functionisgivenbyE() = x−xUxb−x.(20.36)Using diﬀerent sparsity patternsin theweight matrixU, wecan implementstructuresofBoltzmannmachines,suchasRBMs,orDBMswithdiﬀerentnumbersoflayers.ThisisaccomplishedbypartitioningxintovisibleandhiddenunitsandzeroingoutelementsofUforunitsthatdonotinteract.ThecenteredBoltzmannmachineintroducesavectorthatissubtractedfromallofthestates:µE(;) = ()xUb,−xµ−Uxµxµ(−)(−−)b.(20.37)Typicallyµisahyperparameterﬁxedatthebeginningoftraining.Itisusu-allychosentomakesurethatxµ−≈0whenthemodelisinitialized.Thisreparametrizationdoesnotchangethesetofprobabilitydistributionsthatthemodelcanrepresent,butitdoeschangethedynamicsofstochasticgradientdescentappliedtothelikelihood.Speciﬁcally,inmanycases,thisreparametrizationresults673 CHAPTER20.DEEPGENERATIVEMODELSinaHessianmatrixthatisbetterconditioned.()experimentallyMelchioretal.2013conﬁrmedthattheconditioningoftheHessianmatriximproves,andobservedthatthecenteringtrickisequivalenttoanotherBoltzmannmachinelearningtechnique,theenhancedgradient(,).TheimprovedconditioningoftheChoetal.2011Hessianmatrixallowslearningtosucceed,evenindiﬃcultcasesliketrainingadeepBoltzmannmachinewithmultiplelayers.TheotherapproachtojointlytrainingdeepBoltzmannmachinesisthemulti-predictiondeepBoltzmannmachine(MP-DBM)whichworksbyviewingthemeanﬁeldequationsasdeﬁningafamilyofrecurrentnetworksforapproximatelysolvingeverypossibleinferenceproblem(,).RatherthantrainingGoodfellowetal.2013bthemodeltomaximizethelikelihood,themodelistrainedtomakeeachrecurrentnetworkobtainanaccurateanswertothecorrespondinginferenceproblem.Thetrainingprocessisillustratedinﬁgure. Itconsistsofrandomlysamplinga20.5trainingexample,randomlysamplingasubsetofinputstotheinferencenetwork,andthentrainingtheinferencenetworktopredictthevaluesoftheremainingunits.Thisgeneralprincipleofback-propagatingthroughthecomputationalgraphforapproximateinferencehasbeenappliedtoothermodels(Stoyanov2011etal.,;Brakel2013etal.,).InthesemodelsandintheMP-DBM,theﬁnallossisnotthelowerboundonthelikelihood.Instead,theﬁnallossistypicallybasedontheapproximateconditionaldistributionthattheapproximateinferencenetworkimposesoverthemissingvalues.Thismeansthatthetrainingofthesemodelsissomewhatheuristicallymotivated.Ifweinspectthep(v)representedbytheBoltzmannmachinelearnedbytheMP-DBM,ittendstobesomewhatdefective,inthesensethatGibbssamplingyieldspoorsamples.Back-propagationthroughtheinferencegraphhastwomainadvantages.First,ittrainsthemodelasitisreallyused—withapproximateinference.Thismeansthatapproximateinference,forexample,toﬁllinmissinginputs,ortoperformclassiﬁcationdespitethepresenceofmissinginputs,ismoreaccurateintheMP-DBMthanintheoriginalDBM.TheoriginalDBMdoesnotmakeanaccurateclassiﬁeronitsown;thebestclassiﬁcationresultswiththeoriginalDBMwerebasedontrainingaseparateclassiﬁertousefeaturesextractedbytheDBM,ratherthanbyusinginferenceintheDBMtocomputethedistributionovertheclasslabels.MeanﬁeldinferenceintheMP-DBMperformswellasaclassiﬁerwithoutspecialmodiﬁcations.Theotheradvantageofback-propagatingthroughapproximateinferenceisthatback-propagationcomputestheexactgradientoftheloss.ThisisbetterforoptimizationthantheapproximategradientsofSMLtraining,whichsuﬀerfrombothbiasandvariance.ThisprobablyexplainswhyMP-674 CHAPTER20.DEEPGENERATIVEMODELS Figure20.5:Anillustrationofthemulti-predictiontrainingprocessforadeepBoltzmannmachine.Eachrowindicatesadiﬀerentexamplewithinaminibatchforthesametrainingstep. Eachcolumnrepresentsatimestepwithinthemeanﬁeldinferenceprocess. Foreachexample,wesampleasubsetofthedatavariablestoserveasinputstotheinferenceprocess.Thesevariablesareshadedblacktoindicateconditioning.Wethenrunthemeanﬁeldinferenceprocess,witharrowsindicatingwhichvariablesinﬂuencewhichothervariablesintheprocess.Inpracticalapplications,weunrollmeanﬁeldforseveralsteps.Inthisillustration,weunrollforonlytwosteps.Dashedarrowsindicatehowtheprocesscouldbeunrolledformoresteps.Thedatavariablesthatwerenotusedasinputstotheinferenceprocessbecometargets,shadedingray.Wecanviewtheinferenceprocessforeachexampleasarecurrentnetwork.Weusegradientdescentandback-propagationtotraintheserecurrentnetworkstoproducethecorrecttargetsgiventheirinputs.ThistrainsthemeanﬁeldprocessfortheMP-DBMtoproduceaccurateestimates.Figureadaptedfrom().Goodfellowetal.2013b675 CHAPTER20.DEEPGENERATIVEMODELSDBMsmaybetrainedjointlywhileDBMsrequireagreedylayer-wisepretraining.Thedisadvantageofback-propagatingthroughtheapproximateinferencegraphisthatitdoesnotprovideawaytooptimizethelog-likelihood,butratheraheuristicapproximationofthegeneralizedpseudolikelihood.TheMP-DBMinspiredtheNADE-k(Raiko2014etal.,)extensiontotheNADEframework,whichisdescribedinsection.20.10.10TheMP-DBMhassomeconnectionstodropout.Dropoutsharesthesamepa-rametersamongmanydiﬀerentcomputationalgraphs,withthediﬀerencebetweeneachgraphbeingwhetheritincludesorexcludeseachunit.TheMP-DBMalsosharesparametersacrossmanycomputationalgraphs.InthecaseoftheMP-DBM,thediﬀerencebetweenthegraphsiswhethereachinputunitisobservedornot.Whenaunitisnotobserved,theMP-DBMdoesnotdeleteitentirelyasdropoutdoes.Instead,theMP-DBMtreatsitasalatentvariabletobeinferred.OnecouldimagineapplyingdropouttotheMP-DBMbyadditionallyremovingsomeunitsratherthanmakingthemlatent.20.5BoltzmannMachinesforReal-ValuedDataWhileBoltzmannmachineswereoriginallydevelopedforusewithbinarydata,manyapplicationssuchasimageandaudiomodelingseemtorequiretheabilitytorepresentprobabilitydistributionsoverrealvalues.Insomecases,itispossibletotreatreal-valueddataintheinterval[0,1]asrepresentingtheexpectationofabinaryvariable.Forexample,()treatsgrayscaleimagesinthetrainingHinton2000setasdeﬁning[0,1]probabilityvalues.Eachpixeldeﬁnestheprobabilityofabinaryvaluebeing1,andthebinarypixelsareallsampledindependentlyfromeachother.Thisisacommonprocedureforevaluatingbinarymodelsongrayscaleimagedatasets.However,itisnotaparticularlytheoreticallysatisfyingapproach,andbinaryimagessampledindependentlyinthiswayhaveanoisyappearance.Inthissection,wepresentBoltzmannmachinesthatdeﬁneaprobabilitydensityoverreal-valueddata.20.5.1Gaussian-BernoulliRBMsRestrictedBoltzmannmachinesmaybedevelopedformanyexponentialfamilyconditionaldistributions(Welling2005etal.,).Ofthese,themostcommonistheRBMwithbinaryhiddenunitsandreal-valuedvisibleunits,withtheconditionaldistributionoverthevisibleunitsbeingaGaussiandistributionwhosemeanisafunctionofthehiddenunits.676 CHAPTER20.DEEPGENERATIVEMODELSTherearemanywaysofparametrizingGaussian-BernoulliRBMs.OnechoiceiswhethertouseacovariancematrixoraprecisionmatrixfortheGaussiandistribution.Herewepresenttheprecisionformulation.Themodiﬁcationtoobtainthecovarianceformulationisstraightforward. Wewishtohavetheconditionaldistributionp,() = (;vh|NvWhβ−1).(20.38)Wecanﬁndthetermsweneedtoaddtotheenergyfunctionbyexpandingtheunnormalizedlogconditionaldistribution:log(;NvWhβ,−1) = −12()vWh−βvWhβ(−)+(f).(20.39)Herefencapsulatesallthetermsthatareafunctiononlyoftheparametersandnottherandomvariablesinthemodel.Wecandiscardfbecauseitsonlyroleistonormalizethedistribution,andthepartitionfunctionofwhateverenergyfunctionwechoosewillcarryoutthatrole.Ifweincludealloftheterms(withtheirsignﬂipped)involvingvfromequa-tioninourenergyfunctionanddonotaddanyothertermsinvolving20.39v,thenourenergyfunctionwillrepresentthedesiredconditional.p()vh|Wehavesomefreedomregardingtheotherconditionaldistribution,p(hv|).Notethatequationcontainsaterm20.3912hWβWh.(20.40)Thistermcannotbeincludedinitsentiretybecauseitincludeshihjterms.Thesecorrespondtoedgesbetweenthehiddenunits.Ifweincludedtheseterms,wewouldhavealinearfactormodelinsteadofarestrictedBoltzmannmachine.WhendesigningourBoltzmannmachine,wesimplyomitthesehihjcrossterms.Omittingthemdoesnotchangetheconditionalp(vh|)soequationisstillrespected.20.39However,westillhaveachoiceaboutwhethertoincludethetermsinvolvingonlyasinglehi.Ifweassumeadiagonalprecisionmatrix,weﬁndthatforeachhiddenunithiwehaveaterm12hijβjW2j,i.(20.41)Intheabove,weusedthefactthath2i=hibecausehi∈{0,1}.Ifweincludethisterm(withitssignﬂipped)intheenergyfunction,thenitwillnaturallybiashitobeturnedoﬀwhentheweightsforthatunitarelargeandconnectedtovisibleunitswithhighprecision.Thechoiceofwhetherornottoincludethisbiastermdoesnotaﬀectthefamilyofdistributionsthemodelcanrepresent(assumingthat677 CHAPTER20.DEEPGENERATIVEMODELSweincludebiasparametersforthehiddenunits)butitdoesaﬀectthelearningdynamicsofthemodel.Includingthetermmayhelpthehiddenunitactivationsremainreasonableevenwhentheweightsrapidlyincreaseinmagnitude.OnewaytodeﬁnetheenergyfunctiononaGaussian-BernoulliRBMisthusE,(vh) =12v()()βv−vβWhb−h(20.42)butwemayalsoaddextratermsorparametrizetheenergyintermsofthevarianceratherthanprecisionifwechoose.Inthisderivation,wehavenotincludedabiastermonthevisibleunits,butonecouldeasilybeadded.OneﬁnalsourceofvariabilityintheparametrizationofaGaussian-BernoulliRBMisthechoiceofhowtotreattheprecisionmatrix.Itmayeitherbeﬁxedtoaconstant(perhapsestimatedbasedonthemarginalprecisionofthedata)orlearned.Itmayalsobeascalartimestheidentitymatrix,oritmaybeadiagonalmatrix.Typicallywedonotallowtheprecisionmatrixtobenon-diagonalinthiscontext,becausesomeoperationsontheGaussiandistributionrequireinvertingthematrix,andadiagonalmatrixcanbeinvertedtrivially.Inthesectionsahead,wewillseethatotherformsofBoltzmannmachinespermitmodelingthecovariancestructure,usingvarioustechniquestoavoidinvertingtheprecisionmatrix.20.5.2UndirectedModelsofConditionalCovarianceWhiletheGaussianRBMhasbeenthecanonicalenergymodelforreal-valueddata,()arguethattheGaussianRBMinductivebiasisnotRanzatoetal.2010awellsuitedtothestatisticalvariationspresentinsometypesofreal-valueddata,especiallynaturalimages.Theproblemisthatmuchoftheinformationcontentpresentinnaturalimagesisembeddedinthecovariancebetweenpixelsratherthanintherawpixelvalues.Inotherwords,itistherelationshipsbetweenpixelsandnottheirabsolutevalueswheremostoftheusefulinformationinimagesresides.SincetheGaussianRBMonlymodelstheconditionalmeanoftheinputgiventhehiddenunits,itcannotcaptureconditionalcovarianceinformation.Inresponsetothesecriticisms,alternativemodelshavebeenproposedthatattempttobetteraccountforthecovarianceofreal-valueddata.ThesemodelsincludethemeanandcovarianceRBM(mcRBM1),themean-productoft-distribution(mPoT)modelandthespikeandslabRBM(ssRBM).1Theterm“mcRBM”ispronouncedbysayingthenameofthelettersM-C-R-B-M;the“mc”isnotpronouncedlikethe“Mc”in“McDonald’s.”678 CHAPTER20.DEEPGENERATIVEMODELSMeanandCovarianceRBMThemcRBMusesitshiddenunitstoindepen-dentlyencodetheconditionalmeanandcovarianceofallobservedunits.ThemcRBMhiddenlayerisdividedintotwogroupsofunits:meanunitsandcovarianceunits.ThegroupthatmodelstheconditionalmeanissimplyaGaussianRBM.TheotherhalfisacovarianceRBM(,),alsocalledacRBM,Ranzatoetal.2010awhosecomponentsmodeltheconditionalcovariancestructure,asdescribedbelow.Speciﬁcally,withbinarymeanunitsh()mandbinarycovarianceunitsh()c,themcRBMmodelisdeﬁnedasthecombinationoftwoenergyfunctions:Emc(xh,()m,h()c) = Em(xh,()m)+Ec(xh,()c),(20.43)whereEmisthestandardGaussian-BernoulliRBMenergyfunction:2Em(xh,()m) =12xx−jxW:,jh()mj−jb()mjh()mj,(20.44)andEcisthecRBMenergy functionthat models theconditionalcovarianceinformation:Ec(xh,()c) =12jh()cjxr()j2−jb()cjh()cj.(20.45)Theparameterr()jcorrespondstothecovarianceweightvectorassociatedwithh()cjandb()cisavectorofcovarianceoﬀsets.Thecombinedenergyfunctiondeﬁnesajointdistribution:pmc(xh,()m,h()c) =1Zexp−Emc(xh,()m,h()c),(20.46)andacorrespondingconditionaldistributionovertheobservationsgivenh()mandh()casamultivariateGaussiandistribution:pmc(xh|()m,h()c) = NxC;mcxh|jW:,jh()mj,Cmcxh|.(20.47)NotethatthecovariancematrixCmcxh|=jh()cjr()jr()j+I−1isnon-diagonalandthatWistheweightmatrixassociatedwiththeGaussianRBMmodelingthe2ThisversionoftheGaussian-BernoulliRBMenergyfunctionassumestheimagedatahaszeromean,perpixel.Pixeloﬀsetscaneasilybeaddedtothemodeltoaccountfornonzeropixelmeans.679 CHAPTER20.DEEPGENERATIVEMODELSconditionalmeans.ItisdiﬃculttotrainthemcRBMviacontrastivedivergenceorpersistentcontrastivedivergencebecauseofitsnon-diagonalconditionalcovariancestructure.CDandPCDrequiresamplingfromthejointdistributionofxh,()m,h()cwhich,inastandardRBM,isaccomplishedbyGibbssamplingovertheconditionals.However,inthemcRBM,samplingfrompmc(xh|()m,h()c)requirescomputing(Cmc)−1ateveryiterationoflearning.Thiscanbeanimpracticalcomputationalburdenforlargerobservations.()avoiddirectsamplingRanzatoandHinton2010fromtheconditionalpmc(xh|()m,h()c)bysamplingdirectlyfromthemarginalp(x)usingHamiltonian(hybrid)MonteCarlo(,)onthemcRBMfreeNeal1993energy.Mean-ProductofStudent’s-distributionstThemean-productofStudent’st-distribution(mPoT)model(,)extendsthePoTmodel(Ranzatoetal.2010bWellingetal.,)inamannersimilartohowthemcRBMextendsthecRBM.This2003aisachievedbyincludingnonzeroGaussianmeansbytheadditionofGaussianRBM-likehiddenunits.LikethemcRBM,thePoTconditionaldistributionovertheobservationisamultivariateGaussian(withnon-diagonalcovariance)distribution;however,unlikethemcRBM,thecomplementaryconditionaldistributionoverthehiddenvariablesisgivenbyconditionallyindependentGammadistributions.TheGammadistributionG(k,θ) isaprobabilitydistributionoverpositiverealnumbers,withmeankθ.ItisnotnecessarytohaveamoredetailedunderstandingoftheGammadistributiontounderstandthebasicideasunderlyingthemPoTmodel.ThemPoTenergyfunctionis:EmPoT(xh,()m,h()c)(20.48)= Em(xh,()m)+jh()cj1+12r()jx2+(1−γj)logh()cj(20.49)wherer()jisthecovarianceweightvectorassociatedwithunith()cjandEm(xh,()m)isasdeﬁnedinequation.20.44JustaswiththemcRBM,themPoTmodelenergyfunctionspeciﬁesamul-tivariateGaussian,withaconditionaldistributionoverxthathasnon-diagonalcovariance.LearninginthemPoTmodel—again,likethemcRBM—iscompli-catedbytheinabilityto samplefromthenon-diagonalGaussianconditionalpmPoT(xh|()m,h()c),so()alsoadvocatedirectsamplingofRanzatoetal.2010bp()xviaHamiltonian(hybrid)MonteCarlo.680 CHAPTER20.DEEPGENERATIVEMODELSSpikeandSlabRestrictedBoltzmannMachinesSpikeandslabrestrictedBoltzmannmachines(,)orssRBMsprovideanothermeansCourvilleetal.2011ofmodelingthecovariancestructureofreal-valueddata.ComparedtomcRBMs,ssRBMshavetheadvantageofrequiringneithermatrixinversionnorHamiltonianMonteCarlomethods.LikethemcRBMandthemPoTmodel,thessRBM’sbinaryhiddenunitsencodetheconditionalcovarianceacrosspixelsthroughtheuseofauxiliaryreal-valuedvariables.ThespikeandslabRBMhastwosetsofhiddenunits:binaryspikeunitsh,andreal-valuedslabunitss.Themeanofthevisibleunitsconditionedonthehiddenunitsisgivenby(hs)W.Inotherwords,eachcolumnW:,ideﬁnesacomponentthatcanappearintheinputwhenhi= 1.Thecorrespondingspikevariablehidetermineswhetherthatcomponentispresentatall.Thecorrespondingslabvariablesideterminestheintensityofthatcomponent,ifitispresent.Whenaspikevariableisactive,thecorrespondingslabvariableaddsvariancetotheinputalongtheaxisdeﬁnedbyW:,i.Thisallowsustomodelthecovarianceoftheinputs.Fortunately,contrastivedivergenceandpersistentcontrastivedivergencewithGibbssamplingarestillapplicable.Thereisnoneedtoinvertanymatrix.Formally,thessRBMmodelisdeﬁnedviaitsenergyfunction:Ess() =xsh,,−ixW:,isihi+12xΛ+iΦihix(20.50)+12iαis2i−iαiµisihi−ibihi+iαiµ2ihi,(20.51)wherebiistheoﬀsetofthespikehiandΛisadiagonalprecisionmatrixontheobservationsx.Theparameterαi>0isascalarprecisionparameterforthereal-valuedslabvariablesi.TheparameterΦiisanon-negativediagonalmatrixthatdeﬁnesanh-modulatedquadraticpenaltyonx.Eachµiisameanparameterfortheslabvariablesi.Withthejointdistributiondeﬁnedviatheenergyfunction,itisrelativelystraightforwardto derivethessRBM conditionaldistributions.For example,bymarginalizingouttheslabvariabless,theconditionaldistributionovertheobservationsgiventhebinaryspikevariablesisgivenby:hpss()=xh|1P()h1Zexp(){−Exsh,,}ds(20.52)=NxC;ssxh|iW:,iµihi,Cssxh|(20.53)681 CHAPTER20.DEEPGENERATIVEMODELSwhereCssxh|=Λ+iΦihi−iα−1ihiW:,iW:,i−1.ThelastequalityholdsonlyifthecovariancematrixCssxh|ispositivedeﬁnite.Gatingbythespikevariablesmeansthatthetruemarginaldistributionoverhsissparse.Thisisdiﬀerentfromsparsecoding,wheresamplesfromthemodel“almostnever”(inthemeasuretheoreticsense)containzerosinthecode,andMAPinferenceisrequiredtoimposesparsity.ComparingthessRBMtothemcRBMandthemPoTmodels,thessRBMparametrizestheconditionalcovarianceoftheobservationinasigniﬁcantlydiﬀerentway.ThemcRBMandmPoTbothmodelthecovariancestructureoftheobservationasjh()cjr()jr()j+I−1,usingtheactivationofthehiddenunitshj>0toenforceconstraintsontheconditionalcovarianceinthedirectionr()j.Incontrast,thessRBMspeciﬁestheconditionalcovarianceoftheobservationsusingthehiddenspikeactivationshi= 1topinchtheprecisionmatrixalongthedirectionspeciﬁedbythecorrespondingweightvector. ThessRBMconditionalcovarianceisverysimilartothatgivenbyadiﬀerentmodel:theproductofprobabilisticprincipalcomponentsanalysis(PoPPCA)(WilliamsandAgakov2002,).Intheovercompletesetting,sparseactivationswiththessRBMparametrizationpermitsigniﬁcantvariance(abovethenominalvariancegivenbyΛ−1)onlyintheselecteddirectionsofthesparselyactivatedhi. InthemcRBMormPoTmodels,anovercompleterepresentationwouldmeanthattocapturevariationinaparticulardirectionintheobservationspacerequiresremovingpotentiallyallconstraintswithpositiveprojectioninthatdirection. Thiswouldsuggestthatthesemodelsarelesswellsuitedtotheovercompletesetting.TheprimarydisadvantageofthespikeandslabrestrictedBoltzmannmachineisthatsomesettingsoftheparameterscancorrespondtoacovariancematrixthatisnotpositivedeﬁnite.Suchacovariancematrixplacesmoreunnormalizedprobabilityonvaluesthatarefartherfromthemean,causingtheintegraloverallpossibleoutcomestodiverge.Generallythisissuecanbeavoidedwithsimpleheuristictricks.Thereisnotyetanytheoreticallysatisfyingsolution.Usingconstrainedoptimizationtoexplicitlyavoidtheregionswheretheprobabilityisundeﬁnedisdiﬃculttodowithoutbeingoverlyconservativeandalsopreventingthemodelfromaccessinghigh-performingregionsofparameterspace.Qualitatively,convolutionalvariantsofthessRBMproduceexcellentsamplesofnaturalimages.Someexamplesareshowninﬁgure.16.1ThessRBMallowsforseveralextensions.Includinghigher-orderinteractionsandaverage-poolingoftheslabvariables(,)enablesthemodelCourvilleetal.2014tolearnexcellentfeaturesforaclassiﬁerwhenlabeleddataisscarce. Addinga682 CHAPTER20.DEEPGENERATIVEMODELStermtotheenergyfunctionthatpreventsthepartitionfunctionfrombecomingundeﬁnedresultsinasparsecodingmodel,spikeandslabsparsecoding(Goodfellowetal.,),alsoknownasS3C.2013d20.6ConvolutionalBoltzmannMachinesAsseeninchapter,extremelyhighdimensionalinputssuchasimagesplace9greatstrainonthecomputation,memoryandstatisticalrequirementsofmachinelearningmodels.Replacingmatrixmultiplicationbydiscreteconvolutionwithasmallkernelisthestandardwayofsolvingtheseproblemsforinputsthathavetranslationinvariantspatialortemporalstructure.()DesjardinsandBengio2008showedthatthisapproachworkswellwhenappliedtoRBMs.Deepconvolutionalnetworksusuallyrequireapoolingoperationsothatthespatialsizeofeachsuccessivelayerdecreases.Feedforwardconvolutionalnetworksoftenuseapoolingfunctionsuchasthemaximumoftheelementstobepooled.Itisunclearhowtogeneralizethistothesettingofenergy-basedmodels.Wecouldintroduceabinarypoolingunitpovernbinarydetectorunitsdandenforcep=maxidibysettingtheenergyfunctiontobe∞wheneverthatconstraintisviolated.Thisdoesnotscalewellthough,asitrequiresevaluating2ndiﬀerentenergyconﬁgurationstocomputethenormalizationconstant.Forasmall3×3poolingregionthisrequires29= 512energyfunctionevaluationsperpoolingunit!Lee2009etal.()developedasolutiontothisproblemcalledprobabilisticmaxpooling(nottobeconfusedwith“stochasticpooling,”whichisatechniqueforimplicitlyconstructingensemblesofconvolutionalfeedforwardnetworks).Thestrategybehindprobabilisticmaxpoolingistoconstrainthedetectorunitssoatmostonemaybeactiveatatime.Thismeansthereareonlyn+ 1totalstates(onestateforeachofthendetectorunitsbeingon,andanadditionalstatecorrespondingtoallofthedetectorunitsbeingoﬀ).Thepoolingunitisonifandonlyifoneofthedetectorunitsison.Thestatewithallunitsoﬀisassignedenergyzero.Wecanthinkofthisasdescribingamodelwithasinglevariablethathasn+1states,orequivalentlyasamodelthathasn+1variablesthatassignsenergytoallbutjointassignmentsofvariables.∞n+1Whileeﬃcient,probabilisticmaxpoolingdoesforcethedetectorunitstobemutuallyexclusive,whichmaybeausefulregularizingconstraintinsomecontextsoraharmfullimitonmodelcapacityinothercontexts.Italsodoesnotsupportoverlappingpoolingregions.Overlappingpoolingregionsareusuallyrequiredtoobtainthebestperformancefromfeedforwardconvolutionalnetworks,sothisconstraintprobablygreatlyreducestheperformanceofconvolutionalBoltzmann683 CHAPTER20.DEEPGENERATIVEMODELSmachines.Lee2009etal.()demonstratedthatprobabilisticmaxpoolingcouldbeusedtobuildconvolutionaldeepBoltzmannmachines.3Thismodelisabletoperformoperationssuchasﬁllinginmissingportionsofitsinput.Whileintellectuallyappealing,thismodelischallengingtomakeworkinpractice,andusuallydoesnotperformaswellasaclassiﬁerastraditionalconvolutionalnetworkstrainedwithsupervisedlearning.Manyconvolutionalmodelsworkequallywellwithinputsofmanydiﬀerentspatialsizes.ForBoltzmannmachines,itisdiﬃculttochangetheinputsizeforavarietyofreasons. Thepartitionfunctionchangesasthesizeoftheinputchanges.Moreover,manyconvolutionalnetworksachievesizeinvariancebyscalingupthesizeoftheirpoolingregionsproportionaltothesizeoftheinput,butscalingBoltzmannmachinepoolingregionsisawkward.Traditionalconvolutionalneuralnetworkscanuseaﬁxednumberofpoolingunitsanddynamicallyincreasethesizeoftheirpoolingregionsinordertoobtainaﬁxed-sizerepresentationofavariable-sizedinput.ForBoltzmannmachines,largepoolingregionsbecometooexpensiveforthenaiveapproach. Theapproachof()ofmakingLeeetal.2009eachofthedetectorunitsinthesamepoolingregionmutuallyexclusivesolvesthecomputationalproblems,butstilldoesnotallowvariable-sizepoolingregions.Forexample,supposewelearnamodelwith2×2probabilisticmaxpoolingoverdetectorunitsthatlearnedgedetectors. Thisenforcestheconstraintthatonlyoneoftheseedgesmayappearineach2×2region.Ifwethenincreasethesizeoftheinputimageby50%ineachdirection,wewouldexpectthenumberofedgestoincreasecorrespondingly.Instead,ifweincreasethesizeofthepoolingregionsby50%ineachdirectionto3×3,thenthemutualexclusivityconstraintnowspeciﬁesthateachoftheseedgesmayonlyappearonceina3×3region.Aswegrowamodel’sinputimageinthisway,themodelgeneratesedgeswithlessdensity.Ofcourse,theseissuesonlyarisewhenthemodelmustusevariableamountsofpoolinginordertoemitaﬁxed-sizeoutputvector.Modelsthatuseprobabilisticmaxpoolingmaystillacceptvariable-sizedinputimagessolongastheoutputofthemodelisafeaturemapthatcanscaleinsizeproportionaltotheinputimage.Pixelsattheboundaryoftheimagealsoposesomediﬃculty,whichisexac-erbatedbythefactthatconnectionsinaBoltzmannmachinearesymmetric.Ifwedonotimplicitlyzero-padtheinput,thentherearefewerhiddenunitsthanvisibleunits,andthevisibleunitsattheboundaryoftheimagearenotmodeled3Thepublicationdescribesthemodelasa“deepbeliefnetwork”butbecauseitcanbedescribedasapurelyundirectedmodelwithtractablelayer-wisemeanﬁeldﬁxedpointupdates,itbestﬁtsthedeﬁnitionofadeepBoltzmannmachine.684 CHAPTER20.DEEPGENERATIVEMODELSwellbecausetheylieinthereceptiveﬁeldoffewerhiddenunits.However,ifwedoimplicitlyzero-padtheinput,thenthehiddenunitsattheboundaryaredrivenbyfewerinputpixels,andmayfailtoactivatewhenneeded.20.7BoltzmannMachinesforStructuredorSequentialOutputsInthestructuredoutputscenario,wewishtotrainamodelthatcanmapfromsomeinputxtosomeoutputy,andthediﬀerententriesofyarerelatedtoeachotherandmustobeysomeconstraints.Forexample,inthespeechsynthesistask,yisawaveform,andtheentirewaveformmustsoundlikeacoherentutterance.Anaturalwaytorepresenttherelationshipsbetweentheentriesinyistouseaprobabilitydistributionp(y|x).Boltzmannmachines,extendedtomodelconditionaldistributions,cansupplythisprobabilisticmodel.ThesametoolofconditionalmodelingwithaBoltzmannmachinecanbeusednotjustforstructuredoutputtasks,butalsoforsequencemodeling.Inthelattercase,ratherthanmappinganinputxtoanoutputy,themodelmustestimateaprobabilitydistributionoverasequenceofvariables,p(x(1),...,x()τ).ConditionalBoltzmannmachinescanrepresentfactorsoftheformp(x()t|x(1),...,x(1)t−)inordertoaccomplishthistask.Animportantsequencemodelingtaskforthevideogameandﬁlmindustryismodelingsequencesofjointanglesofskeletonsusedtorender3-Dcharacters.Thesesequencesareoftencollectedusingmotioncapturesystemstorecordthemovementsofactors.Aprobabilisticmodelofacharacter’smovementallowsthegenerationofnew, previouslyunseen, but realisticanimations.Tosolvethissequencemodelingtask,Taylor2007etal.()introducedaconditionalRBMmodelingp(x()t|x(1)t−,...,x()tm−)forsmallm.ThemodelisanRBMoverp(x()t)whosebiasparametersarealinearfunctionoftheprecedingmvaluesofx.Whenweconditionondiﬀerentvaluesofx(1)t−andearliervariables,wegetanewRBMoverx.TheweightsintheRBMoverxneverchange,butbyconditioningondiﬀerentpastvalues,wecanchangetheprobabilityofdiﬀerenthiddenunitsintheRBMbeingactive.Byactivatinganddeactivatingdiﬀerentsubsetsofhiddenunits,wecanmakelargechangestotheprobabilitydistributioninducedonx. OthervariantsofconditionalRBM(,)andothervariantsofsequenceMnihetal.2011modelingusingconditionalRBMsarepossible(TaylorandHinton2009Sutskever,;etal.,;2009Boulanger-Lewandowski2012etal.,).Anothersequencemodelingtaskistomodelthedistributionoversequences685 CHAPTER20.DEEPGENERATIVEMODELSofmusicalnotesusedtocomposesongs.Boulanger-Lewandowski2012etal.()introducedtheRNN-RBMsequencemodelandappliedittothistask.TheRNN-RBMisagenerativemodelofasequenceofframesx()tconsistingofanRNNthatemitstheRBMparametersforeachtimestep.UnlikepreviousapproachesinwhichonlythebiasparametersoftheRBMvariedfromonetimesteptothenext,theRNN-RBMusestheRNNtoemitalloftheparametersoftheRBM,includingtheweights.Totrainthemodel,weneedtobeabletoback-propagatethegradientofthelossfunctionthroughtheRNN.ThelossfunctionisnotapplieddirectlytotheRNNoutputs.Instead,itisappliedtotheRBM.ThismeansthatwemustapproximatelydiﬀerentiatethelosswithrespecttotheRBMparametersusingcontrastivedivergenceorarelatedalgorithm. Thisapproximategradientmaythenbeback-propagatedthroughtheRNNusingtheusualback-propagationthroughtimealgorithm.20.8OtherBoltzmannMachinesManyothervariantsofBoltzmannmachinesarepossible.Boltzmannmachinesmaybeextendedwithdiﬀerenttrainingcriteria.WehavefocusedonBoltzmannmachinestrainedtoapproximatelymaximizethegenerativecriterionlogp(v).ItisalsopossibletotraindiscriminativeRBMsthataimtomaximizelogp(y|v)instead(,).ThisapproachoftenLarochelleandBengio2008performsthebestwhenusingalinearcombinationofboththegenerativeandthediscriminativecriteria.Unfortunately,RBMsdonotseemtobeaspowerfulsupervisedlearnersasMLPs,atleastusingexistingmethodology.MostBoltzmannmachinesusedinpracticehaveonlysecond-orderinteractionsintheirenergyfunctions,meaningthattheirenergyfunctionsarethesumofmanytermsandeachindividualtermonlyincludestheproductbetweentworandomvariables.AnexampleofsuchatermisviWi,jhj.Itisalsopossibletotrainhigher-orderBoltzmannmachines(,)whoseenergyfunctiontermsSejnowski1987involvetheproductsbetweenmanyvariables.Three-wayinteractionsbetweenahiddenunitandtwodiﬀerentimagescanmodelspatialtransformationsfromoneframeofvideotothenext(MemisevicandHinton20072010,,).Multiplicationbyaone-hotclassvariablecanchangetherelationshipbetweenvisibleandhiddenunitsdependingonwhichclassispresent(,).OnerecentexampleNairandHinton2009oftheuseofhigher-orderinteractionsisaBoltzmannmachinewithtwogroupsofhiddenunits,withonegroupofhiddenunitsthatinteractwithboththevisibleunitsvandtheclasslabely,andanothergroupofhiddenunitsthatinteractonlywiththevinputvalues(,).ThiscanbeinterpretedasencouragingLuoetal.2011686 CHAPTER20.DEEPGENERATIVEMODELSsomehiddenunitstolearntomodeltheinputusingfeaturesthatarerelevanttotheclassbutalsotolearnextrahiddenunitsthatexplainnuisancedetailsthatarenecessaryforthesamplesofvtoberealisticbutdonotdeterminetheclassoftheexample.Anotheruseofhigher-orderinteractionsistogatesomefeatures.Sohn2013etal.()introducedaBoltzmannmachinewiththird-orderinteractionswithbinarymaskvariablesassociatedwitheachvisibleunit.Whenthesemaskingvariablesaresettozero,theyremovetheinﬂuenceofavisibleunitonthehiddenunits.Thisallowsvisibleunitsthatarenotrelevanttotheclassiﬁcationproblemtoberemovedfromtheinferencepathwaythatestimatestheclass.Moregenerally,theBoltzmannmachineframeworkisarichspaceofmodelspermittingmanymoremodelstructuresthanhavebeenexploredsofar.DevelopinganewformofBoltzmannmachinerequiressomemorecareandcreativitythandevelopinganewneuralnetworklayer,becauseitisoftendiﬃculttoﬁndanenergyfunctionthatmaintainstractabilityofallofthediﬀerentconditionaldistributionsneededtousetheBoltzmannmachine,butdespitethisrequiredeﬀorttheﬁeldremainsopentoinnovation.20.9Back-PropagationthroughRandomOperationsTraditionalneuralnetworksimplementadeterministictransformationofsomeinputvariablesx.Whendevelopinggenerativemodels,weoftenwishtoextendneuralnetworkstoimplementstochastictransformationsofx.Onestraightforwardwaytodothisistoaugmenttheneuralnetworkwithextrainputszthataresampledfromsomesimpleprobabilitydistribution,suchasauniformorGaussiandistribution.Theneuralnetworkcanthencontinuetoperformdeterministiccomputationinternally, butthefunctionf(xz,)willappearstochasticto anobserverwhodoesnothaveaccesstoz.Providedthatfiscontinuousanddiﬀerentiable,wecanthencomputethegradientsnecessaryfortrainingusingback-propagationasusual.Asanexample,letusconsidertheoperationconsistingofdrawingsamplesyfromaGaussiandistributionwithmeanandvarianceµσ2:y∼N(µ,σ2).(20.54)Becauseanindividualsampleofyisnotproducedbyafunction,butratherbyasamplingprocesswhoseoutputchangeseverytimewequeryit,itmayseemcounterintuitivetotakethederivativesofywithrespecttotheparametersofitsdistribution,µandσ2.However, wecanrewritethesamplingprocessas687 CHAPTER20.DEEPGENERATIVEMODELStransforminganunderlyingrandomvaluez∼N(z;0,1)toobtainasamplefromthedesireddistribution:yµσz= +(20.55)Wearenowabletoback-propagatethroughthesamplingoperation,byregard-ingitasadeterministicoperationwithanextrainputz.Crucially,theextrainputisarandomvariablewhosedistributionisnotafunctionofanyofthevariableswhosederivativeswewanttocalculate. Theresulttellsushowaninﬁnitesimalchangeinµorσwouldchangetheoutputifwecouldrepeatthesamplingoperationagainwiththesamevalueofz.Beingabletoback-propagatethroughthissamplingoperationallowsustoincorporateitintoalargergraph.Wecanbuildelementsofthegraphontopoftheoutputofthesamplingdistribution.Forexample,wecancomputethederivativesofsomelossfunctionJ(y).Wecanalsobuildelementsofthegraphwhoseoutputsaretheinputsortheparametersofthesamplingoperation.Forexample,wecouldbuildalargergraphwithµ=f(x;θ)andσ=g(x;θ).Inthisaugmentedgraph,wecanuseback-propagationthroughthesefunctionstoderive∇θJy().TheprincipleusedinthisGaussiansamplingexampleismoregenerallyappli-cable.Wecanexpressanyprobabilitydistributionoftheformp(y;θ)orp(y|x;θ)asp(y|ω),whereωisavariablecontainingbothparametersθ,andifapplicable,theinputsx.Givenavalueysampledfromdistributionp(y|ω),whereωmayinturnbeafunctionofothervariables,wecanrewriteyy ∼p(|ω)(20.56)asyzω= (f;),(20.57)wherezisasourceofrandomness.Wemaythencomputethederivativesofywithrespecttoωusingtraditionaltoolssuchastheback-propagationalgorithmappliedtof,solongasfiscontinuousanddiﬀerentiablealmosteverywhere.Crucially,ωmustnotbeafunctionofz,andzmustnotbeafunctionofω.Thistechniqueisoftencalledthereparametrizationtrick,stochasticback-propagationorperturbationanalysis.Therequirementthatfbecontinuousanddiﬀerentiableofcourserequiresytobecontinuous.Ifwewishtoback-propagatethroughasamplingprocessthatproducesdiscrete-valuedsamples,itmaystillbepossibletoestimateagradientonω,usingreinforcementlearningalgorithmssuchasvariantsoftheREINFORCEalgorithm(,),discussedinsection.Williams199220.9.1688 CHAPTER20.DEEPGENERATIVEMODELSInneuralnetworkapplications,wetypicallychooseztobedrawnfromsomesimpledistribution,suchasaunituniformorunitGaussiandistribution,andachievemorecomplexdistributionsbyallowingthedeterministicportionofthenetworktoreshapeitsinput.Theideaofpropagatinggradientsoroptimizingthroughstochasticoperationsdatesbacktothemid-twentiethcentury(,;,)andwasPrice1958Bonnet1964ﬁrstusedformachinelearninginthecontextofreinforcementlearning(,Williams1992). Morerecently,ithasbeenappliedtovariationalapproximations(OpperandArchambeau2009,)andstochasticorgenerativeneuralnetworks(Bengioetal.,;,;2013bKingma2013KingmaandWelling2014baRezende2014,,;etal.,;Goodfellow2014cetal.,).Manynetworks,suchasdenoisingautoencodersornetworksregularized withdropout, are alsonaturally designedto take noiseasaninputwithoutrequiringanyspecialreparametrizationtomakethenoiseindependentfromthemodel.20.9.1Back-PropagatingthroughDiscreteStochasticOperationsWhenamodelemitsadiscretevariabley,thereparametrizationtrickisnotapplicable.Suppose thatthemodel takesinputsxandparametersθ, bothencapsulatedinthevectorω,andcombinesthemwithrandomnoiseztoproducey:yzω= (f;).(20.58)Becauseyisdiscrete,fmustbeastepfunction.Thederivativesofastepfunctionarenotusefulatanypoint.Rightateachstepboundary,thederivativesareundeﬁned,butthatisasmallproblem.Thelargeproblemisthatthederivativesarezeroalmosteverywhere,ontheregionsbetweenstepboundaries.ThederivativesofanycostfunctionJ(y)thereforedonotgiveanyinformationforhowtoupdatethemodelparameters.θTheREINFORCEalgorithm(REwardIncrement=Non-negativeFactor×OﬀsetReinforcement×CharacteristicEligibility)providesaframeworkdeﬁningafamilyofsimplebutpowerfulsolutions(,). ThecoreideaisthatWilliams1992eventhoughJ(f(z;ω))isastepfunctionwithuselessderivatives,theexpectedcostEzz∼p()Jf((;))zωisoftenasmoothfunctionamenabletogradientdescent.Althoughthatexpectationistypicallynottractablewhenyishigh-dimensional(oristheresultofthecompositionofmanydiscretestochasticdecisions),itcanbeestimatedwithoutbiasusingaMonteCarloaverage.ThestochasticestimateofthegradientcanbeusedwithSGDorotherstochasticgradient-basedoptimizationtechniques.689 CHAPTER20.DEEPGENERATIVEMODELSThesimplestversionofREINFORCEcanbederivedbysimplydiﬀerentiatingtheexpectedcost:Ez[()] =JyyJp()y()y(20.59)∂JE[()]y∂ω=yJ()y∂p()y∂ω(20.60)=yJp()y()y∂plog()y∂ω(20.61)≈1mmy()i∼p,i()y=1J(y()i)∂plog(y()i)∂ω.(20.62)Equationreliesontheassumptionthat20.60Jdoesnotreferenceωdirectly.Itistrivialtoextendtheapproachtorelaxthisassumption.Equationexploits20.61thederivativeruleforthelogarithm,∂plog()y∂ω=1p()y∂p()y∂ω.Equationgives20.62anunbiasedMonteCarloestimatorofthegradient.Anywherewewritep(y)inthissection,onecouldequallywritep(yx|).Thisisbecausep(y)isparametrizedbyω,andωcontainsbothθandx,ifxispresent.OneissuewiththeabovesimpleREINFORCEestimatoristhatithasaveryhighvariance,sothatmanysamplesofyneedtobedrawntoobtainagoodestimatorofthegradient,orequivalently,ifonlyonesampleisdrawn,SGDwillconvergeveryslowlyandwillrequireasmallerlearningrate.Itispossibletoconsiderablyreducethevarianceofthatestimatorbyusingvariancereductionmethods(,;,).TheideaistomodifytheestimatorsoWilson1984L’Ecuyer1994thatitsexpectedvalueremainsunchangedbutitsvariancegetreduced. InthecontextofREINFORCE,theproposedvariancereductionmethodsinvolvethecomputationofabaselinethatisusedtooﬀsetJ(y).Notethatanyoﬀsetb(ω)thatdoesnotdependonywouldnotchangetheexpectationoftheestimatedgradientbecauseEp()y∂plog()y∂ω=yp()y∂plog()y∂ω(20.63)=y∂p()y∂ω(20.64)=∂∂ωyp() =y∂∂ω1 = 0,(20.65)690 CHAPTER20.DEEPGENERATIVEMODELSwhichmeansthatEp()y(()())Jy−bω∂plog()y∂ω= Ep()yJ()y∂plog()y∂ω−bE()ωp()y∂plog()y∂ω(20.66)= Ep()yJ()y∂plog()y∂ω.(20.67)Furthermore,wecanobtaintheoptimalb(ω) bycomputingthevarianceof(J(y)−b(ω))∂plog()y∂ωunderp(y)andminimizingwithrespecttob(ω).Whatweﬁndisthatthisoptimalbaselineb∗()ωiisdiﬀerentforeachelementωiofthevector:ωb∗()ωi=Ep()yJ()y∂plog()y∂ωi2Ep()y∂plog()y∂ωi2.(20.68)Thegradientestimatorwithrespecttoωithenbecomes(()()Jy−bωi)∂plog()y∂ωi(20.69)whereb(ω)iestimatestheaboveb∗(ω)i.TheestimatebisusuallyobtainedbyaddingextraoutputstotheneuralnetworkandtrainingthenewoutputstoestimateEp()y[J(y)∂plog()y∂ωi2]andEp()y∂plog()y∂ωi2foreachelementofω.Theseextraoutputscanbetrainedwiththemeansquarederrorobjective,usingrespectivelyJ(y)∂plog()y∂ωi2and∂plog()y∂ωi2astargetswhenyissampledfromp(y),foragivenω.Theestimatebmaythenberecoveredbysubstitutingtheseestimatesintoequation.()preferredtouseasinglesharedoutput20.68MnihandGregor2014(acrossallelementsiofω)trainedwiththetargetJ(y),usingasbaselineb(ω)≈Ep()y[()]Jy.Variancereductionmethodshavebeenintroducedinthereinforcementlearningcontext(,;Suttonetal.2000WeaverandTao2001,),generalizingpreviousworkonthecaseofbinaryrewardbyDayan1990Bengio2013bMnih(). Seeetal.(),andGregor2014Ba2014Mnih2014Xu2015(),etal.(),etal.(),oretal.()forexamplesofmodernusesoftheREINFORCEalgorithmwithreducedvarianceinthecontextofdeeplearning.Inadditiontotheuseofaninput-dependentbaselineb(ω)(,()foundthatthescaleofMnihandGregor2014J(y)−b(ω))couldbeadjustedduringtrainingbydividingitbyitsstandarddeviationestimatedbyamovingaverageduringtraining,asakindofadaptivelearningrate,tocountertheeﬀectofimportantvariationsthatoccurduringthecourseoftraininginthe691 CHAPTER20.DEEPGENERATIVEMODELSmagnitudeofthisquantity.()calledthisheuristicMnihandGregor2014variancenormalization.REINFORCE-basedestimatorscanbeunderstoodasestimatingthegradientbycorrelatingchoicesofywithcorrespondingvaluesofJ(y).Ifagoodvalueofyisunlikelyunderthecurrentparametrization,itmighttakealongtimetoobtainitbychance,andgettherequiredsignalthatthisconﬁgurationshouldbereinforced.20.10DirectedGenerativeNetsAsdiscussedinchapter,directedgraphicalmodelsmakeupaprominentclass16ofgraphicalmodels.Whiledirectedgraphicalmodelshavebeenverypopularwithinthegreatermachinelearningcommunity,withinthesmallerdeeplearningcommunitytheyhaveuntilroughly2013beenovershadowedbyundirectedmodelssuchastheRBM.Inthissectionwereviewsomeofthestandarddirectedgraphicalmodelsthathavetraditionallybeenassociatedwiththedeeplearningcommunity.Wehavealreadydescribeddeepbeliefnetworks,whichareapartiallydirectedmodel.Wehavealsoalreadydescribedsparsecodingmodels,whichcanbethoughtofasshallowdirectedgenerativemodels.Theyareoftenusedasfeaturelearnersinthecontextofdeeplearning,thoughtheytendtoperformpoorlyatsamplegenerationanddensityestimation.Wenowdescribeavarietyofdeep,fullydirectedmodels.20.10.1SigmoidBeliefNetsSigmoidbeliefnetworks(,)areasimpleformofdirectedgraphicalmodelNeal1990withaspeciﬁckindofconditionalprobabilitydistribution.Ingeneral,wecanthinkofasigmoidbeliefnetworkashavingavectorofbinarystatess,witheachelementofthestateinﬂuencedbyitsancestors:ps(i) = σj<iWj,isj+bi.(20.70)Themostcommonstructureofsigmoidbeliefnetworkisonethatisdividedintomanylayers,withancestralsamplingproceedingthroughaseriesofmanyhiddenlayersandthenultimatelygeneratingthevisiblelayer.Thisstructureisverysimilartothedeepbeliefnetwork,exceptthattheunitsatthebeginningof692 CHAPTER20.DEEPGENERATIVEMODELSthesamplingprocessareindependentfromeachother,ratherthansampledfromarestrictedBoltzmannmachine. Suchastructureisinterestingforavarietyofreasons.Onereasonisthatthestructureisauniversalapproximatorofprobabilitydistributionsoverthevisibleunits,inthesensethatitcanapproximateanyprobabilitydistributionoverbinaryvariablesarbitrarilywell,givenenoughdepth,evenifthewidthoftheindividuallayersisrestrictedtothedimensionalityofthevisiblelayer(SutskeverandHinton2008,).Whilegeneratingasampleofthevisibleunitsisveryeﬃcientinasigmoidbeliefnetwork,mostotheroperationsarenot.Inferenceoverthehiddenunitsgiventhevisibleunitsisintractable.Meanﬁeldinferenceisalsointractablebecausethevariationallowerboundinvolvestakingexpectationsofcliquesthatencompassentirelayers.Thisproblemhasremaineddiﬃcultenoughtorestrictthepopularityofdirecteddiscretenetworks.Oneapproachforperforminginferenceinasigmoidbeliefnetworkistoconstructadiﬀerentlowerboundthatisspecializedforsigmoidbeliefnetworks(,Sauletal.1996).Thisapproachhasonlybeenappliedtoverysmallnetworks.Anotherapproachistouselearnedinferencemechanismsasdescribedinsection.The19.5Helmholtzmachine(Dayan1995DayanandHinton1996etal.,;,)isasigmoidbeliefnetworkcombinedwithaninferencenetworkthatpredictstheparametersofthemeanﬁelddistributionoverthehiddenunits.Modernapproaches(,Gregoretal.2014MnihandGregor2014;,)tosigmoidbeliefnetworksstillusethisinferencenetworkapproach.Thesetechniquesremaindiﬃcultduetothediscretenatureofthelatentvariables.Onecannotsimplyback-propagatethroughtheoutputoftheinferencenetwork,butinsteadmustusetherelativelyunreliablemachineryforback-propagatingthroughdiscretesamplingprocesses,describedinsection.Recent20.9.1approachesbasedonimportancesampling,reweightedwake-sleep(BornscheinandBengio2015Bornschein2015,)andbidirectionalHelmholtzmachines(etal.,)makeitpossibletoquicklytrainsigmoidbeliefnetworksandreachstate-of-the-artperformanceonbenchmarktasks.Aspecialcaseofsigmoidbeliefnetworksisthecasewheretherearenolatentvariables.Learninginthiscaseiseﬃcient,becausethereisnoneedtomarginalizelatentvariablesoutofthelikelihood. Afamilyofmodelscalledauto-regressivenetworksgeneralizethisfullyvisiblebeliefnetworktootherkindsofvariablesbesidesbinaryvariablesandotherstructuresofconditionaldistributionsbesideslog-linearrelationships.Auto-regressivenetworksaredescribedlater,insection.20.10.7693 CHAPTER20.DEEPGENERATIVEMODELS20.10.2DiﬀerentiableGeneratorNetsManygenerativemodelsarebasedontheideaofusingadiﬀerentiablegeneratornetwork.Themodeltransformssamplesoflatentvariablesztosamplesxortodistributionsoversamplesxusingadiﬀerentiablefunctiong(z;θ()g)whichistypicallyrepresentedbyaneuralnetwork.Thismodelclassincludesvariationalautoencoders, whichpair thegeneratornetwithaninferencenet,generativeadversarial networks, which pairthe generator network witha discriminatornetwork,andtechniquesthattraingeneratornetworksinisolation.Generatornetworksareessentiallyjustparametrizedcomputationalproceduresforgeneratingsamples,wherethearchitectureprovidesthefamilyofpossibledistributionstosamplefromandtheparametersselectadistributionfromwithinthatfamily.Asanexample,thestandardprocedurefordrawingsamplesfromanormaldistributionwithmeanµandcovarianceΣistofeedsampleszfromanormaldistributionwithzeromeanandidentitycovarianceintoaverysimplegeneratornetwork.Thisgeneratornetworkcontainsjustoneaﬃnelayer:xzLz= (g) = +µ(20.71)whereisgivenbytheCholeskydecompositionof.LΣPseudorandomnumbergeneratorscanalsousenonlineartransformationsofsimpledistributions.Forexample,inversetransformsampling(Devroye2013,)drawsascalarzfromU(0,1)andappliesanonlineartransformationtoascalarx.Inthiscaseg(z)isgivenbytheinverseofthecumulativedistributionfunctionF(x) =x−∞p(v)dv.Ifweareabletospecifyp(x),integrateoverx,andinverttheresultingfunction,wecansamplefromwithoutusingmachinelearning.px()Togeneratesamplesfrommorecomplicateddistributionsthatarediﬃculttospecifydirectly, diﬃculttointegrateover, orwhoseresultingintegralsarediﬃculttoinvert,weuseafeedforwardnetworktorepresentaparametricfamilyofnonlinearfunctionsg,andusetrainingdatatoinfertheparametersselectingthedesiredfunction.Wecanthinkofgasprovidinganonlinearchangeofvariablesthattransformsthedistributionoverintothedesireddistributionover.z xRecallfromequationthat,forinvertible,diﬀerentiable,continuous,3.47gpz() = zpx(())gzdet(∂g∂z).(20.72)694 CHAPTER20.DEEPGENERATIVEMODELSThisimplicitlyimposesaprobabilitydistributionover:xpx() =xpz(g−1())xdet(∂g∂z).(20.73)Ofcourse,thisformulamaybediﬃculttoevaluate,dependingonthechoiceofg,soweoftenuseindirectmeansoflearningg,ratherthantryingtomaximizelog()pxdirectly.Insomecases,ratherthanusinggtoprovideasampleofxdirectly,weusegtodeﬁneaconditionaldistributionoverx.Forexample,wecoulduseageneratornetwhoseﬁnallayerconsistsofsigmoidoutputstoprovidethemeanparametersofBernoullidistributions:p(xi= 1 ) = ()|zgzi.(20.74)Inthiscase,whenweusegtodeﬁnep(xz|),weimposeadistributionoverxbymarginalizing:zp() = xEzp.()xz|(20.75)Bothapproachesdeﬁneadistributionpg(x)andallowustotrainvariouscriteriaofpgusingthereparametrizationtrickofsection.20.9Thetwodiﬀerentapproachestoformulatinggeneratornets—emittingtheparametersofaconditionaldistributionversusdirectlyemittingsamples—havecomplementarystrengthsandweaknesses.Whenthegeneratornetdeﬁnesaconditionaldistributionoverx,itiscapableofgeneratingdiscretedataaswellascontinuousdata.Whenthegeneratornetprovidessamplesdirectly,itiscapableofgeneratingonlycontinuousdata(wecouldintroducediscretizationintheforwardpropagation,butdoingsowouldmeanthemodelcouldnolongerbetrainedusingback-propagation).Theadvantagetodirectsamplingisthatwearenolongerforcedtouseconditionaldistributionswhoseformcanbeeasilywrittendownandalgebraicallymanipulatedbyahumandesigner.Approachesbasedondiﬀerentiablegeneratornetworksaremotivatedbythesuccessof gradient descentappliedtodiﬀerentiablefeedforwardnetworksforclassiﬁcation. Inthecontextofsupervisedlearning,deepfeedforwardnetworkstrainedwithgradient-basedlearningseempracticallyguaranteedtosucceedgivenenoughhiddenunitsandenoughtrainingdata.Canthissamerecipeforsuccesstransfertogenerativemodeling?Generativemodelingseemstobemorediﬃcultthanclassiﬁcationorregressionbecausethelearningprocessrequiresoptimizingintractablecriteria.Inthecontext695 CHAPTER20.DEEPGENERATIVEMODELSofdiﬀerentiablegeneratornets,thecriteriaareintractablebecausethedatadoesnotspecifyboththeinputszandtheoutputsxofthegeneratornet.Inthecaseofsupervisedlearning,boththeinputsxandtheoutputsyweregiven,andtheoptimizationprocedureneedsonlytolearnhowtoproducethespeciﬁedmapping.Inthecaseofgenerativemodeling,thelearningprocedureneedstodeterminehowtoarrangespaceinausefulwayandadditionallyhowtomapfromto.zzxDosovitskiy2015etal.()studiedasimpliﬁedproblem,wherethecorrespondencebetweenzandxisgiven.Speciﬁcally,thetrainingdataiscomputer-renderedimageryofchairs.Thelatentvariableszareparametersgiventotherenderingenginedescribingthechoiceofwhichchairmodeltouse,thepositionofthechair,andotherconﬁgurationdetailsthataﬀecttherenderingoftheimage.Usingthissyntheticallygenerateddata,aconvolutionalnetworkisabletolearntomapzdescriptionsofthecontentofanimagetoxapproximationsofrenderedimages.Thissuggeststhatcontemporarydiﬀerentiablegeneratornetworkshavesuﬃcientmodelcapacitytobegoodgenerativemodels,andthatcontemporaryoptimizationalgorithmshavetheabilitytoﬁtthem.Thediﬃcultyliesindetermininghowtotraingeneratornetworkswhenthevalueofzforeachxisnotﬁxedandknownaheadofeachtime.Thefollowingsectionsdescribeseveralapproachestotrainingdiﬀerentiablegeneratornetsgivenonlytrainingsamplesof.x20.10.3VariationalAutoencodersThevariationalautoencoderorVAE(,;,)isaKingma2013Rezendeetal.2014directedmodelthatuseslearnedapproximateinferenceandcanbetrainedpurelywithgradient-basedmethods.Togenerateasamplefromthemodel,theVAEﬁrstdrawsasamplezfromthecodedistributionpmodel(z).Thesampleisthenrunthroughadiﬀerentiablegeneratornetworkg(z).Finally,xissampledfromadistributionpmodel(x;g(z)) =pmodel(xz|). However,duringtraining,theapproximateinferencenetwork(orencoder)q(zx|)isusedtoobtainzandpmodel(xz|)isthenviewedasadecodernetwork.Thekeyinsightbehindvariationalautoencodersisthattheymaybetrainedbymaximizingthevariationallowerboundassociatedwithdatapoint:L()qxL() = qEzzx∼q(|)logpmodel()+(())zx,Hqz|x(20.76)= Ezzx∼q(|)logpmodel()xz|−DKL(()qz|x||pmodel())z(20.77)≤logpmodel()x.(20.78)696 CHAPTER20.DEEPGENERATIVEMODELSInequation,werecognizetheﬁrsttermasthejointlog-likelihoodofthevisible20.76andhiddenvariablesundertheapproximateposterioroverthelatentvariables(justlikewithEM,exceptthatweuseanapproximateratherthantheexactposterior).Werecognizealsoasecondterm,theentropyoftheapproximateposterior. WhenqischosentobeaGaussiandistribution,withnoiseaddedtoapredictedmeanvalue,maximizingthisentropytermencouragesincreasingthestandarddeviationofthisnoise.Moregenerally,thisentropytermencouragesthevariationalposteriortoplacehighprobabilitymassonmanyzvaluesthatcouldhavegeneratedx,ratherthancollapsingtoasinglepointestimateofthemostlikelyvalue.Inequation,werecognizetheﬁrsttermasthereconstruction20.77log-likelihoodfoundinotherautoencoders.Thesecondtermtriestomaketheapproximateposteriordistributionq(z|x) andthemodelpriorpmodel(z) approacheachother.Traditionalapproachestovariationalinferenceandlearninginferqviaanopti-mizationalgorithm,typicallyiteratedﬁxedpointequations(section).These19.4approachesareslowandoftenrequiretheabilitytocomputeEz∼qlogpmodel(zx,)inclosedform.Themainideabehindthevariationalautoencoderistotrainaparametricencoder(alsosometimescalledaninferencenetworkorrecognitionmodel)thatproducestheparametersofq.Solongaszisacontinuousvariable,wecanthenback-propagatethroughsamplesofzdrawnfromq(zx|) =q(z;f(x;θ))inordertoobtainagradientwithrespecttoθ.LearningthenconsistssolelyofmaximizingLwithrespecttotheparametersoftheencoderanddecoder.AlloftheexpectationsinmaybeapproximatedbyMonteCarlosampling.LThevariationalautoencoderapproachiselegant,theoreticallypleasing,andsimpletoimplement.Italsoobtainsexcellentresultsandisamongthestateoftheartapproachestogenerativemodeling.Itsmaindrawbackisthatsamplesfromvariationalautoencoderstrainedonimagestendtobesomewhatblurry.Thecausesofthisphenomenonarenotyetknown.Onepossibilityisthattheblurrinessisanintrinsiceﬀectofmaximumlikelihood,whichminimizesDKL(pdatapmodel).Asillustratedinﬁgure,thismeansthatthemodelwillassignhighprobabilityto3.6pointsthatoccurinthetrainingset,butmayalsoassignhighprobabilitytootherpoints.Theseotherpointsmayincludeblurryimages.PartofthereasonthatthemodelwouldchoosetoputprobabilitymassonblurryimagesratherthansomeotherpartofthespaceisthatthevariationalautoencodersusedinpracticeusuallyhaveaGaussiandistributionforpmodel(x;g(z)). Maximizingalowerboundonthelikelihoodofsuchadistributionissimilartotrainingatraditionalautoencoderwithmeansquarederror,inthesensethatithasatendencytoignorefeaturesoftheinputthatoccupyfewpixelsorthatcauseonlyasmallchangeinthebrightnessofthepixelsthattheyoccupy.ThisissueisnotspeciﬁctoVAEsand697 CHAPTER20.DEEPGENERATIVEMODELSissharedwithgenerativemodelsthatoptimizealog-likelihood,orequivalently,DKL(pdatapmodel),asarguedby()andby().AnotherTheisetal.2015Huszar2015troublingissuewithcontemporaryVAEmodelsisthattheytendtouseonlyasmallsubsetofthedimensionsofz,asiftheencoderwasnotabletotransformenoughofthelocaldirectionsininputspacetoaspacewherethemarginaldistributionmatchesthefactorizedprior.TheVAEframeworkisverystraightforwardtoextendtoawiderangeofmodelarchitectures.ThisisakeyadvantageoverBoltzmannmachines,whichrequireextremelycarefulmodeldesigntomaintaintractability.VAEsworkverywellwithadiversefamilyofdiﬀerentiableoperators.OneparticularlysophisticatedVAEisthedeeprecurrentattentionwriterorDRAWmodel(,).Gregoretal.2015DRAWusesarecurrentencoderandrecurrentdecodercombinedwithanattentionmechanism.ThegenerationprocessfortheDRAWmodelconsistsofsequentiallyvisitingdiﬀerentsmallimagepatchesanddrawingthevaluesofthepixelsatthosepoints.VAEscanalsobeextendedtogeneratesequencesbydeﬁningvariationalRNNs(,)byusingarecurrentencoderanddecoderwithinChungetal.2015btheVAEframework.GeneratingasamplefromatraditionalRNNinvolvesonlynon-deterministicoperationsattheoutputspace.VariationalRNNsalsohaverandomvariabilityatthepotentiallymoreabstractlevelcapturedbytheVAElatentvariables.TheVAEframeworkhasbeenextendedtomaximizenotjustthetraditionalvariationallowerbound,butinsteadtheimportanceweightedautoencoder(,)objective:Burdaetal.2015Lk() = x,qEz(1),...,z()k∼|q(zx)log1kki=1pmodel(xz,()i)q(z()i|x).(20.79)ThisnewobjectiveisequivalenttothetraditionallowerboundLwhenk=1.However,itmayalsobeinterpretedasforminganestimateofthetruelogpmodel(x)usingimportancesamplingofzfromproposaldistributionq(zx|).Theimportanceweightedautoencoderobjectiveisalsoalowerboundonlogpmodel(x) andbecomestighterasincreases.kVariationalautoencodershavesomeinterestingconnectionstotheMP-DBMandotherapproachesthatinvolveback-propagationthroughtheapproximateinferencegraph(Goodfellow2013bStoyanov2011Brakel2013etal.,;etal.,;etal.,).Thesepreviousapproachesrequiredaninferenceproceduresuchasmeanﬁeldﬁxedpointequationstoprovidethecomputationalgraph.Thevariationalautoencoderisdeﬁnedforarbitrarycomputationalgraphs,whichmakesitapplicabletoawiderrangeofprobabilisticmodelfamiliesbecausethereisnoneedtorestrictthechoice698 CHAPTER20.DEEPGENERATIVEMODELSofmodelstothosewithtractablemeanﬁeldﬁxedpointequations.Thevariationalautoencoderalsohastheadvantagethatitincreasesaboundonthelog-likelihoodofthemodel,whilethecriteriafortheMP-DBMandrelatedmodelsaremoreheuristicandhavelittleprobabilisticinterpretationbeyondmakingtheresultsofapproximateinferenceaccurate.Onedisadvantageofthevariationalautoencoderisthatitlearnsaninferencenetworkforonlyoneproblem,inferringzgivenx.Theoldermethodsareabletoperformapproximateinferenceoveranysubsetofvariablesgivenanyothersubsetofvariables,becausethemeanﬁeldﬁxedpointequationsspecifyhowtoshareparametersbetweenthecomputationalgraphsforallofthesediﬀerentproblems.Oneverynicepropertyofthevariationalautoencoderisthatsimultaneouslytrainingaparametricencoderincombinationwiththegeneratornetworkforcesthemodeltolearnapredictablecoordinatesystemthattheencodercancapture.Thismakesitanexcellentmanifoldlearningalgorithm.Seeﬁgureforexamplesof20.6low-dimensionalmanifoldslearnedbythevariationalautoencoder.Inoneofthecasesdemonstratedintheﬁgure,thealgorithmdiscoveredtwoindependentfactorsofvariationpresentinimagesoffaces:angleofrotationandemotionalexpression.20.10.4GenerativeAdversarialNetworksGenerativeadversarialnetworksorGANs(,)areanotherGoodfellowetal.2014cgenerativemodelingapproachbasedondiﬀerentiablegeneratornetworks.Generativeadversarialnetworksarebasedonagametheoreticscenarioinwhichthegeneratornetworkmustcompeteagainstanadversary.Thegeneratornetworkdirectlyproducessamplesx=g(z;θ()g).Itsadversary,thediscriminatornetwork,attemptstodistinguishbetweensamplesdrawnfromthetrainingdataandsamplesdrawnfromthegenerator.Thediscriminatoremitsaprobabilityvaluegivenbyd(x;θ()d),indicatingtheprobabilitythatxisarealtrainingexampleratherthanafakesampledrawnfromthemodel.Thesimplestwaytoformulatelearningingenerativeadversarialnetworksisasazero-sumgame,inwhichafunctionv(θ()g,θ()d)determinesthepayoﬀofthediscriminator.Thegeneratorreceives−v(θ()g,θ()d)asitsownpayoﬀ.Duringlearning,eachplayerattemptstomaximizeitsownpayoﬀ,sothatatconvergenceg∗= argmingmaxdvg,d.()(20.80)Thedefaultchoiceforisvv(θ()g,θ()d) = Ex∼pdatalog()+dxEx∼pmodellog(1())−dx.(20.81)699 CHAPTER20.DEEPGENERATIVEMODELS Figure20.6:Examplesoftwo-dimensionalcoordinatesystemsforhigh-dimensionalmani-folds,learnedbyavariationalautoencoder(KingmaandWelling2014a,).Twodimensionsmaybeplotteddirectlyonthepageforvisualization,sowecangainanunderstandingofhowthemodelworksbytrainingamodelwitha2-Dlatentcode,evenifwebelievetheintrinsicdimensionalityofthedatamanifoldismuchhigher.Theimagesshownarenotexamplesfromthetrainingsetbutimagesxactuallygeneratedbythemodelp(xz|),simplybychangingthe2-D“code”z(eachimagecorrespondstoadiﬀerentchoiceof“code”zona2-Duniformgrid).(Left)Thetwo-dimensionalmapoftheFreyfacesmanifold.Onedimensionthathasbeendiscovered(horizontal)mostlycorrespondstoarotationoftheface,whiletheother(vertical)correspondstotheemotionalexpression.The(Right)two-dimensionalmapoftheMNISTmanifold.Thisdrivesthediscriminatortoattempttolearntocorrectlyclassifysamplesasrealorfake.Simultaneously,thegeneratorattemptstofooltheclassiﬁerintobelievingitssamplesarereal.Atconvergence,thegenerator’ssamplesareindistinguishablefromrealdata,andthediscriminatoroutputs12everywhere.Thediscriminatormaythenbediscarded.ThemainmotivationforthedesignofGANsisthatthelearningprocessrequiresneitherapproximateinferencenorapproximationofapartitionfunctiongradient.Inthecasewheremaxdv(g,d)isconvexinθ()g(suchasthecasewhereoptimizationisperformeddirectlyinthespaceofprobabilitydensityfunctions)theprocedureisguaranteedtoconvergeandisasymptoticallyconsistent.Unfortunately,learninginGANscanbediﬃcultinpracticewhenganddarerepresentedbyneuralnetworksandmaxdv(g,d)isnotconvex.Goodfellow700 CHAPTER20.DEEPGENERATIVEMODELS()identiﬁednon-convergenceasanissuethatmaycauseGANstounderﬁt.2014Ingeneral,simultaneousgradientdescentontwoplayers’costsisnotguaranteedtoreachanequilibrium.Considerforexamplethevaluefunctionv(a,b)=ab,whereoneplayercontrolsaandincurscostab,whiletheotherplayercontrolsbandreceivesacost−ab.Ifwemodeleachplayerasmakinginﬁnitesimallysmallgradientsteps,eachplayerreducingtheirowncostattheexpenseoftheotherplayer,thenaandbgointoastable,circularorbit,ratherthanarrivingattheequilibriumpointattheorigin.Notethattheequilibriaforaminimaxgamearenotlocalminimaofv.Instead,theyarepointsthataresimultaneouslyminimaforbothplayers’costs.Thismeansthattheyaresaddlepointsofvthatarelocalminimawithrespecttotheﬁrstplayer’sparametersandlocalmaximawithrespecttothesecondplayer’sparameters.Itispossibleforthetwoplayerstotaketurnsincreasingthendecreasingvforever,ratherthanlandingexactlyonthesaddlepointwhereneitherplayeriscapableofreducingitscost.Itisnotknowntowhatextentthisnon-convergenceproblemaﬀectsGANs.Goodfellow2014()identiﬁedanalternativeformulationofthepayoﬀs,inwhichthegameisnolongerzero-sum,thathasthesameexpectedgradientasmaximumlikelihoodlearningwheneverthediscriminatorisoptimal.Becausemaximumlikelihoodtrainingconverges,thisreformulationoftheGANgameshouldalsoconverge,givenenoughsamples.Unfortunately,thisalternativeformulationdoesnotseemtoimproveconvergenceinpractice,possiblyduetosuboptimalityofthediscriminator,orpossiblyduetohighvariancearoundtheexpectedgradient.Inrealisticexperiments,thebest-performingformulationoftheGANgameisadiﬀerentformulationthatisneitherzero-sumnorequivalenttomaximumlikelihood,introducedby()withaheuristicmotivation.InGoodfellowetal.2014cthisbest-performingformulation,thegeneratoraimstoincreasethelogprobabilitythatthediscriminatormakesamistake,ratherthanaimingtodecreasethelogprobabilitythatthediscriminatormakesthecorrectprediction.Thisreformulationismotivatedsolelybytheobservationthatitcausesthederivativeofthegenerator’scostfunctionwithrespecttothediscriminator’slogitstoremainlargeeveninthesituationwherethediscriminatorconﬁdentlyrejectsallgeneratorsamples.StabilizationofGANlearningremainsanopenproblem. Fortunately,GANlearningperformswellwhenthemodelarchitectureandhyperparametersarecare-fullyselected.()craftedadeepconvolutionalGAN(DCGAN)Radfordetal.2015thatperformsverywellforimagesynthesistasks,andshowedthatitslatentrepre-sentationspacecapturesimportantfactorsofvariation,asshowninﬁgure.15.9SeeﬁgureforexamplesofimagesgeneratedbyaDCGANgenerator.20.7TheGANlearningproblemcanalsobesimpliﬁedbybreakingthegeneration701 CHAPTER20.DEEPGENERATIVEMODELS Figure20.7:ImagesgeneratedbyGANstrainedontheLSUNdataset.(Left)ImagesofbedroomsgeneratedbyaDCGANmodel,reproducedwithpermissionfromRadfordetal.().ImagesofchurchesgeneratedbyaLAPGANmodel,reproducedwith2015(Right)permissionfrom().Dentonetal.2015processintomanylevelsofdetail.ItispossibletotrainconditionalGANs(MirzaandOsindero2014,)thatlearntosamplefromadistributionp(xy|)ratherthansimplysamplingfromamarginaldistributionp(x).()Dentonetal.2015showedthataseriesofconditionalGANscanbetrainedtoﬁrstgenerateaverylow-resolutionversionofanimage,thenincrementallyadddetailstotheimage.ThistechniqueiscalledtheLAPGANmodel,duetotheuseofaLaplacianpyramidtogeneratetheimagescontainingvaryinglevelsofdetail.LAPGANgeneratorsareabletofoolnotonlydiscriminatornetworksbutalsohumanobservers,withexperimentalsubjectsidentifyingupto40%oftheoutputsofthenetworkasbeingrealdata.SeeﬁgureforexamplesofimagesgeneratedbyaLAPGAN20.7generator.OneunusualcapabilityoftheGANtrainingprocedureisthatitcanﬁtproba-bilitydistributionsthatassignzeroprobabilitytothetrainingpoints.Ratherthanmaximizingthelogprobabilityofspeciﬁcpoints,thegeneratornetlearnstotraceoutamanifoldwhosepointsresembletrainingpointsinsomeway.Somewhatpara-doxically,thismeansthatthemodelmayassignalog-likelihoodofnegativeinﬁnitytothetestset,whilestillrepresentingamanifoldthatahumanobserverjudgestocapturetheessenceofthegenerationtask.Thisisnotclearlyanadvantageoradisadvantage,andonemayalsoguaranteethatthegeneratornetworkassignsnon-zeroprobabilitytoallpointssimplybymakingthelastlayerofthegeneratornetworkaddGaussiannoisetoallofthegeneratedvalues. GeneratornetworksthataddGaussiannoiseinthismannersamplefromthesamedistributionthatoneobtainsbyusingthegeneratornetworktoparametrizethemeanofaconditional702 CHAPTER20.DEEPGENERATIVEMODELSGaussiandistribution.Dropoutseemstobeimportantinthediscriminatornetwork. Inparticular,units shouldbe stochasticallydropped whilecomputingthe gradientfor thegeneratornetworktofollow.Followingthegradientofthedeterministicversionofthediscriminatorwithitsweightsdividedbytwodoesnotseemtobeaseﬀective.Likewise,neverusingdropoutseemstoyieldpoorresults.WhiletheGANframeworkisdesignedfordiﬀerentiablegeneratornetworks,similarprinciplescanbeusedtotrainotherkindsofmodels.Forexample,self-supervisedboostingcanbeusedtotrainanRBMgeneratortofoolalogisticregressiondiscriminator(Welling2002etal.,).20.10.5GenerativeMomentMatchingNetworksGenerativemomentmatchingnetworks(,;,Lietal.2015Dziugaiteetal.2015)areanotherformofgenerativemodelbasedondiﬀerentiablegeneratornetworks.UnlikeVAEsandGANs,theydonotneedtopairthegeneratornetworkwithanyothernetwork—neitheraninferencenetworkasusedwithVAEsnoradiscriminatornetworkasusedwithGANs.Thesenetworksaretrainedwithatechniquecalledmomentmatching.Thebasicideabehindmomentmatchingistotrainthegeneratorinsuchawaythatmanyofthestatisticsofsamplesgeneratedbythemodelareassimilaraspossibletothoseofthestatisticsoftheexamplesinthetrainingset.Inthiscontext,amomentisanexpectationofdiﬀerentpowersofarandomvariable.Forexample,theﬁrstmomentisthemean,thesecondmomentisthemeanofthesquaredvalues,andsoon.Inmultipledimensions,eachelementoftherandomvectormayberaisedtodiﬀerentpowers,sothatamomentmaybeanyquantityoftheformExΠixnii(20.82)wheren= [n1,n2,...,nd]isavectorofnon-negativeintegers.Uponﬁrstexamination,thisapproachseemstobecomputationallyinfeasible.Forexample,ifwewanttomatchallthemomentsoftheformxixj,thenweneedtominimizethediﬀerencebetweenanumberofvaluesthatisquadraticinthedimensionofx.Moreover,evenmatchingalloftheﬁrstandsecondmomentswouldonlybesuﬃcienttoﬁtamultivariateGaussiandistribution,whichcapturesonlylinearrelationshipsbetweenvalues.Ourambitionsforneuralnetworksaretocapturecomplexnonlinearrelationships,whichwouldrequirefarmoremoments.GANsavoidthisproblemofexhaustivelyenumeratingallmomentsbyusinga703 CHAPTER20.DEEPGENERATIVEMODELSdynamicallyupdateddiscriminatorthatautomaticallyfocusesitsattentiononwhicheverstatisticthegeneratornetworkismatchingtheleasteﬀectively.Instead,generativemomentmatchingnetworkscanbetrainedbyminimizingacostfunctioncalledmaximummeandiscrepancy(SchölkopfandSmola,2002Gretton2012;etal.,)orMMD.Thiscostfunctionmeasurestheerrorintheﬁrstmomentsinaninﬁnite-dimensionalspace,usinganimplicitmappingtofeaturespacedeﬁnedbyakernelfunctioninordertomakecomputationsoninﬁnite-dimensionalvectorstractable.TheMMDcostiszeroifandonlyifthetwodistributionsbeingcomparedareequal.Visually,thesamplesfromgenerativemomentmatchingnetworksaresomewhatdisappointing.Fortunately,theycanbeimprovedbycombiningthegeneratornetworkwithanautoencoder.First,anautoencoderistrainedtoreconstructthetrainingset.Next,theencoderoftheautoencoderisusedtotransformtheentiretrainingsetintocodespace.Thegeneratornetworkisthentrainedtogeneratecodesamples,whichmaybemappedtovisuallypleasingsamplesviathedecoder.UnlikeGANs,thecostfunctionisdeﬁnedonlywithrespecttoabatchofexamplesfromboththetrainingsetandthegeneratornetwork.Itisnotpossibletomakeatrainingupdateasafunctionofonlyonetrainingexampleoronlyonesamplefromthegeneratornetwork.Thisisbecausethemomentsmustbecomputedasanempiricalaverageacrossmanysamples.Whenthebatchsizeistoosmall,MMDcanunderestimatethetrueamountofvariationinthedistributionsbeingsampled.Noﬁnitebatchsizeissuﬃcientlylargetoeliminatethisproblementirely,butlargerbatchesreducetheamountofunderestimation.Whenthebatchsizeistoolarge,thetrainingprocedurebecomesinfeasiblyslow,becausemanyexamplesmustbeprocessedinordertocomputeasinglesmallgradientstep.AswithGANs,itispossibletotrainageneratornetusingMMDevenifthatgeneratornetassignszeroprobabilitytothetrainingpoints.20.10.6ConvolutionalGenerativeNetworksWhengeneratingimages,itisoftenusefultouseageneratornetworkthatincludesaconvolutionalstructure(seeforexampleGoodfellow2014cDosovitskiyetal.()oretal.()).Todoso, weusethe“transpose”oftheconvolutionoperator,2015describedinsection.Thisapproachoftenyieldsmorerealisticimagesanddoes9.5sousingfewerparametersthanusingfullyconnectedlayerswithoutparametersharing.Convolutionalnetworksforrecognitiontaskshaveinformationﬂowfromtheimagetosomesummarizationlayeratthetopofthenetwork,oftenaclasslabel.704 CHAPTER20.DEEPGENERATIVEMODELSAsthisimageﬂowsupwardthroughthenetwork,informationisdiscardedastherepresentationoftheimagebecomesmoreinvarianttonuisancetransformations.Inageneratornetwork, theoppositeistrue.Richdetailsmustbeaddedastherepresentationoftheimagetobegeneratedpropagatesthroughthenetwork,culminatingintheﬁnalrepresentationoftheimage,whichisofcoursetheimageitself,inallofitsdetailedglory,withobjectpositionsandposesandtexturesandlighting. Theprimarymechanismfordiscardinginformationinaconvolutionalrecognitionnetworkisthepoolinglayer.Thegeneratornetworkseemstoneedtoaddinformation.Wecannotputtheinverseofapoolinglayerintothegeneratornetworkbecausemostpoolingfunctionsarenotinvertible.Asimpleroperationistomerelyincreasethespatialsizeoftherepresentation.Anapproachthatseemstoperformacceptablyistousean“un-pooling”asintroducedbyDosovitskiyetal.().Thislayercorrespondstotheinverseofthemax-poolingoperationunder2015certainsimplifyingconditions. First,thestrideofthemax-poolingoperationisconstrainedtobeequaltothewidthofthepoolingregion.Second,themaximuminputwithineachpoolingregionisassumedtobetheinputintheupper-leftcorner.Finally,allnon-maximalinputswithineachpoolingregionareassumedtobezero.Theseareverystrongandunrealisticassumptions,buttheydoallowthemax-poolingoperatortobeinverted.Theinverseun-poolingoperationallocatesatensorofzeros,thencopieseachvaluefromspatialcoordinateioftheinputtospatialcoordinateik×oftheoutput.Theintegervaluekdeﬁnesthesizeofthepoolingregion.Eventhoughtheassumptionsmotivatingthedeﬁnitionoftheun-poolingoperatorareunrealistic,thesubsequentlayersareabletolearntocompensateforitsunusualoutput,sothesamplesgeneratedbythemodelasawholearevisuallypleasing.20.10.7Auto-RegressiveNetworksAuto-regressivenetworksaredirectedprobabilisticmodelswithnolatentrandomvariables.Theconditionalprobabilitydistributionsinthesemodelsarerepresentedbyneuralnetworks(sometimesextremelysimpleneuralnetworkssuchaslogisticregression).Thegraphstructureofthesemodelsisthecompletegraph.TheydecomposeajointprobabilityovertheobservedvariablesusingthechainruleofprobabilitytoobtainaproductofconditionalsoftheformP(xd|xd−1,...,x1).Suchmodelshavebeencalledfully-visibleBayesnetworks(FVBNs)andusedsuccessfully inmany forms, ﬁrstwith logisticregression foreachconditionaldistribution(Frey1998,)andthenwithneuralnetworkswithhiddenunits(BengioandBengio2000bLarochelleandMurray2011,;,).Insomeformsofauto-regressivenetworks,suchasNADE(,),describedLarochelleandMurray2011705 CHAPTER20.DEEPGENERATIVEMODELSinsectionbelow,wecanintroduceaformofparametersharingthat20.10.10bringsbothastatisticaladvantage(feweruniqueparameters)andacomputationaladvantage(lesscomputation).Thisisonemoreinstanceoftherecurringdeeplearningmotifofreuseoffeatures. x1x1x2x2x3x3x4x4Px(4|x1,x2,x3)Px(4|x1,x2,x3)Px(3|x1,x2)Px(3|x1,x2)Px(2|x1)Px(2|x1)Px(1)Px(1)x1x1x2x2x3x3x4x4 Figure20.8:A fullyvisiblebelief networkpredictsthei-thvariable fromthei−1previousones.(Top)(Bottom)ThedirectedgraphicalmodelforanFVBN.Correspondingcomputationalgraph,inthecaseofthelogisticFVBN,whereeachpredictionismadebyalinearpredictor.20.10.8LinearAuto-RegressiveNetworksThesimplestformofauto-regressivenetworkhasnohiddenunitsandnosharingofparametersorfeatures.EachP(xi|xi−1,...,x1)isparametrizedasalinearmodel(linearregressionforreal-valueddata,logisticregressionforbinarydata,softmaxregressionfordiscretedata).ThismodelwasintroducedbyFrey1998()andhasO(d2)parameterswhentherearedvariablestomodel.Itisillustratedinﬁgure.20.8Ifthevariablesarecontinuous,alinearauto-regressivemodelismerelyanotherwaytoformulateamultivariateGaussiandistribution,capturinglinearpairwiseinteractionsbetweentheobservedvariables.Linearauto-regressivenetworksareessentiallythegeneralizationoflinearclassiﬁcationmethodstogenerativemodeling.Theythereforehavethesame706 CHAPTER20.DEEPGENERATIVEMODELSadvantagesanddisadvantagesaslinearclassiﬁers.Likelinearclassiﬁers,theymaybetrainedwithconvexlossfunctions,andsometimesadmitclosedformsolutions(asintheGaussiancase).Likelinearclassiﬁers,themodelitselfdoesnotoﬀerawayofincreasingitscapacity,socapacitymustberaisedusingtechniqueslikebasisexpansionsoftheinputorthekerneltrick. x1x1x2x2x3x3x4x4h1h1h2h2h3h3Px(4|x1,x2,x3)Px(4|x1,x2,x3)Px(3|x1,x2)Px(3|x1,x2)Px(2|x1)Px(2|x1)Px(1)Px(1) Figure20.9:Aneuralauto-regressivenetworkpredictsthei-thvariablexifromthei−1previousones,butisparametrizedsothatfeatures(groupsofhiddenunitsdenotedhi)thatarefunctionsofx1,...,xicanbereusedinpredictingallofthesubsequentvariablesxi+1,xi+2,...,xd.20.10.9NeuralAuto-RegressiveNetworksNeuralauto-regressivenetworks(,,)havethesameBengioandBengio2000ableft-to-rightgraphicalmodelaslogisticauto-regressivenetworks(ﬁgure)but20.8employadiﬀerentparametrizationoftheconditionaldistributionswithinthatgraphicalmodelstructure.Thenewparametrizationismorepowerfulinthesensethatitscapacitycanbeincreasedasmuchasneeded,allowingapproximationofanyjointdistribution.Thenewparametrizationcanalsoimprovegeneralizationbyintroducingaparametersharingandfeaturesharingprinciplecommontodeeplearningingeneral.Themodelsweremotivatedbytheobjectiveofavoidingthecurseofdimensionalityarisingoutoftraditionaltabulargraphicalmodels,sharingthesamestructureasﬁgure.Intabulardiscreteprobabilisticmodels,each20.8conditionaldistributionisrepresentedbyatableofprobabilities,withoneentryandoneparameterforeachpossibleconﬁgurationofthevariablesinvolved.Byusinganeuralnetworkinstead,twoadvantagesareobtained:707 CHAPTER20.DEEPGENERATIVEMODELS1.TheparametrizationofeachP(xi|xi−1,...,x1)byaneuralnetworkwith(i−1)×kinputsandkoutputs(ifthevariablesarediscreteandtakekvalues,encodedone-hot)allowsonetoestimatetheconditionalprobabilitywithoutrequiringanexponentialnumberofparameters(andexamples),yetstillisabletocapturehigh-orderdependenciesbetweentherandomvariables.2.Insteadofhavingadiﬀerentneuralnetworkforthepredictionofeachxi,aconnectivityillustratedinﬁgureallowsonetomergeallleft-to-right20.9theneuralnetworksintoone.Equivalently,itmeansthatthehiddenlayerfeaturescomputedforpredictingxicanbereusedforpredictingxik+(k>0).Thehiddenunitsarethusorganizedingroupsthathavetheparticularitythatalltheunitsinthei-thgrouponlydependontheinputvaluesx1,...,xi.Theparametersusedtocomputethesehiddenunitsarejointlyoptimizedto improvethe predictionofall thevariables inthe sequence.This isaninstanceofthereuseprinciplethatrecursthroughoutdeeplearninginscenariosrangingfromrecurrentandconvolutionalnetworkarchitecturestomulti-taskandtransferlearning.EachP(xi|xi−1,...,x1)canrepresentaconditionaldistributionbyhavingoutputsoftheneuralnetworkpredictparametersoftheconditionaldistributionofxi,asdiscussedinsection.Althoughtheoriginalneuralauto-regressive6.2.1.1networkswereinitiallyevaluatedinthecontextofpurelydiscretemultivariatedata(withasigmoidoutputforaBernoullivariableorsoftmaxoutputforamultinoullivariable)itisnaturaltoextendsuchmodelstocontinuousvariablesorjointdistributionsinvolvingbothdiscreteandcontinuousvariables.20.10.10NADETheneuralautoregressivedensityestimator(NADE)isaverysuccessfulrecentformofneuralauto-regressivenetwork(LarochelleandMurray2011,).Theconnectivityisthesameasfortheoriginalneuralauto-regressivenetworkofBengioandBengio2000b()butNADEintroducesanadditionalparametersharingscheme,asillustratedinﬁgure.Theparametersofthehiddenunitsofdiﬀerentgroups20.10jareshared.TheweightsWj,k,ifromthei-thinputxitothek-thelementofthej-thgroupofhiddenunith()jk()aresharedamongthegroups:ji≥Wj,k,i= Wk,i.(20.83)Theremainingweights,where,arezero.j<i708 CHAPTER20.DEEPGENERATIVEMODELS x1x1x2x2x3x3x4x4h1h1h2h2h3h3Px(4|x1,x2,x3)Px(4|x1,x2,x3)Px(3|x1,x2)Px(3|x1,x2)Px(2|x1)Px(2|x1)Px(1)Px(1) W:1,W:1,W:1,W:2,W:2,W:3,Figure20.10:Anillustrationoftheneuralautoregressivedensityestimator(NADE).Thehiddenunitsareorganizedingroupsh()jsothatonlytheinputsx1,...,xiparticipateincomputingh()iandpredictingP(xj|xj−1,...,x1),forj>i.NADEisdiﬀerentiatedfromearlierneuralauto-regressivenetworksbytheuseofaparticularweightsharingpattern:Wj,k,i=Wk,iisshared(indicatedintheﬁgurebytheuseofthesamelinepatternforeveryinstanceofareplicatedweight)foralltheweightsgoingoutfromxitothek-thunitofanygroup.Recallthatthevectorji≥(W1,i,W2,i,...,Wn,i)isdenotedW:,i.LarochelleandMurray2011()chosethissharingschemesothatforwardpropagationinaNADEmodellooselyresemblesthecomputationsperformedinmeanﬁeldinferencetoﬁllinmissinginputsinanRBM.ThismeanﬁeldinferencecorrespondstorunningarecurrentnetworkwithsharedweightsandtheﬁrststepofthatinferenceisthesameasinNADE.TheonlydiﬀerenceisthatwithNADE,theoutputweightsconnectingthehiddenunitstotheoutputareparametrizedindependentlyfromtheweightsconnectingtheinputunitstothehiddenunits.IntheRBM,thehidden-to-outputweightsarethetransposeoftheinput-to-hiddenweights.TheNADEarchitecturecanbeextendedtomimicnotjustonetimestepofthemeanﬁeldrecurrentinferencebuttomimicksteps.ThisapproachiscalledNADE-(,).kRaikoetal.2014Asmentionedpreviously,auto-regressivenetworksmaybeextendtoprocesscontinuous-valueddata.AparticularlypowerfulandgenericwayofparametrizingacontinuousdensityisasaGaussianmixture(introducedinsection)with3.9.6mixtureweightsαi(thecoeﬃcientorpriorprobabilityforcomponenti),per-componentconditionalmeanµiandper-componentconditionalvarianceσ2i. AmodelcalledRNADE(,)usesthisparametrizationtoextendNADEUriaetal.2013torealvalues.Aswithothermixturedensitynetworks,theparametersofthis709 CHAPTER20.DEEPGENERATIVEMODELSdistributionareoutputsofthenetwork,withthemixtureweightprobabilitiesproducedbyasoftmaxunit,andthevariancesparametrizedsothattheyarepositive. Stochasticgradientdescentcanbenumericallyill-behavedduetotheinteractionsbetweentheconditionalmeansµiandtheconditionalvariancesσ2i.Toreducethisdiﬃculty,()useapseudo-gradientthatreplacestheUriaetal.2013gradientonthemean,intheback-propagationphase.Anotherveryinterestingextensionoftheneuralauto-regressivearchitecturesgetsridoftheneedtochooseanarbitraryorderfortheobservedvariables(MurrayandLarochelle2014,).Inauto-regressivenetworks,theideaistotrainthenetworktobeabletocopewithanyorderbyrandomlysamplingordersandprovidingtheinformationtohiddenunitsspecifyingwhichoftheinputsareobserved(ontherightsideoftheconditioningbar)andwhicharetobepredictedandarethusconsideredmissing(ontheleftsideoftheconditioningbar).Thisisnicebecauseitallowsonetouseatrainedauto-regressivenetworktoperformanyinferenceproblem(i.e.predictorsamplefromtheprobabilitydistributionoveranysubsetofvariablesgivenanysubset)extremelyeﬃciently.Finally,sincemanyordersofvariablesarepossible(n!fornvariables)andeachorderoofvariablesyieldsadiﬀerent,wecanformanensembleofmodelsformanyvaluesof:po(x|)opensemble() =x1kki=1po(x|()i).(20.84)Thisensemblemodelusuallygeneralizesbetterandassignshigherprobabilitytothetestsetthandoesanindividualmodeldeﬁnedbyasingleordering.Inthesamepaper,theauthorsproposedeepversionsofthearchitecture,butunfortunatelythatimmediatelymakescomputationasexpensiveasintheoriginalneuralauto-regressiveneuralnetwork(,).TheﬁrstlayerBengioandBengio2000bandtheoutputlayercanstillbecomputedinO(nh)multiply-addoperations,asintheregularNADE,wherehisthenumberofhiddenunits(thesizeofthegroupshi,inﬁguresand),whereasitis20.1020.9O(n2h)inBengioandBengio().However,fortheotherhiddenlayers,thecomputationis2000bO(n2h2)ifevery“previous”groupatlayerlparticipatesinpredictingthe“next”groupatlayerl+1,assumingngroupsofhhiddenunitsateachlayer.Makingthei-thgroupatlayerl+1onlydependonthei-thgroup,asinMurrayandLarochelle2014()atlayerlreducesittoOnh(2),whichisstilltimesworsethantheregularNADE.h710 CHAPTER20.DEEPGENERATIVEMODELS20.11DrawingSamplesfromAutoencodersInchapter,wesawthatmanykindsofautoencoderslearnthedatadistribution.14Therearecloseconnectionsbetweenscorematching,denoisingautoencoders,andcontractiveautoencoders.Theseconnectionsdemonstratethatsomekindsofautoencoderslearnthedatadistributioninsomeway.Wehavenotyetseenhowtodrawsamplesfromsuchmodels.Somekindsofautoencoders,suchasthevariationalautoencoder,explicitlyrepresentaprobabilitydistributionandadmitstraightforwardancestralsampling.MostotherkindsofautoencodersrequireMCMCsampling.Contractiveautoencodersaredesignedtorecoveranestimateofthetangentplaneofthedatamanifold.Thismeansthatrepeatedencodinganddecodingwithinjectednoisewillinducearandomwalkalongthesurfaceofthemanifold(Rifaietal.etal.,;2012Mesnil,).Thismanifolddiﬀusiontechniqueisakindof2012Markovchain.ThereisalsoamoregeneralMarkovchainthatcansamplefromanydenoisingautoencoder.20.11.1MarkovChainAssociatedwithanyDenoisingAutoen-coderTheabovediscussionleftopenthequestionofwhatnoisetoinjectandwhere,inordertoobtainaMarkovchainthatwouldgeneratefromthedistributionestimatedbytheautoencoder.()showedhowtoconstructBengioetal.2013csuchaMarkovchainforgeneralizeddenoisingautoencoders.Generalizeddenoisingautoencodersarespeciﬁedbyadenoisingdistributionforsamplinganestimateofthecleaninputgiventhecorruptedinput.EachstepoftheMarkovchainthatgeneratesfromtheestimateddistributionconsistsofthefollowingsub-steps,illustratedinﬁgure:20.111.Startingfromthepreviousstatex,injectcorruptionnoise,sampling˜xfromC(˜xx|).2. Encode˜xintoh= (f˜x).3. Decodetoobtaintheparametersofhωh= (g)pgp(= x |ω()) = h(x|˜x).4. Samplethenextstatefromxpgp(= x|ω()) = h(x|˜x).711 CHAPTER20.DEEPGENERATIVEMODELS x x˜x˜xh hω ωˆxˆxC(˜xx|)p()x|ωfg Figure20.11:EachstepoftheMarkovchainassociatedwithatraineddenoisingautoen-coder,thatgeneratesthesamplesfromtheprobabilisticmodelimplicitlytrainedbythedenoisinglog-likelihoodcriterion.Eachstepconsistsin(a)injectingnoiseviacorruptionprocessCinstatex,yielding˜x,(b)encodingitwithfunctionf,yieldingh=f(˜x),(c)decodingtheresultwithfunctiong,yieldingparametersωforthereconstructiondistribution,and(d)givenω,samplinganewstatefromthereconstructiondistributionp(x |ω=g(f(˜x))).Inthetypicalsquaredreconstructionerrorcase,g(h)=ˆx,whichestimatesE[x|˜x],corruptionconsistsinaddingGaussiannoiseandsamplingfromp(x|ω)consistsinaddingGaussiannoise,asecondtime,tothereconstructionˆx.Thelatternoiselevelshouldcorrespondtothemeansquarederrorofreconstructions,whereastheinjectednoiseisahyperparameterthatcontrolsthemixingspeedaswellastheextenttowhichtheestimatorsmoothstheempiricaldistribution(,).IntheVincent2011exampleillustratedhere,onlytheCandpconditionalsarestochasticsteps(fandgaredeterministiccomputations),althoughnoisecanalsobeinjectedinsidetheautoencoder,asingenerativestochasticnetworks(,).Bengioetal.2014 712 CHAPTER20.DEEPGENERATIVEMODELSBengio2014etal.()showedthatiftheautoencoderp(x |˜x)formsaconsistentestimatorofthecorrespondingtrueconditionaldistribution,thenthestationarydistributionoftheaboveMarkovchainformsaconsistentestimator(albeitanimplicitone)ofthedatageneratingdistributionof.x20.11.2ClampingandConditionalSamplingSimilarlytoBoltzmannmachines,denoisingautoencodersandtheirgeneralizations(suchasGSNs,describedbelow)canbeusedtosamplefromaconditionaldistri-butionp(xf|xo),simplybyclampingtheobservedunitsxfandonlyresamplingthefreeunitsxogivenxfandthesampledlatentvariables(ifany).Forexample,MP-DBMscanbeinterpretedasaformofdenoisingautoencoder,andareabletosamplemissinginputs.GSNslatergeneralizedsomeoftheideaspresentinMP-DBMstoperformthesameoperation(,).()Bengioetal.2014Alainetal.2015identiﬁedamissingconditionfromProposition1of(),whichisBengioetal.2014thatthetransitionoperator(deﬁnedbythestochasticmappinggoingfromonestateofthechaintothenext)shouldsatisfyapropertycalleddetailedbalance,whichspeciﬁesthataMarkovChainatequilibriumwillremaininequilibriumwhetherthetransitionoperatorisruninforwardorreverse.Anexperimentinclampinghalfofthepixels(therightpartoftheimage)andrunningtheMarkovchainontheotherhalfisshowninﬁgure.20.12 713 CHAPTER20.DEEPGENERATIVEMODELS Figure20.12:IllustrationofclampingtherighthalfoftheimageandrunningtheMarkovChainbyresamplingonlythelefthalfateachstep. ThesesamplescomefromaGSNtrainedtoreconstructMNISTdigitsateachtimestepusingthewalkbackprocedure.20.11.3Walk-BackTrainingProcedureThewalk-backtrainingprocedurewasproposedby()asawayBengioetal.2013ctoacceleratetheconvergenceofgenerativetrainingofdenoisingautoencoders.Insteadofperformingaone-stepencode-decodereconstruction,thisprocedureconsistsinalternativemultiplestochasticencode-decodesteps(asinthegenerativeMarkovchain)initializedatatrainingexample(justlikewiththecontrastivedivergencealgorithm,describedinsection)andpenalizingthelastprobabilistic18.2reconstructions(orallofthereconstructionsalongtheway).Trainingwithkstepsisequivalent(inthesenseofachievingthesamestationarydistribution)astrainingwithonestep,butpracticallyhastheadvantagethatspuriousmodesfurtherfromthedatacanberemovedmoreeﬃciently.20.12GenerativeStochasticNetworksGenerativestochasticnetworksorGSNs(,)aregeneraliza-Bengioetal.2014tionsofdenoisingautoencodersthatincludelatentvariableshinthegenerative714 CHAPTER20.DEEPGENERATIVEMODELSMarkovchain,inadditiontothevisiblevariables(usuallydenoted).xAGSNisparametrizedbytwoconditionalprobabilitydistributionswhichspecifyonestepoftheMarkovchain:1. p(x()k|h()k)tellshowtogeneratethenextvisiblevariablegiventhecurrentlatentstate.Sucha“reconstructiondistribution”isalsofoundindenoisingautoencoders,RBMs,DBNsandDBMs.2. p(h()k|h(1)k−,x(1)k−)tellshowtoupdatethelatentstatevariable,giventhepreviouslatentstateandvisiblevariable.DenoisingautoencodersandGSNsdiﬀerfromclassicalprobabilisticmodels(directedorundirected)inthattheyparametrizethegenerativeprocessitselfratherthanthemathematicalspeciﬁcationofthejointdistributionofvisibleandlatentvariables.Instead,thelatterisdeﬁned,,asthestationaryimplicitlyifitexistsdistributionofthegenerativeMarkovchain.TheconditionsforexistenceofthestationarydistributionaremildandarethesameconditionsrequiredbystandardMCMCmethods(seesection).Theseconditionsarenecessarytoguarantee17.3thatthechainmixes,buttheycanbeviolatedbysomechoicesofthetransitiondistributions(forexample,iftheyweredeterministic).OnecouldimaginediﬀerenttrainingcriteriaforGSNs.Theoneproposedandevaluatedby()issimplyreconstructionlog-probabilityontheBengioetal.2014visibleunits,justlikefordenoisingautoencoders.Thisisachievedbyclampingx(0)=xtotheobservedexampleandmaximizingtheprobabilityofgeneratingxatsomesubsequenttimesteps,i.e.,maximizinglogp(x()k=x|h()k),whereh()kissampledfromthechain,givenx(0)=x. Inordertoestimatethegradientoflogp(x()k=x|h()k)withrespecttotheotherpiecesofthemodel,Bengioetal.()usethereparametrizationtrick,introducedinsection.201420.9Thewalk-backtrainingprotocol(describedinsection)wasused(20.11.3Ben-gio2014etal.,)toimprovetrainingconvergenceofGSNs.20.12.1DiscriminantGSNsTheoriginalformulationofGSNs(,)wasmeantforunsupervisedBengioetal.2014learningandimplicitlymodelingp(x)forobserveddatax,butitispossibletomodifytheframeworktooptimize.p()y|xForexample,ZhouandTroyanskaya2014()generalizeGSNsinthisway,byonlyback-propagatingthereconstructionlog-probabilityovertheoutputvariables,keepingtheinputvariablesﬁxed.Theyappliedthissuccessfullytomodelsequences715 CHAPTER20.DEEPGENERATIVEMODELS(proteinsecondarystructure)andintroduceda(one-dimensional)convolutionalstructureinthetransitionoperatoroftheMarkovchain.Itisimportanttorememberthat,foreachstepoftheMarkovchain,onegeneratesanewsequenceforeachlayer,andthatsequenceistheinputforcomputingotherlayervalues(saytheonebelowandtheoneabove)atthenexttimestep.HencetheMarkovchainisreallyovertheoutputvariable(andassociatedhigher-levelhiddenlayers),andtheinputsequenceonlyservestoconditionthatchain,withback-propagationallowingtolearnhowtheinputsequencecanconditiontheoutputdistributionimplicitlyrepresentedbytheMarkovchain.ItisthereforeacaseofusingtheGSNinthecontextofstructuredoutputs.ZöhrerandPernkopf2014()introducedahybridmodelthatcombinesasuper-visedobjective(asintheabovework)andanunsupervisedobjective(asintheoriginalGSNwork),bysimplyadding(withadiﬀerentweight)thesupervisedandunsupervisedcostsi.e.,thereconstructionlog-probabilitiesofyandxrespectively.SuchahybridcriterionhadpreviouslybeenintroducedforRBMsbyLarochelleandBengio2008().Theyshowimprovedclassiﬁcationperformanceusingthisscheme.20.13OtherGenerationSchemesThemethodswehavedescribedsofaruseeitherMCMCsampling,ancestralsampling,orsomemixtureofthetwotogeneratesamples. Whilethesearethemostpopularapproachestogenerativemodeling,theyarebynomeanstheonlyapproaches.Sohl-Dickstein2015etal.()developedadiﬀusioninversiontrainingschemeforlearningagenerativemodel,basedonnon-equilibriumthermodynamics.Theapproachisbasedontheideathattheprobabilitydistributionswewishtosamplefromhavestructure.Thisstructurecangraduallybedestroyedbyadiﬀusionprocessthatincrementallychangestheprobabilitydistributiontohave moreentropy.Toformagenerativemodel,wecanruntheprocessinreverse,bytrainingamodelthatgraduallyrestoresthestructuretoanunstructureddistribution.Byiterativelyapplyingaprocessthatbringsadistributionclosertothetargetone,wecangraduallyapproachthattargetdistribution.ThisapproachresemblesMCMCmethodsinthesensethatitinvolvesmanyiterationstoproduceasample.However,themodelisdeﬁnedtobetheprobabilitydistributionproducedbytheﬁnalstepofthechain. Inthissense,thereisnoapproximationinducedbytheiterativeprocedure.Theapproachintroducedby()Sohl-Dicksteinetal.2015isalsoveryclosetothegenerativeinterpretationofthedenoisingautoencoder716 CHAPTER20.DEEPGENERATIVEMODELS(section).Aswiththedenoisingautoencoder,diﬀusioninversiontrainsa20.11.1transitionoperatorthatattemptstoprobabilisticallyundotheeﬀectofaddingsomenoise.Thediﬀerenceisthatdiﬀusioninversionrequresundoingonlyonestepofthediﬀusionprocess,ratherthantravelingallthewaybacktoacleandatapoint.Thisaddressesthefollowingdilemmapresentwiththeordinaryreconstructionlog-likelihoodobjectiveofdenoisingautoencoders:withsmalllevelsofnoisethelearneronlyseesconﬁgurationsnearthedatapoints,whilewithlargelevelsofnoiseitisaskedtodoanalmostimpossiblejob(becausethedenoisingdistributionishighlycomplexandmulti-modal).Withthediﬀusioninversionobjective,thelearnercanlearntheshapeofthedensityaroundthedatapointsmorepreciselyaswellasremovespuriousmodesthatcouldshowupfarfromthedatapoints.AnotherapproachtosamplegenerationistheapproximateBayesiancom-putation(ABC)framework(,).Inthisapproach,samplesareRubinetal.1984rejectedormodiﬁedinordertomakethemomentsofselectedfunctionsofthesamplesmatchthoseofthedesireddistribution.Whilethisideausesthemomentsofthesampleslikeinmomentmatching,itisdiﬀerentfrommomentmatchingbecauseitmodiﬁesthesamplesthemselves,ratherthantrainingthemodeltoautomaticallyemitsampleswiththecorrectmoments.()BachmanandPrecup2015showedhowtouseideasfromABCinthecontextofdeeplearning,byusingABCtoshapetheMCMCtrajectoriesofGSNs.Weexpectthatmanyotherpossibleapproachestogenerativemodelingawaitdiscovery.20.14EvaluatingGenerativeModelsResearchersstudyinggenerativemodelsoftenneedtocompareonegenerativemodeltoanother,usuallyinordertodemonstratethatanewlyinventedgenerativemodelisbetteratcapturingsomedistributionthanthepre-existingmodels.Thiscanbeadiﬃcultandsubtletask.Inmanycases,wecannotactuallyevaluatethelogprobabilityofthedataunderthemodel,butonlyanapproximation.Inthesecases,itisimportanttothinkandcommunicateclearlyaboutexactlywhatisbeingmeasured.Forexample,supposewecanevaluateastochasticestimateofthelog-likelihoodformodelA,andadeterministiclowerboundonthelog-likelihoodformodelB.IfmodelAgetsahigherscorethanmodelB,whichisbetter?Ifwecareaboutdeterminingwhichmodelhasabetterinternalrepresentationofthedistribution,weactuallycannottell,unlesswehavesomewayofdetermininghowloosetheboundformodelBis.However,ifwecareabouthowwellwecanusethemodelinpractice,forexampletoperformanomalydetection,thenitisfairto717 CHAPTER20.DEEPGENERATIVEMODELSsaythatamodelispreferablebasedonacriterionspeciﬁctothepracticaltaskofinterest,e.g.,basedonrankingtestexamplesandrankingcriteriasuchasprecisionandrecall.Anothersubtletyofevaluatinggenerativemodelsisthattheevaluationmetricsareoftenhardresearchproblemsinandofthemselves.Itcanbeverydiﬃculttoestablishthatmodelsarebeingcomparedfairly.Forexample,supposeweuseAIStoestimatelogZinordertocomputelog ˜p(x)−logZforanewmodelwehavejustinvented.AcomputationallyeconomicalimplementationofAISmayfailtoﬁndseveralmodesofthemodeldistributionandunderestimateZ,whichwillresultinusoverestimatinglogp(x).ItcanthusbediﬃculttotellwhetherahighlikelihoodestimateisduetoagoodmodelorabadAISimplementation.Otherﬁeldsofmachinelearningusuallyallowforsomevariationinthepre-processingofthedata.Forexample,whencomparingtheaccuracyofobjectrecognitionalgorithms,itisusuallyacceptabletopreprocesstheinputimagesslightlydiﬀerentlyforeachalgorithmbasedonwhatkindofinputrequirementsithas.Generativemodelingisdiﬀerentbecausechangesinpreprocessing,evenverysmallandsubtleones,arecompletelyunacceptable.Anychangetotheinputdatachangesthedistributiontobecapturedandfundamentallyaltersthetask.Forexample,multiplyingtheinputby0.1willartiﬁciallyincreaselikelihoodbyafactorof10.IssueswithpreprocessingcommonlyarisewhenbenchmarkinggenerativemodelsontheMNISTdataset,oneofthemorepopulargenerativemodelingbenchmarks.MNISTconsistsofgrayscaleimages.SomemodelstreatMNISTimagesaspointsinarealvectorspace,whileotherstreatthemasbinary.Yetotherstreatthegrayscalevaluesasprobabilitiesforabinarysamples.Itisessentialtocomparereal-valuedmodelsonlytootherreal-valuedmodelsandbinary-valuedmodelsonlytootherbinary-valuedmodels. Otherwisethelikelihoodsmeasuredarenotonthesamespace.Forbinary-valuedmodels,thelog-likelihoodcanbeatmostzero,whileforreal-valuedmodelsitcanbearbitrarilyhigh,sinceitisthemeasurementofadensity.Amongbinarymodels,itisimportanttocomparemodelsusingexactlythesamekindofbinarization.Forexample,wemightbinarizeagraypixelto0or1bythresholdingat0.5,orbydrawingarandomsamplewhoseprobabilityofbeing1isgivenbythegraypixelintensity.Ifweusetherandombinarization,wemightbinarizethewholedatasetonce,orwemightdrawadiﬀerentrandomexampleforeachstepoftrainingandthendrawmultiplesamplesforevaluation.Eachofthesethreeschemesyieldswildlydiﬀerentlikelihoodnumbers,andwhencomparingdiﬀerentmodelsitisimportantthatbothmodelsusethesamebinarizationschemefortrainingandforevaluation.Infact,researcherswhoapplyasinglerandom718 CHAPTER20.DEEPGENERATIVEMODELSbinarizationstepshareaﬁlecontainingtheresultsoftherandombinarization,sothatthereisnodiﬀerenceinresultsbasedondiﬀerentoutcomesofthebinarizationstep.Becausebeingabletogeneraterealisticsamplesfromthedatadistributionisoneofthegoalsofagenerativemodel,practitionersoftenevaluategenerativemodelsbyvisuallyinspectingthesamples.Inthebestcase,thisisdonenotbytheresearchersthemselves,butbyexperimentalsubjectswhodonotknowthesourceofthesamples(Denton2015etal.,).Unfortunately,itispossibleforaverypoorprobabilisticmodeltoproduceverygoodsamples.Acommonpracticetoverifyifthemodelonlycopiessomeofthetrainingexamplesisillustratedinﬁgure.16.1Theideaistoshowforsomeofthegeneratedsamplestheirnearestneighborinthetrainingset,accordingtoEuclideandistanceinthespaceofx. Thistestisintendedtodetectthecasewherethemodeloverﬁtsthetrainingsetandjustreproducestraininginstances.Itisevenpossibletosimultaneouslyunderﬁtandoverﬁtyetstillproducesamplesthatindividuallylookgood.Imagineagenerativemodeltrainedonimagesofdogsandcatsthatsimplylearnstoreproducethetrainingimagesofdogs.Suchamodelhasclearlyoverﬁt,becauseitdoesnotproducesimagesthatwerenotinthetrainingset,butithasalsounderﬁt,becauseitassignsnoprobabilitytothetrainingimagesofcats.Yetahumanobserverwouldjudgeeachindividualimageofadogtobehighquality.Inthissimpleexample,itwouldbeeasyforahumanobserverwhocaninspectmanysamplestodeterminethatthecatsareabsent.Inmorerealisticsettings,agenerativemodeltrainedondatawithtensofthousandsofmodesmayignoreasmallnumberofmodes,andahumanobserverwouldnoteasilybeabletoinspectorrememberenoughimagestodetectthemissingvariation.Since thevisual quality ofsamples is not areliable guide, we oftenalsoevaluatethelog-likelihoodthatthemodelassignstothetestdata,whenthisiscomputationallyfeasible.Unfortunately,insomecasesthelikelihoodseemsnottomeasureanyattributeofthemodelthatwereallycareabout.Forexample,real-valuedmodelsofMNISTcanobtainarbitrarilyhighlikelihoodbyassigningarbitrarilylowvariancetobackgroundpixelsthatneverchange.Modelsandalgorithmsthatdetecttheseconstantfeaturescanreapunlimitedrewards,eventhoughthisisnotaveryusefulthingtodo.Thepotentialtoachieveacostapproaching negativeinﬁnityis present foranykind of maximum likelihoodproblemwithrealvalues,butitisespeciallyproblematicforgenerativemodelsofMNISTbecausesomanyoftheoutputvaluesaretrivialtopredict.Thisstronglysuggestsaneedfordevelopingotherwaysofevaluatinggenerativemodels.Theis2015etal.()reviewmanyoftheissuesinvolvedinevaluatinggenerative719 CHAPTER20.DEEPGENERATIVEMODELSmodels,includingmanyoftheideasdescribedabove.Theyhighlightthefactthattherearemanydiﬀerentusesofgenerativemodelsandthatthechoiceofmetricmustmatchtheintendeduseofthemodel.Forexample,somegenerativemodelsarebetteratassigninghighprobabilitytomostrealisticpointswhileothergenerativemodelsarebetteratrarelyassigninghighprobabilitytounrealisticpoints.ThesediﬀerencescanresultfromwhetheragenerativemodelisdesignedtominimizeDKL(pdatapmodel)orDKL(pmodelpdata),asillustratedinﬁgure.3.6Unfortunately,evenwhenwerestricttheuseofeachmetrictothetaskitismostsuitedfor,allofthemetricscurrentlyinusecontinuetohaveseriousweaknesses.Oneofthemostimportantresearchtopicsingenerativemodelingisthereforenotjusthowtoimprovegenerativemodels,butinfact,designingnewtechniquestomeasureourprogress.20.15ConclusionTraininggenerativemodelswithhiddenunitsisapowerfulwaytomakemodelsunderstandtheworldrepresentedinthegiventrainingdata.Bylearningamodelpmodel(x)andarepresentationpmodel(hx|),agenerativemodelcanprovideanswerstomanyinferenceproblemsabouttherelationshipsbetweeninputvariablesinxandcanprovidemanydiﬀerentwaysofrepresentingxbytakingexpectationsofhatdiﬀerentlayersofthehierarchy. GenerativemodelsholdthepromisetoprovideAIsystemswithaframeworkforallofthemanydiﬀerentintuitiveconceptstheyneedtounderstand,andtheabilitytoreasonabouttheseconceptsinthefaceofuncertainty.Wehopethatourreaderswillﬁndnewwaystomaketheseapproachesmorepowerfulandcontinuethejourneytounderstandingtheprinciplesthatunderlielearningandintelligence. 720 BibliographyAbadi,M.,Agarwal,A.,Barham,P.,Brevdo,E.,Chen,Z.,Citro,C.,Corrado,G.S.,Davis,A.,Dean,J.,Devin,M.,Ghemawat,S.,Goodfellow,I.,Harp,A.,Irving,G.,Isard,M.,Jia,Y.,Jozefowicz,R.,Kaiser,L.,Kudlur,M.,Levenberg,J.,Mané,D.,Monga,R.,Moore,S.,Murray,D.,Olah,C.,Schuster,M.,Shlens,J.,Steiner,B.,Sutskever,I.,Talwar,K.,Tucker,P.,Vanhoucke,V.,Vasudevan,V.,Viégas,F.,Vinyals,O.,Warden,P.,Wattenberg,M.,Wicke,M.,Yu,Y.,andZheng,X.(2015).TensorFlow:Large-scalemachinelearningonheterogeneoussystems.Softwareavailablefromtensorﬂow.org.,25214446,Ackley,D.H.,Hinton,G.E.,andSejnowski,T.J.(1985).AlearningalgorithmforBoltzmannmachines.CognitiveScience,,147–169.,9570654Alain,G.andBengio,Y.(2013). Whatregularizedauto-encoderslearnfromthedatageneratingdistribution.In.,,,ICLR’2013,arXiv:1211.4246507513514521Alain,G.,Bengio,Y.,Yao,L.,ÉricThibodeau-Laufer,Yosinski,J.,andVincent,P.(2015).GSNs:Generativestochasticnetworks.arXiv:1503.05571.,510713Anderson,E.(1935).TheIrisesoftheGaspéPeninsula.BulletinoftheAmericanIrisSociety,,2–5.5921Ba,J.,Mnih,V.,andKavukcuoglu,K.(2014).Multipleobjectrecognitionwithvisualattention..arXiv:1412.7755691Bachman,P.andPrecup,D.(2015).Variationalgenerativestochasticnetworkswithcollaborativeshaping.InProceedingsofthe32ndInternationalConferenceonMachineLearning,ICML2015,Lille,France,6-11July2015,pages1964–1972.717Bacon,P.-L.,Bengio,E.,Pineau,J.,andPrecup,D.(2015).Conditionalcomputationinneuralnetworksusingadecision-theoreticapproach.In2ndMultidisciplinaryConferenceonReinforcementLearningandDecisionMaking(RLDM2015).450Bagnell,J.A.andBradley,D.M.(2009).Diﬀerentiablesparsecoding.InD.Koller,D.Schuurmans,Y.Bengio,andL.Bottou,editors,AdvancesinNeuralInformationProcessingSystems21(NIPS’08),pages113–120.498721 BIBLIOGRAPHYBahdanau,D.,Cho,K.,andBengio,Y.(2015).Neuralmachinetranslationbyjointlylearningtoalignandtranslate.In.,,,,,ICLR’2015,arXiv:1409.047325101397418420465475476,,Bahl,L.R.,Brown,P.,deSouza,P.V.,andMercer,R.L.(1987). Speechrecognitionwithcontinuous-parameterhiddenMarkovmodels.Computer,SpeechandLanguage,2,219–234.458Baldi,P.andHornik,K.(1989).Neuralnetworksandprincipalcomponentanalysis:Learningfromexampleswithoutlocalminima.NeuralNetworks,,53–58.2286Baldi,P.,Brunak,S.,Frasconi,P.,Soda,G.,andPollastri,G.(1999).Exploitingthepastandthefutureinproteinsecondarystructureprediction.,Bioinformatics15(11),937–946.395Baldi, P., Sadowski, P., andWhiteson, D.(2014).Searchingforexoticparticlesinhigh-energyphysicswithdeeplearning.Naturecommunications,.526Ballard,D.H.,Hinton,G.E.,andSejnowski,T.J.(1983).Parallelvisioncomputation.Nature.452Barlow,H.B.(1989).Unsupervisedlearning.NeuralComputation,,295–311.1147Barron,A.E.(1993).Universalapproximationboundsforsuperpositionsofasigmoidalfunction.IEEETrans.onInformationTheory,,930–945.39199Bartholomew,D.J.(1987).Latentvariablemodelsandfactoranalysis.OxfordUniversityPress.490Basilevsky,A.(1994).StatisticalFactorAnalysisandRelatedMethods:TheoryandApplications.Wiley.490Bastien,F.,Lamblin,P., Pascanu,R.,Bergstra,J., Goodfellow,I.J.,Bergeron,A.,Bouchard,N.,andBengio,Y.(2012).Theano:newfeaturesandspeedimprovements.DeepLearningandUnsupervisedFeatureLearningNIPS2012Workshop.,,,2582214222446,Basu,S.andChristensen,J.(2013). Teachingclassiﬁcationboundariestohumans. InAAAI’2013.329Baxter,J.(1995).Learninginternalrepresentations.InProceedingsofthe8thInternationalConferenceonComputationalLearningTheory(COLT’95),pages311–320,SantaCruz,California.ACMPress.245Bayer,J.andOsendorfer,C.(2014).Learningstochasticrecurrentnetworks.ArXive-prints.265Becker,S.andHinton,G.(1992).Aself-organizingneuralnetworkthatdiscoverssurfacesinrandom-dotstereograms.Nature,,161–163.355541722 BIBLIOGRAPHYBehnke,S.(2001).Learningiterativeimagereconstructionintheneuralabstractionpyramid.Int.J.ComputationalIntelligenceandApplications,(4),427–438.1515Beiu,V.,Quintana,J.M.,andAvedillo,M.J.(2003).VLSIimplementationsofthresholdlogic-acomprehensivesurvey.NeuralNetworks,IEEETransactionson,14(5),1217–1243.451Belkin, M.and Niyogi, P.(2002).Laplacianeigenmapsandspectraltechniquesforembeddingandclustering. InT.Dietterich,S.Becker,andZ.Ghahramani,editors,AdvancesinNeuralInformationProcessingSystems14(NIPS’01),Cambridge,MA.MITPress.244Belkin,M.andNiyogi,P.(2003).Laplacianeigenmapsfordimensionalityreductionanddatarepresentation.NeuralComputation,(6),1373–1396.,15164518Bengio,E.,Bacon,P.-L.,Pineau,J.,andPrecup,D.(2015a).Conditionalcomputationinneuralnetworksforfastermodels.arXiv:1511.06297.450Bengio, S. andBengio, Y. (2000a).Taking onthecurseofdimensionalityinjointdistributionsusingneuralnetworks.IEEETransactionsonNeuralNetworks,specialissueonDataMiningandKnowledgeDiscovery,(3),550–557.11707Bengio,S.,Vinyals,O.,Jaitly,N.,andShazeer,N.(2015b).Scheduledsamplingforsequencepredictionwithrecurrentneuralnetworks.Technicalreport,arXiv:1506.03099.384Bengio,Y.(1991).ArtiﬁcialNeuralNetworksandtheirApplicationtoSequenceRecognition.Ph.D.thesis,McGillUniversity,(ComputerScience),Montreal,Canada.407Bengio,Y.(2000).Gradient-basedoptimizationofhyperparameters.NeuralComputation,12(8),1889–1900.435Bengio,Y.(2002).Newdistributedprobabilisticlanguagemodels.TechnicalReport1215,Dept.IRO,UniversitédeMontréal.467Bengio,Y.(2009).LearningdeeparchitecturesforAI.NowPublishers.,201622Bengio,Y.(2013).Deeplearningofrepresentations:lookingforward.InStatisticalLanguageandSpeechProcessing,volume7978ofLectureNotesinComputerScience,pages1–37.Springer,alsoinarXivathttp://arxiv.org/abs/1305.0445.448Bengio,Y.(2015).Earlyinferenceinenergy-basedmodelsapproximatesback-propagation.TechnicalReportarXiv:1510.02777,UniversitedeMontreal.656Bengio,Y.andBengio,S.(2000b).Modelinghigh-dimensionaldiscretedatawithmulti-layerneuralnetworks.In,pages400–406.MITPress.,,,NIPS12705707708710Bengio,Y.andDelalleau,O.(2009).Justifyingandgeneralizingcontrastivedivergence.NeuralComputation,(6),1601–1621.,21513611723 BIBLIOGRAPHYBengio,Y.andGrandvalet,Y.(2004).Nounbiasedestimatorofthevarianceofk-foldcross-validation.InS.Thrun,L.Saul,andB.Schölkopf,editors,AdvancesinNeuralInformationProcessingSystems16(NIPS’03),Cambridge,MA.MITPress,Cambridge.122Bengio,Y.andLeCun,Y.(2007).ScalinglearningalgorithmstowardsAI.InLargeScaleKernelMachines.19Bengio,Y.andMonperrus,M.(2005).Non-localmanifoldtangentlearning.InL.Saul,Y.Weiss,andL.Bottou,editors,AdvancesinNeuralInformationProcessingSystems17(NIPS’04),pages129–136.MITPress.,160519Bengio,Y.andSénécal,J.-S.(2003).Quicktrainingofprobabilisticneuralnetsbyimportancesampling.InProceedingsofAISTATS2003.470Bengio,Y.andSénécal,J.-S.(2008).Adaptiveimportancesamplingtoacceleratetrainingofaneuralprobabilisticlanguagemodel.IEEETrans.NeuralNetworks,19(4),713–722.470Bengio,Y.,DeMori,R.,Flammia,G.,andKompe,R.(1991).Phoneticallymotivatedacousticparametersforcontinuousspeechrecognitionusingartiﬁcialneuralnetworks.InProceedingsofEuroSpeech’91.,27459Bengio,Y.,DeMori,R.,Flammia,G.,andKompe,R.(1992).Neuralnetwork-Gaussianmixturehybridforspeechrecognitionordensityestimation.In,pages175–182.NIPS4MorganKaufmann.459Bengio,Y.,Frasconi,P.,andSimard,P.(1993).Theproblemoflearninglong-termdependenciesinrecurrentnetworks.InIEEEInternationalConferenceonNeuralNetworks,pages1183–1195,SanFrancisco.IEEEPress.(invitedpaper).403Bengio,Y.,Simard,P.,andFrasconi,P.(1994).Learninglong-termdependencieswithgradientdescentisdiﬃcult.IEEETr.NeuralNets.,,,18401403411Bengio,Y.,Latendresse,S.,andDugas,C.(1999).Gradient-basedlearningofhyper-parameters.LearningConference,Snowbird.435Bengio,Y.,Ducharme,R.,andVincent,P.(2001).Aneuralprobabilisticlanguagemodel.InT.K.Leen,T.G.Dietterich,andV.Tresp,editors,,pages932–938.MITNIPS’2000Press.,,,,,,18447464466472477482Bengio,Y.,Ducharme,R.,Vincent,P.,andJauvin,C.(2003).Aneuralprobabilisticlanguagemodel.,,1137–1155.,JMLR3466472Bengio,Y.,LeRoux,N.,Vincent,P.,Delalleau,O.,andMarcotte,P.(2006a).Convexneuralnetworks.In,pages123–130.NIPS’2005258Bengio,Y.,Delalleau,O.,andLeRoux,N.(2006b).Thecurseofhighlyvariablefunctionsforlocalkernelmachines.In.NIPS’2005158724 BIBLIOGRAPHYBengio,Y.,Larochelle,H.,andVincent,P.(2006c).Non-localmanifoldParzenwindows.In.MITPress.,NIPS’2005160520Bengio,Y.,Lamblin,P.,Popovici,D.,andLarochelle,H.(2007).Greedylayer-wisetrainingofdeepnetworks.In.,,,,,,NIPS’20061419201323324528530Bengio,Y.,Louradour,J.,Collobert,R.,andWeston,J.(2009).Curriculumlearning.InICML’09.328Bengio,Y.,Mesnil,G.,Dauphin,Y.,andRifai,S.(2013a).Bettermixingviadeeprepresentations.In.ICML’2013604Bengio,Y.,Léonard,N.,andCourville,A.(2013b).Estimatingorpropagatinggradientsthroughstochasticneuronsforconditionalcomputation. arXiv:1308.3432.,,448450689691,Bengio,Y.,Yao,L.,Alain,G.,andVincent,P.(2013c).Generalizeddenoisingauto-encodersasgenerativemodels.In.,,NIPS’2013507711714Bengio,Y.,Courville,A.,andVincent,P.(2013d).Representationlearning:Areviewandnewperspectives.IEEETrans.PatternAnalysisandMachineIntelligence(PAMI),35(8),1798–1828.555Bengio,Y.,Thibodeau-Laufer,E.,Alain,G.,andYosinski,J.(2014). Deepgenerativestochasticnetworkstrainablebybackprop.In.,,,,ICML’2014711712713714715Bennett,C.(1976).EﬃcientestimationoffreeenergydiﬀerencesfromMonteCarlodata.JournalofComputationalPhysics,(2),245–268.22628Bennett,J.andLanning,S.(2007).TheNetﬂixprize.479Berger,A.L.,DellaPietra,V.J.,andDellaPietra,S.A.(1996).Amaximumentropyapproachtonaturallanguageprocessing.,,39–71.ComputationalLinguistics22473Berglund,M.andRaiko,T.(2013).Stochasticgradientestimatevarianceincontrastivedivergenceandpersistentcontrastivedivergence.,.CoRRabs/1312.6002614Bergstra, J.(2011).IncorporatingComplexCellsintoNeuralNetworksfor PatternClassiﬁcation.Ph.D.thesis,UniversitédeMontréal.255Bergstra,J.andBengio,Y.(2009).Slow,decorrelatedfeaturesforpretrainingcomplexcell-likenetworks.In.NIPS’2009494Bergstra,J.andBengio,Y.(2012).Randomsearchforhyper-parameteroptimization.J.MachineLearningRes.,,281–305.,,13433434435Bergstra,J.,Breuleux,O.,Bastien,F.,Lamblin,P.,Pascanu,R.,Desjardins,G.,Turian,J.,Warde-Farley,D.,andBengio,Y.(2010).Theano:aCPUandGPUmathexpressioncompiler.InProc.SciPy.,,,,2582214222446725 BIBLIOGRAPHYBergstra,J.,Bardenet,R.,Bengio,Y.,andKégl,B.(2011).Algorithmsforhyper-parameteroptimization.In.NIPS’2011436Berkes,P.andWiskott,L.(2005).Slowfeatureanalysisyieldsarichrepertoireofcomplexcellproperties.,(6),579–602.JournalofVision5495Bertsekas,D.P.andTsitsiklis,J.(1996).Neuro-DynamicProgramming.AthenaScientiﬁc.106Besag,J.(1975).Statisticalanalysisofnon-latticedata.,TheStatistician24(3),179–195.615Bishop,C.M.(1994).Mixturedensitynetworks.189Bishop,C.M.(1995a).Regularizationandcomplexitycontrolinfeed-forwardnetworks.InProceedingsInternationalConferenceonArtiﬁcialNeuralNetworksICANN’95,volume1,page141–148.,242250Bishop,C.M.(1995b).TrainingwithnoiseisequivalenttoTikhonovregularization.NeuralComputation,(1),108–116.7242Bishop,C.M.(2006).PatternRecognitionandMachineLearning.Springer.,98146Blum,A.L.andRivest,R.L.(1992).Traininga3-nodeneuralnetworkisNP-complete.293Blumer,A.,Ehrenfeucht,A.,Haussler,D.,andWarmuth,M.K.(1989).LearnabilityandtheVapnik–Chervonenkisdimension.,(4),929––865.JournaloftheACM36114Bonnet,G.(1964).Transformationsdessignauxaléatoiresàtraverslessystèmesnonlinéairessansmémoire.AnnalesdesTélécommunications,(9–10),203–220.19689Bordes, A., Weston, J., Collobert, R., andBengio, Y.(2011).Learningstructuredembeddingsofknowledgebases.In.AAAI2011484Bordes,A.,Glorot,X.,Weston,J.,andBengio,Y.(2012).Jointlearningofwordsandmeaningrepresentationsforopen-textsemanticparsing.AISTATS’2012.,,401484485Bordes,A.,Glorot,X.,Weston,J.,andBengio,Y.(2013a).Asemanticmatchingenergyfunctionforlearningwithmulti-relationaldata.MachineLearning:SpecialIssueonLearningSemantics.483Bordes,A.,Usunier, N.,Garcia-Duran,A.,Weston,J.,andYakhnenko,O.(2013b).Translatingembeddingsformodelingmulti-relationaldata.InC.Burges,L.Bottou,M.Welling,Z.Ghahramani,andK.Weinberger,editors,AdvancesinNeuralInformationProcessingSystems26,pages2787–2795.CurranAssociates,Inc.484Bornschein,J.andBengio,Y.(2015).Reweightedwake-sleep.InICLR’2015,arXiv:1406.2751.693726 BIBLIOGRAPHYBornschein,J.,Shabanian,S.,Fischer,A.,andBengio,Y.(2015).TrainingbidirectionalHelmholtzmachines.Technicalreport,arXiv:1506.03877.693Boser,B.E.,Guyon,I.M.,andVapnik,V.N.(1992).Atrainingalgorithmforopti-malmarginclassiﬁers.InCOLT’92:ProceedingsoftheﬁfthannualworkshoponComputationallearningtheory,pages144–152,NewYork,NY,USA.ACM.,18141Bottou,L.(1998).Onlinealgorithmsandstochasticapproximations.InD.Saad,editor,OnlineLearninginNeuralNetworks.CambridgeUniversityPress,Cambridge,UK.296Bottou, L.(2011).Frommachinelearning tomachinereasoning.Technicalreport,arXiv.1102.1808.401Bottou,L.(2015).Multilayerneuralnetworks.DeepLearningSummerSchool.440Bottou,L.andBousquet,O.(2008).Thetradeoﬀsoflargescalelearning.In.NIPS’2008282295,Boulanger-Lewandowski,N.,Bengio,Y.,andVincent,P.(2012).Modelingtemporaldependenciesinhigh-dimensionalsequences:Applicationtopolyphonicmusicgenerationandtranscription.In.,ICML’12685686Boureau,Y.,Ponce,J.,andLeCun,Y.(2010).Atheoreticalanalysisoffeaturepoolinginvisionalgorithms.InProc.InternationalConferenceonMachinelearning(ICML’10).345Boureau,Y.,LeRoux,N.,Bach,F.,Ponce,J.,andLeCun,Y.(2011). Askthelocals:multi-waylocalpoolingforimagerecognition.InProc.InternationalConferenceonComputerVision(ICCV’11).IEEE.345Bourlard,H.andKamp,Y.(1988).Auto-associationbymultilayerperceptronsandsingularvaluedecomposition.BiologicalCybernetics,,291–294.59502Bourlard,H.andWellekens,C.(1989).Speechpatterndiscriminationandmulti-layeredperceptrons.ComputerSpeechandLanguage,,1–19.3459Boyd,S.andVandenberghe,L.(2004)..CambridgeUniversityConvexOptimizationPress,NewYork,NY,USA.93Brady,M.L.,Raghavan,R.,andSlawny,J.(1989).Back-propagationfailstoseparatewhereperceptronssucceed.IEEETransactionsonCircuitsandSystems,36,665–674.284Brakel,P.,Stroobandt,D.,andSchrauwen,B.(2013).Trainingenergy-basedmodelsfortime-seriesimputation.JournalofMachineLearningResearch,14,2771–2797.,674698Brand,M.(2003).Chartingamanifold.In,pages961–968.MITPress.,NIPS’2002164518727 BIBLIOGRAPHYBreiman,L.(1994).Baggingpredictors.MachineLearning,(2),123–140.24256Breiman,L.,Friedman,J.H.,Olshen,R.A.,andStone,C.J.(1984).ClassiﬁcationandRegressionTrees.WadsworthInternationalGroup,Belmont,CA.146Bridle,J.S.(1990).Alphanets:arecurrent‘neural’networkarchitecturewithahiddenMarkovmodelinterpretation.SpeechCommunication,(1),83–92.9186Briggman,K.,Denk,W.,Seung,S.,Helmstaedter,M.N.,andTuraga,S.C.(2009).Maximinaﬃnitylearningofimagesegmentation.In,pages1865–1873.NIPS’2009360Brown,P.F.,Cocke,J.,Pietra,S.A.D.,Pietra,V.J.D.,Jelinek,F.,Laﬀerty,J.D.,Mercer,R.L.,andRoossin,P.S.(1990).Astatisticalapproachtomachinetranslation.Computationallinguistics,(2),79–85.1621Brown,P.F.,Pietra,V.J.D.,DeSouza,P.V.,Lai,J.C.,andMercer,R.L.(1992).Class-based-grammodelsofnaturallanguage.,nComputationalLinguistics18,467–479.463Bryson,A.andHo,Y.(1969). Appliedoptimalcontrol: optimization,estimation,andcontrol.BlaisdellPub.Co.225Bryson,Jr.,A.E.andDenham,W.F.(1961).Asteepest-ascentmethodforsolvingoptimumprogrammingproblems.TechnicalReportBR-1303,RaytheonCompany,MissleandSpaceDivision.225Buciluˇa, C.,Caruana,R., and Niculescu-Mizil, A. (2006).Model compression.InProceedingsofthe12thACMSIGKDDinternationalconferenceonKnowledgediscoveryanddatamining,pages535–541.ACM.448Burda,Y.,Grosse,R.,andSalakhutdinov,R.(2015).Importanceweightedautoencoders.arXivpreprintarXiv:1509.00519.698Cai,M.,Shi,Y.,andLiu,J.(2013).Deepmaxoutneuralnetworksforspeechrecognition.InAutomaticSpeechRecognitionandUnderstanding(ASRU),2013IEEEWorkshopon,pages291–296.IEEE.194Carreira-Perpiñan,M.A.andHinton,G.E.(2005).Oncontrastivedivergencelearning.InR.G.CowellandZ.Ghahramani,editors,ProceedingsoftheTenthInternationalWorkshoponArtiﬁcialIntelligenceandStatistics(AISTATS’05),pages33–40.SocietyforArtiﬁcialIntelligenceandStatistics.611Caruana,R.(1993).Multitaskconnectionistlearning.InProc.1993ConnectionistModelsSummerSchool,pages372–379.244Cauchy,A.(1847).Méthodegénéralepourlarésolutiondesystèmesd’équationssimul-tanées.InCompterendudesséancesdel’académiedessciences,pages536–538.,83225728 BIBLIOGRAPHYCayton,L.(2005).Algorithmsformanifoldlearning.TechnicalReportCS2008-0923,UCSD.164Chandola,V.,Banerjee,A.,andKumar,V.(2009).Anomalydetection:Asurvey.ACMcomputingsurveys(CSUR),(3),15.41102Chapelle,O.,Weston,J.,andSchölkopf,B.(2003).Clusterkernelsforsemi-supervisedlearning.InS.Becker,S.Thrun,andK.Obermayer,editors,AdvancesinNeuralInformationProcessingSystems15(NIPS’02),pages585–592,Cambridge,MA.MITPress.244Chapelle,O.,Schölkopf,B.,andZien,A.,editors(2006).Semi-SupervisedLearning.MITPress,Cambridge,MA.,244541Chellapilla,K.,Puri,S.,andSimard,P.(2006).HighPerformanceConvolutionalNeuralNetworks forDocumentProcessing.In GuyLorette, editor, Tenth InternationalWorkshoponFrontiersinHandwritingRecognition,LaBaule(France).UniversitédeRennes1,Suvisoft.http://www.suvisoft.com.,,2427445Chen,B.,Ting,J.-A.,Marlin,B.M.,anddeFreitas,N.(2010).Deeplearningofinvariantspatio-temporalfeaturesfromvideo.NIPS*2010DeepLearningandUnsupervisedFeatureLearningWorkshop.360Chen,S.F.andGoodman,J.T.(1999).Anempiricalstudyofsmoothingtechniquesforlanguagemodeling.Computer,SpeechandLanguage,(4),359–393.,,13462463473Chen,T.,Du,Z.,Sun,N.,Wang,J.,Wu,C.,Chen,Y.,andTemam,O.(2014a).DianNao:Asmall-footprinthigh-throughputacceleratorforubiquitousmachine-learning.InPro-ceedingsofthe19thinternationalconferenceonArchitecturalsupportforprogramminglanguagesandoperatingsystems,pages269–284.ACM.451Chen,T.,Li,M.,Li,Y.,Lin,M.,Wang,N.,Wang,M.,Xiao,T.,Xu,B.,Zhang,C.,andZhang,Z.(2015).MXNet: Aﬂexibleandeﬃcientmachinelearninglibraryforheterogeneousdistributedsystems.arXivpreprintarXiv:1512.01274.25Chen,Y.,Luo,T.,Liu,S.,Zhang,S.,He,L.,Wang,J.,Li,L.,Chen,T.,Xu,Z.,Sun,N.,etal.Microarchitecture(2014b).DaDianNao:Amachine-learningsupercomputer.In(MICRO),201447thAnnualIEEE/ACMInternationalSymposiumon,pages609–622.IEEE.451Chilimbi,T.,Suzue,Y.,Apacible,J.,andKalyanaraman,K.(2014).ProjectAdam:Buildinganeﬃcientandscalabledeeplearningtrainingsystem.In11thUSENIXSymposiumonOperatingSystemsDesignandImplementation(OSDI’14).447Cho,K.,Raiko,T.,andIlin,A.(2010).ParalleltemperingiseﬃcientforlearningrestrictedBoltzmannmachines.In.,IJCNN’2010603614729 BIBLIOGRAPHYCho,K.,Raiko,T.,andIlin,A.(2011).EnhancedgradientandadaptivelearningratefortrainingrestrictedBoltzmannmachines.In,pages105–112.ICML’2011674Cho,K.,vanMerriënboer,B.,Gulcehre,C.,Bougares,F.,Schwenk,H.,andBengio,Y.(2014a).LearningphraserepresentationsusingRNNencoder-decoderforstatisticalmachinetranslation. InProceedingsoftheEmpiricialMethodsinNaturalLanguageProcessing(EMNLP2014).,,397474475Cho,K.,VanMerriënboer,B.,Bahdanau,D.,andBengio,Y.(2014b).Ontheprop-ertiesofneuralmachinetranslation:Encoder-decoderapproaches.,ArXive-printsabs/1409.1259.412Choromanska,A.,Henaﬀ,M.,Mathieu,M.,Arous,G.B.,andLeCun,Y.(2014). Thelosssurfaceofmultilayernetworks.,285286Chorowski,J.,Bahdanau,D.,Cho,K.,andBengio,Y.(2014). End-to-endcontinuousspeechrecognitionusingattention-basedrecurrentNN:Firstresults.arXiv:1412.1602.461Christianson,B.(1992).AutomaticHessiansbyreverseaccumulation.IMAJournalofNumericalAnalysis,(2),135–150.12224Chrupala,G.,Kadar,A.,andAlishahi,A.(2015).Learninglanguagethroughpictures.arXiv1506.03694.412Chung,J.,Gulcehre,C.,Cho,K.,andBengio,Y.(2014).Empiricalevaluationofgatedrecurrentneuralnetworksonsequencemodeling.NIPS’2014DeepLearningworkshop,arXiv1412.3555.,412460Chung,J.,Gülçehre,Ç.,Cho,K.,andBengio,Y.(2015a).Gatedfeedbackrecurrentneuralnetworks.In.ICML’15412Chung,J.,Kastner,K.,Dinh,L.,Goel,K.,Courville,A.,andBengio,Y.(2015b).Arecurrentlatentvariablemodelforsequentialdata.In.NIPS’2015698Ciresan,D.,Meier,U.,Masci,J.,andSchmidhuber,J.(2012).Multi-columndeepneuralnetworkfortraﬃcsignclassiﬁcation.NeuralNetworks,,333–338.,3223201Ciresan,D.C.,Meier,U.,Gambardella,L.M.,andSchmidhuber,J.(2010). Deepbigsimpleneuralnetsforhandwrittendigitrecognition.NeuralComputation,22,1–14.2427446,,Coates,A.andNg,A.Y.(2011).Theimportanceofencodingversustrainingwithsparsecodingandvectorquantization.In.,,ICML’201127256498Coates, A.,Lee, H.,andNg,A.Y. (2011).Ananalysisofsingle-layernetworksinunsupervisedfeaturelearning.InProceedingsoftheThirteenthInternationalConferenceonArtiﬁcialIntelligenceandStatistics(AISTATS2011).,,363364455730 BIBLIOGRAPHYCoates,A.,Huval,B.,Wang, T.,Wu, D.,Catanzaro, B.,and Andrew,N. (2013).DeeplearningwithCOTSHPCsystems.InS.DasguptaandD.McAllester,editors,Proceedingsofthe30thInternationalConferenceonMachineLearning(ICML-13),volume28(3),pages1337–1345.JMLRWorkshopandConferenceProceedings.,,2427364447,Cohen,N.,Sharir,O.,andShashua,A.(2015).Ontheexpressivepowerofdeeplearning:Atensoranalysis.arXiv:1509.05009.554Collobert,R.(2004).LargeScaleMachineLearning.Ph.D.thesis,UniversitédeParisVI,LIP6.197Collobert,R.(2011).Deeplearningforeﬃcientdiscriminativeparsing.InAISTATS’2011.101477,Collobert,R.andWeston,J.(2008a).Auniﬁedarchitecturefornaturallanguageprocessing:Deepneuralnetworkswithmultitasklearning.In.,ICML’2008471477Collobert, R. and Weston,J. (2008b).A uniﬁed architecture fornatural languageprocessing:Deepneuralnetworkswithmultitasklearning.In.ICML’2008535Collobert,R.,Bengio,S.,andBengio,Y.(2001). AparallelmixtureofSVMsforverylargescaleproblems.TechnicalReportIDIAP-RR-01-12,IDIAP.450Collobert,R.,Bengio,S.,andBengio,Y.(2002).ParallelmixtureofSVMsforverylargescaleproblems.NeuralComputation,(5),1105–1114.14450Collobert,R.,Weston,J.,Bottou,L.,Karlen,M.,Kavukcuoglu,K.,andKuksa,P.(2011a).Naturallanguageprocessing(almost)fromscratch.TheJournalofMachineLearningResearch,,2493–2537.,,,12328477535536Collobert,R.,Kavukcuoglu,K.,andFarabet,C.(2011b).Torch7:AMatlab-likeenviron-mentformachinelearning.InBigLearn,NIPSWorkshop.,,25214446Comon,P.(1994).Independentcomponentanalysis-anewconcept?SignalProcessing,36,287–314.491Cortes,C.andVapnik,V.(1995).Supportvectornetworks.MachineLearning,20,273–297.,18141Couprie,C.,Farabet,C.,Najman,L.,andLeCun,Y.(2013).Indoorsemanticsegmentationusingdepthinformation.InInternationalConferenceonLearningRepresentations(ICLR2013).,23201Courbariaux,M.,Bengio,Y.,andDavid,J.-P.(2015).Lowprecisionarithmeticfordeeplearning.InArxiv:1412.7024,ICLR’2015Workshop.452Courville,A.,Bergstra,J.,andBengio,Y.(2011).Unsupervisedmodelsofimagesbyspike-and-slabRBMs.In.,ICML’11561681731 BIBLIOGRAPHYCourville,A.,Desjardins,G.,Bergstra,J.,andBengio,Y.(2014).Thespike-and-slabRBMandextensionstodiscreteandsparsedatadistributions.PatternAnalysisandMachineIntelligence,IEEETransactionson,(9),1874–1887.36682Cover,T.M.andThomas,J.A.(2006).ElementsofInformationTheory,2ndEdition.Wiley-Interscience.73Cox,D.andPinto,N.(2011).Beyondsimplefeatures:Alarge-scalefeaturesearchapproachtounconstrainedfacerecognition.InAutomaticFace&GestureRecognitionandWorkshops(FG2011),2011IEEEInternationalConferenceon,pages8–15.IEEE.363Cramér,H.(1946).Mathematicalmethodsofstatistics.PrincetonUniversityPress.,135295Crick,F.H.C.andMitchison,G.(1983).Thefunctionofdreamsleep.Nature,304,111–114.609Cybenko,G.(1989).Approximationbysuperpositionsofasigmoidalfunction.MathematicsofControl,Signals,andSystems,,303–314.2198Dahl,G.E.,Ranzato,M.,Mohamed,A.,andHinton,G.E.(2010).Phonerecognitionwiththemean-covariancerestrictedBoltzmannmachine.In.NIPS’201023Dahl,G.E.,Yu,D.,Deng,L.,andAcero,A.(2012).Context-dependentpre-traineddeepneuralnetworksforlargevocabularyspeechrecognition.IEEETransactionsonAudio,Speech,andLanguageProcessing,(1),33–42.20459Dahl,G.E.,Sainath,T.N.,andHinton,G.E.(2013).ImprovingdeepneuralnetworksforLVCSRusingrectiﬁedlinearunitsanddropout.In.ICASSP’2013460Dahl,G.E.,Jaitly,N.,andSalakhutdinov,R.(2014).Multi-taskneuralnetworksforQSARpredictions.arXiv:1406.1231.26Dauphin, Y.andBengio, Y.(2013).Stochasticratiomatchingof RBMsforsparsehigh-dimensionalinputs.In.NIPSFoundation.NIPS26619Dauphin,Y.,Glorot,X.,andBengio,Y.(2011).Large-scalelearningofembeddingswithreconstructionsampling.In.ICML’2011471Dauphin,Y.,Pascanu,R.,Gulcehre,C.,Cho,K.,Ganguli,S.,andBengio,Y.(2014).Identifyingandattackingthesaddlepointprobleminhigh-dimensionalnon-convexoptimization.In.,,NIPS’2014285286288Davis,A.,Rubinstein,M.,Wadhwa,N.,Mysore,G.,Durand,F.,andFreeman,W.T.(2014).Thevisualmicrophone:Passiverecoveryofsoundfromvideo.ACMTransactionsonGraphics(Proc.SIGGRAPH),(4),79:1–79:10.33452732 BIBLIOGRAPHYDayan,P.(1990).Reinforcementcomparison.InConnectionistModels:Proceedingsofthe1990ConnectionistSummerSchool,SanMateo,CA.691Dayan,P.andHinton,G.E.(1996).VarietiesofHelmholtzmachine.NeuralNetworks,9(8),1385–1403.693Dayan,P.,Hinton,G.E.,Neal,R.M.,andZemel,R.S.(1995).TheHelmholtzmachine.Neuralcomputation,(5),889–904.7693Dean,J.,Corrado,G.,Monga,R.,Chen,K.,Devin,M.,Le,Q.,Mao,M.,Ranzato,M.,Senior,A.,Tucker,P.,Yang,K.,andNg,A.Y.(2012).Largescaledistributeddeepnetworks.In.,NIPS’201225447Dean,T.andKanazawa,K.(1989).Amodelforreasoningaboutpersistenceandcausation.ComputationalIntelligence,(3),142–150.5662Deerwester,S.,Dumais,S.T.,Furnas,G.W.,Landauer,T.K.,andHarshman,R.(1990).Indexingbylatentsemanticanalysis.JournaloftheAmericanSocietyforInformationScience,(6),391–407.,41477482Delalleau,O.andBengio,Y.(2011).Shallowvs.deepsum-productnetworks.In.NIPS19554,Deng,J.,Dong,W.,Socher,R.,Li,L.-J.,Li,K.,andFei-Fei,L.(2009).ImageNet: ALarge-ScaleHierarchicalImageDatabase.In.CVPR0921Deng,J.,Berg,A.C.,Li,K.,andFei-Fei,L.(2010a).Whatdoesclassifyingmorethan10,000imagecategoriestellus?InProceedingsofthe11thEuropeanConferenceonComputerVision:PartV,ECCV’10,pages71–84,Berlin,Heidelberg.Springer-Verlag.21Deng,L.andYu,D.(2014).Deeplearning–methodsandapplications.FoundationsandTrendsinSignalProcessing.460Deng,L.,Seltzer,M.,Yu,D.,Acero,A.,Mohamed,A.,andHinton,G.(2010b).Binarycodingofspeechspectrogramsusingadeepauto-encoder.InInterspeech2010,Makuhari,Chiba,Japan.23Denil,M.,Bazzani,L.,Larochelle,H.,anddeFreitas,N.(2012).Learningwheretoattendwithdeeparchitecturesforimagetracking.NeuralComputation,24(8),2151–2184.367Denton,E.,Chintala,S.,Szlam,A.,andFergus,R.(2015).DeepgenerativeimagemodelsusingaLaplacianpyramidofadversarialnetworks..,NIPS702719Desjardins,G.andBengio,Y.(2008).EmpiricalevaluationofconvolutionalRBMsforvision. TechnicalReport1327,Départementd’InformatiqueetdeRechercheOpéra-tionnelle,UniversitédeMontréal.683733 BIBLIOGRAPHYDesjardins, G., Courville, A.C., Bengio, Y., Vincent, P., andDelalleau, O.(2010).TemperedMarkovchainMonteCarlofortrainingofrestrictedBoltzmannmachines.InInternationalConferenceonArtiﬁcialIntelligenceandStatistics,pages145–152.,603614Desjardins,G.,Courville,A.,andBengio,Y.(2011).Ontrackingthepartitionfunction.In.NIPS’2011629Desjardins,G.,Simonyan,K.,Pascanu,R.,(2015). Naturalneuralnetworks. Inetal.AdvancesinNeuralInformationProcessingSystems,pages2062–2070.320Devlin,J.,Zbib,R.,Huang,Z.,Lamar,T.,Schwartz,R.,andMakhoul,J.(2014).Fastandrobustneuralnetworkjointmodelsforstatisticalmachinetranslation. InProc.ACL’2014.473Devroye,L.(2013).Non-UniformRandomVariateGeneration.SpringerLink:Bücher.SpringerNewYork.694DiCarlo,J.J.(2013).Mechanismsunderlyingvisualobjectrecognition:Humansvs.neuronsvs.machines.NIPSTutorial.,26366Dinh,L.,Krueger,D.,andBengio,Y.(2014).NICE:Non-linearindependentcomponentsestimation.arXiv:1410.8516.493Donahue,J.,Hendricks,L.A.,Guadarrama,S.,Rohrbach,M.,Venugopalan,S.,Saenko,K.,andDarrell,T.(2014).Long-termrecurrentconvolutionalnetworksforvisualrecognitionanddescription.arXiv:1411.4389.102Donoho,D.L.andGrimes,C.(2003).Hessianeigenmaps:newlocallylinearembeddingtechniquesforhigh-dimensional data.TechnicalReport2003-08, Dept.Statistics,StanfordUniversity.,164519Dosovitskiy,A.,Springenberg,J.T.,andBrox,T.(2015).Learningtogeneratechairswithconvolutionalneuralnetworks.InProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition,pages1538–1546.,,696704705Doya,K.(1993).Bifurcationsofrecurrentneuralnetworksingradientdescentlearning.IEEETransactionsonNeuralNetworks,,75–80.,1401403Dreyfus, S. E.(1962).The numerical solutionofvariational problems.JournalofMathematicalAnalysisandApplications,,30–45.5(1)225Dreyfus,S.E.(1973).Thecomputationalsolutionofoptimalcontrolproblemswithtimelag.IEEETransactionsonAutomaticControl,,383–385.18(4)225Drucker,H.andLeCun,Y.(1992).Improvinggeneralisationperformanceusingdoubleback-propagation.IEEETransactionsonNeuralNetworks,(6),991–997.3271734 BIBLIOGRAPHYDuchi,J.,Hazan,E.,andSinger,Y.(2011). Adaptivesubgradientmethodsforonlinelearningandstochasticoptimization.JournalofMachineLearningResearch.307Dudik,M.,Langford,J.,andLi,L.(2011).Doublyrobustpolicyevaluationandlearning.InProceedingsofthe28thInternationalConferenceonMachinelearning,ICML’11.482Dugas,C.,Bengio,Y.,Bélisle,F.,andNadeau,C.(2001). Incorporatingsecond-orderfunctionalknowledgeforbetteroptionpricing.InT.Leen,T.Dietterich,andV.Tresp,editors, AdvancesinNeural InformationProcessingSystems 13(NIPS’00), pages472–478.MITPress.,68197Dziugaite,G.K.,Roy,D.M.,andGhahramani,Z.(2015).Traininggenerativeneuralnet-worksviamaximummeandiscrepancyoptimization.arXivpreprintarXiv:1505.03906.703ElHihi,S.andBengio,Y.(1996).Hierarchicalrecurrentneuralnetworksforlong-termdependencies.In.,,NIPS’1995398407408Elkahky,A.M.,Song,Y.,andHe,X.(2015).Amulti-viewdeeplearningapproachforcrossdomainusermodelinginrecommendationsystems. InProceedingsofthe24thInternationalConferenceonWorldWideWeb,pages278–288.480Elman,J.L.(1993).Learninganddevelopmentinneuralnetworks:Theimportanceofstartingsmall.Cognition,,781–799.48328Erhan,D.,Manzagol,P.-A.,Bengio,Y.,Bengio,S.,andVincent,P.(2009).Thediﬃcultyoftrainingdeeparchitecturesandtheeﬀectofunsupervisedpre-training.InProceedingsofAISTATS’2009.201Erhan,D.,Bengio,Y.,Courville,A.,Manzagol,P.,Vincent,P.,andBengio,S.(2010).Whydoesunsupervisedpre-traininghelpdeeplearning?J.MachineLearningRes.529533534,,Fahlman,S.E.,Hinton,G.E.,andSejnowski,T.J.(1983).Massivelyparallelarchitecturesfor AI:NETL,thistle, andBoltzmann machines.In Proceedings ofthe NationalConferenceonArtiﬁcialIntelligenceAAAI-83.,570654Fang,H.,Gupta,S.,Iandola,F.,Srivastava,R.,Deng,L.,Dollár,P.,Gao,J.,He,X.,Mitchell,M.,Platt,J.C.,Zitnick,C.L.,andZweig,G.(2015).Fromcaptionstovisualconceptsandback.arXiv:1411.4952.102Farabet,C.,LeCun,Y.,Kavukcuoglu,K.,Culurciello,E.,Martini,B.,Akselrod,P.,andTalay,S.(2011).Large-scaleFPGA-basedconvolutionalnetworks.InR.Bekkerman,M.Bilenko,andJ.Langford, editors,ScalingupMachineLearning:ParallelandDistributedApproaches.CambridgeUniversityPress.523735 BIBLIOGRAPHYFarabet,C.,Couprie,C.,Najman,L.,andLeCun,Y.(2013).Learninghierarchicalfeaturesforscenelabeling.IEEETransactionsonPatternAnalysisandMachineIntelligence,35(8),1915–1929.,,23201360Fei-Fei,L.,Fergus,R.,andPerona,P.(2006).One-shotlearningofobjectcategories.IEEETransactionsonPatternAnalysisandMachineIntelligence,28(4),594–611.538Finn,C.,Tan,X.Y.,Duan,Y.,Darrell,T.,Levine,S.,andAbbeel,P.(2015).Learningvisualfeaturespacesforroboticmanipulationwithdeepspatialautoencoders.arXivpreprintarXiv:1509.06113.25Fisher,R.A.(1936).Theuseofmultiplemeasurementsintaxonomicproblems.AnnalsofEugenics,,179–188.,721105Földiák,P.(1989).Adaptivenetworkforoptimallinearfeatureextraction.InInternationalJointConferenceonNeuralNetworks(IJCNN),volume1,pages401–405,Washington1989.IEEE,NewYork.494Franzius,M.,Sprekeler,H.,andWiskott,L.(2007).Slownessandsparsenessleadtoplace,head-direction,andspatial-viewcells.495Franzius,M.,Wilbert,N.,andWiskott,L.(2008).Invariantobjectrecognitionwithslowfeatureanalysis.InArtiﬁcialNeuralNetworks-ICANN2008,pages961–970.Springer.496Frasconi,P.,Gori,M.,andSperduti,A.(1997).Ontheeﬃcientclassiﬁcationofdatastructuresbyneuralnetworks.InProc.Int.JointConf.onArtiﬁcialIntelligence.401Frasconi, P., Gori, M., andSperduti, A.(1998).Ageneralframeworkforadaptiveprocessingofdatastructures.IEEETransactionsonNeuralNetworks,9(5),768–786.401Freund,Y.andSchapire,R.E.(1996a).Experimentswithanewboostingalgorithm.InMachineLearning:ProceedingsofThirteenthInternationalConference,pages148–156,USA.ACM.258Freund,Y.andSchapire,R.E.(1996b).Gametheory,on-linepredictionandboosting.InProceedingsoftheNinthAnnualConferenceonComputationalLearningTheory,pages325–332.258Frey,B.J.(1998).Graphicalmodelsformachinelearninganddigitalcommunication.MITPress.,705706Frey,B.J.,Hinton,G.E.,andDayan,P.(1996).Doesthewake-sleepalgorithmlearngooddensityestimators?InD.Touretzky,M.Mozer,andM.Hasselmo,editors,AdvancesinNeuralInformationProcessingSystems8(NIPS’95),pages661–670.MITPress,Cambridge,MA.651736 BIBLIOGRAPHYFrobenius,G.(1908).Übermatrizenauspositivenelementen,s.B.Preuss.Akad.Wiss.Berlin,Germany.597Fukushima,K.(1975).Cognitron:Aself-organizingmultilayeredneuralnetwork.BiologicalCybernetics,,121–136.,,2016226528Fukushima, K.(1980).Neocognitron:Aself-organizingneuralnetworkmodelforamechanismofpatternrecognitionunaﬀectedbyshiftinposition.BiologicalCybernetics,36,193–202.,,,,162427226367Gal,Y.andGhahramani,Z.(2015).BayesianconvolutionalneuralnetworkswithBernoulliapproximatevariationalinference.arXivpreprintarXiv:1506.02158.264Gallinari,P.,LeCun,Y.,Thiria,S.,andFogelman-Soulie,F.(1987).Memoiresassociativesdistribuees.InProceedingsofCOGNITIVA87,Paris,LaVillette.515Garcia-Duran,A.,Bordes,A.,Usunier,N.,andGrandvalet,Y.(2015).Combiningtwoandthree-wayembeddingsmodelsforlinkpredictioninknowledgebases.arXivpreprintarXiv:1506.00999.484Garofolo,J.S.,Lamel,L.F.,Fisher,W.M.,Fiscus,J.G.,andPallett,D.S.(1993).Darpatimitacoustic-phoneticcontinousspeechcorpuscd-rom.nistspeechdisc1-1.1.NASASTI/ReconTechnicalReportN,,27403.93459Garson,J.(1900).Themetricsystemofidentiﬁcationofcriminals,asusedinGreatBritainandIreland.TheJournaloftheAnthropologicalInstituteofGreatBritainandIreland,(2),177–227.21Gers,F.A.,Schmidhuber,J.,andCummins,F.(2000). Learningtoforget:ContinualpredictionwithLSTM.Neuralcomputation,(10),2451–2471.,12410412Ghahramani,Z.andHinton,G.E.(1996).TheEMalgorithmformixturesoffactoranalyzers.TechnicalReportCRG-TR-96-1,Dpt.ofComp.Sci.,Univ.ofToronto.489Gillick,D.,Brunk,C.,Vinyals,O.,andSubramanya,A.(2015).Multilinguallanguageprocessingfrombytes.arXivpreprintarXiv:1512.00103.477Girshick,R.,Donahue,J.,Darrell,T.,andMalik,J.(2015).Region-basedconvolutionalnetworksforaccurateobjectdetectionandsegmentation.426Giudice,M.D.,Manera,V.,andKeysers,C.(2009).Programmedtolearn?Theontogenyofmirrorneurons.,(2),350––363.Dev.Sci.12656Glorot,X.andBengio,Y.(2010).Understandingthediﬃcultyoftrainingdeepfeedforwardneuralnetworks.InAISTATS’2010.303Glorot,X.,Bordes,A.,andBengio,Y.(2011a).Deepsparserectiﬁerneuralnetworks.InAISTATS’2011.,,,,16174197226227737 BIBLIOGRAPHYGlorot, X.,Bordes, A.,andBengio, Y.(2011b).Domainadaptationforlarge-scalesentimentclassiﬁcation:Adeeplearningapproach.In.,ICML’2011507537Goldberger,J.,Roweis,S.,Hinton,G.E.,andSalakhutdinov,R.(2005).Neighbourhoodcomponentsanalysis.InL.Saul,Y.Weiss,andL.Bottou,editors,AdvancesinNeuralInformationProcessingSystems17(NIPS’04).MITPress.115Gong,S.,McKenna,S.,andPsarrou,A.(2000).DynamicVision:FromImagestoFaceRecognition.ImperialCollegePress.,165519Goodfellow,I.,Le,Q.,Saxe,A., andNg,A.(2009).Measuringinvariancesindeepnetworks.In,pages646–654.NIPS’2009255Goodfellow,I.,Koenig,N.,Muja,M.,Pantofaru,C.,Sorokin,A.,andTakayama,L.(2010).Helpmehelpyou:Interfacesforpersonalrobots.InProc.ofHumanRobotInteraction(HRI),Osaka,Japan.ACMPress,ACMPress.100Goodfellow,I.J.(2010).Technicalreport:Multidimensional,downsampledconvolutionforautoencoders.Technicalreport,UniversitédeMontréal.357Goodfellow,I.J.(2014).Ondistinguishabilitycriteriaforestimatinggenerativemodels.InInternationalConferenceonLearningRepresentations,WorkshopsTrack.,,622700701Goodfellow,I.J.,Courville,A.,andBengio,Y.(2011).Spike-and-slabsparsecodingforunsupervisedfeaturediscovery.InNIPSWorkshoponChallengesinLearningHierarchicalModels.,532538Goodfellow,I.J.,Warde-Farley,D.,Mirza,M.,Courville,A.,andBengio,Y.(2013a).Maxoutnetworks.InS.DasguptaandD.McAllester,editors,,pages1319–ICML’131327.,,,,193264344365455Goodfellow,I.J.,Mirza,M.,Courville,A.,andBengio,Y.(2013b).Multi-predictiondeepBoltzmannmachines.In.NIPSFoundation.,,,,,,,NIPS26100617671672673674675698Goodfellow,I.J.,Warde-Farley,D.,Lamblin,P.,Dumoulin,V.,Mirza,M.,Pascanu,R.,Bergstra,J.,Bastien,F.,andBengio,Y.(2013c).Pylearn2:amachinelearningresearchlibrary.arXivpreprintarXiv:1308.4214.,25446Goodfellow,I.J.,Courville,A.,andBengio,Y.(2013d).Scalingupspike-and-slabmodelsforunsupervisedfeaturelearning.IEEETransactionsonPatternAnalysisandMachineIntelligence,(8),1902–1914.,,,,35497498499650683Goodfellow,I.J.,Mirza,M.,Xiao,D.,Courville,A.,andBengio,Y.(2014a).Anempiricalinvestigationofcatastrophicforgetingingradient-basedneuralnetworks.In.ICLR’2014194738 BIBLIOGRAPHYGoodfellow,I.J.,Shlens,J.,andSzegedy,C.(2014b).Explainingandharnessingadver-sarialexamples.,.,,,,CoRRabs/1412.6572268269271555556Goodfellow,I.J.,Pouget-Abadie,J.,Mirza,M.,Xu,B.,Warde-Farley,D.,Ozair,S.,Courville,A.,andBengio,Y.(2014c).Generativeadversarialnetworks.In.NIPS’2014544689699701704,,,,Goodfellow,I.J.,Bulatov,Y.,Ibarz,J.,Arnoud,S.,andShet,V.(2014d).Multi-digitnumberrecognitionfromStreetViewimageryusingdeepconvolutionalneuralnetworks.InInternationalConferenceonLearningRepresentations.,,,,,,25101201202203391422449,Goodfellow,I.J.,Vinyals,O.,andSaxe,A.M.(2015).Qualitativelycharacterizingneuralnetworkoptimizationproblems.InInternationalConferenceonLearningRepresenta-tions.,,,285286287291Goodman,J.(2001).Classes forfast maximumentropytraining.InInternationalConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),Utah.467Gori,M.andTesi,A.(1992).Ontheproblemoflocalminimainbackpropagation.IEEETransactionsonPatternAnalysisandMachineIntelligence,PAMI-14(1),76–86.284Gosset,W.S.(1908).Theprobableerrorofamean.,Biometrika6(1),1–25.Originallypublishedunderthepseudonym“Student”.21Gouws,S.,Bengio,Y.,andCorrado,G.(2014).BilBOWA:Fastbilingualdistributedrepresentationswithoutwordalignments.Technicalreport,arXiv:1410.2455.,476539Graf,H.P.andJackel,L.D.(1989).Analogelectronicneuralnetworkcircuits.CircuitsandDevicesMagazine,IEEE,(4),44–49.5451Graves,A.(2011).Practicalvariationalinferenceforneuralnetworks.In.NIPS’2011242Graves,A.(2012).SupervisedSequenceLabellingwithRecurrentNeuralNetworks.StudiesinComputationalIntelligence.Springer.,,,374395411460Graves,A.(2013).Generatingsequenceswithrecurrentneuralnetworks.Technicalreport,arXiv:1308.0850.,,,190410415420Graves,A.andJaitly,N.(2014).Towardsend-to-endspeechrecognitionwithrecurrentneuralnetworks.In.ICML’2014410Graves,A.andSchmidhuber,J.(2005).Framewisephonemeclassiﬁcationwithbidirec-tionalLSTMandotherneuralnetworkarchitectures.NeuralNetworks,18(5),602–610.395Graves,A.andSchmidhuber,J.(2009).Oﬄinehandwritingrecognitionwithmultidi-mensionalrecurrentneuralnetworks.InD.Koller,D.Schuurmans,Y.Bengio,andL.Bottou,editors,,pages545–552.NIPS’2008395739 BIBLIOGRAPHYGraves,A.,Fernández,S.,Gomez,F.,andSchmidhuber,J.(2006).Connectionisttemporalclassiﬁcation:Labellingunsegmentedsequencedatawithrecurrentneuralnetworks.InICML’2006,pages369–376,Pittsburgh,USA.460Graves,A.,Liwicki,M.,Bunke,H.,Schmidhuber,J.,andFernández,S.(2008).Uncon-strainedon-linehandwritingrecognitionwithrecurrentneuralnetworks.InJ.Platt,D.Koller,Y.Singer,andS.Roweis,editors,,pages577–584.NIPS’2007395Graves,A.,Liwicki,M.,Fernández,S.,Bertolami,R.,Bunke,H.,andSchmidhuber,J.(2009).Anovelconnectionistsystemforunconstrainedhandwritingrecognition.PatternAnalysisandMachineIntelligence,IEEETransactionson,(5),855–868.31410Graves,A.,Mohamed,A.,andHinton,G.(2013).Speechrecognitionwithdeeprecurrentneuralnetworks.In,pages6645–6649.,,,,ICASSP’2013395398410411460Graves,A.,Wayne,G.,andDanihelka,I.(2014a).NeuralTuringmachines.arXiv:1410.5401.25Graves,A.,Wayne,G.,andDanihelka,I.(2014b).NeuralTuringmachines.arXivpreprintarXiv:1410.5401.418Grefenstette,E.,Hermann,K.M.,Suleyman,M.,andBlunsom,P.(2015).Learningtotransducewithunboundedmemory.In.NIPS’2015418Greﬀ,K.,Srivastava,R.K.,Koutník,J.,Steunebrink,B.R.,andSchmidhuber,J.(2015).LSTM:asearchspaceodyssey.arXivpreprintarXiv:1503.04069.412Gregor,K.andLeCun,Y.(2010a).Emergenceofcomplex-likecellsinatemporalproductnetworkwithlocalreceptiveﬁelds.Technicalreport,arXiv:1006.0448.352Gregor,K.andLeCun,Y.(2010b).Learningfastapproximationsofsparsecoding.InL.BottouandM.Littman,editors,ProceedingsoftheTwenty-seventhInternationalConferenceonMachineLearning(ICML-10).ACM.652Gregor, K.,Danihelka, I.,Mnih,A., Blundell,C.,and Wierstra, D. (2014).Deepautoregressivenetworks.InInternationalConferenceonMachineLearning(ICML’2014).693Gregor,K.,Danihelka,I.,Graves,A.,andWierstra,D.(2015).DRAW:Arecurrentneuralnetworkforimagegeneration.arXivpreprintarXiv:1502.04623.698Gretton,A.,Borgwardt,K.M.,Rasch,M.J.,Schölkopf,B.,andSmola,A.(2012).Akerneltwo-sampletest.TheJournalofMachineLearningResearch,13(1),723–773.704Gülçehre,Ç.andBengio,Y.(2013).Knowledgematters:Importanceofpriorinformationforoptimization.InInternationalConferenceonLearningRepresentations(ICLR’2013).25740 BIBLIOGRAPHYGuo,H.andGelfand,S.B.(1992).Classiﬁcationtreeswithneuralnetworkfeatureextraction.NeuralNetworks,IEEETransactionson,(6),923–933.3450Gupta,S.,Agrawal,A.,Gopalakrishnan,K.,andNarayanan,P.(2015).Deeplearningwithlimitednumericalprecision.,.CoRRabs/1502.02551452Gutmann,M.andHyvarinen,A.(2010).Noise-contrastiveestimation: Anewestima-tionprincipleforunnormalizedstatisticalmodels. InProceedingsofTheThirteenthInternationalConferenceonArtiﬁcialIntelligenceandStatistics(AISTATS’10).620Hadsell, R., Sermanet, P., Ben, J.,Erkan,A., Han, J., Muller,U., andLeCun, Y.(2007).Onlinelearningforoﬀroadrobots:Spatiallabelpropagationtolearnlong-rangetraversability.InProceedingsofRobotics:ScienceandSystems,Atlanta,GA,USA.453Hajnal,A.,Maass,W.,Pudlak,P.,Szegedy,M.,andTuran,G.(1993).Thresholdcircuitsofboundeddepth.,,129–154.J.Comput.System.Sci.46199Håstad,J.(1986).Almostoptimallowerboundsforsmalldepthcircuits.InProceedingsofthe18thannualACMSymposiumonTheoryofComputing,pages6–20,Berkeley,California.ACMPress.199Håstad,J.andGoldmann,M.(1991).Onthepowerofsmall-depththresholdcircuits.ComputationalComplexity,,113–129.1199Hastie,T.,Tibshirani,R.,andFriedman,J.(2001).Theelementsofstatisticallearning:datamining,inferenceandprediction. SpringerSeriesinStatistics.SpringerVerlag.146He,K.,Zhang,X.,Ren,S.,andSun,J.(2015).Delvingdeepintorectiﬁers:Surpassinghuman-levelperformanceonImageNetclassiﬁcation.arXivpreprintarXiv:1502.01852.28193,Hebb,D.O.(1949).TheOrganizationofBehavior.Wiley,NewYork.,,1417656Henaﬀ,M.,Jarrett,K.,Kavukcuoglu,K.,andLeCun,Y.(2011).Unsupervisedlearningofsparsefeaturesforscalableaudioclassiﬁcation.In.ISMIR’11523Henderson,J.(2003).Inducinghistoryrepresentationsforbroadcoveragestatisticalparsing.InHLT-NAACL,pages103–110.477Henderson,J.(2004).Discriminativetrainingofaneuralnetworkstatisticalparser.InProceedingsofthe42ndAnnualMeetingonAssociationforComputationalLinguistics,page95.477Henniges,M.,Puertas,G.,Bornschein,J.,Eggert,J.,andLücke,J.(2010).Binarysparsecoding.InLatentVariableAnalysisandSignalSeparation,pages450–457.Springer.640741 BIBLIOGRAPHYHerault,J.andAns,B.(1984).Circuitsneuronauxàsynapsesmodiﬁables:Décodagedemessagescompositesparapprentissagenonsupervisé.ComptesRendusdel’AcadémiedesSciences,,525––528.299(III-13)491Hinton,G.(2012).Neuralnetworksformachinelearning.Coursera,videolectures.307Hinton,G.,Deng,L.,Dahl,G.E.,Mohamed,A.,Jaitly,N.,Senior,A.,Vanhoucke,V.,Nguyen,P.,Sainath,T.,andKingsbury,B.(2012a).Deepneuralnetworksforacousticmodelinginspeechrecognition.IEEESignalProcessingMagazine,29(6),82–97.,23460Hinton,G.,Vinyals,O.,andDean,J.(2015).Distillingtheknowledgeinaneuralnetwork.arXivpreprintarXiv:1503.02531.448Hinton,G.E.(1989).Connectionistlearningprocedures.ArtiﬁcialIntelligence,40,185–234.494Hinton,G.E.(1990).Mappingpart-wholehierarchiesintoconnectionistnetworks.ArtiﬁcialIntelligence,(1),47–75.46418Hinton,G.E.(1999).Productsofexperts.In.ICANN’1999571Hinton,G.E.(2000).Trainingproductsofexpertsbyminimizingcontrastivedivergence.TechnicalReportGCNUTR2000-004,GatsbyUnit,UniversityCollegeLondon.,610676Hinton,G.E.(2006).Torecognizeshapes,ﬁrstlearntogenerateimages.TechnicalReportUTMLTR2006-003,UniversityofToronto.,528595Hinton,G.E.(2007a).Howtodobackpropagationinabrain.InvitedtalkattheNIPS’2007DeepLearningWorkshop.656Hinton,G.E.(2007b). Learningmultiplelayersofrepresentation. Trendsincognitivesciences,(10),428–434.11660Hinton, G.E.(2010).ApracticalguidetotrainingrestrictedBoltzmannmachines.TechnicalReportUTMLTR2010-003,DepartmentofComputerScience,UniversityofToronto.610Hinton,G.E.andGhahramani,Z.(1997).Generativemodelsfordiscoveringsparsedistributedrepresentations.PhilosophicalTransactionsoftheRoyalSocietyofLondon.147Hinton,G.E.andMcClelland,J.L.(1988).Learningrepresentationsbyrecirculation.InNIPS’1987,pages358–366.502Hinton,G.E.andRoweis,S.(2003).Stochasticneighborembedding.In.NIPS’2002519742 BIBLIOGRAPHYHinton,G.E.andSalakhutdinov,R.(2006).Reducingthedimensionalityofdatawithneuralnetworks.Science,(5786),504–507.,,,,313509524528529534Hinton,G.E.andSejnowski,T.J.(1986).LearningandrelearninginBoltzmannmachines.InD.E.RumelhartandJ.L.McClelland,editors,ParallelDistributedProcessing,volume1,chapter7,pages282–317.MITPress,Cambridge.,570654Hinton,G.E.andSejnowski,T.J.(1999).Unsupervisedlearning:foundationsofneuralcomputation.MITpress.541Hinton,G.E.andShallice,T.(1991).Lesioninganattractornetwork:investigationsofacquireddyslexia.Psychologicalreview,(1),74.9813Hinton,G.E.andZemel,R.S.(1994).Autoencoders,minimumdescriptionlength,andHelmholtzfreeenergy.In.NIPS’1993502Hinton,G.E.,Sejnowski,T.J.,andAckley,D.H.(1984).Boltzmannmachines:Constraintsatisfactionnetworksthatlearn.TechnicalReportTR-CMU-CS-84-119,Carnegie-MellonUniversity,Dept.ofComputerScience.,570654Hinton,G.E.,McClelland,J.,andRumelhart,D.(1986). Distributedrepresentations.InD.E.RumelhartandJ.L.McClelland,editors,ParallelDistributedProcessing:ExplorationsintheMicrostructureofCognition,volume1,pages77–109.MITPress,Cambridge.,,17225526Hinton,G.E.,Revow,M.,andDayan,P.(1995a).Recognizinghandwrittendigitsusingmixturesoflinearmodels.InG.Tesauro,D.Touretzky,andT.Leen,editors,AdvancesinNeuralInformationProcessingSystems7(NIPS’94),pages1015–1022.MITPress,Cambridge,MA.489Hinton,G.E.,Dayan,P.,Frey,B.J.,andNeal,R.M.(1995b).Thewake-sleepalgorithmforunsupervisedneuralnetworks.Science,,1558–1161.,268504651Hinton,G.E.,Dayan,P.,andRevow,M.(1997).Modellingthemanifoldsofimagesofhandwrittendigits.IEEETransactionsonNeuralNetworks,,65–74.8499Hinton,G.E.,Welling,M.,Teh,Y.W.,andOsindero,S.(2001).AnewviewofICA.InProceedingsof3rdInternationalConferenceonIndependentComponentAnalysisandBlindSignalSeparation(ICA’01),pages746–751,SanDiego,CA.491Hinton,G.E.,Osindero,S.,andTeh,Y.(2006).Afastlearningalgorithmfordeepbeliefnets.NeuralComputation,,1527–1554.,,,,,,,18141927143528529660661Hinton,G.E., Deng,L.,Yu, D.,Dahl,G.E.,Mohamed, A.,Jaitly, N.,Senior,A.,Vanhoucke,V.,Nguyen,P.,Sainath,T.N.,andKingsbury,B.(2012b).Deepneuralnetworksforacousticmodelinginspeechrecognition:Thesharedviewsoffourresearchgroups.IEEESignalProcess.Mag.,(6),82–97.29101743 BIBLIOGRAPHYHinton,G.E.,Srivastava,N.,Krizhevsky,A.,Sutskever,I.,andSalakhutdinov,R.(2012c).Improvingneuralnetworksbypreventingco-adaptationoffeaturedetectors.Technicalreport,arXiv:1207.0580.,,238263267Hinton,G.E.,Vinyals,O.,andDean,J.(2014).Darkknowledge. InvitedtalkattheBayLearnBayAreaMachineLearningSymposium.448Hochreiter,S.(1991).UntersuchungenzudynamischenneuronalenNetzen.Diplomathesis,T.U.München.,,18401403Hochreiter,S.andSchmidhuber,J.(1995). Simplifyingneuralnetsbydiscoveringﬂatminima.InAdvancesinNeuralInformationProcessingSystems7,pages529–536.MITPress.243Hochreiter,S.andSchmidhuber,J.(1997).Longshort-termmemory.NeuralComputation,9(8),1735–1780.,,18410411Hochreiter,S.,Bengio,Y.,andFrasconi,P.(2001).Gradientﬂowinrecurrentnets:thediﬃcultyoflearninglong-termdependencies.InJ.KolenandS.Kremer,editors,FieldGuidetoDynamicalRecurrentNetworks.IEEEPress.411Holi,J.L.andHwang,J.-N.(1993).Finiteprecisionerroranalysisofneuralnetworkhardwareimplementations.Computers,IEEETransactionson,(3),281–290.42451Holt,J.L.andBaker,T.E.(1991). Backpropagationsimulationsusinglimitedpreci-sioncalculations.InNeuralNetworks,1991.,IJCNN-91-SeattleInternationalJointConferenceon,volume2,pages121–126.IEEE.451Hornik,K.,Stinchcombe,M.,andWhite,H.(1989).Multilayerfeedforwardnetworksareuniversalapproximators.NeuralNetworks,,359–366.2198Hornik,K.,Stinchcombe,M.,andWhite,H.(1990).Universalapproximationofanunknownmappinganditsderivativesusingmultilayerfeedforwardnetworks.Neuralnetworks,(5),551–560.3198Hsu,F.-H.(2002).BehindDeepBlue:BuildingtheComputerThatDefeatedtheWorldChessChampion.PrincetonUniversityPress,Princeton,NJ,USA.2Huang,F.andOgata,Y.(2002).Generalizedpseudo-likelihoodestimatesforMarkovrandomﬁeldsonlattice.AnnalsoftheInstituteofStatisticalMathematics,54(1),1–18.616Huang,P.-S.,He,X.,Gao,J.,Deng,L.,Acero,A.,andHeck,L.(2013).Learningdeepstructuredsemanticmodelsforwebsearchusingclickthroughdata.InProceedingsofthe22ndACMinternationalconferenceonConferenceoninformation&knowledgemanagement,pages2333–2338.ACM.480Hubel,D.andWiesel,T.(1968).Receptiveﬁeldsandfunctionalarchitectureofmonkeystriatecortex.JournalofPhysiology(London),,215–243.195364744 BIBLIOGRAPHYHubel,D.H.andWiesel,T.N.(1959).Receptiveﬁeldsofsingleneuronsinthecat’sstriatecortex.JournalofPhysiology,,574–591.148364Hubel,D.H.andWiesel, T.N.(1962).Receptiveﬁelds, binocularinteraction,andfunctionalarchitectureinthecat’svisualcortex.JournalofPhysiology(London),160,106–154.364Huszar,F.(2015).How(not)totrainyourgenerativemodel:schedulesampling,likelihood,adversary?.arXiv:1511.05101698Hutter,F.,Hoos,H.,andLeyton-Brown,K.(2011). Sequentialmodel-basedoptimizationforgeneralalgorithmconﬁguration.In.ExtendedversionasUBCTechreportLION-5TR-2010-10.436Hyotyniemi,H.(1996).Turingmachinesarerecurrentneuralnetworks.InSTeP’96,pages13–24.379Hyvärinen,A.(1999). Surveyonindependentcomponentanalysis.NeuralComputingSurveys,,94–128.2491Hyvärinen,A.(2005).Estimationofnon-normalizedstatisticalmodelsusingscorematching.JournalofMachineLearningResearch,,695–709.,6513617Hyvärinen,A.(2007a).Connectionsbetweenscorematching,contrastivedivergence,andpseudolikelihoodforcontinuous-valuedvariables.IEEETransactionsonNeuralNetworks,,1529–1531.18618Hyvärinen,A.(2007b).Someextensionsofscorematching.ComputationalStatisticsandDataAnalysis,,2499–2512.51618Hyvärinen,A.andHoyer,P.O.(1999).Emergenceoftopographyandcomplexcellpropertiesfromnaturalimagesusingextensionsofica.In,pages827–833.NIPS493Hyvärinen, A.andPajunen, P.(1999).Nonlinearindependentcomponentanalysis:Existenceanduniquenessresults.NeuralNetworks,(3),429–439.12493Hyvärinen,A.,Karhunen,J.,andOja,E.(2001a).IndependentComponentAnalysis.Wiley-Interscience.491Hyvärinen,A.,Hoyer,P.O.,andInki,M.O.(2001b).Topographicindependentcomponentanalysis.NeuralComputation,(7),1527–1558.13493Hyvärinen,A.,Hurri,J.,andHoyer,P.O.(2009).NaturalImageStatistics:Aprobabilisticapproachtoearlycomputationalvision.Springer-Verlag.370Iba,Y.(2001).ExtendedensembleMonteCarlo.InternationalJournalofModernPhysics,C12,623–656.603745 BIBLIOGRAPHYInayoshi, H. and Kurita, T. (2005).Improved generalizationbyadding both auto-associationandhidden-layernoisetoneural-network-based-classiﬁers.IEEEWorkshoponMachineLearningforSignalProcessing,pages141—-146.515Ioﬀe,S.andSzegedy,C.(2015).Batchnormalization:Acceleratingdeepnetworktrainingbyreducinginternalcovariateshift.,,100317320Jacobs,R.A.(1988). Increasedratesofconvergencethroughlearningrateadaptation.Neuralnetworks,(4),295–307.1307Jacobs,R.A.,Jordan,M.I.,Nowlan,S.J.,andHinton,G.E.(1991).Adaptivemixturesoflocalexperts.NeuralComputation,,79–87.,3189450Jaeger,H.(2003).Adaptivenonlinearsystemidentiﬁcationwithechostatenetworks.InAdvancesinNeuralInformationProcessingSystems15.404Jaeger,H.(2007a).Discoveringmultiscaledynamicalfeatureswithhierarchicalechostatenetworks.Technicalreport,JacobsUniversity.398Jaeger,H.(2007b).Echostatenetwork.Scholarpedia,(9),2330.2404Jaeger,H.(2012).Longshort-termmemoryinechostatenetworks:Detailsofasimulationstudy.Technicalreport,Technicalreport,JacobsUniversityBremen.405Jaeger,H.andHaas,H.(2004).Harnessingnonlinearity:Predictingchaoticsystemsandsavingenergyinwirelesscommunication.Science,(5667),78–80.,30427404Jaeger,H.,Lukosevicius,M.,Popovici,D.,andSiewert,U.(2007).Optimizationandapplicationsofechostatenetworkswithleaky-integratorneurons.NeuralNetworks,20(3),335–352.407Jain,V.,Murray,J.F.,Roth,F.,Turaga,S.,Zhigulin,V.,Briggman,K.L.,Helmstaedter,M.N.,Denk,W.,andSeung,H.S.(2007).Supervisedlearningofimagerestorationwithconvolutionalnetworks.InComputer Vision,2007.ICCV2007.IEEE11thInternationalConferenceon,pages1–8.IEEE.359Jaitly,N.andHinton,G.(2011).LearningabetterrepresentationofspeechsoundwavesusingrestrictedBoltzmannmachines.InAcoustics, SpeechandSignalProcessing(ICASSP),2011IEEEInternationalConferenceon,pages5884–5887.IEEE.458Jaitly,N.andHinton,G.E.(2013).Vocaltractlengthperturbation(VTLP)improvesspeechrecognition.In.ICML’2013241Jarrett,K.,Kavukcuoglu,K.,Ranzato,M.,andLeCun,Y.(2009).Whatisthebestmulti-stagearchitectureforobjectrecognition?In.,,,,,,ICCV’09162427174193226363364523,,Jarzynski,C.(1997).Nonequilibriumequalityforfreeenergydiﬀerences.Phys.Rev.Lett.,78,2690–2693.,625628746 BIBLIOGRAPHYJaynes,E.T.(2003).ProbabilityTheory:TheLogicofScience.CambridgeUniversityPress.53Jean,S.,Cho,K.,Memisevic,R.,andBengio,Y.(2014).Onusingverylargetargetvocabularyforneuralmachinetranslation.arXiv:1412.2007.,474475Jelinek,F.andMercer,R.L.(1980).InterpolatedestimationofMarkovsourceparametersfromsparsedata.InE.S.GelsemaandL.N.Kanal,editors,PatternRecognitioninPractice.North-Holland,Amsterdam.,462473Jia,Y.(2013).Caﬀe:Anopensourceconvolutionalarchitectureforfastfeatureembedding.http://caffe.berkeleyvision.org/.,25214Jia,Y.,Huang,C.,andDarrell,T.(2012).Beyondspatialpyramids:Receptiveﬁeldlearningforpooledimagefeatures.InComputerVisionandPatternRecognition(CVPR),2012IEEEConferenceon,pages3370–3377.IEEE.345Jim,K.-C.,Giles,C.L.,andHorne,B.G.(1996).Ananalysisofnoiseinrecurrentneuralnetworks: convergenceandgeneralization.IEEETransactionsonNeuralNetworks,7(6),1424–1438.242Jordan,M.I.(1998).LearninginGraphicalModels.Kluwer,Dordrecht,Netherlands.18Joulin,A.andMikolov,T.(2015).Inferringalgorithmicpatternswithstack-augmentedrecurrentnets.arXivpreprintarXiv:1503.01007.418Jozefowicz,R.,Zaremba,W.,andSutskever,I.(2015).Anempiricalevaluationofrecurrentnetworkarchitectures.In.,ICML’2015306412Judd,J.S.(1989).NeuralNetworkDesignandtheComplexityofLearning.MITpress.293Jutten, C.andHerault, J.(1991).Blindseparationofsources, partI:anadaptivealgorithmbasedonneuromimeticarchitecture.SignalProcessing,,1–10.24491Kahou,S.E.,Pal,C.,Bouthillier,X.,Froumenty,P.,Gülçehre,c.,Memisevic,R.,Vincent,P.,Courville,A.,Bengio,Y.,Ferrari,R.C.,Mirza,M.,Jean,S.,Carrier,P.L.,Dauphin,Y.,Boulanger-Lewandowski,N.,Aggarwal,A.,Zumer, J.,Lamblin,P.,Raymond,J.-P.,Desjardins,G.,Pascanu,R.,Warde-Farley,D.,Torabi,A.,Sharma,A.,Bengio,E.,Côté,M.,Konda,K.R.,andWu,Z.(2013).Combiningmodalityspeciﬁcdeepneuralnetworksforemotionrecognitioninvideo.InProceedingsofthe15thACMonInternationalConferenceonMultimodalInteraction.201Kalchbrenner,N.andBlunsom,P.(2013).Recurrentcontinuoustranslationmodels.InEMNLP’2013.,474475Kalchbrenner,N.,Danihelka,I.,andGraves,A.(2015). Gridlongshort-termmemory.arXivpreprintarXiv:1507.01526.395747 BIBLIOGRAPHYKamyshanska,H.andMemisevic,R.(2015).Thepotentialenergyofanautoencoder.IEEETransactionsonPatternAnalysisandMachineIntelligence.515Karpathy,A.andLi,F.-F.(2015).Deepvisual-semanticalignmentsforgeneratingimagedescriptions.In.arXiv:1412.2306.CVPR’2015102Karpathy,A.,Toderici,G.,Shetty,S.,Leung,T.,Sukthankar,R.,andFei-Fei,L.(2014).Large-scalevideoclassiﬁcationwithconvolutionalneuralnetworks.In.CVPR21Karush,W.(1939).MinimaofFunctionsofSeveralVariableswithInequalitiesasSideConstraints.Master’sthesis,Dept.ofMathematics,Univ.ofChicago.95Katz,S.M.(1987).Estimationofprobabilitiesfromsparsedataforthelanguagemodelcomponentofaspeechrecognizer.IEEETransactionsonAcoustics,Speech,andSignalProcessing,(3),400–401.,ASSP-35462473Kavukcuoglu,K.,Ranzato,M.,andLeCun,Y.(2008). Fastinferenceinsparsecodingalgorithmswithapplicationstoobjectrecognition.Technicalreport,ComputationalandBiologicalLearningLab,CourantInstitute,NYU.TechReportCBLL-TR-2008-12-01.523Kavukcuoglu,K.,Ranzato,M.-A.,Fergus,R.,andLeCun,Y.(2009).Learninginvariantfeaturesthroughtopographicﬁltermaps.In.CVPR’2009523Kavukcuoglu,K.,Sermanet,P.,Boureau,Y.-L.,Gregor,K.,Mathieu,M.,andLeCun,Y.(2010).Learningconvolutionalfeaturehierarchiesforvisualrecognition.In.NIPS’2010364523,Kelley,H.J.(1960).Gradienttheoryofoptimalﬂightpaths.,ARSJournal30(10),947–954.225Khan,F.,Zhu,X.,andMutlu,B.(2011).Howdohumansteach:Oncurriculumlearningandteachingdimension.InAdvancesinNeuralInformationProcessingSystems24(NIPS’11),pages1449–1457.328Kim,S.K.,McAfee,L.C.,McMahon,P.L.,andOlukotun,K.(2009).AhighlyscalablerestrictedBoltzmannmachineFPGAimplementation.InFieldProgrammableLogicandApplications,2009.FPL2009.InternationalConferenceon,pages367–372.IEEE.451Kindermann,R.(1980).MarkovRandomFieldsandTheirApplications(ContemporaryMathematics;V.1).AmericanMathematicalSociety.566Kingma,D.andBa,J.(2014).Adam:Amethodforstochasticoptimization.arXivpreprintarXiv:1412.6980.308Kingma,D.andLeCun,Y.(2010).Regularizedestimationofimagestatisticsbyscorematching.In.,NIPS’2010513620748 BIBLIOGRAPHYKingma,D.,Rezende,D.,Mohamed,S.,andWelling,M.(2014).Semi-supervisedlearningwithdeepgenerativemodels.In.NIPS’2014426Kingma,D.P.(2013).Fastgradient-basedinferencewithcontinuouslatentvariablemodelsinauxiliaryform.Technicalreport,arxiv:1306.0733.,,652689696Kingma,D.P.andWelling,M.(2014a).Auto-encodingvariationalbayes.InProceedingsoftheInternationalConferenceonLearningRepresentations(ICLR).,689700Kingma, D.P.andWelling, M.(2014b).Eﬃcientgradient-basedinferencethroughtransformationsbetweenbayesnetsandneuralnets.Technicalreport,arxiv:1402.0480.689Kirkpatrick,S.,Jr.,C.D.G.,,andVecchi,M.P.(1983).Optimizationbysimulatedannealing.Science,,671–680.220327Kiros,R.,Salakhutdinov,R.,andZemel,R.(2014a).Multimodalneurallanguagemodels.In.ICML’2014102Kiros,R.,Salakhutdinov,R.,andZemel,R.(2014b).Unifyingvisual-semanticembeddingswithmultimodalneurallanguagemodels..,arXiv:1411.2539[cs.LG]102410Klementiev,A.,Titov,I.,andBhattarai,B.(2012).Inducingcrosslingualdistributedrepresentationsofwords.InProceedingsofCOLING2012.,476539Knowles-Barley,S.,Jones,T.R.,Morgan,J.,Lee,D.,Kasthuri,N.,Lichtman,J.W.,andPﬁster,H.(2014).Deeplearningfortheconnectome.GPUTechnologyConference.26Koller,D.andFriedman, N.(2009).ProbabilisticGraphicalModels:PrinciplesandTechniques.MITPress.,,583595645Konig,Y.,Bourlard,H.,andMorgan,N.(1996).REMAP:Recursiveestimationandmaximizationofaposterioriprobabilities–applicationtotransition-basedconnectionistspeechrecognition.InD.Touretzky,M.Mozer,andM.Hasselmo,editors,AdvancesinNeuralInformationProcessingSystems8(NIPS’95).MITPress,Cambridge,MA.459Koren,Y.(2009).TheBellKorsolutiontotheNetﬂixgrandprize.,258480Kotzias,D.,Denil,M.,deFreitas,N.,andSmyth,P.(2015).Fromgrouptoindividuallabelsusingdeepfeatures.In.ACMSIGKDD106Koutnik,J.,Greﬀ,K.,Gomez,F.,andSchmidhuber,J.(2014).AclockworkRNN.InICML’2014.408Kočiský,T.,Hermann,K.M.,andBlunsom,P.(2014).LearningBilingualWordRepre-sentationsbyMarginalizingAlignments.InProceedingsofACL.476Krause,O.,Fischer,A.,Glasmachers,T.,andIgel,C.(2013).ApproximationpropertiesofDBNswithbinaryhiddenunitsandreal-valuedvisibleunits.In.ICML’2013553749 BIBLIOGRAPHYKrizhevsky,A.(2010).ConvolutionaldeepbeliefnetworksonCIFAR-10.Technicalreport,UniversityofToronto.UnpublishedManuscript:http://www.cs.utoronto.ca/kriz/conv-cifar10-aug2010.pdf.446Krizhevsky,A.andHinton,G.(2009).Learningmultiplelayersoffeaturesfromtinyimages.Technicalreport,UniversityofToronto.,21561Krizhevsky,A.andHinton,G.E.(2011).Usingverydeepautoencodersforcontent-basedimageretrieval.In.ESANN525Krizhevsky,A.,Sutskever,I.,andHinton,G.(2012).ImageNetclassiﬁcationwithdeepconvolutionalneuralnetworks.In.,,,,,,,NIPS’2012232427100201371454458Krueger,K.A.andDayan,P.(2009).Flexibleshaping:howlearninginsmallstepshelps.Cognition,,380–394.110328Kuhn,H.W.andTucker,A.W.(1951).Nonlinearprogramming.InProceedingsoftheSecondBerkeleySymposiumonMathematicalStatisticsandProbability,pages481–492,Berkeley,Calif.UniversityofCaliforniaPress.95Kumar,A.,Irsoy,O.,Su,J.,Bradbury,J.,English,R.,Pierce,B.,Ondruska,P.,Iyyer,M.,Gulrajani,I.,andSocher,R.(2015).Askmeanything:Dynamicmemorynetworksfornaturallanguageprocessing..,arXiv:1506.07285418485Kumar,M.P.,Packer,B.,andKoller,D.(2010).Self-pacedlearningforlatentvariablemodels.In.NIPS’2010328Lang,K.J.andHinton,G.E.(1988).Thedevelopmentofthetime-delayneuralnetworkarchitectureforspeechrecognition.TechnicalReportCMU-CS-88-152,Carnegie-MellonUniversity.,,367374407Lang,K.J.,Waibel,A.H.,andHinton,G.E.(1990).Atime-delayneuralnetworkarchitectureforisolatedwordrecognition.Neuralnetworks,(1),23–43.3374Langford,J.andZhang,T.(2008).Theepoch-greedyalgorithmforcontextualmulti-armedbandits.In,pages1096––1103.NIPS’2008480Lappalainen,H.,Giannakopoulos,X.,Honkela,A.,andKarhunen,J.(2000).Nonlinearindependentcomponentanalysisusingensemblelearning:Experimentsanddiscussion.InProc.ICA.Citeseer.493Larochelle, H. and Bengio, Y.(2008).Classiﬁcation usingdiscriminative restrictedBoltzmannmachines.In.,,,,ICML’2008244255530686716Larochelle,H.andHinton,G.E.(2010).Learningtocombinefovealglimpseswithathird-orderBoltzmannmachine.InAdvancesinNeuralInformationProcessingSystems23,pages1243–1251.367750 BIBLIOGRAPHYLarochelle,H.andMurray,I.(2011).TheNeuralAutoregressiveDistributionEstimator.InAISTATS’2011.,,705708709Larochelle,H.,Erhan,D.,andBengio,Y.(2008). Zero-datalearningofnewtasks.InAAAIConferenceonArtiﬁcialIntelligence.539Larochelle,H.,Bengio,Y.,Louradour,J.,andLamblin,P.(2009).Exploringstrategiesfortrainingdeepneuralnetworks.JournalofMachineLearningResearch,,1–40.10535Lasserre,J.A.,Bishop,C.M.,andMinka,T.P.(2006).Principledhybridsofgenerativeanddiscriminativemodels.InProceedingsoftheComputerVisionandPatternRecognitionConference(CVPR’06),pages87–94,Washington,DC,USA.IEEEComputerSociety.244253,Le,Q.,Ngiam,J.,Chen,Z.,haoChia,D.J.,Koh,P.W.,andNg,A.(2010).Tiledconvolutionalneuralnetworks.InJ.Laﬀerty,C.K.I.Williams,J.Shawe-Taylor,R.Zemel,andA.Culotta,editors,AdvancesinNeuralInformationProcessingSystems23(NIPS’10),pages1279–1287.352Le,Q.,Ngiam,J.,Coates,A.,Lahiri,A.,Prochnow,B.,andNg,A.(2011).Onoptimizationmethodsfordeeplearning.InProc.ICML’2011.ACM.316Le,Q.,Ranzato,M.,Monga,R.,Devin,M.,Corrado,G.,Chen,K.,Dean,J.,andNg,A.(2012).Buildinghigh-levelfeaturesusinglargescaleunsupervisedlearning.InICML’2012.,2427LeRoux,N.andBengio,Y.(2008).RepresentationalpowerofrestrictedBoltzmannmachinesanddeepbeliefnetworks.NeuralComputation,(6),1631–1649.,20553655LeRoux,N.andBengio,Y.(2010).Deepbeliefnetworksarecompactuniversalapproxi-mators.NeuralComputation,(8),2192–2207.22553LeCun,Y.(1985).Uneprocédured’apprentissagepourRéseauàseuilassymétrique.InCognitiva85:AlaFrontièredel’IntelligenceArtiﬁcielle,desSciencesdelaConnaissanceetdesNeurosciences,pages599–604,Paris1985.CESTA,Paris.225LeCun,Y.(1986).Learningprocessesinanasymmetricthresholdnetwork.InF.Fogelman-Soulié,E.Bienenstock,andG.Weisbuch,editors,DisorderedSystemsandBiologicalOrganization,pages233–240.Springer-Verlag,LesHouches,France.352LeCun,Y.(1987).Modèlesconnexionistesdel’apprentissage.Ph.D.thesis,UniversitédeParisVI.,,18502515LeCun, Y.(1989).Generalizationandnetworkdesignstrategies.TechnicalReportCRG-TR-89-4,UniversityofToronto.,330352751 BIBLIOGRAPHYLeCun,Y.,Jackel,L.D.,Boser,B.,Denker,J.S.,Graf,H.P.,Guyon,I.,Henderson,D.,Howard,R.E.,andHubbard,W.(1989).Handwrittendigitrecognition:Applicationsofneuralnetworkchipsandautomaticlearning.IEEECommunicationsMagazine,27(11),41–46.368LeCun,Y.,Bottou,L.,Orr,G.B.,andMüller,K.-R.(1998a).Eﬃcientbackprop.InNeuralNetworks,TricksoftheTrade,LectureNotesinComputerScienceLNCS1524.SpringerVerlag.,310429LeCun,Y.,Bottou,L.,Bengio,Y.,andHaﬀner,P.(1998b).Gradientbasedlearningappliedtodocumentrecognition.Proc.IEEE.,,,,,,16182127371458460LeCun, Y., Kavukcuoglu, K., andFarabet, C.(2010).Convolutionalnetworksandapplicationsinvision. InCircuitsandSystems(ISCAS),Proceedingsof2010IEEEInternationalSymposiumon,pages253–256.IEEE.371L’Ecuyer,P.(1994).Eﬃciencyimprovementandvariancereduction.InProceedingsofthe1994WinterSimulationConference,pages122––132.690Lee,C.-Y.,Xie,S.,Gallagher,P.,Zhang,Z.,andTu,Z.(2014).Deeply-supervisednets.arXivpreprintarXiv:1409.5185.326Lee,H.,Battle,A.,Raina,R.,andNg,A.(2007).Eﬃcientsparsecodingalgorithms.InB.Schölkopf,J.Platt,andT.Hoﬀman,editors,AdvancesinNeuralInformationProcessingSystems19(NIPS’06),pages801–808.MITPress.637Lee,H.,Ekanadham,C.,andNg,A.(2008).SparsedeepbeliefnetmodelforvisualareaV2.In.NIPS’07255Lee,H.,Grosse,R.,Ranganath,R.,andNg,A.Y.(2009).Convolutionaldeepbeliefnetworksforscalableunsupervisedlearningofhierarchicalrepresentations.InL.BottouandM.Littman,editors,ProceedingsoftheTwenty-sixthInternationalConferenceonMachineLearning(ICML’09).ACM,Montreal,Canada.,,363683684Lee,Y.J.andGrauman,K.(2011).Learningtheeasythingsﬁrst:self-pacedvisualcategorydiscovery.In.CVPR’2011328Leibniz,G.W.(1676).Memoirusingthechainrule.(CitedinTMME7:2&3p321-332,2010).225Lenat,D.B.andGuha,R.V.(1989).Buildinglargeknowledge-basedsystems;representa-tionandinferenceintheCycproject.Addison-WesleyLongmanPublishingCo.,Inc.2Leshno,M.,Lin,V.Y.,Pinkus,A.,andSchocken,S.(1993).Multilayerfeedforwardnetworkswithanonpolynomialactivationfunctioncanapproximateanyfunction.NeuralNetworks,,861––867.,6198199752 BIBLIOGRAPHYLevenberg,K.(1944).Amethodforthesolutionofcertainnon-linearproblemsinleastsquares.QuarterlyJournalofAppliedMathematics,(2),164–168.II312L’Hôpital,G.F.A.(1696).Analysedesinﬁnimentpetits,pourl’intelligencedeslignescourbes.Paris:L’ImprimerieRoyale.225Li,Y.,Swersky,K.,andZemel,R.S.(2015).Generativemomentmatchingnetworks.CoRR,.abs/1502.02761703Lin,T.,Horne,B.G.,Tino,P.,andGiles,C.L.(1996).Learninglong-termdependenciesisnotasdiﬃcultwithNARXrecurrentneuralnetworks.IEEETransactionsonNeuralNetworks,(6),1329–1338.7407Lin,Y.,Liu,Z.,Sun,M.,Liu,Y.,andZhu,X.(2015).Learningentityandrelationembeddingsforknowledgegraphcompletion.InProc.AAAI’15.484Linde,N.(1992).Themachinethatchangedtheworld,episode3.Documentaryminiseries.2Lindsey,C.andLindblad,T.(1994).Reviewofhardwareneuralnetworks:auser’sperspective.InProc.ThirdWorkshoponNeuralNetworks:FromBiologytoHighEnergyPhysics,pages195––202,Isolad’Elba,Italy.451Linnainmaa, S. (1976).Taylorexpansionofthe accumulated roundingerror.BITNumericalMathematics,(2),146–160.16225LISA(2008).Deeplearningtutorials:RestrictedBoltzmannmachines.Technicalreport,LISALab,UniversitédeMontréal.589Long,P.M.andServedio,R.A.(2010).RestrictedBoltzmannmachinesarehardtoapproximatelyevaluateorsimulate.InProceedingsofthe27thInternationalConferenceonMachineLearning(ICML’10).658Lotter,W.,Kreiman,G.,andCox,D.(2015).Unsupervisedlearningofvisualstructureusingpredictivegenerativenetworks.arXivpreprintarXiv:1511.06380.,544545Lovelace,A.(1842).NotesuponL.F.Menabrea’s“SketchoftheAnalyticalEngineinventedbyCharlesBabbage”.1Lu,L.,Zhang,X.,Cho,K.,andRenals,S.(2015).Astudyoftherecurrentneuralnetworkencoder-decoderforlargevocabularyspeechrecognition.InProc.Interspeech.461Lu,T.,Pál,D.,andPál,M.(2010).Contextualmulti-armedbandits.InInternationalConferenceonArtiﬁcialIntelligenceandStatistics,pages485–492.480Luenberger,D.G.(1984).LinearandNonlinearProgramming.AddisonWesley.316Lukoševičius,M.andJaeger,H.(2009).Reservoircomputingapproachestorecurrentneuralnetworktraining.ComputerScienceReview,(3),127–149.3404753 BIBLIOGRAPHYLuo,H.,Shen,R.,Niu,C.,andUllrich,C.(2011).Learningclass-relevantfeaturesandclass-irrelevantfeaturesviaahybridthird-orderRBM.InInternationalConferenceonArtiﬁcialIntelligenceandStatistics,pages470–478.686Luo,H.,Carrier,P.L.,Courville,A.,andBengio,Y.(2013).Texturemodelingwithconvolutionalspike-and-slabRBMsanddeepextensions.InAISTATS’2013.102Lyu,S.(2009).Interpretationandgeneralizationofscorematching.InProceedingsoftheTwenty-ﬁfthConferenceinUncertaintyinArtiﬁcialIntelligence(UAI’09).618Ma,J.,Sheridan,R.P.,Liaw,A.,Dahl,G.E.,andSvetnik,V.(2015).Deepneuralnetsasamethodforquantitativestructure–activityrelationships.J.Chemicalinformationandmodeling.530Maas,A.L.,Hannun,A.Y.,andNg,A.Y.(2013).Rectiﬁernonlinearitiesimproveneuralnetworkacousticmodels.InICMLWorkshoponDeepLearningforAudio,Speech,andLanguageProcessing.193Maass,W.(1992).Boundsforthecomputationalpowerandlearningcomplexityofanalogneuralnets(extendedabstract).InProc.ofthe25thACMSymp.TheoryofComputing,pages335–344.199Maass,W.,Schnitger,G.,andSontag,E.D.(1994).AcomparisonofthecomputationalpowerofsigmoidandBooleanthresholdcircuits.TheoreticalAdvancesinNeuralComputationandLearning,pages127–151.199Maass,W.,Natschlaeger,T.,andMarkram,H.(2002).Real-timecomputingwithoutstablestates:Anewframeworkforneuralcomputationbasedonperturbations.NeuralComputation,(11),2531–2560.14404MacKay,D.(2003). InformationTheory,InferenceandLearningAlgorithms.CambridgeUniversityPress.73Maclaurin,D.,Duvenaud,D.,andAdams,R.P.(2015).Gradient-basedhyperparameteroptimizationthroughreversiblelearning.arXivpreprintarXiv:1502.03492.435Mao,J.,Xu,W.,Yang,Y.,Wang,J.,Huang,Z.,andYuille,A.L.(2015).Deepcaptioningwithmultimodalrecurrentneuralnetworks.In.arXiv:1410.1090.ICLR’2015102Marcotte,P.andSavard,G.(1992).Novelapproachestothediscriminationproblem.ZeitschriftfürOperationsResearch(Theory),,517–545.36276Marlin,B.anddeFreitas,N.(2011).Asymptoticeﬃciencyofdeterministicestimatorsfordiscreteenergy-basedmodels:Ratiomatchingandpseudolikelihood.In.,UAI’2011617619754 BIBLIOGRAPHYMarlin,B.,Swersky,K.,Chen,B.,anddeFreitas,N.(2010).InductiveprinciplesforrestrictedBoltzmannmachinelearning.InProceedingsofTheThirteenthInternationalConferenceonArtiﬁcialIntelligenceandStatistics(AISTATS’10),volume9,pages509–516.,,613618619Marquardt,D.W.(1963).Analgorithmforleast-squaresestimationofnon-linearparam-eters.JournaloftheSocietyofIndustrialandAppliedMathematics,11(2),431–441.312Marr,D.andPoggio,T.(1976).Cooperativecomputationofstereodisparity.Science,194.367Martens, J.(2010).DeeplearningviaHessian-freeoptimization.InL.BottouandM.Littman,editors,ProceedingsoftheTwenty-seventhInternationalConferenceonMachineLearning(ICML-10),pages735–742.ACM.304Martens,J.andMedabalimi,V.(2014).Ontheexpressiveeﬃciencyofsumproductnetworks..arXiv:1411.7717554Martens,J.andSutskever,I.(2011).LearningrecurrentneuralnetworkswithHessian-freeoptimization.InProc.ICML’2011.ACM.413Mase,S.(1995).Consistencyofthemaximumpseudo-likelihoodestimatorofcontinuousstatespaceGibbsianprocesses.TheAnnalsofAppliedProbability,5(3),pp.603–612.616McClelland,J.,Rumelhart,D.,andHinton,G.(1995).Theappealofparalleldistributedprocessing.InComputation&intelligence,pages305–341.AmericanAssociationforArtiﬁcialIntelligence.17McCulloch,W.S.andPitts,W.(1943).Alogicalcalculusofideasimmanentinnervousactivity.BulletinofMathematicalBiophysics,,115–133.,51415Mead,C.andIsmail,M.(2012).AnalogVLSIimplementationofneuralsystems,volume80.SpringerScience&BusinessMedia.451Melchior,J.,Fischer,A.,andWiskott,L.(2013).HowtocenterbinarydeepBoltzmannmachines.arXivpreprintarXiv:1311.1354.674Memisevic,R.andHinton,G.E.(2007).Unsupervisedlearningofimagetransformations.InProceedingsoftheComputerVisionandPatternRecognitionConference(CVPR’07).686Memisevic,R.andHinton,G.E.(2010).Learningtorepresentspatialtransformationswithfactoredhigher-orderBoltzmannmachines.NeuralComputation,22(6),1473–1492.686755 BIBLIOGRAPHYMesnil,G.,Dauphin,Y.,Glorot,X.,Rifai,S.,Bengio,Y.,Goodfellow,I.,Lavoie,E.,Muller,X.,Desjardins,G.,Warde-Farley,D.,Vincent,P.,Courville,A.,andBergstra,J.(2011).Unsupervisedandtransferlearningchallenge:adeeplearningapproach.InJMLRW&CP:Proc.UnsupervisedandTransferLearning,volume7.,,201532538Mesnil,G.,Rifai,S.,Dauphin,Y.,Bengio,Y.,andVincent,P.(2012).Surﬁngonthemanifold.LearningWorkshop,Snowbird.711Miikkulainen,R.andDyer,M.G.(1991).NaturallanguageprocessingwithmodularPDPnetworksanddistributedlexicon.CognitiveScience,,343–399.15477Mikolov,T.(2012).StatisticalLanguageModelsbasedonNeuralNetworks.Ph.D.thesis,BrnoUniversityofTechnology.414Mikolov,T.,Deoras,A.,Kombrink,S.,Burget,L.,andCernocky,J.(2011a).Empiricalevaluationandcombinationofadvancedlanguagemodelingtechniques.InProc.12than-nualconferenceoftheinternationalspeechcommunicationassociation(INTERSPEECH2011).472Mikolov,T.,Deoras,A.,Povey,D.,Burget,L.,andCernocky,J.(2011b).Strategiesfortraininglargescaleneuralnetworklanguagemodels.InProc.ASRU’2011.,328472Mikolov,T.,Chen,K.,Corrado,G.,andDean,J.(2013a).Eﬃcientestimationofwordrep-resentationsinvectorspace.InInternationalConferenceonLearningRepresentations:WorkshopsTrack.536Mikolov,T.,Le,Q.V.,andSutskever,I.(2013b).Exploitingsimilaritiesamonglanguagesformachinetranslation.Technicalreport,arXiv:1309.4168.539Minka,T.(2005).Divergencemeasuresandmessagepassing.MicrosoftResearchCambridgeUKTechRepMSRTR2005173,(TR-2005-173).72625Minsky,M.L.andPapert,S.A.(1969).Perceptrons.MITPress,Cambridge.15Mirza,M.andOsindero,S.(2014).Conditionalgenerativeadversarialnets.arXivpreprintarXiv:1411.1784.702Mishkin,D.and Matas,J.(2015).Allyouneedisagoodinit.arXivpreprintarXiv:1511.06422.305Misra,J.andSaha,I.(2010). Artiﬁcialneuralnetworksinhardware:Asurveyoftwodecadesofprogress.Neurocomputing,(1),239–255.74451Mitchell,T.M.(1997).MachineLearning.McGraw-Hill,NewYork.99Miyato,T.,Maeda,S.,Koyama,M.,Nakae,K.,andIshii,S.(2015).Distributionalsmoothingwithvirtualadversarialtraining.In.Preprint:arXiv:1507.00677.ICLR269756 BIBLIOGRAPHYMnih,A.andGregor, K.(2014).Neuralvariationalinferenceandlearninginbeliefnetworks.In.,,ICML’2014691692693Mnih,A.andHinton,G.E.(2007).Threenewgraphicalmodelsforstatisticallanguagemodelling.InZ.Ghahramani,editor,ProceedingsoftheTwenty-fourthInternationalConferenceonMachineLearning(ICML’07),pages641–648.ACM.465Mnih,A.andHinton,G.E.(2009).Ascalablehierarchicaldistributedlanguagemodel.InD.Koller,D.Schuurmans,Y.Bengio,andL.Bottou,editors,AdvancesinNeuralInformationProcessingSystems21(NIPS’08),pages1081–1088.467Mnih,A.andKavukcuoglu,K.(2013).Learningwordembeddingseﬃcientlywithnoise-contrastiveestimation.InC.Burges,L.Bottou,M.Welling,Z.Ghahramani,andK.Weinberger,editors,AdvancesinNeuralInformationProcessingSystems26,pages2265–2273.CurranAssociates,Inc.,472622Mnih, A.andTeh, Y. W.(2012).Afastandsimple algorithmfortrainingneuralprobabilisticlanguagemodels.In,pages1751–1758.ICML’2012472Mnih,V.andHinton,G.(2010).Learningtodetectroadsinhigh-resolutionaerialimages.InProceedingsofthe11thEuropeanConferenceonComputerVision(ECCV).102Mnih,V.,Larochelle,H., andHinton,G.(2011).ConditionalrestrictedBoltzmannmachinesforstructureoutputprediction.InProc.Conf.onUncertaintyinArtiﬁcialIntelligence(UAI).685Mnih,V.,Kavukcuoglo,K.,Silver,D.,Graves,A.,Antonoglou,I.,andWierstra,D.(2013).PlayingAtariwithdeepreinforcementlearning.Technicalreport,arXiv:1312.5602.106Mnih,V.,Heess,N.,Graves,A.,andKavukcuoglu,K.(2014).Recurrentmodelsofvisualattention.InZ.Ghahramani,M.Welling,C.Cortes,N.Lawrence,andK.Weinberger,editors,,pages2204–2212.NIPS’2014691Mnih,V.,Kavukcuoglo,K.,Silver,D.,Rusu,A.A.,Veness,J.,Bellemare,M.G.,Graves,A.,Riedmiller,M.,Fidgeland,A.K.,Ostrovski,G.,Petersen,S.,Beattie,C.,Sadik,A.,Antonoglou,I.,King,H.,Kumaran,D.,Wierstra,D.,Legg,S.,andHassabis,D.(2015).Human-levelcontrolthroughdeepreinforcementlearning.Nature,,529–533.51825Mobahi,H.andFisher, III,J.W.(2015).AtheoreticalanalysisofoptimizationbyGaussiancontinuation.In.AAAI’2015327Mobahi,H.,Collobert,R.,andWeston,J.(2009).Deeplearningfromtemporalcoherenceinvideo.InL.BottouandM.Littman,editors,Proceedingsofthe26thInternationalConferenceonMachineLearning,pages737–744,Montreal.Omnipress.494Mohamed,A.,Dahl,G.,andHinton,G.(2009).Deepbeliefnetworksforphonerecognition.459757 BIBLIOGRAPHYMohamed,A.,Sainath,T.N.,Dahl,G.,Ramabhadran,B.,Hinton,G.E.,andPicheny,M.A.(2011).Deepbeliefnetworksusingdiscriminativefeaturesforphonerecognition.InAcoustics,SpeechandSignalProcessing(ICASSP),2011IEEEInternationalConferenceon,pages5060–5063.IEEE.459Mohamed,A.,Dahl,G.,andHinton,G.(2012a). Acousticmodelingusingdeepbeliefnetworks.IEEETrans.onAudio,SpeechandLanguageProcessing,20(1),14–22.459Mohamed,A.,Hinton,G.,andPenn,G.(2012b).Understandinghowdeepbeliefnetworksperformacousticmodelling.InAcoustics,SpeechandSignalProcessing(ICASSP),2012IEEEInternationalConferenceon,pages4273–4276.IEEE.459Moller,M.F.(1993).Ascaledconjugategradientalgorithmforfastsupervisedlearning.NeuralNetworks,,525–533.6316Montavon,G.andMuller,K.-R.(2012). DeepBoltzmannmachinesandthecenteringtrick.InG.Montavon,G.Orr,andK.-R.Müller,editors,NeuralNetworks:TricksoftheTrade,volume7700ofLectureNotesinComputerScience,pages621–637.Preprint:http://arxiv.org/abs/1203.3783.673Montúfar,G.(2014).Universalapproximationdepthanderrorsofnarrowbeliefnetworkswithdiscreteunits.NeuralComputation,.26553Montúfar,G.andAy,N.(2011).ReﬁnementsofuniversalapproximationresultsfordeepbeliefnetworksandrestrictedBoltzmannmachines.NeuralComputation,23(5),1306–1319.553Montufar,G.F.,Pascanu,R.,Cho,K.,andBengio,Y.(2014).Onthenumberoflinearregionsofdeepneuralnetworks.In.,,NIPS’201419199200Mor-Yosef,S.,Samueloﬀ,A.,Modan,B.,Navot,D.,andSchenker,J.G.(1990).Rankingtheriskfactorsforcesarean:logisticregressionanalysisofanationwidestudy.ObstetGynecol,(6),944–7.753Morin,F.andBengio,Y.(2005).Hierarchicalprobabilisticneuralnetworklanguagemodel.InAISTATS’2005.,467469Mozer,M.C.(1992).Theinductionofmultiscaletemporalstructure.InJ.M.S.HansonandR.Lippmann, editors, AdvancesinNeural InformationProcessingSystems4(NIPS’91),pages275–282,SanMateo,CA.MorganKaufmann.,407408Murphy,K. P.(2012).MachineLearning:a Probabilistic Perspective.MIT Press,Cambridge,MA,USA.,,6298146Murray,B.U.I.andLarochelle,H.(2014).Adeepandtractabledensityestimator.InICML’2014.,190710Nair,V.andHinton,G.(2010).RectiﬁedlinearunitsimproverestrictedBoltzmannmachines.In.,,ICML’201016174197758 BIBLIOGRAPHYNair,V.andHinton,G.E.(2009).3dobjectrecognitionwithdeepbeliefnets.InY.Bengio,D.Schuurmans,J.D.Laﬀerty,C.K.I.Williams,andA.Culotta,editors,AdvancesinNeuralInformationProcessingSystems22,pages1339–1347.CurranAssociates,Inc.686Narayanan,H.andMitter,S.(2010).Samplecomplexityoftestingthemanifoldhypothesis.In.NIPS’2010164Naumann,U.(2008).OptimalJacobianaccumulationisNP-complete.MathematicalProgramming,(2),427–441.112222Navigli,R.andVelardi,P.(2005). Structuralsemanticinterconnections:aknowledge-basedapproachtowordsensedisambiguation.IEEETrans.PatternAnalysisandMachineIntelligence,(7),1075––1086.27485Neal,R.andHinton,G.(1999).AviewoftheEMalgorithmthatjustiﬁesincremental,sparse,andothervariants.InM.I.Jordan,editor,LearninginGraphicalModels.MITPress,Cambridge,MA.634Neal,R.M.(1990).Learningstochasticfeedforwardnetworks.Technicalreport.692Neal,R.M.(1993).ProbabilisticinferenceusingMarkovchainMonte-Carlomethods.TechnicalReportCRG-TR-93-1,Dept.ofComputerScience,UniversityofToronto.680Neal,R.M.(1994).Samplingfrommultimodaldistributionsusingtemperedtransitions.TechnicalReport9421,Dept.ofStatistics,UniversityofToronto.603Neal,R.M.(1996).BayesianLearningforNeuralNetworks.LectureNotesinStatistics.Springer.265Neal,R.M.(2001).Annealedimportancesampling.,StatisticsandComputing11(2),125–139.,,625627628Neal,R.M.(2005).Estimatingratiosofnormalizingconstantsusinglinkedimportancesampling.629Nesterov,Y.(1983).AmethodofsolvingaconvexprogrammingproblemwithconvergencerateO/k(12).,,372–376.SovietMathematicsDoklady27300Nesterov,Y.(2004).Introductorylecturesonconvexoptimization:abasiccourse.Appliedoptimization.KluwerAcademicPubl.,Boston,Dordrecht,London.300Netzer,Y.,Wang,T.,Coates,A.,Bissacco,A.,Wu,B.,andNg,A.Y.(2011).Readingdigits in naturalimages withunsupervised feature learning.Deep Learning andUnsupervisedFeatureLearningWorkshop,NIPS.21Ney,H.andKneser,R.(1993).Improvedclusteringtechniquesforclass-basedstatisticallanguagemodelling.InEuropeanConferenceonSpeechCommunicationandTechnology(Eurospeech),pages973–976,Berlin.463759 BIBLIOGRAPHYNg,A.(2015).Adviceforapplyingmachinelearning.https://see.stanford.edu/materials/aimlcs229/ML-advice.pdf.421Niesler,T.R.,Whittaker,E.W.D.,andWoodland,P.C.(1998).Comparisonofpart-of-speechandautomaticallyderivedcategory-basedlanguagemodelsforspeechrecognition.InInternationalConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),pages177–180.463Ning,F.,Delhomme,D.,LeCun,Y.,Piano,F.,Bottou,L.,andBarbano,P.E.(2005).Towardautomaticphenotypingofdevelopingembryosfromvideos.ImageProcessing,IEEETransactionson,(9),1360–1371.14360Nocedal,J.andWright,S.(2006).NumericalOptimization.Springer.,9296Norouzi,M.andFleet,D.J.(2011).Minimallosshashingforcompactbinarycodes.InICML’2011.525Nowlan,S.J.(1990).Competingexperts:Anexperimentalinvestigationofassociativemixturemodels.TechnicalReportCRG-TR-90-5,UniversityofToronto.450Nowlan,S.J.andHinton,G.E.(1992).Simplifyingneuralnetworksbysoftweight-sharing.NeuralComputation,(4),473–493.4139Olshausen,B.andField,D.J.(2005).HowclosearewetounderstandingV1?NeuralComputation,,1665–1699.1716Olshausen,B.A.andField,D.J.(1996).Emergenceofsimple-cellreceptiveﬁeldpropertiesbylearningasparsecodefornaturalimages. Nature,381,607–609.,,,147255370496Olshausen,B.A.,Anderson,C.H.,andVanEssen,D.C.(1993).Aneurobiologicalmodelofvisualattentionandinvariantpatternrecognitionbasedondynamicroutingofinformation.J.Neurosci.,(11),4700–4719.13450Opper,M.andArchambeau,C.(2009).ThevariationalGaussianapproximationrevisited.Neuralcomputation,(3),786–792.21689Oquab,M.,Bottou,L.,Laptev,I.,andSivic,J.(2014).Learningandtransferringmid-levelimagerepresentationsusingconvolutionalneuralnetworks.InComputerVisionandPatternRecognition(CVPR),2014IEEEConferenceon,pages1717–1724.IEEE.536Osindero,S.andHinton,G.E.(2008).ModelingimagepatcheswithadirectedhierarchyofMarkovrandomﬁelds.InJ.Platt,D.Koller,Y.Singer,andS.Roweis,editors,AdvancesinNeuralInformationProcessingSystems20(NIPS’07),pages1121–1128,Cambridge,MA.MITPress.632OvidandMartin,C.(2004)..W.W.Norton.Metamorphoses1760 BIBLIOGRAPHYPaccanaro,A.andHinton,G.E.(2000).Extractingdistributedrepresentationsofconceptsandrelationsfrompositiveandnegativepropositions.InInternationalJointConferenceonNeuralNetworks(IJCNN),Como,Italy.IEEE,NewYork.484Paine,T.L.,Khorrami,P.,Han,W.,andHuang,T.S.(2014).Ananalysisofunsupervisedpre-traininginlightofrecentadvances.arXivpreprintarXiv:1412.6597.532Palatucci,M.,Pomerleau,D.,Hinton,G.E.,andMitchell,T.M.(2009).Zero-shotlearningwithsemanticoutputcodes.InY.Bengio,D.Schuurmans,J.D.Laﬀerty,C.K.I.Williams,andA.Culotta,editors,AdvancesinNeuralInformationProcessingSystems22,pages1410–1418.CurranAssociates,Inc.539Parker,D.B.(1985).Learning-logic.TechnicalReportTR-47,CenterforComp.ResearchinEconomicsandManagementSci.,MIT.225Pascanu,R.,Mikolov,T.,andBengio,Y.(2013).Onthediﬃcultyoftrainingrecurrentneuralnetworks.In.,,,,,ICML’2013289402403408414416Pascanu,R.,Gülçehre,Ç.,Cho,K.,andBengio,Y.(2014a).Howtoconstructdeeprecurrentneuralnetworks.In.,,,,,ICLR’201419265398399410460Pascanu,R.,Montufar,G.,andBengio,Y.(2014b).Onthenumberofinferenceregionsofdeepfeedforwardnetworkswithpiece-wiselinearactivations.In.ICLR’2014550Pati,Y.,Rezaiifar,R.,andKrishnaprasad,P.(1993).Orthogonalmatchingpursuit:Recursivefunctionapproximationwithapplicationstowaveletdecomposition.InPro-ceedingsofthe27thAnnualAsilomarConferenceonSignals,Systems,andComputers,pages40–44.255Pearl,J.(1985).Bayesiannetworks:Amodelofself-activatedmemoryforevidentialreasoning.In Proceedingsofthe7thConferenceofthe CognitiveScience Society,UniversityofCalifornia,Irvine,pages329–334.563Pearl,J.(1988). ProbabilisticReasoninginIntelligentSystems: NetworksofPlausibleInference.MorganKaufmann.54Perron,O.(1907).Zurtheoriedermatrices.MathematischeAnnalen,64(2),248–263.597Petersen,K.B.andPedersen,M.S.(2006).Thematrixcookbook.Version20051003.31Peterson,G.B.(2004).Adayofgreatillumination:B.F.Skinner’sdiscoveryofshaping.JournaloftheExperimentalAnalysisofBehavior,(3),317–328.82328Pham,D.-T.,Garat,P.,andJutten,C.(1992).Separationofamixtureofindependentsourcesthroughamaximumlikelihoodapproach.In,pages771–774.EUSIPCO491761 BIBLIOGRAPHYPham,P.-H.,Jelaca,D.,Farabet,C.,Martini,B.,LeCun,Y.,andCulurciello,E.(2012).NeuFlow:dataﬂowvisionprocessingsystem-on-a-chip.InCircuitsandSystems(MWS-CAS),2012IEEE55thInternationalMidwestSymposiumon,pages1044–1047.IEEE.451Pinheiro,P.H.O.andCollobert,R.(2014).Recurrentconvolutionalneuralnetworksforscenelabeling.In.ICML’2014359Pinheiro,P.H.O.andCollobert,R.(2015).Fromimage-leveltopixel-levellabelingwithconvolutionalnetworks.InConferenceonComputerVisionandPatternRecognition(CVPR).359Pinto,N.,Cox,D.D.,andDiCarlo,J.J.(2008).Whyisreal-worldvisualobjectrecognitionhard?PLoSComputBiol,.4456Pinto,N.,Stone,Z.,Zickler,T.,andCox,D.(2011).Scalingupbiologically-inspiredcomputervision:Acasestudyinunconstrainedfacerecognitiononfacebook.InComputerVisionandPatternRecognitionWorkshops(CVPRW),2011IEEEComputerSocietyConferenceon,pages35–42.IEEE.363Pollack,J.B.(1990).Recursivedistributedrepresentations.ArtiﬁcialIntelligence,46(1),77–105.401Polyak,B.andJuditsky,A.(1992).Accelerationofstochasticapproximationbyaveraging.SIAMJ.ControlandOptimization,,838–855.30(4)322Polyak,B.T.(1964).Somemethodsofspeedinguptheconvergenceofiterationmethods.USSRComputationalMathematicsandMathematicalPhysics,(5),1–17.4296Poole,B.,Sohl-Dickstein,J.,andGanguli,S.(2014). Analyzingnoiseinautoencodersanddeepnetworks.,.CoRRabs/1406.1831241Poon,H.andDomingos,P.(2011).Sum-productnetworks:Anewdeeparchitecture.InProceedingsoftheTwenty-seventhConferenceinUncertaintyinArtiﬁcialIntelligence(UAI),Barcelona,Spain.554Presley,R.K.andHaggard,R.L.(1994).Aﬁxedpointimplementationofthebackpropa-gationlearningalgorithm.InSoutheastcon’94.CreativeTechnologyTransfer-AGlobalAﬀair.,Proceedingsofthe1994IEEE,pages136–138.IEEE.451Price,R.(1958).AusefultheoremfornonlineardeviceshavingGaussianinputs.IEEETransactionsonInformationTheory,(2),69–72.4689Quiroga,R.Q.,Reddy,L.,Kreiman,G.,Koch,C.,andFried,I.(2005).Invariantvisualrepresentationbysingleneuronsinthehumanbrain.Nature,435(7045),1102–1107.366762 BIBLIOGRAPHYRadford,A.,Metz,L.,andChintala,S.(2015).Unsupervisedrepresentationlearningwithdeepconvolutionalgenerativeadversarialnetworks.arXivpreprintarXiv:1511.06434.552701702,,Raiko,T.,Yao,L.,Cho,K.,andBengio, Y.(2014).Iterativeneuralautoregressivedistributionestimator(NADE-k).Technicalreport,arXiv:1406.1485.,676709Raina,R.,Madhavan,A.,andNg,A.Y.(2009).Large-scaledeepunsupervisedlearningusinggraphicsprocessors. InL.BottouandM.Littman,editors,ProceedingsoftheTwenty-sixthInternationalConferenceonMachineLearning(ICML’09),pages873–880,NewYork,NY,USA.ACM.,27446Ramsey,F.P.(1926).Truthandprobability.InR.B.Braithwaite,editor,TheFoundationsofMathematicsandotherLogicalEssays,chapter7,pages156–198.McMasterUniversityArchivefortheHistoryofEconomicThought.56Ranzato,M.andHinton,G.H.(2010).Modelingpixelmeansandcovariancesusingfactorizedthird-orderBoltzmannmachines.In,pages2551–2558.CVPR’2010680Ranzato,M.,Poultney,C.,Chopra,S.,andLeCun,Y.(2007a).Eﬃcientlearningofsparserepresentationswithanenergy-basedmodel.In.,,,,NIPS’20061419507528530Ranzato,M.,Huang,F.,Boureau,Y.,andLeCun,Y.(2007b).Unsupervisedlearningofinvariantfeaturehierarchieswithapplicationstoobjectrecognition.InProceedingsoftheComputerVisionandPatternRecognitionConference(CVPR’07).IEEEPress.364Ranzato,M.,Boureau,Y.,andLeCun,Y.(2008).Sparsefeaturelearningfordeepbeliefnetworks.In.NIPS’2007507Ranzato,M.,Krizhevsky,A.,andHinton,G.E.(2010a).Factored3-wayrestrictedBoltzmannmachinesformodelingnaturalimages.InProceedingsofAISTATS2010.678679,Ranzato,M.,Mnih,V.,andHinton,G.(2010b).GeneratingmorerealisticimagesusinggatedMRFs.In.NIPS’2010680Rao,C.(1945).Informationandtheaccuracyattainableintheestimationofstatisticalparameters.BulletinoftheCalcuttaMathematicalSociety,,81–89.,37135295Rasmus,A.,Valpola,H.,Honkala,M.,Berglund,M.,andRaiko,T.(2015).Semi-supervisedlearningwithladdernetwork.arXivpreprintarXiv:1507.02672.,426530Recht,B.,Re,C.,Wright,S.,andNiu,F.(2011).Hogwild:Alock-freeapproachtoparallelizingstochasticgradientdescent.In.NIPS’2011447Reichert,D.P.,Seriès,P.,andStorkey,A.J.(2011).Neuronaladaptationforsampling-basedprobabilisticinferenceinperceptualbistability.InAdvancesinNeuralInformationProcessingSystems,pages2357–2365.666763 BIBLIOGRAPHYRezende,D.J.,Mohamed,S.,andWierstra,D.(2014).Stochasticbackpropagationand approximateinferencein deepgenerative models.In .Preprint:ICML’2014arXiv:1401.4082.,,652689696Rifai, S., Vincent, P., Muller,X., Glorot, X.,andBengio,Y.(2011a).Contractiveauto-encoders:Explicitinvarianceduringfeatureextraction.In.,,ICML’2011521522523Rifai,S.,Mesnil,G.,Vincent,P.,Muller,X.,Bengio,Y.,Dauphin,Y.,andGlorot,X.(2011b).Higherordercontractiveauto-encoder.In.,ECMLPKDD521522Rifai,S.,Dauphin,Y.,Vincent,P.,Bengio,Y.,andMuller,X.(2011c). Themanifoldtangentclassiﬁer.In.,,NIPS’2011271272523Rifai,S.,Bengio,Y.,Dauphin,Y.,andVincent,P.(2012).Agenerativeprocessforsamplingcontractiveauto-encoders.In.ICML’2012711Ringach,D.andShapley,R.(2004).Reversecorrelationinneurophysiology.CognitiveScience,(2),147–166.28368Roberts,S.andEverson,R.(2001).Independentcomponentanalysis:principlesandpractice.CambridgeUniversityPress.493Robinson,A.J.andFallside,F.(1991).Arecurrenterrorpropagationnetworkspeechrecognitionsystem.ComputerSpeechandLanguage,(3),259–274.,527459Rockafellar,R.T.(1997).Convexanalysis.princetonlandmarksinmathematics.93Romero,A.,Ballas,N.,EbrahimiKahou,S.,Chassang,A.,Gatta,C.,andBengio,Y.(2015).Fitnets:Hintsforthindeepnets.In.ICLR’2015,arXiv:1412.6550325Rosen,J.B.(1960).Thegradientprojectionmethodfornonlinearprogramming.parti.linearconstraints.JournaloftheSocietyforIndustrialandAppliedMathematics,8(1),pp.181–217.93Rosenblatt,F.(1958).Theperceptron:Aprobabilisticmodelforinformationstorageandorganizationinthebrain.PsychologicalReview,,386–408.,,65141527Rosenblatt,F.(1962).PrinciplesofNeurodynamics.Spartan,NewYork.,1527Roweis,S.andSaul,L.K.(2000).Nonlineardimensionalityreductionbylocallylinearembedding.Science,(5500).,290164518Roweis,S.,Saul,L.,andHinton,G.(2002).Globalcoordinationoflocallinearmodels.InT.Dietterich,S.Becker,andZ.Ghahramani,editors,AdvancesinNeuralInformationProcessingSystems14(NIPS’01),Cambridge,MA.MITPress.489Rubin,D.B.(1984).Bayesianlyjustiﬁableandrelevantfrequencycalculationsforetal.theappliedstatistician.,(4),1151–1172.TheAnnalsofStatistics12717764 BIBLIOGRAPHYRumelhart, D., Hinton, G., andWilliams, R.(1986a).Learningrepresentationsbyback-propagatingerrors.Nature,,533–536.,,,,,,,323141823204225373476482Rumelhart,D.E.,Hinton,G.E.,andWilliams,R.J.(1986b).Learninginternalrepresen-tationsbyerrorpropagation.InD.E.RumelhartandJ.L.McClelland,editors,ParallelDistributedProcessing,volume1,chapter8,pages318–362.MITPress,Cambridge.,2127225,Rumelhart,D.E.,McClelland,J.L.,andthePDPResearchGroup(1986c).ParallelDistributedProcessing:ExplorationsintheMicrostructureofCognition.MITPress,Cambridge.17Russakovsky,O.,Deng,J.,Su,H.,Krause,J.,Satheesh,S.,Ma,S.,Huang,Z.,Karpathy,A.,Khosla,A.,Bernstein,M.,Berg,A.C.,andFei-Fei,L.(2014a).ImageNetLargeScaleVisualRecognitionChallenge.21Russakovsky,O.,Deng,J.,Su,H.,Krause,J.,Satheesh,S.,Ma,S.,Huang,Z.,Karpathy,A.,Khosla,A.,Bernstein,M.,(2014b).Imagenetlargescalevisualrecognitionetal.challenge.arXivpreprintarXiv:1409.0575.28Russel,S.J.andNorvig,P.(2003).ArtiﬁcialIntelligence:aModernApproach.PrenticeHall.86Rust,N.,Schwartz,O.,Movshon,J.A.,andSimoncelli,E.(2005).SpatiotemporalelementsofmacaqueV1receptiveﬁelds.Neuron,(6),945–956.46367Sainath,T.,Mohamed,A.,Kingsbury,B.,andRamabhadran,B.(2013).Deepconvolu-tionalneuralnetworksforLVCSR.In.ICASSP2013460Salakhutdinov,R.(2010).LearninginMarkovrandomﬁeldsusingtemperedtransitions.InY.Bengio,D.Schuurmans,C.Williams,J.Laﬀerty,andA.Culotta,editors,AdvancesinNeuralInformationProcessingSystems22(NIPS’09).603Salakhutdinov,R.andHinton,G.(2009a).DeepBoltzmannmachines.InProceedingsoftheInternationalConferenceonArtiﬁcialIntelligenceandStatistics,volume5,pages448–455.,,,,,,2427529663666671672Salakhutdinov,R.andHinton,G.(2009b).Semantichashing.InInternationalJournalofApproximateReasoning.525Salakhutdinov, R. andHinton, G. E.(2007a).Learning a nonlinearembedding bypreservingclassneighbourhoodstructure.InProceedingsoftheEleventhInternationalConferenceonArtiﬁcialIntelligenceandStatistics(AISTATS’07),SanJuan,PortoRico.Omnipress.527Salakhutdinov,R.andHinton,G.E.(2007b).Semantichashing.In.SIGIR’2007525765 BIBLIOGRAPHYSalakhutdinov,R.andHinton,G.E.(2008).UsingdeepbeliefnetstolearncovariancekernelsforGaussianprocesses.InJ.Platt,D.Koller,Y.Singer,andS.Roweis,editors,AdvancesinNeuralInformationProcessingSystems20(NIPS’07),pages1249–1256,Cambridge,MA.MITPress.244Salakhutdinov,R.andLarochelle,H.(2010).EﬃcientlearningofdeepBoltzmannmachines.InProceedingsoftheThirteenthInternationalConferenceonArtiﬁcialIntelligenceandStatistics(AISTATS2010),JMLRW&CP,volume9,pages693–700.652Salakhutdinov,R.andMnih,A.(2008).Probabilisticmatrixfactorization.In.NIPS’2008480Salakhutdinov,R.andMurray,I.(2008).Onthequantitativeanalysisofdeepbeliefnetworks.InW.W.Cohen,A.McCallum,andS.T.Roweis,editors,ProceedingsoftheTwenty-ﬁfthInternationalConferenceonMachineLearning(ICML’08),volume25,pages872–879.ACM.,628662Salakhutdinov,R.,Mnih,A.,andHinton,G.(2007).RestrictedBoltzmannmachinesforcollaborativeﬁltering.In.ICML480Sanger, T.D. (1994).Neuralnetworklearningcontrolofrobotmanipulatorsusinggraduallyincreasingtaskdiﬃculty.IEEETransactionsonRoboticsandAutomation,10(3).328Saul,L.K.andJordan,M.I.(1996).Exploitingtractablesubstructuresinintractablenetworks.InD.Touretzky,M.Mozer,andM.Hasselmo,editors,AdvancesinNeuralInformationProcessingSystems8(NIPS’95).MITPress,Cambridge,MA.638Saul,L.K.,Jaakkola,T.,andJordan,M.I.(1996).Meanﬁeldtheoryforsigmoidbeliefnetworks.JournalofArtiﬁcialIntelligenceResearch,,61–76.,427693Savich,A.W.,Moussa,M.,andAreibi,S.(2007).Theimpactofarithmeticrepresentationonimplementingmlp-bponfpgas:Astudy.NeuralNetworks,IEEETransactionson,18(1),240–252.451Saxe,A.M.,Koh,P.W.,Chen,Z.,Bhand,M.,Suresh,B.,andNg,A.(2011).Onrandomweightsandunsupervisedfeaturelearning.InProc.ICML’2011.ACM.363Saxe,A.M.,McClelland,J.L.,andGanguli,S.(2013).Exactsolutionstothenonlineardynamicsoflearningindeeplinearneuralnetworks.In.,,ICLR285286303Schaul,T.,Antonoglou,I.,andSilver,D.(2014).Unittestsforstochasticoptimization.InInternationalConferenceonLearningRepresentations.309Schmidhuber,J.(1992).Learningcomplex,extendedsequencesusingtheprincipleofhistorycompression.NeuralComputation,(2),234–242.4398Schmidhuber,J.(1996).Sequentialneuraltextcompression.IEEETransactionsonNeuralNetworks,(1),142–146.7477766 BIBLIOGRAPHYSchmidhuber,J.(2012).Self-delimitingneuralnetworks.arXivpreprintarXiv:1210.0118.390Schölkopf,B.andSmola,A.J.(2002).Learningwithkernels:Supportvectormachines,regularization,optimization,andbeyond.MITpress.704Schölkopf,B.,Smola,A.,andMüller,K.-R.(1998).Nonlinearcomponentanalysisasakerneleigenvalueproblem.NeuralComputation,,1299–1319.,10164518Schölkopf,B.,Burges,C.J.C.,andSmola,A.J.(1999).AdvancesinKernelMethods—SupportVectorLearning.MITPress,Cambridge,MA.,18142Schölkopf,B.,Janzing,D.,Peters,J.,Sgouritsa,E.,Zhang,K.,andMooij,J.(2012).Oncausalandanticausallearning.In,pages1255–1262.ICML’2012545Schuster,M.(1999).Onsupervisedlearningfromsequentialdatawithapplicationsforspeechrecognition.190Schuster,M.andPaliwal,K.(1997).Bidirectionalrecurrentneuralnetworks.IEEETransactionsonSignalProcessing,(11),2673–2681.45395Schwenk,H.(2007).Continuousspacelanguagemodels.Computerspeechandlanguage,21,492–518.466Schwenk,H.(2010).Continuousspacelanguagemodelsforstatisticalmachinetranslation.ThePragueBulletinofMathematicalLinguistics,,137–146.93473Schwenk,H.(2014).CleanedsubsetofWMT’14dataset.21Schwenk,H.andBengio,Y.(1998).Trainingmethodsforadaptiveboostingofneuralnet-works.InM.Jordan,M.Kearns,andS.Solla,editors,AdvancesinNeuralInformationProcessingSystems10(NIPS’97),pages647–653.MITPress.258Schwenk, H.andGauvain, J.-L.(2002).Connectionistlanguagemodeling forlargevocabularycontinuousspeechrecognition.InInternationalConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),pages765–768,Orlando,Florida.466Schwenk,H.,Costa-jussà,M.R.,andFonollosa,J.A.R.(2006).ContinuousspacelanguagemodelsfortheIWSLT2006task.InInternationalWorkshoponSpokenLanguageTranslation,pages166–173.473Seide,F.,Li,G.,andYu,D.(2011).Conversationalspeechtranscriptionusingcontext-dependentdeepneuralnetworks.InInterspeech2011,pages437–440.23Sejnowski,T.(1987).Higher-orderBoltzmannmachines.InAIPConferenceProceedings151onNeuralNetworksforComputing,pages398–403.AmericanInstituteofPhysicsInc.686767 BIBLIOGRAPHYSeries,P.,Reichert,D.P.,andStorkey,A.J.(2010).HallucinationsinCharlesBonnetsyndromeinducedbyhomeostasis:adeepBoltzmannmachinemodel.InAdvancesinNeuralInformationProcessingSystems,pages2020–2028.666Sermanet,P.,Chintala,S.,andLeCun,Y.(2012).Convolutionalneuralnetworksappliedtohousenumbersdigitclassiﬁcation.,.CoRRabs/1204.3968457Sermanet,P.,Kavukcuoglu,K.,Chintala,S.,andLeCun,Y.(2013).Pedestriandetectionwithunsupervisedmulti-stagefeaturelearning.InProc.InternationalConferenceonComputerVisionandPatternRecognition(CVPR’13).IEEE.,23201Shilov,G.(1977).LinearAlgebra.DoverBooksonMathematicsSeries.DoverPublications.31Siegelmann,H.(1995).ComputationbeyondtheTuringlimit.Science,268(5210),545–548.379Siegelmann,H.andSontag,E.(1991).Turingcomputabilitywithneuralnets.AppliedMathematicsLetters,(6),77–80.4379Siegelmann,H.T.andSontag,E.D.(1995).Onthecomputationalpowerofneuralnets.JournalofComputerandSystemsSciences,(1),132–150.,50379403Sietsma,J.andDow,R.(1991).Creatingartiﬁcialneuralnetworksthatgeneralize.NeuralNetworks,(1),67–79.4241Simard,D.,Steinkraus,P.Y.,andPlatt,J.C.(2003).Bestpracticesforconvolutionalneuralnetworks.In.ICDAR’2003371Simard,P.andGraf,H.P.(1994).Backpropagationwithoutmultiplication.InAdvancesinNeuralInformationProcessingSystems,pages232–239.451Simard,P.,Victorri,B.,LeCun,Y.,andDenker,J.(1992).Tangentprop-Aformalismforspecifyingselectedinvariancesinanadaptivenetwork.In.,,,NIPS’1991270271272356Simard,P.Y.,LeCun,Y.,andDenker,J.(1993).Eﬃcientpatternrecognitionusinganewtransformationdistance.In.NIPS’92270Simard,P.Y.,LeCun,Y.A.,Denker,J.S.,andVictorri,B.(1998).Transformationinvarianceinpatternrecognition—tangentdistanceandtangentpropagation.LectureNotesinComputerScience,.1524270Simons,D.J.andLevin,D.T.(1998).Failuretodetectchangestopeopleduringareal-worldinteraction.PsychonomicBulletin&Review,(4),644–649.5543Simonyan,K.andZisserman,A.(2015).Verydeepconvolutionalnetworksforlarge-scaleimagerecognition.In.ICLR323768 BIBLIOGRAPHYSjöberg,J.andLjung,L.(1995).Overtraining,regularizationandsearchingforaminimum,withapplicationtoneuralnetworks.InternationalJournalofControl,62(6),1391–1407.250Skinner,B.F.(1958).Reinforcementtoday.AmericanPsychologist,,94–99.13328Smolensky,P.(1986).Informationprocessingindynamicalsystems:Foundationsofharmonytheory.InD.E.RumelhartandJ.L.McClelland,editors,ParallelDistributedProcessing,volume1,chapter6,pages194–281.MITPress,Cambridge.,,571587656Snoek,J.,Larochelle,H.,andAdams,R.P.(2012).PracticalBayesianoptimizationofmachinelearningalgorithms.In.NIPS’2012436Socher,R.,Huang,E.H.,Pennington,J.,Ng,A.Y.,andManning,C.D.(2011a).Dynamicpoolingandunfoldingrecursiveautoencodersforparaphrasedetection.In.NIPS’2011401Socher,R.,Manning,C.,andNg,A.Y.(2011b).Parsingnaturalscenesandnaturallan-guagewithrecursiveneuralnetworks.InProceedingsoftheTwenty-EighthInternationalConferenceonMachineLearning(ICML’2011).401Socher, R., Pennington, J., Huang, E.H., Ng, A.Y.,andManning, C.D.(2011c).Semi-supervisedrecursiveautoencoders forpredictingsentimentdistributions.InEMNLP’2011.401Socher,R.,Perelygin,A.,Wu,J.Y.,Chuang,J.,Manning,C.D.,Ng,A.Y.,andPotts,C.(2013a).Recursivedeepmodelsforsemanticcompositionalityoverasentimenttreebank.In.EMNLP’2013401Socher,R.,Ganjoo,M.,Manning,C.D.,andNg,A.Y.(2013b).Zero-shotlearningthroughcross-modaltransfer.In27thAnnualConferenceonNeuralInformationProcessingSystems(NIPS2013).539Sohl-Dickstein,J.,Weiss,E.A.,Maheswaranathan,N.,andGanguli,S.(2015).Deepunsupervisedlearningusingnonequilibriumthermodynamics.716Sohn,K.,Zhou,G.,andLee,H.(2013).Learningandselectingfeaturesjointlywithpoint-wisegatedBoltzmannmachines.In.ICML’2013687Solomonoﬀ,R.J.(1989).Asystemforincrementallearningbasedonalgorithmicproba-bility.328Sontag,E.D.(1998).VCdimensionofneuralnetworks.NATOASISeriesFComputerandSystemsSciences,,69–96.,168547551Sontag,E.D.andSussman,H.J.(1989).Backpropagationcangiverisetospuriouslocalminimaevenfornetworkswithouthiddenlayers.,,91–106.ComplexSystems3284769 BIBLIOGRAPHYSparkes,B.(1996).TheRedandtheBlack:StudiesinGreekPottery.Routledge.1Spitkovsky,V.I.,Alshawi,H.,andJurafsky,D.(2010).Frombabystepstoleapfrog:how“lessismore”inunsuperviseddependencyparsing.InHLT’10.328Squire,W.andTrapp,G.(1998). Usingcomplexvariablestoestimatederivativesofrealfunctions.SIAMRev.,(1),110––112.40439Srebro,N.andShraibman,A.(2005).Rank,trace-normandmax-norm.InProceedingsofthe18thAnnualConferenceonLearningTheory,pages545–560.Springer-Verlag.238Srivastava,N.(2013).ImprovingNeuralNetworksWithDropout.Master’sthesis,U.Toronto.535Srivastava,N.andSalakhutdinov,R.(2012).MultimodallearningwithdeepBoltzmannmachines.In.NIPS’2012541Srivastava,N.,Salakhutdinov,R.R.,andHinton,G.E.(2013).ModelingdocumentswithdeepBoltzmannmachines.arXivpreprintarXiv:1309.6865.663Srivastava,N.,Hinton,G.,Krizhevsky,A.,Sutskever,I.,andSalakhutdinov,R.(2014).Dropout:Asimplewaytopreventneuralnetworksfromoverﬁtting.JournalofMachineLearningResearch,,1929–1958.,,,15258265267672Srivastava,R.K.,Greﬀ,K.,andSchmidhuber,J.(2015).Highwaynetworks.arXiv:1505.00387.326Steinkrau,D.,Simard,P.Y.,andBuck,I.(2005).UsingGPUsformachinelearningalgorithms.201312thInternationalConferenceonDocumentAnalysisandRecognition,0,1115–1119.445Stoyanov,V.,Ropson,A.,andEisner,J.(2011).Empiricalriskminimizationofgraphicalmodelparametersgivenapproximateinference,decoding,andmodelstructure.InProceedingsofthe14thInternationalConferenceonArtiﬁcialIntelligenceandStatistics(AISTATS)JMLRWorkshopandConferenceProceedings,volume15of,pages725–733,FortLauderdale.Supplementarymaterial(4pages)alsoavailable.,674698Sukhbaatar,S.,Szlam,A.,Weston,J.,andFergus,R.(2015).Weaklysupervisedmemorynetworks.arXivpreprintarXiv:1503.08895.418Supancic,J.andRamanan,D.(2013).Self-pacedlearningforlong-termtracking.InCVPR’2013.328Sussillo,D.(2014).Randomwalks:Trainingverydeepnonlinearfeed-forwardnetworkswithsmartinitialization.,.,,,CoRRabs/1412.6558290303305403Sutskever,I.(2012).TrainingRecurrentNeuralNetworks.Ph.D.thesis,Departmentofcomputerscience,UniversityofToronto.,406413770 BIBLIOGRAPHYSutskever,I.andHinton,G.E.(2008).Deepnarrowsigmoidbeliefnetworksareuniversalapproximators.NeuralComputation,(11),2629–2636.20693Sutskever,I.andTieleman,T.(2010).OntheConvergencePropertiesofContrastiveDivergence.InY.W.TehandM.Titterington,editors,Proc.oftheInternationalConferenceonArtiﬁcialIntelligenceandStatistics(AISTATS),volume9,pages789–795.612Sutskever,I.,Hinton,G.,andTaylor,G.(2009).TherecurrenttemporalrestrictedBoltzmannmachine.In.NIPS’2008685Sutskever,I.,Martens,J.,andHinton,G.E.(2011).Generatingtextwithrecurrentneuralnetworks.In,pages1017–1024.ICML’2011477Sutskever, I.,Martens,J.,Dahl, G.,andHinton,G.(2013).Ontheimportanceofinitializationandmomentumindeeplearning.In.,,ICML300406413Sutskever,I.,Vinyals,O.,andLe,Q.V.(2014).Sequencetosequencelearningwithneuralnetworks.In.,,,,,,NIPS’2014,arXiv:1409.321525101397410411474475Sutton,R.andBarto,A.(1998).ReinforcementLearning:AnIntroduction.MITPress.106Sutton,R.S.,Mcallester,D.,Singh,S.,andMansour,Y.(2000).Policygradientmethodsforreinforcementlearningwithfunctionapproximation.In,pages1057–NIPS’1999–1063.MITPress.691Swersky,K.,Ranzato,M.,Buchman,D.,Marlin,B.,anddeFreitas,N.(2011).Onautoencodersandscorematchingforenergybasedmodels.In.ACM.ICML’2011513Swersky,K.,Snoek,J.,andAdams,R.P.(2014).Freeze-thawBayesianoptimization.arXivpreprintarXiv:1406.3896.436Szegedy,C.,Liu,W.,Jia,Y.,Sermanet,P.,Reed,S.,Anguelov,D.,Erhan,D.,Vanhoucke,V.,andRabinovich,A.(2014a).Goingdeeperwithconvolutions.Technicalreport,arXiv:1409.4842.,,,,,,2427201258269326347Szegedy,C.,Zaremba,W.,Sutskever,I.,Bruna,J.,Erhan,D.,Goodfellow,I.J.,andFergus,R.(2014b).Intriguingpropertiesofneuralnetworks.,ICLRabs/1312.6199.268271,Szegedy,C.,Vanhoucke,V.,Ioﬀe,S.,Shlens,J.,andWojna,Z.(2015).RethinkingtheInceptionArchitectureforComputerVision..,ArXive-prints243322Taigman,Y.,Yang,M.,Ranzato,M.,andWolf,L.(2014).DeepFace:Closingthegaptohuman-levelperformanceinfaceveriﬁcation.In.CVPR’2014100Tandy,D.W.(1997).WorksandDays:ATranslationandCommentaryfortheSocialSciences.UniversityofCaliforniaPress.1771 BIBLIOGRAPHYTang,Y.andEliasmith,C.(2010).Deepnetworksforrobustvisualrecognition.InProceedingsofthe27thInternationalConferenceonMachineLearning,June21-24,2010,Haifa,Israel.241Tang,Y.,Salakhutdinov,R.,andHinton,G.(2012).Deepmixturesoffactoranalysers.arXivpreprintarXiv:1206.4635.489Taylor,G.andHinton,G.(2009).FactoredconditionalrestrictedBoltzmannmachinesformodelingmotionstyle.InL.BottouandM.Littman, editors,ProceedingsoftheTwenty-sixthInternationalConferenceonMachineLearning(ICML’09),pages1025–1032,Montreal,Quebec,Canada.ACM.685Taylor,G.,Hinton,G.E.,andRoweis,S.(2007).Modelinghumanmotionusingbinarylatentvariables.InB.Schölkopf,J.Platt,andT.Hoﬀman,editors,AdvancesinNeuralInformationProcessingSystems19(NIPS’06),pages1345–1352.MITPress,Cambridge,MA.685Teh,Y.,Welling,M.,Osindero,S.,andHinton,G.E.(2003).Energy-basedmodelsforsparseovercompleterepresentations.JournalofMachineLearningResearch,4,1235–1260.491Tenenbaum,J.,deSilva,V.,andLangford,J.C.(2000).Aglobalgeometricframeworkfornonlineardimensionalityreduction.Science,(5500),2319–2323.,,290164518533Theis,L.,vandenOord,A.,andBethge,M.(2015).Anoteontheevaluationofgenerativemodels.arXiv:1511.01844.,698719Thompson,J.,Jain,A.,LeCun,Y.,andBregler,C.(2014).Jointtrainingofaconvolutionalnetworkandagraphicalmodelforhumanposeestimation.In.NIPS’2014360Thrun,S.(1995).Learningtoplaythegameofchess.In.NIPS’1994271Tibshirani,R.J.(1995).Regressionshrinkageandselectionviathelasso.JournaloftheRoyalStatisticalSocietyB,,267–288.58236Tieleman,T.(2008).TrainingrestrictedBoltzmannmachinesusingapproximationstothelikelihoodgradient.InW.W.Cohen,A.McCallum,andS.T.Roweis,editors,Pro-ceedingsoftheTwenty-ﬁfthInternationalConferenceonMachineLearning(ICML’08),pages1064–1071.ACM.612Tieleman,T.andHinton,G.(2009).Usingfastweightstoimprovepersistentcontrastivedivergence.InL.BottouandM.Littman,editors,ProceedingsoftheTwenty-sixthInternationalConferenceonMachineLearning(ICML’09),pages1033–1040.ACM.614Tipping,M.E.andBishop,C.M.(1999).Probabilisticprincipalcomponentsanalysis.JournaloftheRoyalStatisticalSocietyB,(3),611–622.61491772 BIBLIOGRAPHYTorralba,A.,Fergus,R.,andWeiss,Y.(2008).Smallcodesandlargedatabasesforrecognition.InProceedingsoftheComputerVisionandPatternRecognitionConference(CVPR’08),pages1–8.525Touretzky,D.S.andMinton,G.E.(1985).Symbolsamongtheneurons:Detailsofaconnectionistinferencearchitecture. InProceedingsofthe9thInternationalJointConferenceonArtiﬁcialIntelligence-Volume1,IJCAI’85,pages238–243,SanFrancisco,CA,USA.MorganKaufmannPublishersInc.17Tu,K.andHonavar,V.(2011). Ontheutilityofcurriculainunsupervisedlearningofprobabilisticgrammars.In.IJCAI’2011328Turaga,S.C.,Murray,J.F.,Jain,V.,Roth,F.,Helmstaedter,M.,Briggman,K.,Denk,W.,andSeung,H.S.(2010).Convolutionalnetworkscanlearntogenerateaﬃnitygraphsforimagesegmentation.NeuralComputation,(2),511–538.22360Turian,J.,Ratinov,L.,andBengio,Y.(2010).Wordrepresentations:Asimpleandgeneralmethodforsemi-supervisedlearning.InProc.ACL’2010,pages384–394.535Töscher,A.,Jahrer,M.,andBell,R.M.(2009). TheBigChaossolutiontotheNetﬂixgrandprize.480Uria,B.,Murray,I.,andLarochelle,H.(2013).Rnade:Thereal-valuedneuralautoregres-sivedensity-estimator.In.,NIPS’2013709710vandenOörd,A.,Dieleman,S.,andSchrauwen,B.(2013).Deepcontent-basedmusicrecommendation.In.NIPS’2013480vanderMaaten,L.andHinton,G.E.(2008).Visualizingdatausingt-SNE.J.MachineLearningRes.,.,9477519Vanhoucke,V.,Senior,A.,andMao,M.Z.(2011).ImprovingthespeedofneuralnetworksonCPUs.InProc.DeepLearningandUnsupervisedFeatureLearningNIPSWorkshop.444452,Vapnik,V.N.(1982).EstimationofDependencesBasedonEmpiricalData.Springer-Verlag,Berlin.114Vapnik,V.N.(1995).TheNatureofStatisticalLearningTheory.Springer,NewYork.114Vapnik,V.N.andChervonenkis,A.Y.(1971).Ontheuniformconvergenceofrelativefrequenciesofeventstotheirprobabilities.TheoryofProbabilityandItsApplications,16,264–280.114Vincent,P.(2011). Aconnectionbetweenscorematchinganddenoisingautoencoders.NeuralComputation,(7).,,23513515712773 BIBLIOGRAPHYVincent,P.andBengio,Y.(2003).ManifoldParzenwindows.In.MITPress.NIPS’2002520Vincent,P.,Larochelle,H.,Bengio,Y.,andManzagol,P.-A.(2008).Extractingandcomposingrobustfeatureswithdenoisingautoencoders.In.,ICML2008241515Vincent,P.,Larochelle,H.,Lajoie,I.,Bengio,Y.,andManzagol,P.-A.(2010).Stackeddenoisingautoencoders:Learningusefulrepresentationsinadeepnetworkwithalocaldenoisingcriterion.J.MachineLearningRes.,.11515Vincent,P.,deBrébisson,A.,andBouthillier,X.(2015).Eﬃcientexactgradientupdatefortrainingdeepnetworkswithverylargesparsetargets.InC.Cortes,N.D.Lawrence,D.D.Lee,M.Sugiyama,andR.Garnett,editors,AdvancesinNeuralInformationProcessingSystems28,pages1108–1116.CurranAssociates,Inc.466Vinyals, O., Kaiser, L., Koo, T., Petrov, S., Sutskever, I., andHinton, G.(2014a).Grammarasaforeignlanguage.Technicalreport,arXiv:1412.7449.410Vinyals,O.,Toshev,A.,Bengio,S.,andErhan,D.(2014b).Showandtell:aneuralimagecaptiongenerator.arXiv1411.4555.410Vinyals,O.,Fortunato,M.,andJaitly,N.(2015a).Pointernetworks.arXivpreprintarXiv:1506.03134.418Vinyals,O.,Toshev,A.,Bengio,S.,andErhan,D.(2015b).Showandtell:aneuralimagecaptiongenerator.In.arXiv:1411.4555.CVPR’2015102Viola,P.andJones,M.(2001).Robustreal-timeobjectdetection.InInternationalJournalofComputerVision.449Visin,F.,Kastner,K.,Cho,K.,Matteucci,M.,Courville,A.,andBengio,Y.(2015).ReNet:Arecurrentneuralnetworkbasedalternativetoconvolutionalnetworks.arXivpreprintarXiv:1505.00393.395VonMelchner,L.,Pallas,S.L.,andSur,M.(2000).Visualbehaviourmediatedbyretinalprojectionsdirectedtotheauditorypathway.Nature,(6780),871–876.40416Wager,S.,Wang,S.,andLiang,P.(2013).Dropouttrainingasadaptiveregularization.InAdvancesinNeuralInformationProcessingSystems26,pages351–359.265Waibel,A.,Hanazawa,T.,Hinton,G.E.,Shikano,K.,andLang,K.(1989).Phonemerecognitionusingtime-delayneuralnetworks.IEEETransactionsonAcoustics,Speech,andSignalProcessing,,328–339.,,37374453459Wan,L.,Zeiler,M.,Zhang,S.,LeCun,Y.,andFergus,R.(2013).Regularizationofneuralnetworksusingdropconnect.In.ICML’2013266Wang,S.andManning,C.(2013).Fastdropouttraining.In.ICML’2013266774 BIBLIOGRAPHYWang,Z.,Zhang,J.,Feng,J.,andChen,Z.(2014a).Knowledgegraphandtextjointlyembedding.InProc.EMNLP’2014.484Wang,Z.,Zhang,J.,Feng,J.,andChen,Z.(2014b). Knowledgegraphembeddingbytranslatingonhyperplanes.InProc.AAAI’2014.484Warde-Farley,D.,Goodfellow,I.J.,Courville,A.,andBengio,Y.(2014).Anempiricalanalysisofdropoutinpiecewiselinearnetworks.In.,,ICLR’2014262266267Wawrzynek,J.,Asanovic,K.,Kingsbury,B.,Johnson,D.,Beck,J.,andMorgan,N.(1996).Spert-II:Avectormicroprocessorsystem.,(3),79–86.Computer29451Weaver,L.andTao,N.(2001).Theoptimalrewardbaselineforgradient-basedreinforce-mentlearning.InProc.UAI’2001,pages538–545.691Weinberger,K.Q.andSaul,L.K.(2004).Unsupervisedlearningofimagemanifoldsbysemideﬁniteprogramming.In,pages988–995.,CVPR’2004164519Weiss, Y., Torralba, A., andFergus, R.(2008).Spectral hashing.In,pagesNIPS1753–1760.525Welling,M.,Zemel,R.S.,andHinton,G.E.(2002).Selfsupervisedboosting.InAdvancesinNeuralInformationProcessingSystems,pages665–672.703Welling, M.,Hinton,G.E., andOsindero, S.(2003a).LearningsparsetopographicrepresentationswithproductsofStudent-tdistributions.In.NIPS’2002680Welling,M.,Zemel,R.,andHinton,G.E.(2003b).Self-supervisedboosting.InS.Becker,S.Thrun,andK.Obermayer,editors,AdvancesinNeuralInformationProcessingSystems15(NIPS’02),pages665–672.MITPress.622Welling,M.,Rosen-Zvi,M.,andHinton,G.E.(2005).Exponentialfamilyharmoniumswithanapplicationtoinformationretrieval.InL.Saul,Y.Weiss,andL.Bottou,editors,AdvancesinNeuralInformationProcessingSystems17(NIPS’04),volume17,Cambridge,MA.MITPress.676Werbos, P.J.(1981).Applicationsofadvancesinnonlinearsensitivityanalysis.InProceedingsofthe10thIFIPConference,31.8-4.9,NYC,pages762–770.225Weston,J.,Bengio,S.,andUsunier,N.(2010).Largescaleimageannotation:learningtorankwithjointword-imageembeddings.MachineLearning,(1),21–35.81401Weston, J., Chopra, S., andBordes, A.(2014).Memorynetworks.arXivpreprintarXiv:1410.3916.,418485Widrow,B.andHoﬀ,M.E.(1960).Adaptiveswitchingcircuits.In1960IREWESCONConventionRecord,volume4,pages96–104.IRE,NewYork.,,,15212427775 BIBLIOGRAPHYWikipedia(2015).Listofanimalsbynumberofneurons—Wikipedia,thefreeencyclopedia.[Online;accessed4-March-2015].,2427Williams,C.K.I.andAgakov,F.V.(2002). ProductsofGaussiansandProbabilisticMinorComponentAnalysis.NeuralComputation,,1169–1182.14(5)682Williams,C.K.I.andRasmussen,C.E.(1996). Gaussianprocessesforregression. InD.Touretzky,M.Mozer,andM.Hasselmo,editors,AdvancesinNeuralInformationProcessingSystems8(NIPS’95),pages514–520.MITPress,Cambridge,MA.142Williams,R.J.(1992).Simplestatisticalgradient-followingalgorithmsconnectionistreinforcementlearning.MachineLearning,,229–256.,8688689Williams,R.J.andZipser,D.(1989).Alearningalgorithmforcontinuallyrunningfullyrecurrentneuralnetworks.NeuralComputation,,270–280.1223Wilson,D.R.andMartinez,T.R.(2003).Thegeneralineﬃciencyofbatchtrainingforgradientdescentlearning.NeuralNetworks,(10),1429–1451.16279Wilson,J.R.(1984).Variancereductiontechniquesfordigitalsimulation.AmericanJournalofMathematicalandManagementSciences,(3),277––312.4690Wiskott,L.andSejnowski,T.J.(2002).Slowfeatureanalysis:Unsupervisedlearningofinvariances.NeuralComputation,(4),715–770.14494Wolpert,D.andMacReady,W.(1997).Nofreelunchtheoremsforoptimization.IEEETransactionsonEvolutionaryComputation,,67–82.1293Wolpert,D.H.(1996).Thelackofaprioridistinctionbetweenlearningalgorithms.NeuralComputation,(7),1341–1390.8116Wu,R.,Yan,S.,Shan,Y.,Dang,Q.,andSun,G.(2015).Deepimage:Scalingupimagerecognition.arXiv:1501.02876.447Wu,Z.(1997).Globalcontinuationfordistancegeometryproblems.SIAMJournalofOptimization,,814–836.7327Xiong,H.Y.,Barash,Y.,andFrey,B.J.(2011).Bayesianpredictionoftissue-regulatedsplicingusingRNAsequenceandcellularcontext.,Bioinformatics27(18),2554–2562.265Xu,K.,Ba,J.L.,Kiros,R.,Cho,K.,Courville,A.,Salakhutdinov,R.,Zemel,R.S.,andBengio,Y.(2015).Show,attendandtell:Neuralimagecaptiongenerationwithvisualattention.In.,,ICML’2015,arXiv:1502.03044102410691Yildiz,I.B.,Jaeger,H.,andKiebel,S.J.(2012).Re-visitingtheechostateproperty.Neuralnetworks,,1–9.35405776 BIBLIOGRAPHYYosinski,J.,Clune,J.,Bengio,Y.,andLipson,H.(2014).Howtransferablearefeaturesindeepneuralnetworks?In.,NIPS’2014325536Younes,L.(1998).OntheconvergenceofMarkovianstochasticalgorithmswithrapidlydecreasingergodicityrates.InStochasticsandStochasticsModels,pages177–228.612Yu,D.,Wang,S.,and Deng, L. (2010).Sequential labeling using deep-structuredconditionalrandomﬁelds.IEEEJournalofSelectedTopicsinSignalProcessing.323Zaremba,W.andSutskever,I.(2014).Learningtoexecute.arXiv1410.4615.329Zaremba,W.andSutskever,I.(2015).ReinforcementlearningneuralTuringmachines.arXiv:1505.00521.419Zaslavsky,T.(1975).FacingUptoArrangements:Face-CountFormulasforPartitionsofSpacebyHyperplanes.Numberno.154inMemoirsoftheAmericanMathematicalSociety.AmericanMathematicalSociety.550Zeiler,M.D.andFergus,R.(2014).Visualizingandunderstandingconvolutionalnetworks.In.ECCV’146Zeiler,M.D.,Ranzato,M.,Monga,R.,Mao,M.,Yang,K.,Le,Q.,Nguyen,P.,Senior,A.,Vanhoucke,V.,Dean,J.,andHinton,G.E.(2013). Onrectiﬁedlinearunitsforspeechprocessing.In.ICASSP2013460Zhou,B.,Khosla,A.,Lapedriza,A.,Oliva,A.,andTorralba,A.(2015). ObjectdetectorsemergeindeepsceneCNNs.ICLR’2015,arXiv:1412.6856.551Zhou,J.andTroyanskaya,O.G.(2014).Deepsupervisedandconvolutionalgenerativestochasticnetworkforproteinsecondarystructureprediction.In.ICML’2014715Zhou,Y.andChellappa,R.(1988).Computationofopticalﬂowusinganeuralnetwork.InNeuralNetworks,1988.,IEEEInternationalConferenceon,pages71–78.IEEE.339Zöhrer,M.andPernkopf,F.(2014).Generalstochasticnetworksforclassiﬁcation.InNIPS’2014.716 777 Index0-1loss,,102274Absolutevaluerectiﬁcation,191Accuracy,420Activationfunction,169Activeconstraint,94AdaGrad,305ADALINE,seeadaptivelinearelementAdam,,307422Adaptivelinearelement,,,152326Adversarialexample,265Adversarialtraining,,,266268526Aﬃne,109AIS,seeannealedimportancesamplingAlmosteverywhere,70Almostsureconvergence,128Ancestralsampling,,576591ANN,seeArtiﬁcialneuralnetworkAnnealedimportancesampling, ,,621662711ApproximateBayesiancomputation,710Approximateinference,579Artiﬁcialintelligence,1Artiﬁcialneuralnetwork, seeNeuralnet-workASR,seeautomaticspeechrecognitionAsymptoticallyunbiased,123Audio,,,101357455Autoencoder,,,4353498Automaticspeechrecognition,455Back-propagation,201Back-propagationthroughtime,381Backprop,seeback-propagationBagofwords,467Bagging,252Batchnormalization,,264422Bayeserror,116Bayes’rule,69Bayesianhyperparameteroptimization,433Bayesian network, seedirected graphicalmodelBayesianprobability,54Bayesianstatistics,134Beliefnetwork,seedirectedgraphicalmodelBernoullidistribution,61BFGS,314Bias,,123227Biasparameter,109Biasedimportancesampling,589Bigram,458Binaryrelation,478BlockGibbssampling,595Boltzmanndistribution,566Boltzmannmachine,,566648BPTT,seeback-propagationthroughtimeBroadcasting,33Burn-in,593CAE,seecontractiveautoencoderCalculusofvariations,178Categoricaldistribution,seemultinoullidis-tributionCD,seecontrastivedivergenceCenteringtrick(DBM),667Centrallimittheorem,63Chainrule(calculus),203Chainruleofprobability,58778 INDEXChess,2Chord,575Chordalgraph,575Class-basedlanguagemodels,460Classicaldynamicalsystem,372Classiﬁcation,99Cliquepotential,seefactor(graphicalmodel)CNN,seeconvolutionalneuralnetworkCollaborativeFiltering,474Collider,seeexplainingawayColorimages,357Complexcell,362Computationalgraph,202Computervision,449Conceptdrift,533Conditionnumber,277Conditionalcomputation,seedynamicstruc-tureConditionalindependence,,xiii59Conditionalprobability,58ConditionalRBM,679Connectionism,,17440Connectionisttemporalclassiﬁcation,457Consistency,,128509Constrainedoptimization,,92235Content-basedaddressing,416Content-basedrecommendersystems,475Context-speciﬁcindependence,569Contextualbandits,476Continuationmethods,324Contractiveautoencoder,516Contrast,451Contrastivedivergence,,,289606666Convexoptimization,140Convolution,,327677Convolutionalnetwork,16Convolutionalneuralnetwork,,250327,,422456Coordinatedescent,,319665Correlation,60Costfunction,seeobjectivefunctionCovariance,,xiii60Covariancematrix,61Coverage,421Criticaltemperature,599Cross-correlation,329Cross-entropy,,74131Cross-validation,121CTC,seeconnectionisttemporalclassiﬁca-tionCurriculumlearning,326Curseofdimensionality,153Cyc,2D-separation,568DAE,seedenoisingautoencoderDatageneratingdistribution,,110130Datageneratingprocess,110Dataparallelism,444Dataset,103Datasetaugmentation,,268454DBM,seedeepBoltzmannmachineDCGAN,,,547548695Decisiontree,,144544Decoder,4Deepbeliefnetwork,,,,,,26525626651654678686,DeepBlue,2DeepBoltzmannmachine,,,,,2326525626647651657666678,,,,Deepfeedforwardnetwork,,166422Deeplearning,,25Denoisingautoencoder,,506683Denoisingscorematching,615Densityestimation,102Derivative,,xiii82Designmatrix,105Detectorlayer,336Determinant,xiiDiagonalmatrix,40Diﬀerentialentropy,,73641Diracdeltafunction,64Directedgraphicalmodel,,,,76503559685Directionalderivative,84Discriminativeﬁne-tuning,seesupervisedﬁne-tuningDiscriminativeRBM,680Distributedrepresentation,,,17149542Domainadaptation,532779 INDEXDotproduct,,33139Doublebackprop,268Doublyblockcirculantmatrix,330Dreamsleep,,605647DropConnect,263Dropout,,,,,,255422427428666683Dynamicstructure,445E-step,629Earlystopping,,,,,244246270271422EBM,seeenergy-basedmodelEchostatenetwork,,,2326401Eﬀectivecapacity,113Eigendecomposition,41Eigenvalue,41Eigenvector,41ELBO,seeevidencelowerboundElement-wiseproduct,seeHadamardprod-uct,seeHadamardproductEM,seeexpectationmaximizationEmbedding,512Empiricaldistribution,65Empiricalrisk,274Empiricalriskminimization,274Encoder,4Energyfunction,565Energy-basedmodel,,,,565591648657Ensemblemethods,252Epoch,244Equalityconstraint,93Equivariance,335Errorfunction,seeobjectivefunctionESN,seeechostatenetworkEuclideannorm,38Euler-Lagrangeequation,641Evidencelowerbound,,628655Example,98Expectation,59Expectationmaximization,629Expectedvalue,seeexpectationExplainingaway,,,570626639Exploitation,477Exploration,477Exponentialdistribution,64F-score,420Factor(graphicalmodel),563Factoranalysis,486Factorgraph,575Factorsofvariation,4Feature,98Featureselection,234Feedforwardneuralnetwork,166Fine-tuning,321Finitediﬀerences,436Forgetgate,304Forwardpropagation,201Fouriertransform,,357359Fovea,363FPCD,610Freeenergy,,567674Freebase,479Frequentistprobability,54Frequentiststatistics,134Frobeniusnorm,45Fully-visibleBayesnetwork,699Functionalderivatives,640FVBN,seefully-visibleBayesnetworkGaborfunction,365GANs,seegenerativeadversarialnetworksGatedrecurrentunit,422Gaussiandistribution,seenormaldistribu-tionGaussiankernel,140Gaussianmixture,,66187GCN,seeglobalcontrastnormalizationGeneOntology,479Generalization,109GeneralizedLagrangefunction,seegeneral-izedLagrangianGeneralizedLagrangian,93Generativeadversarialnetworks,,683693Generativemomentmatchingnetworks,696Generatornetwork,687Gibbsdistribution,564Gibbssampling,,577595Globalcontrastnormalization,451GPU,seegraphicsprocessingunitGradient,83780 INDEXGradientclipping,,287411Gradientdescent,,8284Graph,xiiGraphicalmodel,seestructuredprobabilis-ticmodelGraphicsprocessingunit,441Greedyalgorithm,321Greedylayer-wiseunsupervisedpretraining,524Greedysupervisedpretraining,321Gridsearch,429Hadamardproduct,,xii33Hard,tanh195Harmonium,seerestrictedBoltzmannma-chineHarmonytheory,567Helmholtzfreeenergy,seeevidencelowerboundHessian,221Hessianmatrix,,xiii86Heteroscedastic,186Hiddenlayer,,6166Hillclimbing,85Hyperparameteroptimization,429Hyperparameters,,119427Hypothesisspace,,111117i.i.d.assumptions,,,110121265Identitymatrix,35ILSVRC,seeImageNetLargeScaleVisualRecognitionChallengeImageNetLargeScaleVisualRecognitionChallenge,22Immorality,573Importancesampling,,,588620691Importanceweightedautoencoder,691Independence,,xiii59Independentandidenticallydistributed,seei.i.d.assumptionsIndependentcomponentanalysis,487Independentsubspaceanalysis,489Inequalityconstraint,93Inference,,,,,,,,558579626628630633643646Informationretrieval,520Initialization,298Integral,xiiiInvariance,339Isotropic,64Jacobianmatrix,,,xiii7185Jointprobability,56k-means,,361542k-nearestneighbors,,141544Karush-Kuhn-Tuckerconditions,,94235Karush–Kuhn–Tucker,93Kernel(convolution),,328329Kernelmachine,544Kerneltrick,139KKT,seeKarush–Kuhn–TuckerKKTconditions,seeKarush-Kuhn-TuckerconditionsKLdivergence,seeKullback-Leiblerdiver-genceKnowledgebase,,2479Krylovmethods,222Kullback-Leiblerdivergence,,xiii73Labelsmoothing,241Lagrangemultipliers,,93641Lagrangian,seegeneralizedLagrangianLAPGAN,695Laplacedistribution,,64492Latentvariable,66Layer(neuralnetwork),166LCN,seelocalcontrastnormalizationLeakyReLU,191Leakyunits,404Learningrate,84Linesearch,,,848592Linearcombination,36Lineardependence,37Linearfactormodels,485Linearregression,,,106109138Linkprediction,480Lipschitzconstant,91Lipschitzcontinuous,91Liquidstatemachine,401781 INDEXLocalconditionalprobabilitydistribution,560Localcontrastnormalization,452Logisticregression,,,3138139Logisticsigmoid,,766Longshort-termmemory,,,,1824304407,422Loop,575Loopybeliefpropagation,581Lossfunction,seeobjectivefunctionLpnorm,38LSTM,seelongshort-termmemoryM-step,629Machinelearning,2Machinetranslation,100Maindiagonal,32Manifold,159Manifoldhypothesis,160Manifoldlearning,160Manifoldtangentclassiﬁer,268MAPapproximation,,137501Marginalprobability,57Markovchain,591MarkovchainMonteCarlo,591Markovnetwork,seeundirectedmodelMarkovrandomﬁeld,seeundirectedmodelMatrix,,,xixii31Matrixinverse,35Matrixproduct,33Maxnorm,39Maxpooling,336Maximumlikelihood,130Maxout,,191422MCMC,seeMarkovchainMonteCarloMeanﬁeld,,,633634666Meansquarederror,107Measuretheory,70Measurezero,70Memorynetwork,,413415Methodof steepestdescent, seegradientdescentMinibatch,277Missinginputs,99Mixing(Markovchain),597Mixturedensitynetworks,187Mixturedistribution,65Mixturemodel,,187506Mixtureofexperts,,446544MLP,seemultilayerperceptionMNIST,,,2021666Modelaveraging,252Modelcompression,444Modelidentiﬁability,282Modelparallelism,444Momentmatching,696Moore-Penrosepseudoinverse,,44237Moralizedgraph,573MP-DBM,seemulti-predictionDBMMRF(Markov RandomField),seeundi-rectedmodelMSE,seemeansquarederrorMulti-modallearning,535Multi-predictionDBM,668Multi-tasklearning,,242533Multilayerperception,5Multilayerperceptron,26Multinomialdistribution,61Multinoullidistribution,61n-gram,458NADE,702NaiveBayes,3Nat,72Naturalimage,555Naturallanguageprocessing,457Nearestneighborregression,114Negativedeﬁnite,88Negativephase,,,466602604Neocognitron,,,,162326364Nesterovmomentum,298NetﬂixGrandPrize,,255475Neurallanguagemodel,,460472Neuralnetwork,13NeuralTuringmachine,415Neuroscience,15Newton’smethod,,88309NLM,seeneurallanguagemodelNLP,seenaturallanguageprocessingNofreelunchtheorem,115782 INDEXNoise-contrastiveestimation,616Non-parametricmodel,113Norm,,xiv38Normaldistribution,,,6263124Normalequations,,,,108108111232Normalizedinitialization,301Numericaldiﬀerentiation,seeﬁnitediﬀer-encesObjectdetection,449Objectrecognition,449Objectivefunction,81OMP-,kseeorthogonalmatchingpursuitOne-shotlearning,534Operation,202Optimization,,7981Orthodoxstatistics,seefrequentiststatisticsOrthogonalmatchingpursuit,,26252Orthogonalmatrix,41Orthogonality,40Outputlayer,166Paralleldistributedprocessing,17Parameterinitialization,,298403Parametersharing,,,,,249332370372386Parametertying,seeParametersharingParametricmodel,113ParametricReLU,191Partialderivative,83Partitionfunction,,,564601663PCA,seeprincipalcomponentsanalysisPCD,seestochasticmaximumlikelihoodPerceptron,,1526Persistentcontrastivedivergence,seestochas-ticmaximumlikelihoodPerturbationanalysis,seereparametrizationtrickPointestimator,121Policy,476Pooling,,327677Positivedeﬁnite,88Positivephase,,,,,466602604650662Precision,420Precision(ofanormaldistribution),,6264Predictivesparsedecomposition,519Preprocessing,450Pretraining,,320524Primaryvisualcortex,362Principalcomponentsanalysis,,,,47145146486626,Priorprobabilitydistribution,134Probabilisticmaxpooling,677ProbabilisticPCA,,,486487627Probabilitydensityfunction,57Probabilitydistribution,55Probabilitymassfunction,55Probabilitymassfunctionestimation,102Productofexperts,566Productruleofprobability,seechainruleofprobabilityPSD,seepredictivesparsedecompositionPseudolikelihood,611Quadraturepair,366Quasi-Newtonmethods,314Radialbasisfunction,195Randomsearch,431Randomvariable,55Ratiomatching,614RBF,195RBM,seerestrictedBoltzmannmachineRecall,420Receptiveﬁeld,334RecommenderSystems,474Rectiﬁedlinearunit,,,,170191422503Recurrentnetwork,26Recurrentneuralnetwork,375Regression,99Regularization,,,,,119119176226427Regularizer,118REINFORCE,683Reinforcementlearning,,,,24105476683Relationaldatabase,479Relations,478Reparametrizationtrick,682Representationlearning,3Representationalcapacity,113RestrictedBoltzmannmachine, , ,353456475583626650651666670,,,,,,,783 INDEX672674677,,Ridgeregression,seeweightdecayRisk,273RNN-RBM,679Saddlepoints,283Samplemean,124Scalar,,,xixii30Scorematching,,509613Secondderivative,85Secondderivativetest,88Self-information,72Semantichashing,521Semi-supervisedlearning,241Separableconvolution,359Separation(probabilisticmodeling),568Set,xiiSGD,seestochasticgradientdescentShannonentropy,,xiii73Shortlist,462Sigmoid,,xivseelogisticsigmoidSigmoidbeliefnetwork,26Simplecell,362Singularvalue,seesingularvaluedecompo-sitionSingularvaluedecomposition,,,43146475Singularvector,seesingularvaluedecom-positionSlowfeatureanalysis,489SML,seestochasticmaximumlikelihoodSoftmax,,,182415446Softplus,,,xiv67195Spamdetection,3Sparsecoding,,,,,319353492626686Sparseinitialization,,302403Sparserepresentation,,,,,145224251501552Spearmint,433Spectralradius,401Speechrecognition,see automaticspeechrecognitionSphering,seewhiteningSpikeand slabrestricted Boltzmannma-chine,674SPN,seesum-productnetworkSquarematrix,37ssRBM,seespikeandslabrestrictedBoltz-mannmachineStandarddeviation,60Standarderror,126Standarderrorofthemean,,126276Statistic,121Statisticallearningtheory,109Steepestdescent,seegradientdescentStochasticback-propagation,seereparametriza-tiontrickStochasticgradientdescent,,,,15149277292,666Stochasticmaximumlikelihood,,608666Stochasticpooling,263Structurelearning,578Structuredoutput,,100679Structuredprobabilisticmodel,,76554Sumruleofprobability,57Sum-productnetwork,549Supervisedﬁne-tuning,,525656Supervisedlearning,104Supportvectormachine,139Surrogatelossfunction,274SVD,seesingularvaluedecompositionSymmetricmatrix,,4042Tangentdistance,267Tangentplane,511Tangentprop,267TDNN,seetime-delayneuralnetworkTeacherforcing,,379380Tempering,599Templatematching,140Tensor,,,xixii32Testset,109Tikhonovregularization,seeweightdecayTiledconvolution,349Time-delayneuralnetwork,,364371Toeplitzmatrix,330TopographicICA,489Traceoperator,45Trainingerror,109Transcription,100Transferlearning,532784 INDEXTranspose,,xii32Triangleinequality,38Triangulatedgraph,seechordalgraphTrigram,458Unbiased,123Undirectedgraphicalmodel,,76503Undirectedmodel,562Uniformdistribution,56Unigram,458Unitnorm,40Unitvector,40Universalapproximationtheorem,196Universalapproximator,549Unnormalizedprobabilitydistribution,563Unsupervisedlearning,,104144Unsupervisedpretraining,,456524V-structure,seeexplainingawayV1,362VAE,seevariationalautoencoderVapnik-Chervonenkisdimension,113Variance,,,xiii60227Variationalautoencoder,,683690Variationalderivatives,seefunctionalderiva-tivesVariationalfreeenergy,seeevidencelowerboundVCdimension,seeVapnik-Chervonenkisdi-mensionVector,,,xixii31Virtualadversarialexamples,266Visiblelayer,6Volumetricdata,357Wake-sleep,,646655Weightdecay,,,,117176229428Weightspacesymmetry,282Weights,,15106Whitening,452Wikibase,479Wikibase,479Wordembedding,460Word-sensedisambiguation,480WordNet,479Zero-datalearning,seezero-shotlearningZero-shotlearning,534 785"
}