{
  "title": "Deep Learning with Python",
  "source": "Libros/Deep Learning with Python.pdf",
  "content": "MANNINGFrançois CholletSECOND EDITION Deep Learning with Python Deep Learningwith PythonSECOND EDITIONFRANÇOIS CHOLLET MANNINGSHELTER ISLAND For online information and ordering of this and other Manning books, please visitwww.manning.com. The publisher offers discounts on this book when ordered in quantity. For more information, please contactSpecial Sales DepartmentManning Publications Co.20 Baldwin RoadPO Box 761Shelter Island, NY 11964Email: orders@manning.com©2021 by Manning Publications Co. All rights reserved.No part of this publication may be reproduced, stored in a retrieval system, or transmitted, in any form or by means electronic, mechanical, photocopying, or otherwise, without prior written permission of the publisher.Many of the designations used by manufacturers and sellers to distinguish their products are claimed as trademarks. Where those designations appear in the book, and Manning Publications was aware of a trademark claim, the designations have been printed in initial caps or all caps.Recognizing the importance of preserving what has been written, it is Manning’s policy to have the books we publish printed on acid-free paper, and we exert our best efforts to that end. Recognizing also our responsibility to conserve the resources of our planet, Manning booksare printed on paper that is at least 15 percent recycled and processed without the use of elemental chlorine.The author and publisher have made every effort to ensure that the information in this book was correct at press time. The author and publisher do not assume and hereby disclaim any liability to any party for any loss, damage, or disruption caused by errors or omissions, whether such errors or omissions result from negligence, accident, or any other cause, or from any usage of the information herein.Development editor: Jennifer StoutTechnical development editor: Frances BuontempoManning Publications Co. Review editor: Aleksandar Dragosavljevic ´20 Baldwin Road Production editor: Keri HalesPO Box 761 Copy editor: Andy CarrollShelter Island, NY 11964 Proofreaders: Katie Tennant and Melody DolabTechnical proofreader: Karsten StrøbækTypesetter: Dennis DalinnikCover designer: Marija TudorISBN: 9781617296864Printed in the United States of America To my son Sylvain: I hope you’ll read this book someday! viibrief contents1IWhat is deep learning?12IThe mathematical building blocks of neural networks263IIntroduction to Keras and TensorFlow684IGetting started with neural networks: Classification and regression955IFundamentals of machine learning1216IThe universal workflow of machine learning1537IWorking with Keras: A deep dive1728IIntroduction to deep learning for computer vision2019IAdvanced deep learning for computer vision23810IDeep learning for timeseries28011IDeep learning for text30912IGenerative deep learning36413IBest practices for the real world41214IConclusions431 ixcontents preface xviiacknowledgments xixabout this book xxabout the author xxiiiabout the cover illustration xxiv1 What is deep learning? 11.1 Artificial intelligence, machine learning, and deep learning 2Artificial intelligence 2IMachine learning 3ILearning rules and representations from data 4IThe “deep” in “deep learning” 7IUnderstanding how deep learning works, in three figures 8IWhat deep learning has achieved so far 10Don’t believe the short-term hype 11IThe promise of AI 121.2 Before deep learning: A brief history of machine learning 13Probabilistic modeling 13IEarly neural networks 14Kernel methods 14IDecision trees, random forests, and gradient boosting machines 15IBack to neural networks 16What makes deep learning different 17IThe modern machine learning landscape 18 CONTENTSx1.3 Why deep learning? Why now? 20Hardware 20IData 21IAlgorithms 22IA new wave of investment 23IThe democratization of deep learning 24Will it last? 242 The mathematical building blocks of neural networks 262.1 A first look at a neural network 272.2 Data representations for neural networks 31Scalars (rank-0 tensors) 31IVectors (rank-1 tensors) 31Matrices (rank-2 tensors) 32IRank-3 and higher-rank tensors 32IKey attributes 32IManipulating tensors in NumPy 34IThe notion of data batches 35IReal-world examples of data tensors 35IVector data 35ITimeseries data or sequence data 36IImage data 37IVideo data 372.3 The gears of neural networks: Tensor operations 38Element-wise operations 38IBroadcasting 40ITensor product 41Tensor reshaping 43IGeometric interpretation of tensor operations 44A geometric interpretation of deep learning 472.4 The engine of neural networks: Gradient-based optimization 48What’s a derivative? 49IDerivative of a tensor operation: The gradient 51IStochastic gradient descent 52IChaining derivatives: The Backpropagation algorithm 552.5 Looking back at our first example 61Reimplementing our first example from scratch in TensorFlow 63Running one training step 64IThe full training loop 65Evaluating the model 663 Introduction to Keras and TensorFlow 683.1 What’s TensorFlow? 693.2 What’s Keras? 693.3 Keras and TensorFlow: A brief history 713.4 Setting up a deep learning workspace 71Jupyter notebooks: The preferred way to run deep learning experiments 72IUsing Colaboratory 733.5 First steps with TensorFlow 75Constant tensors and variables 76ITensor operations: Doing math in TensorFlow 78IA second look at the GradientTape API 78IAn end-to-end example: A linear classifier in pure TensorFlow 79 CONTENTSxi3.6 Anatomy of a neural network: Understanding core Keras APIs 84Layers: The building blocks of deep learning 84IFrom layers to models 87IThe “compile” step: Configuring the learning process 88IPicking a loss function 90IUnderstanding the fit() method 91IMonitoring loss and metrics on validation data 91IInference: Using a model after training 934 Getting started with neural networks: Classification and regression 954.1 Classifying movie reviews: A binary classification example 97The IMDB dataset 97IPreparing the data 98IBuilding your model 99IValidating your approach 102IUsing a trained model to generate predictions on new data 105IFurther experiments 105IWrapping up 1064.2 Classifying newswires: A multiclass classification example 106The Reuters dataset 106IPreparing the data 107IBuilding your model 108IValidating your approach 109IGenerating predictions on new data 111IA different way to handle the labels and the loss 112IThe importance of having sufficiently large intermediate layers 112IFurther experiments 113Wrapping up 1134.3 Predicting house prices: A regression example 113The Boston housing price dataset 114IPreparing the data 114Building your model 115IValidating your approach using K-fold validation 115IGenerating predictions on new data 119IWrapping up 1195 Fundamentals of machine learning 1215.1 Generalization: The goal of machine learning 121Underfitting and overfitting 122IThe nature of generalization in deep learning 1275.2 Evaluating machine learning models 133Training, validation, and test sets 133IBeating a common-sense baseline 136IThings to keep in mind about model evaluation 1375.3 Improving model fit 138Tuning key gradient descent parameters 138ILeveraging better architecture priors 139IIncreasing model capacity 140 CONTENTSxii5.4 Improving generalization 142Dataset curation 142IFeature engineering 143IUsing early stopping 144IRegularizing your model 1456 The universal workflow of machine learning 1536.1 Define the task 155Frame the problem 155ICollect a dataset 156IUnderstand your data 160IChoose a measure of success 1606.2 Develop a model 161Prepare the data 161IChoose an evaluation protocol 162Beat a baseline 163IScale up: Develop a model that overfits 164IRegularize and tune your model 1656.3 Deploy the model 165Explain your work to stakeholders and set expectations 165Ship an inference model 166IMonitor your model in the wild 169IMaintain your model 1707 Working with Keras: A deep dive 1727.1 A spectrum of workflows 1737.2 Different ways to build Keras models 173The Sequential model 174IThe Functional API 176Subclassing the Model class 182IMixing and matching different components 184IRemember: Use the right tool for the job 1857.3 Using built-in training and evaluation loops 185Writing your own metrics 186IUsing callbacks 187Writing your own callbacks 189IMonitoring and visualization with TensorBoard 1907.4 Writing your own training and evaluation loops 192Training versus inference 194ILow-level usage of metrics 195A complete training and evaluation loop 195IMake it fast with tf.function 197ILeveraging fit() with a custom training loop 1988 Introduction to deep learning for computer vision 2018.1 Introduction to convnets 202The convolution operation 204IThe max-pooling operation 2098.2 Training a convnet from scratch on a small dataset 211The relevance of deep learning for small-data problems 212Downloading the data 212IBuilding the model 215Data preprocessing 217IUsing data augmentation 221 CONTENTSxiii8.3 Leveraging a pretrained model 224Feature extraction with a pretrained model 225IFine-tuning a pretrained model 2349 Advanced deep learning for computer vision 2389.1 Three essential computer vision tasks 2389.2 An image segmentation example 2409.3 Modern convnet architecture patterns 248Modularity, hierarchy, and reuse 249IResidual connections 251Batch normalization 255IDepthwise separable convolutions 257Putting it together: A mini Xception-like model 2599.4 Interpreting what convnets learn 261Visualizing intermediate activations 262IVisualizing convnet filters 268IVisualizing heatmaps of class activation 27310 Deep learning for timeseries 28010.1 Different kinds of timeseries tasks 28010.2 A temperature-forecasting example 281Preparing the data 285IA common-sense, non-machine learning baseline 288ILet’s try a basic machine learning model 289Let’s try a 1D convolutional model 290IA first recurrent baseline 29210.3 Understanding recurrent neural networks 293A recurrent layer in Keras 29610.4 Advanced use of recurrent neural networks 300Using recurrent dropout to fight overfitting 300IStacking recurrent layers 303IUsing bidirectional RNNs 304Going even further 30711 Deep learning for text 30911.1 Natural language processing: The bird’s eye view 30911.2 Preparing text data 311Text standardization 312IText splitting (tokenization) 313Vocabulary indexing 314IUsing the TextVectorization layer 31611.3 Two approaches for representing groups of words: Sets and sequences 319Preparing the IMDB movie reviews data 320IProcessing words as a set: The bag-of-words approach 322IProcessing words as a sequence: The sequence model approach 327 CONTENTSxiv11.4 The Transformer architecture 336Understanding self-attention 337IMulti-head attention 341The Transformer encoder 342IWhen to use sequence models over bag-of-words models 34911.5 Beyond text classification: Sequence-to-sequence learning 350A machine translation example 351ISequence-to-sequence learning with RNNs 354ISequence-to-sequence learning with Transformer 35812 Generative deep learning 36412.1 Text generation 366A brief history of generative deep learning for sequence generation 366IHow do you generate sequence data? 367The importance of the sampling strategy 368IImplementing text generation with Keras 369IA text-generation callback with variable-temperature sampling 372IWrapping up 37612.2 DeepDream 376Implementing DeepDream in Keras 377IWrapping up 38312.3 Neural style transfer 383The content loss 384IThe style loss 384INeural style transfer in Keras 385IWrapping up 39112.4 Generating images with variational autoencoders 391Sampling from latent spaces of images 391IConcept vectors for image editing 393IVariational autoencoders 393Implementing a VAE with Keras 396IWrapping up 40112.5 Introduction to generative adversarial networks 401A schematic GAN implementation 402IA bag of tricks 403IGetting our hands on the CelebA dataset 404The discriminator 405IThe generator 407IThe adversarial network 408IWrapping up 41013 Best practices for the real world 41213.1 Getting the most out of your models 413Hyperparameter optimization 413IModel ensembling 42013.2 Scaling-up model training 421Speeding up training on GPU with mixed precision 422Multi-GPU training 425ITPU training 428 CONTENTSxv14 Conclusions 43114.1 Key concepts in review 432Various approaches to AI 432IWhat makes deep learning special within the field of machine learning 432IHow to think about deep learning 433IKey enabling technologies 434IThe universal machine learning workflow 435IKey network architectures 436IThe space of possibilities 44014.2 The limitations of deep learning 442The risk of anthropomorphizing machine learning models 443Automatons vs. intelligent agents 445ILocal generalization vs. extreme generalization 446IThe purpose of intelligence 448Climbing the spectrum of generalization 44914.3 Setting the course toward greater generality in AI 450On the importance of setting the right objective: The shortcut rule 450IA new target 45214.4 Implementing intelligence: The missing ingredients 454Intelligence as sensitivity to abstract analogies 454IThe two poles of abstraction 455IThe missing half of the picture 45814.5 The future of deep learning 459Models as programs 460IBlending together deep learning and program synthesis 461ILifelong learning and modular subroutine reuse 463IThe long-term vision 46514.6 Staying up to date in a fast-moving field 466Practice on real-world problems using Kaggle 466IRead about the latest developments on arXiv 466IExplore the Keras ecosystem 46714.7 Final words 467index 469 xvii prefaceIf you’ve picked up this book, you’re probably aware of the extraordinary progressthat deep learning has represented for the field of artificial intelligence in the recentpast. We went from near-unusable computer vision and natural language processingto highly performant systems deployed at scale in products you use every day. Theconsequences of this sudden progress extend to almost every industry. We’re alreadyapplying deep learning to an amazing range of important problems across domains asdifferent as medical imaging, agriculture, autonomous driving, education, disasterprevention, and manufacturing. Yet, I believe deep learning is still in its early days. It has only realized a small frac-tion of its potential so far. Over time, it will make its way to every problem where it canhelp—a transformation that will take place over multiple decades. In order to begin deploying deep learning technology to every problem that itcould solve, we need to make it accessible to as many people as possible, includingnon-experts—people who aren’t researchers or graduate students. For deep learning toreach its full potential, we need to radically democratize it. And today, I believe thatwe’re at the cusp of a historical transition, where deep learning is moving out of aca-demic labs and the R&D departments of large tech companies to become a ubiquitouspart of the toolbox of every developer out there—not unlike the trajectory of webdevelopment in the late 1990s. Almost anyone can now build a website or web app fortheir business or community of a kind that would have required a small team of special-ist engineers in 1998. In the not-so-distant future, anyone with an idea and basic codingskills will be able to build smart applications that learn from data. PREFACExviii When I released the first version of the Keras deep learning framework in March2015, the democratization of AI wasn’t what I had in mind. I had been doing researchin machine learning for several years and had built Keras to help me with my ownexperiments. But since 2015, hundreds of thousands of newcomers have entered thefield of deep learning; many of them picked up Keras as their tool of choice. As Iwatched scores of smart people use Keras in unexpected, powerful ways, I came tocare deeply about the accessibility and democratization of AI. I realized that the fur-ther we spread these technologies, the more useful and valuable they become. Accessi-bility quickly became an explicit goal in the development of Keras, and over a fewshort years, the Keras developer community has made fantastic achievements on thisfront. We’ve put deep learning into the hands of hundreds of thousands of people,who in turn are using it to solve problems that were until recently thought to beunsolvable. The book you’re holding is another step on the way to making deep learning avail-able to as many people as possible. Keras had always needed a companion course tosimultaneously cover the fundamentals of deep learning, deep learning best practices,and Keras usage patterns. In 2016 and 2017, I did my best to produce such a course,which became the first edition of this book, released in December 2017. It quicklybecame a machine learning best seller that sold over 50,000 copies and was translatedinto 12 languages. However, the field of deep learning advances fast. Since the release of the first edi-tion, many important developments have taken place—the release of TensorFlow 2,the growing popularity of the Transformer architecture, and more. And so, in late2019, I set out to update my book. I originally thought, quite naively, that it would fea-ture about 50% new content and would end up being roughly the same length as thefirst edition. In practice, after two years of work, it turned out to be over a third lon-ger, with about 75% novel content. More than a refresh, it is a whole new book. I wrote it with a focus on making the concepts behind deep learning, and theirimplementation, as approachable as possible. Doing so didn’t require me to dumbdown anything—I strongly believe that there are no difficult ideas in deep learning. Ihope you’ll find this book valuable and that it will enable you to begin building intelli-gent applications and solve the problems that matter to you. xixacknowledgmentsFirst of all, I’d like to thank the Keras community for making this book possible. Overthe past six years, Keras has grown to have hundreds of open source contributors andmore than one million users. Your contributions and feedback have turned Keras intowhat it is today. On a more personal note, I’d like to thank my wife for her endless support duringthe development of Keras and the writing of this book. I’d also like to thank Google for backing the Keras project. It has been fantastic tosee Keras adopted as TensorFlow’s high-level API. A smooth integration betweenKeras and TensorFlow greatly benefits both TensorFlow users and Keras users, andmakes deep learning accessible to most. I want to thank the people at Manning who made this book possible: publisherMarjan Bace and everyone on the editorial and production teams, including MichaelStephens, Jennifer Stout, Aleksandar Dragosavljevic ´, and many others who workedbehind the scenes. M a n y t h a n k s g o t o t h e t e c h n i c a l p e e r r e v i e w e r s : B i l l y O ’ C a l l a g h a n , C h r i s t i a nWeisstanner, Conrad Taylor, Daniela Zapata Riesco, David Jacobs, Edmon Begoli,Edmund Ronald PhD, Hao Liu, Jared Duncan, Kee Nam, Ken Fricklas, Kjell Jansson,Milan Šarenac, Nguyen Cao, Nikos Kanakaris, Oliver Korten, Raushan Jha, Sayak Paul,Sergio Govoni, Shashank Polasa, Todd Cook, and Viton Vitanis—and all the otherpeople who sent us feedback on the draft on the book. On the technical side, special thanks go to Frances Buontempo, who served as thebook’s technical editor, and Karsten Strøbæk, who served as the book’s technicalproofreader. xxabout this bookThis book was written for anyone who wishes to explore deep learning from scratch orbroaden their understanding of deep learning. Whether you’re a practicing machinelearning engineer, a software developer, or a college student, you’ll find value inthese pages. You’ll explore deep learning in an approachable way—starting simply, then work-ing up to state-of-the-art techniques. You’ll find that this book strikes a balance betweenintuition, theory, and hands-on practice. It avoids mathematical notation, preferringinstead to explain the core ideas of machine learning and deep learning via detailedcode snippets and intuitive mental models. You’ll learn from abundant code examplesthat include extensive commentary, practical recommendations, and simple high-levelexplanations of everything you need to know to start using deep learning to solve con-crete problems. The code examples use the Python deep learning framework Keras, with Tensor-Flow 2 as its numerical engine. They demonstrate modern Keras and TensorFlow 2best practices as of 2021. After reading this book, you’ll have a solid understand of what deep learning is,when it’s applicable, and what its limitations are. You’ll be familiar with the standardworkflow for approaching and solving machine learning problems, and you’ll knowhow to address commonly encountered issues. You’ll be able to use Keras to tacklereal-world problems ranging from computer vision to natural language processing:image classification, image segmentation, timeseries forecasting, text classification,machine translation, text generation, and more. ABOUT THIS BOOKxxiWho should read this bookThis book is written for people with Python programming experience who want to getstarted with machine learning and deep learning. But this book can also be valuableto many different types of readers:If you’re a data scientist familiar with machine learning, this book will provideyou with a solid, practical introduction to deep learning, the fastest-growingand most significant subfield of machine learning.If you’re a deep learning researcher or practitioner looking to get started withthe Keras framework, you’ll find this book to be the ideal Keras crash course.If you’re a graduate student studying deep learning in a formal setting, you’llfind this book to be a practical complement to your education, helping youbuild intuition around the behavior of deep neural networks and familiarizingyou with key best practices.Even technically minded people who don’t code regularly will find this book useful asan introduction to both basic and advanced deep learning concepts. In order to understand the code examples, you’ll need reasonable Python profi-ciency. Additionally, familiarity with the NumPy library will be helpful, although itisn’t required. You don’t need previous experience with machine learning or deeplearning: this book covers, from scratch, all the necessary basics. You don’t need anadvanced mathematics background, either—high school–level mathematics shouldsuffice in order to follow along.About the codeThis book contains many examples of source code both in numbered listings and inline with normal text. In both cases, source code is formatted in a fixed-width fontlike this to separate it from ordinary text. In many cases, the original source code has been reformatted; we’ve added linebreaks and reworked indentation to accommodate the available page space in thebook. Additionally, comments in the source code have often been removed from thelistings when the code is described in the text. Code annotations accompany many ofthe listings, highlighting important concepts. All code examples in this book are available from the Manning website at https:/ /www.manning.com/books/deep-learning-with-python-second-edition, and as Jupyternotebooks on GitHub at https:/ /github.com/fchollet/deep-learning-with-python-notebooks. They can be run directly in your browser via Google Colaboratory, ahosted Jupyter notebook environment that you can use for free. An internet connec-tion and a desktop web browser are all you need to get started with deep learning.liveBook discussion forumPurchase of Deep Learning with Python, second edition, includes free access to a privateweb forum run by Manning Publications where you can make comments about the ABOUT THIS BOOKxxiibook, ask technical questions, and receive help from the author and from other users.To access the forum, go to https:/ /livebook.manning.com/#!/book/deep-learning-with-python-second-edition/discussion. You can also learn more about Manning’sforums and the rules of conduct at https:/ /livebook.manning.com/#!/discussion. Manning’s commitment to our readers is to provide a venue where a meaningfuldialogue between individual readers and between readers and the author can takeplace. It is not a commitment to any specific amount of participation on the part ofthe author, whose contribution to the forum remains voluntary (and unpaid). We sug-gest you try asking the author some challenging questions lest his interest stray! Theforum and the archives of previous discussions will be accessible from the publisher’swebsite as long as the book is in print. xxiiiabout the authorFRANÇOIS CHOLLET i s t h e c r e a t o r o f K e r a s , o n e o f t h e m o s twidely used deep learning frameworks. He is currently a soft-ware engineer at Google, where he leads the Keras team. Inaddition, he does research on abstraction, reasoning, and howto achieve greater generality in artificial intelligence. xxivabout the cover illustrationThe figure on the cover of Deep Learning with Python, second edition, is captioned“Habit of a Persian Lady in 1568.” The illustration is taken from Thomas Jefferys’ ACollection of the Dresses of Different Nations, Ancient and Modern (four volumes), London,published between 1757 and 1772. The title page states that these are hand-coloredcopperplate engravings, heightened with gum arabic. Thomas Jefferys (1719–1771) was called “Geographer to King George III.” He wasan English cartographer who was the leading map supplier of his day. He engravedand printed maps for government and other official bodies and produced a widerange of commercial maps and atlases, especially of North America. His work as a mapmaker sparked an interest in local dress customs of the lands he surveyed andmapped, which are brilliantly displayed in this collection. Fascination with farawaylands and travel for pleasure were relatively new phenomena in the late eighteenthcentury, and collections such as this one were popular, introducing both the tourist aswell as the armchair traveler to the inhabitants of other countries. The diversity of the drawings in Jefferys’ volumes speaks vividly of the uniquenessand individuality of the world’s nations some 200 years ago. Dress codes have changedsince then, and the diversity by region and country, so rich at the time, has faded away.It’s now often hard to tell the inhabitants of one continent from another. Perhaps, try-ing to view it optimistically, we’ve traded a cultural and visual diversity for a more variedpersonal life—or a more varied and interesting intellectual and technical life. At a time when it’s difficult to tell one computer book from another, Manning cel-ebrates the inventiveness and initiative of the computer business with book coversbased on the rich diversity of regional life of two centuries ago, brought back to life byJefferys’ pictures. 1What is deep learning? In the past few years, artificial intelligence (AI) has been a subject of intense mediahype. Machine learning, deep learning, and AI come up in countless articles, oftenoutside of technology-minded publications. We’re promised a future of intelligentchatbots, self-driving cars, and virtual assistants—a future sometimes painted in agrim light and other times as utopian, where human jobs will be scarce and mosteconomic activity will be handled by robots or AI agents. For a future or currentpractitioner of machine learning, it’s important to be able to recognize the signalamid the noise, so that you can tell world-changing developments from overhypedpress releases. Our future is at stake, and it’s a future in which you have an activerole to play: after reading this book, you’ll be one of those who develop those AIsystems. So let’s tackle these questions: What has deep learning achieved so far?How significant is it? Where are we headed next? Should you believe the hype? This chapter provides essential context around artificial intelligence, machinelearning, and deep learning.This chapter coversHigh-level definitions of fundamental conceptsTimeline of the development of machine learningKey factors behind deep learning’s rising popularity and future potential 2CHAPTER 1What is deep learning?1.1 Artificial intelligence, machine learning, and deep learningFirst, we need to define clearly what we’re talking about when we mention AI. Whatare artificial intelligence, machine learning, and deep learning (see figure 1.1)? Howdo they relate to each other? 1.1.1 Artificial intelligenceArtificial intelligence was born in the 1950s, when a handful of pioneers from thenascent field of computer science started asking whether computers could be made to“think”—a question whose ramifications we’re still exploring today. W h i l e m a n y o f t h e u n d e r l y i n g i d e a s h a d b e e n b r e w i n g i n t h e y e a r s a n d e v e ndecades prior, “artificial intelligence” finally crystallized as a field of research in 1956,when John McCarthy, then a young Assistant Professor of Mathematics at DartmouthCollege, organized a summer workshop under the following proposal:The study is to proceed on the basis of the conjecture that every aspect of learning or anyother feature of intelligence can in principle be so precisely described that a machine canbe made to simulate it. An attempt will be made to find how to make machines uselanguage, form abstractions and concepts, solve kinds of problems now reserved forhumans, and improve themselves. We think that a significant advance can be made inone or more of these problems if a carefully selected group of scientists work on it togetherfor a summer.At the end of the summer, the workshop concluded without having fully solved theriddle it set out to investigate. Nevertheless, it was attended by many people whowould move on to become pioneers in the field, and it set in motion an intellectualrevolution that is still ongoing to this day. Concisely, AI can be described as the effort to automate intellectual tasks normally per-formed by humans. As such, AI is a general field that encompasses machine learning anddeep learning, but that also includes many more approaches that may not involve anylearning. Consider that until the 1980s, most AI textbooks didn’t mention “learning” atArtiﬁcialintelligenceMachinelearningDeeplearningFigure 1.1 Artificial intelligence, machine learning, and deep learning 3Artificial intelligence, machine learning, and deep learningall! Early chess programs, for instance, only involved hardcoded rules crafted by pro-grammers, and didn’t qualify as machine learning. In fact, for a fairly long time, mostexperts believed that human-level artificial intelligence could be achieved by havingprogrammers handcraft a sufficiently large set of explicit rules for manipulatingknowledge stored in explicit databases. This approach is known as symbolic AI. It wasthe dominant paradigm in AI from the 1950s to the late 1980s, and it reached its peakpopularity during the expert systems boom of the 1980s. A l t h o u g h s y m b o l i c A I p r o v e d s u i t a b l e t o s o l v e w e l l - d e f i n e d , l o g i c a l p r o b l e m s ,such as playing chess, it turned out to be intractable to figure out explicit rules forsolving more complex, fuzzy problems, such as image classification, speech recogni-tion, or natural language translation. A new approach arose to take symbolic AI’s place:machine learning. 1.1.2 Machine learningIn Victorian England, Lady Ada Lovelace was a friend and collaborator of CharlesBabbage, the inventor of the Analytical Engine: the first-known general-purposemechanical computer. Although visionary and far ahead of its time, the AnalyticalEngine wasn’t meant as a general-purpose computer when it was designed in the1830s and 1840s, because the concept of general-purpose computation was yet to beinvented. It was merely meant as a way to use mechanical operations to automate cer-tain computations from the field of mathematical analysis—hence the name Analyti-cal Engine. As such, it was the intellectual descendant of earlier attempts at encodingmathematical operations in gear form, such as the Pascaline, or Leibniz’s step reck-oner, a refined version of the Pascaline. Designed by Blaise Pascal in 1642 (at age 19!),the Pascaline was the world’s first mechanical calculator—it could add, subtract, mul-tiply, or even divide digits. In 1843, Ada Lovelace remarked on the invention of the Analytical Engine,The Analytical Engine has no pretensions whatever to originate anything. It can dowhatever we know how to order it to perform. . . . Its province is to assist us in makingavailable what we’re already acquainted with.Even with 178 years of historical perspective, Lady Lovelace’s observation remainsarresting. Could a general-purpose computer “originate” anything, or would it always bebound to dully execute processes we humans fully understand? Could it ever be capableof any original thought? Could it learn from experience? Could it show creativity? Her remark was later quoted by AI pioneer Alan Turing as “Lady Lovelace’s objec-tion” in his landmark 1950 paper “Computing Machinery and Intelligence,”1 whichintroduced the Turing test as well as key concepts that would come to shape AI.2 Turing1A.M. Turing, “Computing Machinery and Intelligence,” Mind 59, no. 236 (1950): 433–460.2Although the Turing test has sometimes been interpreted as a literal test—a goal the field of AI should set out toreach—Turing merely meant it as a conceptual device in a philosophical discussion about the nature of cognition. 4CHAPTER 1What is deep learning?was of the opinion—highly provocative at the time—that computers could in princi-ple be made to emulate all aspects of human intelligence. The usual way to make a computer do useful work is to have a human programmerwrite down rules—a computer program—to be followed to turn input data into appro-priate answers, just like Lady Lovelace writing down step-by-step instructions for theAnalytical Engine to perform. Machine learning turns this around: the machine looksat the input data and the corresponding answers, and figures out what the rulesshould be (see figure 1.2). A machine learning system is trained rather than explicitlyprogrammed. It’s presented with many examples relevant to a task, and it finds statisti-cal structure in these examples that eventually allows the system to come up with rulesfor automating the task. For instance, if you wished to automate the task of taggingyour vacation pictures, you could present a machine learning system with many exam-ples of pictures already tagged by humans, and the system would learn statistical rulesfor associating specific pictures to specific tags. Although machine learning only started to flourish in the 1990s, it has quicklybecome the most popular and most successful subfield of AI, a trend driven by theavailability of faster hardware and larger datasets. Machine learning is related to math-ematical statistics, but it differs from statistics in several important ways, in the samesense that medicine is related to chemistry but cannot be reduced to chemistry, asmedicine deals with its own distinct systems with their own distinct properties. Unlikestatistics, machine learning tends to deal with large, complex datasets (such as a data-set of millions of images, each consisting of tens of thousands of pixels) for which clas-sical statistical analysis such as Bayesian analysis would be impractical. As a result,machine learning, and especially deep learning, exhibits comparatively little mathe-matical theory—maybe too little—and is fundamentally an engineering discipline.Unlike theoretical physics or mathematics, machine learning is a very hands-on fielddriven by empirical findings and deeply reliant on advances in software and hardware. 1.1.3 Learning rules and representations from dataTo define deep learning and understand the difference between deep learning and othermachine learning approaches, first we need some idea of what machine learningalgorithms do. We just stated that machine learning discovers rules for executing aAnswersRulesDataClassicalprogrammingRulesDataAnswersMachinelearningFigure 1.2 Machine learning: a new programming paradigm 5Artificial intelligence, machine learning, and deep learningdata processing task, given examples of what’s expected. So, to do machine learning,we need three things:Input data points—For instance, if the task is speech recognition, these datapoints could be sound files of people speaking. If the task is image tagging, theycould be pictures.Examples of the expected output—In a speech-recognition task, these could behuman-generated transcripts of sound files. In an image task, expected outputscould be tags such as “dog,” “cat,” and so on.A way to measure whether the algorithm is doing a good job—This is necessary inorder to determine the distance between the algorithm’s current output and itsexpected output. The measurement is used as a feedback signal to adjust theway the algorithm works. This adjustment step is what we call learning.A machine learning model transforms its input data into meaningful outputs, a pro-cess that is “learned” from exposure to known examples of inputs and outputs. There-fore, the central problem in machine learning and deep learning is to meaningfullytransform data: in other words, to learn useful representations o f t h e i n p u t d a t a a thand—representations that get us closer to the expected output. Before we go any further: what’s a representation? At its core, it’s a different way tolook at data—to represent or encode data. For instance, a color image can be encodedin the RGB format (red-green-blue) or in the HSV format (hue-saturation-value):these are two different representations of the same data. Some tasks that may be diffi-cult with one representation can become easy with another. For example, the task“select all red pixels in the image” is simpler in the RGB format, whereas “make theimage less saturated” is simpler in the HSV format. Machine learning models are allabout finding appropriate representations for their input data—transformations ofthe data that make it more amenable to the task at hand. Let’s make this concrete. Consider an x-axis, a y-axis, andsome points represented by their coordinates in the (x, y) sys-tem, as shown in figure 1.3. As you can see, we have a few white points and a few blackpoints. Let’s say we want to develop an algorithm that can takethe coordinates (x, y) of a point and output whether thatpoint is likely to be black or to be white. In this case,The inputs are the coordinates of our points.The expected outputs are the colors of our points.A way to measure whether our algorithm is doing a goodjob could be, for instance, the percentage of points thatare being correctly classified.What we need here is a new representation of our data that cleanly separates the whitepoints from the black points. One transformation we could use, among many otherpossibilities, would be a coordinate change, illustrated in figure 1.4.y xFigure 1.3 Some sample data 6CHAPTER 1What is deep learning? In this new coordinate system, the coordinates of our points can be said to be a newrepresentation of our data. And it’s a good one! With this representation, theblack/white classification problem can be expressed as a simple rule: “Black pointsare such that x > 0,” or “White points are such that x < 0.” This new representation,combined with this simple rule, neatly solves the classification problem. In this case we defined the coordinate change by hand: we used our human intelli-gence to come up with our own appropriate representation of the data. This is fine forsuch an extremely simple problem, but could you do the same if the task were to clas-sify images of handwritten digits? Could you write down explicit, computer-executableimage transformations that would illuminate the difference between a 6 and an 8,between a 1 and a 7, across all kinds of different handwriting? T h i s i s p o s s i b l e t o a n e x t e n t . R u l e s b a s e d o n r e p r e s e n t a t i o n s o f d i g i t s s u c h a s“number of closed loops” or vertical and horizontal pixel histograms can do a decentjob of telling apart handwritten digits. But finding such useful representations byhand is hard work, and, as you can imagine, the resulting rule-based system is brittle—a nightmare to maintain. Every time you come across a new example of handwritingthat breaks your carefully thought-out rules, you will have to add new data transfor-mations and new rules, while taking into account their interaction with every previ-ous rule. You’re probably thinking, if this process is so painful, could we automate it? Whatif we tried systematically searching for different sets of automatically generated repre-sentations of the data and rules based on them, identifying good ones by using asfeedback the percentage of digits being correctly classified in some development data-set? We would then be doing machine learning. Learning, in the context of machinelearning, describes an automatic search process for data transformations that produceuseful representations of some data, guided by some feedback signal—representa-tions that are amenable to simpler rules solving the task at hand. These transformations can be coordinate changes (like in our 2D coordinatesclassification example), or taking a histogram of pixels and counting loops (like inour digits classification example), but they could also be linear projections, transla-tions, nonlinear operations (such as “select all points such that x > 0”), and so on.y2: Coordinate change xy1: Raw data xy3: Better representation xFigure 1.4 Coordinate change 7Artificial intelligence, machine learning, and deep learningMachine learning algorithms aren’t usually creative in finding these transformations;they’re merely searching through a predefined set of operations, called a hypothesisspace. For instance, the space of all possible coordinate changes would be ourhypothesis space in the 2D coordinates classification example. So that’s what machine learning is, concisely: searching for useful representationsand rules over some input data, within a predefined space of possibilities, using guid-ance from a feedback signal. This simple idea allows for solving a remarkably broadrange of intellectual tasks, from speech recognition to autonomous driving. Now that you understand what we mean by learning, let’s take a look at what makesdeep learning special. 1.1.4 The “deep” in “deep learning”Deep learning is a specific subfield of machine learning: a new take on learning rep-resentations from data that puts an emphasis on learning successive layers of increas-ingly meaningful representations. The “deep” in “deep learning” isn’t a reference toany kind of deeper understanding achieved by the approach; rather, it stands forthis idea of successive layers of representations. How many layers contribute to amodel of the data is called the depth of the model. Other appropriate names for thefield could have been layered representations learning or hierarchical representations learn-ing. Modern deep learning often involves tens or even hundreds of successive layersof representations, and they’re all learned automatically from exposure to trainingdata. Meanwhile, other approaches to machine learning tend to focus on learningonly one or two layers of representations of the data (say, taking a pixel histogramand then applying a classification rule); hence, they’re sometimes called shallowlearning. In deep learning, these layered representations are learned via models called neu-ral networks, structured in literal layers stacked on top of each other. The term “neuralnetwork” refers to neurobiology, but although some of the central concepts in deeplearning were developed in part by drawing inspiration from our understanding ofthe brain (in particular, the visual cortex), deep learning models are not models ofthe brain. There’s no evidence that the brain implements anything like the learningmechanisms used in modern deep learning models. You may come across pop-sciencearticles proclaiming that deep learning works like the brain or was modeled after thebrain, but that isn’t the case. It would be confusing and counterproductive for new-comers to the field to think of deep learning as being in any way related to neurobiol-ogy; you don’t need that shroud of “just like our minds” mystique and mystery, andyou may as well forget anything you may have read about hypothetical links betweendeep learning and biology. For our purposes, deep learning is a mathematical frame-work for learning representations from data. What do the representations learned by a deep learning algorithm look like? Let’sexamine how a network several layers deep (see figure 1.5) transforms an image of adigit in order to recognize what digit it is. 8CHAPTER 1What is deep learning? As you can see in figure 1.6, the network transforms the digit image into representa-tions that are increasingly different from the original image and increasingly informa-tive about the final result. You can think of a deep network as a multistage information-distillation process, where information goes through successive filters and comes outincreasingly purified (that is, useful with regard to some task). So that’s what deep learning is, technically: a multistage way to learn data representa-tions. It’s a simple idea—but, as it turns out, very simple mechanisms, sufficientlyscaled, can end up looking like magic. 1.1.5 Understanding how deep learning works, in three figuresAt this point, you know that machine learning is about mapping inputs (such asimages) to targets (such as the label “cat”), which is done by observing many examplesof input and targets. You also know that deep neural networks do this input-to-targetmapping via a deep sequence of simple data transformations (layers) and that these Layer 1OriginalinputFinaloutputLayer 2 Layer 3 Layer 40123456789Figure 1.5 A deep neural network for digit classification Layer 1representationsOriginalinputLayer 2representationsLayer 3representationsLayer 4representations(ﬁnal output) Layer 1 Layer 2 Layer 3 Layer 40123456789 Figure 1.6 Data representations learned by a digit-classification model 9Artificial intelligence, machine learning, and deep learningdata transformations are learned by exposure to examples. Now let’s look at how thislearning happens, concretely. T h e s p e c i f i c a t i o n o f w h a t a l a y e r d o e s t o i t s i n p u t d a t a i s s t o r e d i n t h e l a y e r ’ sweights, which in essence are a bunch of numbers. In technical terms, we’d say that thetransformation implemented by a layer is parameterized by its weights (see figure 1.7).(Weights are also sometimes called the parameters of a layer.) In this context, learningmeans finding a set of values for the weights of all layers in a network, such that thenetwork will correctly map example inputs to their associated targets. But here’s thething: a deep neural network can contain tens of millions of parameters. Finding thecorrect values for all of them may seem like a daunting task, especially given that mod-ifying the value of one parameter will affect the behavior of all the others! To control something, first you need to be able to observe it. To control the output ofa neural network, you need to be able to measure how far this output is from what youexpected. This is the job of the loss function of the network, also sometimes called theobjective function or cost function. The loss function takes the predictions of the networkand the true target (what you wanted the network to output) and computes a distancescore, capturing how well the network has done on this specific example (see figure 1.8).Goal: ﬁnding theright values forthese weightsLayer(data transformation)Input XWeightsLayer(data transformation)PredictionsY'WeightsFigure 1.7 A neural network is parameterized by its weights. Layer(data transformation)Input XWeightsLayer(data transformation)PredictionsY'True targetsYWeightsLoss functionLoss scoreFigure 1.8 A loss function measures the quality of the network’s output. 10CHAPTER 1What is deep learning?The fundamental trick in deep learning is to use this score as a feedback signal toadjust the value of the weights a little, in a direction that will lower the loss score forthe current example (see figure 1.9). This adjustment is the job of the optimizer, whichimplements what’s called the Backpropagation algorithm: the central algorithm in deeplearning. The next chapter explains in more detail how backpropagation works. Initially, the weights of the network are assigned random values, so the networkmerely implements a series of random transformations. Naturally, its output is farfrom what it should ideally be, and the loss score is accordingly very high. But withevery example the network processes, the weights are adjusted a little in the correctdirection, and the loss score decreases. This is the training loop, which, repeated a suffi-cient number of times (typically tens of iterations over thousands of examples), yieldsweight values that minimize the loss function. A network with a minimal loss is one forwhich the outputs are as close as they can be to the targets: a trained network. Onceagain, it’s a simple mechanism that, once scaled, ends up looking like magic. 1.1.6 What deep learning has achieved so farAlthough deep learning is a fairly old subfield of machine learning, it only rose toprominence in the early 2010s. In the few years since, it has achieved nothing short ofa revolution in the field, producing remarkable results on perceptual tasks and evennatural language processing tasks—problems involving skills that seem natural andintuitive to humans but have long been elusive for machines. In particular, deep learning has enabled the following breakthroughs, all in histor-ically difficult areas of machine learning:Near-human-level image classificationNear-human-level speech transcriptionNear-human-level handwriting transcriptionLayer(data transformation)Input XWeightsLayer(data transformation)PredictionsY'WeightupdateTrue targetsYWeightsLoss function OptimizerLoss scoreFigure 1.9 The loss score is used as a feedback signal to adjust the weights. 11Artificial intelligence, machine learning, and deep learningDramatically improved machine translationDramatically improved text-to-speech conversionDigital assistants such as Google Assistant and Amazon AlexaNear-human-level autonomous drivingImproved ad targeting, as used by Google, Baidu, or BingImproved search results on the webAbility to answer natural language questionsSuperhuman Go playingWe’re still exploring the full extent of what deep learning can do. We’ve started apply-ing it with great success to a wide variety of problems that were thought to be impossi-ble to solve just a few years ago—automatically transcribing the tens of thousands ofancient manuscripts held in the Vatican’s Apostolic Archive, detecting and classifyingplant diseases in fields using a simple smartphone, assisting oncologists or radiologistswith interpreting medical imaging data, predicting natural disasters such as floods,hurricanes, or even earthquakes, and so on. With every milestone, we’re getting closerto an age where deep learning assists us in every activity and every field of humanendeavor—science, medicine, manufacturing, energy, transportation, software devel-opment, agriculture, and even artistic creation. 1.1.7 Don’t believe the short-term hypeAlthough deep learning has led to remarkable achievements in recent years, expecta-tions for what the field will be able to achieve in the next decade tend to run muchhigher than what will likely be possible. Although some world-changing applicationslike autonomous cars are already within reach, many more are likely to remain elusivefor a long time, such as believable dialogue systems, human-level machine translationacross arbitrary languages, and human-level natural language understanding. In par-ticular, talk of human-level general intelligence shouldn’t be taken too seriously. Therisk with high expectations for the short term is that, as technology fails to deliver,research investment will dry up, slowing progress for a long time. This has happened before. Twice in the past, AI went through a cycle of intenseoptimism followed by disappointment and skepticism, with a dearth of funding as aresult. It started with symbolic AI in the 1960s. In those early days, projections about AIwere flying high. One of the best-known pioneers and proponents of the symbolicAI approach was Marvin Minsky, who claimed in 1967, “Within a generation . . . theproblem of creating ‘artificial intelligence’ will substantially be solved.” Three yearslater, in 1970, he made a more precisely quantified prediction: “In from three to eightyears we will have a machine with the general intelligence of an average human being.”In 2021 such an achievement still appears to be far in the future—so far that we have noway to predict how long it will take—but in the 1960s and early 1970s, several expertsbelieved it to be right around the corner (as do many people today). A few years later, asthese high expectations failed to materialize, researchers and government funds turned 12CHAPTER 1What is deep learning?away from the field, marking the start of the first AI winter (a reference to a nuclear win-ter, because this was shortly after the height of the Cold War). It wouldn’t be the last one. In the 1980s, a new take on symbolic AI, expert systems,started gathering steam among large companies. A few initial success stories triggereda wave of investment, with corporations around the world starting their own in-houseAI departments to develop expert systems. Around 1985, companies were spendingover $1 billion each year on the technology; but by the early 1990s, these systems hadproven expensive to maintain, difficult to scale, and limited in scope, and interestdied down. Thus began the second AI winter. We may be currently witnessing the third cycle of AI hype and disappointment,and we’re still in the phase of intense optimism. It’s best to moderate our expectationsfor the short term and make sure people less familiar with the technical side of thefield have a clear idea of what deep learning can and can’t deliver. 1.1.8 The promise of AIAlthough we may have unrealistic short-term expectations for AI, the long-term pic-ture is looking bright. We’re only getting started in applying deep learning to manyimportant problems for which it could prove transformative, from medical diagnosesto digital assistants. AI research has been moving forward amazingly quickly in thepast ten years, in large part due to a level of funding never before seen in the shorthistory of AI, but so far relatively little of this progress has made its way into the prod-ucts and processes that form our world. Most of the research findings of deep learningaren’t yet applied, or at least are not applied to the full range of problems they couldsolve across all industries. Your doctor doesn’t yet use AI, and neither does youraccountant. You probably don’t use AI technologies very often in your day-to-day life.Of course, you can ask your smartphone simple questions and get reasonable answers,you can get fairly useful product recommendations on Amazon.com, and you cansearch for “birthday” on Google Photos and instantly find those pictures of yourdaughter’s birthday party from last month. That’s a far cry from where such technolo-gies used to stand. But such tools are still only accessories to our daily lives. AI has yetto transition to being central to the way we work, think, and live. Right now, it may seem hard to believe that AI could have a large impact on ourworld, because it isn’t yet widely deployed—much as, back in 1995, it would have beendifficult to believe in the future impact of the internet. Back then, most people didn’tsee how the internet was relevant to them and how it was going to change their lives.The same is true for deep learning and AI today. But make no mistake: AI is coming.In a not-so-distant future, AI will be your assistant, even your friend; it will answer yourquestions, help educate your kids, and watch over your health. It will deliver your gro-ceries to your door and drive you from point A to point B. It will be your interface toan increasingly complex and information-intensive world. And, even more important,AI will help humanity as a whole move forward, by assisting human scientists in newbreakthrough discoveries across all scientific fields, from genomics to mathematics. 13Before deep learning: A brief history of machine learning On the way, we may face a few setbacks and maybe even a new AI winter—in muchthe same way the internet industry was overhyped in 1998–99 and suffered from acrash that dried up investment throughout the early 2000s. But we’ll get there eventu-ally. AI will end up being applied to nearly every process that makes up our society andour daily lives, much like the internet is today. Don’t believe the short-term hype, but do believe in the long-term vision. It maytake a while for AI to be deployed to its true potential—a potential the full extent ofwhich no one has yet dared to dream—but AI is coming, and it will transform ourworld in a fantastic way. 1.2 Before deep learning: A brief history of machine learningDeep learning has reached a level of public attention and industry investment neverbefore seen in the history of AI, but it isn’t the first successful form of machine learn-ing. It’s safe to say that most of the machine learning algorithms used in the industrytoday aren’t deep learning algorithms. Deep learning isn’t always the right tool for thejob—sometimes there isn’t enough data for deep learning to be applicable, and some-times the problem is better solved by a different algorithm. If deep learning is yourfirst contact with machine learning, you may find yourself in a situation where all youhave is the deep learning hammer, and every machine learning problem starts to looklike a nail. The only way not to fall into this trap is to be familiar with other approachesand practice them when appropriate. A detailed discussion of classical machine learning approaches is outside of thescope of this book, but I’ll briefly go over them and describe the historical context inwhich they were developed. This will allow us to place deep learning in the broadercontext of machine learning and better understand where deep learning comes fromand why it matters.1.2.1 Probabilistic modelingProbabilistic modeling is the application of the principles of statistics to data analysis. It isone of the earliest forms of machine learning, and it’s still widely used to this day. Oneof the best-known algorithms in this category is the Naive Bayes algorithm. Naive Bayes is a type of machine learning classifier based on applying Bayes’ theo-rem while assuming that the features in the input data are all independent (a strong,or “naive” assumption, which is where the name comes from). This form of data analy-sis predates computers and was applied by hand decades before its first computerimplementation (most likely dating back to the 1950s). Bayes’ theorem and the foun-dations of statistics date back to the eighteenth century, and these are all you need tostart using Naive Bayes classifiers. A closely related model is logistic regression (logreg for short), which is sometimesconsidered to be the “Hello World” of modern machine learning. Don’t be misled byits name—logreg is a classification algorithm rather than a regression algorithm. Much 14CHAPTER 1What is deep learning?like Naive Bayes, logreg predates computing by a long time, yet it’s still useful to thisday, thanks to its simple and versatile nature. It’s often the first thing a data scientistwill try on a dataset to get a feel for the classification task at hand. 1.2.2 Early neural networksEarly iterations of neural networks have been completely supplanted by the modernvariants covered in these pages, but it’s helpful to be aware of how deep learning orig-inated. Although the core ideas of neural networks were investigated in toy forms asearly as the 1950s, the approach took decades to get started. For a long time, the miss-ing piece was an efficient way to train large neural networks. This changed in the mid-1980s, when multiple people independently rediscovered the Backpropagation algo-rithm—a way to train chains of parametric operations using gradient-descent optimi-zation (we’ll precisely define these concepts later in the book)—and started applyingit to neural networks. The f irs t s uc ce s s fu l p ra ct ic a l a pp li ca t io n of ne ur a l ne ts c a me in 1 989 fr om B el lLabs, when Yann LeCun combined the earlier ideas of convolutional neural networksand backpropagation, and applied them to the problem of classifying handwrittendigits. The resulting network, dubbed LeNet, was used by the United States Postal Ser-vice in the 1990s to automate the reading of ZIP codes on mail envelopes. 1.2.3 Kernel methodsAs neural networks started to gain some respect among researchers in the 1990s, thanksto this first success, a new approach to machine learning rose to fame and quickly sentneural nets back to oblivion: kernel methods. Kernel methods are a group of classificationalgorithms, the best known of which is the Support Vector Machine (SVM). The modernformulation of an SVM was developed by Vladimir Vapnik and Corinna Cortes in theearly 1990s at Bell Labs and published in 1995,3 although an older linear formulation waspublished by Vapnik and Alexey Chervonenkis as early as 1963.4 SVM is a classification algorithm that works by finding “deci-sion boundaries” separating two classes (see figure 1.10). SVMsproceed to find these boundaries in two steps:1The data is mapped to a new high-dimensional represen-tation where the decision boundary can be expressed asa hyperplane (if the data was two-dimensional, as in fig-ure 1.10, a hyperplane would be a straight line).2A good decision boundary (a separation hyperplane) iscomputed by trying to maximize the distance between thehyperplane and the closest data points from each class, a3Vladimir Vapnik and Corinna Cortes, “Support-Vector Networks,” Machine Learning 20, no. 3 (1995): 273–297.4Vladimir Vapnik and Alexey Chervonenkis, “A Note on One Class of Perceptrons,” Automation and Remote Con-trol 2 5 ( 1 9 6 4 ) .Figure 1.10A decision boundary 15Before deep learning: A brief history of machine learningstep called maximizing the margin. This allows the boundary to generalize well tonew samples outside of the training dataset.The technique of mapping data to a high-dimensional representation where a classifi-cation problem becomes simpler may look good on paper, but in practice it’s oftencomputationally intractable. That’s where the kernel trick comes in (the key idea thatkernel methods are named after). Here’s the gist of it: to find good decision hyper-planes in the new representation space, you don’t have to explicitly compute the coor-dinates of your points in the new space; you just need to compute the distancebetween pairs of points in that space, which can be done efficiently using a kernelfunction. A kernel function is a computationally tractable operation that maps any twopoints in your initial space to the distance between these points in your target repre-sentation space, completely bypassing the explicit computation of the new representa-tion. Kernel functions are typically crafted by hand rather than learned from data—inthe case of an SVM, only the separation hyperplane is learned. At the time they were developed, SVMs exhibited state-of-the-art performance onsimple classification problems and were one of the few machine learning methodsbacked by extensive theory and amenable to serious mathematical analysis, makingthem well understood and easily interpretable. Because of these useful properties,SVMs became extremely popular in the field for a long time. But SVMs proved hard to scale to large datasets and didn’t provide good results forperceptual problems such as image classification. Because an SVM is a shallowmethod, applying an SVM to perceptual problems requires first extracting useful rep-resentations manually (a step called feature engineering), which is difficult and brittle.For instance, if you want to use an SVM to classify handwritten digits, you can’t startfrom the raw pixels; you should first find by hand useful representations that makethe problem more tractable, like the pixel histograms I mentioned earlier. 1.2.4 Decision trees, random forests, and gradient boosting machinesDecision trees are flowchart-like structures that let you classify input data points or pre-dict output values given inputs (see figure 1.11). They’re easy to visualize and inter-pret. Decision trees learned from data began to receive significant research interest inthe 2000s, and by 2010 they were often preferred to kernel methods. QuestionCategory CategoryQuestionInput dataQuestionCategory CategoryFigure 1.11 A decision tree: the parameters that are learned are the questions about the data. A question could be, for instance, “Is coefficient 2 in the data greater than 3.5?” 16CHAPTER 1What is deep learning?In particular, the Random Forest a l g o r i t h m i n t r o d u c e d a r o b u s t , p r a c t i c a l t a k e o ndecision-tree learning that involves building a large number of specialized decision treesand then ensembling their outputs. Random forests are applicable to a wide range ofproblems—you could say that they’re almost always the second-best algorithm for anyshallow machine learning task. When the popular machine learning competition web-site Kaggle (http:/ /kaggle.com) got started in 2010, random forests quickly became afavorite on the platform—until 2014, when gradient boosting machines took over. A gra-dient boosting machine, much like a random forest, is a machine learning techniquebased on ensembling weak prediction models, generally decision trees. It uses gradientboosting, a way to improve any machine learning model by iteratively training newmodels that specialize in addressing the weak points of the previous models. Appliedto decision trees, the use of the gradient boosting technique results in models thatstrictly outperform random forests most of the time, while having similar properties.It may be one of the best, if not the best, algorithm for dealing with nonperceptualdata today. Alongside deep learning, it’s one of the most commonly used techniquesin Kaggle competitions. 1.2.5 Back to neural networksAround 2010, although neural networks were almost completely shunned by the sci-entific community at large, a number of people still working on neural networksstarted to make important breakthroughs: the groups of Geoffrey Hinton at the Uni-versity of Toronto, Yoshua Bengio at the University of Montreal, Yann LeCun at NewYork University, and IDSIA in Switzerland. In 2011, Dan Ciresan from IDSIA began to win academic image-classification com-petitions with GPU-trained deep neural networks—the first practical success of mod-ern deep learning. But the watershed moment came in 2012, with the entry ofHinton’s group in the yearly large-scale image-classification challenge ImageNet(ImageNet Large Scale Visual Recognition Challenge, or ILSVRC for short). TheImageNet challenge was notoriously difficult at the time, consisting of classifying high-resolution color images into 1,000 different categories after training on 1.4 millionimages. In 2011, the top-five accuracy of the winning model, based on classicalapproaches to computer vision, was only 74.3%.5 Then, in 2012, a team led by AlexKrizhevsky and advised by Geoffrey Hinton was able to achieve a top-five accuracy of83.6%—a significant breakthrough. The competition has been dominated by deepconvolutional neural networks every year since. By 2015, the winner reached an accu-racy of 96.4%, and the classification task on ImageNet was considered to be a com-pletely solved problem. Since 2012, deep convolutional neural networks (convnets) have become the go-toalgorithm for all computer vision tasks; more generally, they work on all perceptual5“Top-five accuracy” measures how often the model selects the correct answer as part of its top five guesses (outof 1,000 possible answers, in the case of ImageNet). 17Before deep learning: A brief history of machine learningtasks. At any major computer vision conference after 2015, it was nearly impossible tofind presentations that didn’t involve convnets in some form. At the same time, deeplearning has also found applications in many other types of problems, such as naturallanguage processing. It has completely replaced SVMs and decision trees in a widerange of applications. For instance, for several years, the European Organization forNuclear Research, CERN, used decision tree–based methods for analyzing particledata from the ATLAS detector at the Large Hadron Collider (LHC), but CERN even-tually switched to Keras-based deep neural networks due to their higher performanceand ease of training on large datasets. 1.2.6 What makes deep learning differentThe primary reason deep learning took off so quickly is that it offered better perfor-mance for many problems. But that’s not the only reason. Deep learning also makesproblem-solving much easier, because it completely automates what used to be themost crucial step in a machine learning workflow: feature engineering. Previous machine learning techniques—shallow learning—only involved transform-ing the input data into one or two successive representation spaces, usually via simpletransformations such as high-dimensional non-linear projections (SVMs) or decisiontrees. But the refined representations required by complex problems generally can’tbe attained by such techniques. As such, humans had to go to great lengths to makethe initial input data more amenable to processing by these methods: they had tomanually engineer good layers of representations for their data. This is called featureengineering. Deep learning, on the other hand, completely automates this step: withdeep learning, you learn all features in one pass rather than having to engineer themyourself. This has greatly simplified machine learning workflows, often replacing sophis-ticated multistage pipelines with a single, simple, end-to-end deep learning model. You may ask, if the crux of the issue is to have multiple successive layers of repre-sentations, could shallow methods be applied repeatedly to emulate the effects ofdeep learning? In practice, successive applications of shallow-learning methods pro-duce fast-diminishing returns, because the optimal first representation layer in athree-layer model isn’t the optimal first layer in a one-layer or two-layer model. What istransformative about deep learning is that it allows a model to learn all layers of repre-sentation jointly, at the same time, rather than in succession (greedily, as it’s called).With joint feature learning, whenever the model adjusts one of its internal features, allother features that depend on it automatically adapt to the change, without requiringhuman intervention. Everything is supervised by a single feedback signal: everychange in the model serves the end goal. This is much more powerful than greedilystacking shallow models, because it allows for complex, abstract representations to belearned by breaking them down into long series of intermediate spaces (layers); eachspace is only a simple transformation away from the previous one. These are the two essential characteristics of how deep learning learns from data:the incremental, layer-by-layer way in which increasingly complex representations are developed, 18CHAPTER 1What is deep learning?and the fact that these intermediate incremental representations are learned jointly, each layerbeing updated to follow both the representational needs of the layer above and theneeds of the layer below. Together, these two properties have made deep learningvastly more successful than previous approaches to machine learning. 1.2.7 The modern machine learning landscapeA great way to get a sense of the current landscape of machine learning algorithmsand tools is to look at machine learning competitions on Kaggle. Due to its highlycompetitive environment (some contests have thousands of entrants and million-dollar prizes) and to the wide variety of machine learning problems covered, Kaggleoffers a realistic way to assess what works and what doesn’t. So what kind of algorithmis reliably winning competitions? What tools do top entrants use? In early 2019, Kaggle ran a survey asking teams that ended in the top five of anycompetition since 2017 which primary software tool they had used in the competition(see figure 1.12). It turns out that top teams tend to use either deep learning methods(most often via the Keras library) or gradient boosted trees (most often via theLightGBM or XGBoost libraries). 0 10 20Number of competitionsCaﬀeFastaiScikit-learnTensorFlowPyTorchXGBoostLightGBMKeras Deep Classic30 40Figure 1.12 Machine learning tools used by top teams on Kaggle 19Before deep learning: A brief history of machine learningIt’s not just competition champions, either. Kaggle also runs a yearly survey amongmachine learning and data science professionals worldwide. With tens of thousands ofrespondents, this survey is one of our most reliable sources about the state of theindustry. Figure 1.13 shows the percentage of usage of different machine learningsoftware frameworks. From 2016 to 2020, the entire machine learning and data science industry has beendominated by these two approaches: deep learning and gradient boosted trees. Specif-ically, gradient boosted trees is used for problems where structured data is available,whereas deep learning is used for perceptual problems such as image classification. Users of gradient boosted trees tend to use Scikit-learn, XGBoost, or LightGBM.Meanwhile, most practitioners of deep learning use Keras, often in combination withJAXNoneOtherMXNetH2O3TidymodelsFast.aiProphetCatboostCaretLightGBMPyTorchXgboostKerasTensorFlowScikit-learn 0.7%3.2%3.7%2.1%6%7.2%7.5%10%13.7%14.1%26.1%30.9%48.4%50.5%50.5%82.8% 0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%Figure 1.13 Tool usage across the machine learning and data science industry (Source: www.kaggle.com/kaggle-survey-2020) 20CHAPTER 1What is deep learning?its parent framework TensorFlow. The common point of these tools is they’re allPython libraries: Python is by far the most widely used language for machine learningand data science. These are the two techniques you should be the most familiar with in order to besuccessful in applied machine learning today: gradient boosted trees, for shallow-learning problems; and deep learning, for perceptual problems. In technical terms,this means you’ll need to be familiar with Scikit-learn, XGBoost, and Keras—the threelibraries that currently dominate Kaggle competitions. With this book in hand, you’realready one big step closer. 1.3 Why deep learning? Why now?The two key ideas of deep learning for computer vision—convolutional neural networksand backpropagation—were already well understood by 1990. The Long Short-TermMemory (LSTM) algorithm, which is fundamental to deep learning for timeseries,was developed in 1997 and has barely changed since. So why did deep learning onlytake off after 2012? What changed in these two decades? In general, three technical forces are driving advances in machine learning:HardwareDatasets and benchmarksAlgorithmic advancesBecause the field is guided by experimental findings rather than by theory, algorith-mic advances only become possible when appropriate data and hardware are availableto try new ideas (or to scale up old ideas, as is often the case). Machine learning isn’tmathematics or physics, where major advances can be done with a pen and a piece ofpaper. It’s an engineering science. The real bottlenecks throughout the 1990s and 2000s were data and hardware. Buthere’s what happened during that time: the internet took off and high-performancegraphics chips were developed for the needs of the gaming market.1.3.1 HardwareBetween 1990 and 2010, off-the-shelf CPUs became faster by a factor of approximately5,000. As a result, nowadays it’s possible to run small deep learning models on yourlaptop, whereas this would have been intractable 25 years ago. But typical deep learning models used in computer vision or speech recognitionrequire orders of magnitude more computational power than your laptop can deliver.Throughout the 2000s, companies like NVIDIA and AMD invested billions of dollarsin developing fast, massively parallel chips (graphical processing units, or GPUs) topower the graphics of increasingly photorealistic video games—cheap, single-purposesupercomputers designed to render complex 3D scenes on your screen in real time.This investment came to benefit the scientific community when, in 2007, NVIDIAlaunched CUDA (https:/ /developer.nvidia.com/about-cuda), a programming interface 21Why deep learning? Why now?for its line of GPUs. A small number of GPUs started replacing massive clusters ofCPUs in various highly parallelizable applications, beginning with physics modeling.Deep neural networks, consisting mostly of many small matrix multiplications, arealso highly parallelizable, and around 2011 some researchers began to write CUDAimplementations of neural nets—Dan Ciresan6 a n d A l e x K r i z h e v s k y7 w e r e a m o n gthe first. What happened is that the gaming market subsidized supercomputing for the nextgeneration of artificial intelligence applications. Sometimes, big things begin asgames. Today, the NVIDIA Titan RTX, a GPU that cost $2,500 at the end of 2019, candeliver a peak of 16 teraFLOPS in single precision (16 trillion float32 operations persecond). That’s about 500 times more computing power than the world’s fastest super-computer from 1990, the Intel Touchstone Delta. On a Titan RTX, it takes only a fewhours to train an ImageNet model of the sort that would have won the ILSVRC com-petition around 2012 or 2013. Meanwhile, large companies train deep learning mod-els on clusters of hundreds of GPUs. What’s more, the deep learning industry has been moving beyond GPUs and isinvesting in increasingly specialized, efficient chips for deep learning. In 2016, at itsannual I/O convention, Google revealed its Tensor Processing Unit (TPU) project: anew chip design developed from the ground up to run deep neural networks signifi-cantly faster and far more energy efficient than top-of-the-line GPUs. Today, in 2020,the third iteration of the TPU card represents 420 teraFLOPS of computing power.That’s 10,000 times more than the Intel Touchstone Delta from 1990. T h e s e T P U c a r d s a r e d e s i g n e d t o b e a s s e m b l e d i n t o l a r g e - s c a l e c o n f i g u r a t i o n s ,called “pods.” One pod (1024 TPU cards) peaks at 100 petaFLOPS. For scale, that’sabout 10% of the peak computing power of the current largest supercomputer, theIBM Summit at Oak Ridge National Lab, which consists of 27,000 NVIDIA GPUs andpeaks at around 1.1 exaFLOPS. 1.3.2 DataAI is sometimes heralded as the new industrial revolution. If deep learning is the steamengine of this revolution, then data is its coal: the raw material that powers our intelli-gent machines, without which nothing would be possible. When it comes to data, inaddition to the exponential progress in storage hardware over the past 20 years (follow-ing Moore’s law), the game changer has been the rise of the internet, making it feasibleto collect and distribute very large datasets for machine learning. Today, large companieswork with image datasets, video datasets, and natural language datasets that couldn’thave been collected without the internet. User-generated image tags on Flickr, for6See “Flexible, High Performance Convolutional Neural Networks for Image Classification,” Proceedings of the22nd International Joint Conference on Artificial Intelligence ( 2 0 1 1 ) , www.ijcai.org/Proceedings/11/Papers/210.pdf.7See “ImageNet Classification with Deep Convolutional Neural Networks,” Advances in Neural Information Pro-cessing Systems 25 (2012), http:/ /mng.bz/2286. 22CHAPTER 1What is deep learning?instance, have been a treasure trove of data for computer vision. So are YouTube videos.And Wikipedia is a key dataset for natural language processing. If there’s one dataset that has been a catalyst for the rise of deep learning, it’s theImageNet dataset, consisting of 1.4 million images that have been hand annotatedwith 1,000 image categories (one category per image). But what makes ImageNet spe-cial isn’t just its large size, but also the yearly competition associated with it.8 As Kaggle has been demonstrating since 2010, public competitions are an excel-lent way to motivate researchers and engineers to push the envelope. Having commonbenchmarks that researchers compete to beat has greatly helped the rise of deeplearning, by highlighting its success against classical machine learning approaches. 1.3.3 AlgorithmsIn addition to hardware and data, until the late 2000s, we were missing a reliable wayto train very deep neural networks. As a result, neural networks were still fairly shal-low, using only one or two layers of representations; thus, they weren’t able to shineagainst more-refined shallow methods such as SVMs and random forests. The keyissue was that of gradient propagation through deep stacks of layers. The feedback signalused to train neural networks would fade away as the number of layers increased. This changed around 2009–2010 with the advent of several simple but importantalgorithmic improvements that allowed for better gradient propagation:Better activation functions for neural layersBetter weight-initialization schemes, starting with layer-wise pretraining, which wasthen quickly abandonedBetter optimization schemes, such as RMSProp and AdamOnly when these improvements began to allow for training models with 10 or morelayers did deep learning start to shine. Finally, in 2014, 2015, and 2016, even more advanced ways to improve gradientpropagation were discovered, such as batch normalization, residual connections, anddepthwise separable convolutions. T o d a y , w e c a n t r a i n m o d e l s t h a t a r e a r b i t r a r i l y d e e p f r o m s c r a t c h . T h i s h a sunlocked the use of extremely large models, which hold considerable representa-tional power—that is to say, which encode very rich hypothesis spaces. This extremescalability is one of the defining characteristics of modern deep learning. Large-scalemodel architectures, which feature tens of layers and tens of millions of parameters,have brought about critical advances both in computer vision (for instance, architec-tures such as ResNet, Inception, or Xception) and natural language processing (forinstance, large Transformer-based architectures such as BERT, GPT-3, or XLNet). 8The ImageNet Large Scale Visual Recognition Challenge (ILSVRC), www.image-net.org/challenges/LSVRC. 23Why deep learning? Why now?1.3.4 A new wave of investmentAs deep learning became the new state of the art for computer vision in 2012–2013,and eventually for all perceptual tasks, industry leaders took note. What followed wasa gradual wave of industry investment far beyond anything previously seen in the his-tory of AI (see figure 1.14). In 2011, right before deep learning took the spotlight, the total venture capital invest-ment in AI worldwide was less than a billion dollars, which went almost entirely topractical applications of shallow machine learning approaches. In 2015, it had risento over $5 billion, and in 2017, to a staggering $16 billion. Hundreds of startupslaunched in these few years, trying to capitalize on the deep learning hype. Mean-while, large tech companies such as Google, Amazon, and Microsoft have invested ininternal research departments in amounts that would most likely dwarf the flow ofventure-capital money. Machine learning—in particular, deep learning—has become central to the prod-uct strategy of these tech giants. In late 2015, Google CEO Sundar Pichai stated,“Machine learning is a core, transformative way by which we’re rethinking how we’re24681012141618USD billion 02011 2012 2013 2014 2015 2016 2017US USUSUS USEUEUEUChinaChinaIsraelIsrael USUSTotal estimated investments in Al start-ups, 2011 17 and ﬁrst semester 2018 –By start-up locationUS China EU Israel Canada Japan Other India Figure 1.14 OECD estimate of total investments in AI startups (Source: http://mng.bz/zGN6) 24CHAPTER 1What is deep learning?doing everything. We’re thoughtfully applying it across all our products, be it search,ads, YouTube, or Play. And we’re in early days, but you’ll see us—in a systematic way—apply machine learning in all these areas.” 9 A s a r e s u l t o f t h i s w a v e o f i n v e s t m e n t , t h e n u m b e r o f p e o p l e w o r k i n g o n d e e plearning went from a few hundred to tens of thousands in less than 10 years, andresearch progress has reached a frenetic pace. 1.3.5 The democratization of deep learningOne of the key factors driving this inflow of new faces in deep learning has been thedemocratization of the toolsets used in the field. In the early days, doing deep learn-ing required significant C++ and CUDA expertise, which few people possessed. N o w a d a y s , b a s i c P y t h o n s c r i p t i n g s k i l l s s u f f i c e t o d o a d v a n c e d d e e p l e a r n i n gresearch. This has been driven most notably by the development of the now-defunctTheano library, and then the TensorFlow library—two symbolic tensor-manipulationframeworks for Python that support autodifferentiation, greatly simplifying the imple-mentation of new models—and by the rise of user-friendly libraries such as Keras,which makes deep learning as easy as manipulating LEGO bricks. After its release inearly 2015, Keras quickly became the go-to deep learning solution for large numbersof new startups, graduate students, and researchers pivoting into the field. 1.3.6 Will it last?Is there anything special about deep neural networks that makes them the “right”approach for companies to be investing in and for researchers to flock to? Or isdeep learning just a fad that may not last? Will we still be using deep neural networksin 20 years? Deep learning has several properties that justify its status as an AI revolution, andit’s here to stay. We may not be using neural networks two decades from now, but what-ever we use will directly inherit from modern deep learning and its core concepts.These important properties can be broadly sorted into three categories:Simplicity—Deep learning removes the need for feature engineering, replacingcomplex, brittle, engineering-heavy pipelines with simple, end-to-end trainablemodels that are typically built using only five or six different tensor operations.Scalability—Deep learning is highly amenable to parallelization on GPUs or TPUs,so it can take full advantage of Moore’s law. In addition, deep learning modelsare trained by iterating over small batches of data, allowing them to be trained ondatasets of arbitrary size. (The only bottleneck is the amount of parallel computa-tional power available, which, thanks to Moore’s law, is a fast-moving barrier.)Versatility and reusability—Unlike many prior machine learning approaches, deeplearning models can be trained on additional data without restarting from9Sundar Pichai, Alphabet earnings call, Oct. 22, 2015. 25Why deep learning? Why now?scratch, making them viable for continuous online learning—an importantproperty for very large production models. Furthermore, trained deep learningmodels are repurposable and thus reusable: for instance, it’s possible to take adeep learning model trained for image classification and drop it into a video-processing pipeline. This allows us to reinvest previous work into increasinglycomplex and powerful models. This also makes deep learning applicable tofairly small datasets.Deep learning has only been in the spotlight for a few years, and we may not yet haveestablished the full scope of what it can do. With every passing year, we learn aboutnew use cases and engineering improvements that lift previous limitations. Followinga scientific revolution, progress generally follows a sigmoid curve: it starts with aperiod of fast progress, which gradually stabilizes as researchers hit hard limitations,and then further improvements become incremental. When I was writing the first edition of this book, in 2016, I predicted that deeplearning was still in the first half of that sigmoid, with much more transformativeprogress to come in the following few years. This has proven true in practice, as2017 and 2018 have seen the rise of Transformer-based deep learning models fornatural language processing, which have been a revolution in the field, while deeplearning also kept delivering steady progress in computer vision and speech recog-nition. Today, in 2021, deep learning seems to have entered the second half of thatsigmoid. We should still expect significant progress in the years to come, but we’reprobably out of the initial phase of explosive progress. Today, I’m extremely excited about the deployment of deep learning technologyto every problem it can solve—the list is endless. Deep learning is still a revolution inthe making, and it will take many years to realize its full potential. 26The mathematicalbuilding blocksof neural networks Understanding deep learning requires familiarity with many simple mathematicalconcepts: tensors, tensor operations, differentiation, gradient descent, and so on. Our goalin this chapter will be to build up your intuition about these notions without get-ting overly technical. In particular, we’ll steer away from mathematical notation,which can introduce unnecessary barriers for those without any mathematics back-ground and isn’t necessary to explain things well. The most precise, unambiguousdescription of a mathematical operation is its executable code. To provide sufficient context for introducing tensors and gradient descent, we’llbegin the chapter with a practical example of a neural network. Then we’ll go overevery new concept that’s been introduced, point by point. Keep in mind that theseconcepts will be essential for you to understand the practical examples in the fol-lowing chapters!This chapter coversA first example of a neural networkTensors and tensor operationsHow neural networks learn via backpropagation and gradient descent 27A first look at a neural network After reading this chapter, you’ll have an intuitive understanding of the mathemat-ical theory behind deep learning, and you’ll be ready to start diving into Keras andTensorFlow in chapter 3.2.1 A first look at a neural networkLet’s look at a concrete example of a neural network that uses the Python libraryKeras to learn to classify handwritten digits. Unless you already have experience withKeras or similar libraries, you won’t understand everything about this first exampleright away. That’s fine. In the next chapter, we’ll review each element in the exampleand explain them in detail. So don’t worry if some steps seem arbitrary or look likemagic to you! We’ve got to start somewhere. The problem we’re trying to solve here is to classify grayscale images of handwrit-ten digits (28 × 28 pixels) into their 10 categories (0 through 9). We’ll use the MNISTdataset, a classic in the machine learning community, which has been around almostas long as the field itself and has been intensively studied. It’s a set of 60,000 trainingimages, plus 10,000 test images, assembled by the National Institute of Standards andTechnology (the NIST in MNIST) in the 1980s. You can think of “solving” MNIST asthe “Hello World” of deep learning—it’s what you do to verify that your algorithms areworking as expected. As you become a machine learning practitioner, you’ll seeMNIST come up over and over again in scientific papers, blog posts, and so on. Youcan see some MNIST samples in figure 2.1. NOTEIn machine learning, a category in a classification problem is called aclass. Data points are called samples. The class associated with a specific sampleis called a label.You don’t need to try to reproduce this example on your machine just now. If you wishto, you’ll first need to set up a deep learning workspace, which is covered in chapter 3. The MNIST dataset comes preloaded in Keras, in the form of a set of four NumPyarrays.from tensorflow.keras.datasets import mnist(train_images, train_labels), (test_images, test_labels) = mnist.load_data()train_images and train_labels form the training set, the data that the model willlearn from. The model will then be tested on the test set, test_images and test_labels.Listing 2.1 Loading the MNIST dataset in KerasFigure 2.1 MNIST sample digits 28CHAPTER 2The mathematical building blocks of neural networksThe images are encoded as NumPy arrays, and the labels are an array of digits, rang-ing from 0 to 9. The images and labels have a one-to-one correspondence. Let’s look at the training data:>>> train_images.shape(60000, 28, 28)>>> len(train_labels) 60000 >>> train_labelsarray([5, 0, 4, ..., 5, 6, 8], dtype=uint8)And here’s the test data:>>> test_images.shape(10000, 28, 28)>>> len(test_labels) 10000 >>> test_labelsarray([7, 2, 1, ..., 4, 5, 6], dtype=uint8)The workflow will be as follows: First, we’ll feed the neural network the training data,train_images and train_labels. The network will then learn to associate images andlabels. Finally, we’ll ask the network to produce predictions for test_images, and we’llverify whether these predictions match the labels from test_labels. Let’s build the network—again, remember that you aren’t expected to understandeverything about this example yet.from tensorflow import keras from tensorflow.keras import layersmodel = keras.Sequential([ layers.Dense(512, activation=\"relu\"), layers.Dense(10, activation=\"softmax\")])The core building block of neural networks is the layer. You can think of a layer as a fil-ter for data: some data goes in, and it comes out in a more useful form. Specifically,layers extract representations out of the data fed into them—hopefully, representationsthat are more meaningful for the problem at hand. Most of deep learning consists ofchaining together simple layers that will implement a form of progressive data distilla-tion. A deep learning model is like a sieve for data processing, made of a succession ofincreasingly refined data filters—the layers. Here, our model consists of a sequence of two Dense layers, which are densely con-nected (also called fully connected) neural layers. The second (and last) layer is a 10-waysoftmax classification layer, which means it will return an array of 10 probability scores(summing to 1). Each score will be the probability that the current digit imagebelongs to one of our 10 digit classes.Listing 2.2 The network architecture 29A first look at a neural network To make the model ready for training, we need to pick three more things as part ofthe compilation step:An optimizer—The mechanism through which the model will update itself basedon the training data it sees, so as to improve its performance.A loss function—How the model will be able to measure its performance on thetraining data, and thus how it will be able to steer itself in the right direction.Metrics to monitor during training and testing—Here, we’ll only care about accu-racy (the fraction of the images that were correctly classified).The exact purpose of the loss function and the optimizer will be made clear through-out the next two chapters.model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])Before training, we’ll preprocess the data by reshaping it into the shape the modelexpects and scaling it so that all values are in the [0, 1] interval. Previously, our train-ing images were stored in an array of shape (60000, 28, 28) of type uint8 with valuesin the [0, 255] interval. We’ll transform it into a float32 array of shape (60000, 28 *28) with values between 0 and 1.train_images = train_images.reshape((60000, 28 * 28))train_images = train_images.astype(\"float32\") / 255 test_images = test_images.reshape((10000, 28 * 28))test_images = test_images.astype(\"float32\") / 255We’re now ready to train the model, which in Keras is done via a call to the model’sfit() method—we fit the model to its training data.>>> model.fit(train_images, train_labels, epochs=5, batch_size=128)Epoch 1/5 60000/60000 [===========================] - 5s - loss: 0.2524 - acc: 0.9273 Epoch 2/5 51328/60000 [=====================>.....] - ETA: 1s - loss: 0.1035 - acc: 0.9692Two quantities are displayed during training: the loss of the model over the trainingdata, and the accuracy of the model over the training data. We quickly reach an accu-racy of 0.989 (98.9%) on the training data. Now that we have a trained model, we can use it to predict class probabilities fornew digits—images that weren’t part of the training data, like those from the test set.Listing 2.3 The compilation step Listing 2.4 Preparing the image data Listing 2.5 “Fitting” the model 30CHAPTER 2The mathematical building blocks of neural networks>>> test_digits = test_images[0:10]>>> predictions = model.predict(test_digits)>>> predictions[0]array([1.0726176e-10, 1.6918376e-10, 6.1314843e-08, 8.4106023e-06, 2.9967067e-11, 3.0331331e-09, 8.3651971e-14, 9.9999106e-01, 2.6657624e-08, 3.8127661e-07], dtype=float32)Each number of index i in that array corresponds to the probability that digit imagetest_digits[0] belongs to class i. T h i s f i r s t t e s t d i g i t h a s t h e h i g h e s t p r o b a b i l i t y s c o r e ( 0 . 9 9 9 9 9 1 0 6 , a l m o s t 1 ) a tindex 7, so according to our model, it must be a 7:>>> predictions[0].argmax() 7 >>> predictions[0][7] 0.99999106We can check that the test label agrees:>>> test_labels[0] 7 On average, how good is our model at classifying such never-before-seen digits? Let’scheck by computing average accuracy over the entire test set.>>> test_loss, test_acc = model.evaluate(test_images, test_labels)>>> print(f\"test_acc: {test_acc}\")test_acc: 0.9785The test-set accuracy turns out to be 97.8%—that’s quite a bit lower than the training-set accuracy (98.9%). This gap between training accuracy and test accuracy is anexample of overfitting: the fact that machine learning models tend to perform worseon new data than on their training data. Overfitting is a central topic in chapter 3. This concludes our first example—you just saw how you can build and train aneural network to classify handwritten digits in less than 15 lines of Python code. Inthis chapter and the next, we’ll go into detail about every moving piece we just pre-viewed and clarify what’s going on behind the scenes. You’ll learn about tensors,the data-storing objects going into the model; tensor operations, which layers aremade of; and gradient descent, which allows your model to learn from its trainingexamples. Listing 2.6 Using the model to make predictions Listing 2.7 Evaluating the model on new data 31Data representations for neural networks2.2 Data representations for neural networksIn the previous example, we started from data stored in multidimensional NumPyarrays, also called tensors. In general, all current machine learning systems use tensorsas their basic data structure. Tensors are fundamental to the field—so fundamentalthat TensorFlow was named after them. So what’s a tensor? At its core, a tensor is a container for data—usually numerical data. So, it’s a con-tainer for numbers. You may be already familiar with matrices, which are rank-2 ten-sors: tensors are a generalization of matrices to an arbitrary number of dimensions(note that in the context of tensors, a dimension is often called an axis).2.2.1 Scalars (rank-0 tensors)A tensor that contains only one number is called a scalar (or scalar tensor, or rank-0tensor, or 0D tensor). In NumPy, a float32 or float64 number is a scalar tensor (orscalar array). You can display the number of axes of a NumPy tensor via the ndim attri-bute; a scalar tensor has 0 axes (ndim == 0). The number of axes of a tensor is alsocalled its rank. Here’s a NumPy scalar:>>> import numpy as np>>> x = np.array(12)>>> xarray(12)>>> x.ndim 02.2.2 Vectors (rank-1 tensors)An array of numbers is called a vector, or rank-1 tensor, or 1D tensor. A rank-1 tensor issaid to have exactly one axis. Following is a NumPy vector:>>> x = np.array([12, 3, 6, 14, 7])>>> xarray([12, 3, 6, 14, 7])>>> x.ndim 1This vector has five entries and so is called a 5-dimensional vector. Don’t confuse a 5Dvector with a 5D tensor! A 5D vector has only one axis and has five dimensions alongits axis, whereas a 5D tensor has five axes (and may have any number of dimensionsalong each axis). Dimensionality can denote either the number of entries along a spe-cific axis (as in the case of our 5D vector) or the number of axes in a tensor (such as a5D tensor), which can be confusing at times. In the latter case, it’s technically morecorrect to talk about a tensor of rank 5 (the rank of a tensor being the number of axes),but the ambiguous notation 5D tensor is common regardless. 32CHAPTER 2The mathematical building blocks of neural networks2.2.3 Matrices (rank-2 tensors)An array of vectors is a matrix, or rank-2 tensor, or 2D tensor. A matrix has two axes(often referred to as rows and columns). You can visually interpret a matrix as a rectan-gular grid of numbers. This is a NumPy matrix:>>> x = np.array([[5, 78, 2, 34, 0], [6, 79, 3, 35, 1], [7, 80, 4, 36, 2]])>>> x.ndim 2The entries from the first axis are called the rows, and the entries from the second axisare called the columns. In the previous example, [5, 78, 2, 34, 0] is the first row of x,and [5, 6, 7] is the first column. 2.2.4 Rank-3 and higher-rank tensorsIf you pack such matrices in a new array, you obtain a rank-3 tensor (or 3D tensor),which you can visually interpret as a cube of numbers. Following is a NumPy rank-3tensor:>>> x = np.array([[[5, 78, 2, 34, 0], [6, 79, 3, 35, 1], [7, 80, 4, 36, 2]], [[5, 78, 2, 34, 0], [6, 79, 3, 35, 1], [7, 80, 4, 36, 2]], [[5, 78, 2, 34, 0], [6, 79, 3, 35, 1], [7, 80, 4, 36, 2]]])>>> x.ndim 3 By packing rank-3 tensors in an array, you can create a rank-4 tensor, and so on. Indeep learning, you’ll generally manipulate tensors with ranks 0 to 4, although youmay go up to 5 if you process video data. 2.2.5 Key attributesA tensor is defined by three key attributes:Number of axes (rank)—For instance, a rank-3 tensor has three axes, and a matrixhas two axes. This is also called the tensor’s ndim in Python libraries such asNumPy or TensorFlow.Shape—This is a tuple of integers that describes how many dimensions the ten-sor has along each axis. For instance, the previous matrix example has shape(3, 5), and the rank-3 tensor example has shape (3, 3, 5). A vector has a shapewith a single element, such as (5,), whereas a scalar has an empty shape, (). 33Data representations for neural networksData type (usually called dtype i n P y t h o n l i b r a r i e s )—This is the type of the datacontained in the tensor; for instance, a tensor’s type could be float16, float32,float64, uint8, and so on. In TensorFlow, you are also likely to come acrossstring tensors.To make this more concrete, let’s look back at the data we processed in the MNISTexample. First, we load the MNIST dataset:from tensorflow.keras.datasets import mnist(train_images, train_labels), (test_images, test_labels) = mnist.load_data()Next, we display the number of axes of the tensor train_images, the ndim attribute:>>> train_images.ndim 3 Here’s its shape:>>> train_images.shape(60000, 28, 28)And this is its data type, the dtype attribute:>>> train_images.dtypeuint8So what we have here is a rank-3 tensor of 8-bit integers. More precisely, it’s an array of60,000 matrices of 28 × 28 integers. Each such matrix is a grayscale image, with coeffi-cients between 0 and 255. Let’s display the fourth digit in this rank-3 tensor, using the Matplotlib library (awell-known Python data visualization library, which comes preinstalled in Colab); seefigure 2.2. Figure 2.2 The fourth sample in our dataset 34CHAPTER 2The mathematical building blocks of neural networksimport matplotlib.pyplot as pltdigit = train_images[4]plt.imshow(digit, cmap=plt.cm.binary)plt.show()Naturally, the corresponding label is the integer 9:>>> train_labels[4] 92.2.6 Manipulating tensors in NumPyIn the previous example, we selected a specific digit alongside the first axis using thesyntax train_images[i]. Selecting specific elements in a tensor is called tensor slicing.Let’s look at the tensor-slicing operations you can do on NumPy arrays. The following example selects digits #10 to #100 (#100 isn’t included) and putsthem in an array of shape (90, 28, 28):>>> my_slice = train_images[10:100]>>> my_slice.shape(90, 28, 28)It’s equivalent to this more detailed notation, which specifies a start index and stopindex for the slice along each tensor axis. Note that : is equivalent to selecting theentire axis:>>> my_slice = train_images[10:100, :, :] >>> my_slice.shape(90, 28, 28)>>> my_slice = train_images[10:100, 0:28, 0:28] >>> my_slice.shape(90, 28, 28)In general, you may select slices between any two indices along each tensor axis. Forinstance, in order to select 14 × 14 pixels in the bottom-right corner of all images, youwould do this:my_slice = train_images[:, 14:, 14:]It’s also possible to use negative indices. Much like negative indices in Python lists,they indicate a position relative to the end of the current axis. In order to crop theimages to patches of 14 × 14 pixels centered in the middle, you’d do this:my_slice = train_images[:, 7:-7, 7:-7]Listing 2.8 Displaying the fourth digit Equivalent to the previous exampleAlso equivalent to the previous example 35Data representations for neural networks2.2.7 The notion of data batchesIn general, the first axis (axis 0, because indexing starts at 0) in all data tensors you’llcome across in deep learning will be the samples axis (sometimes called the samplesdimension). In the MNIST example, “samples” are images of digits. In addition, deep learning models don’t process an entire dataset at once; rather,they break the data into small batches. Concretely, here’s one batch of our MNISTdigits, with a batch size of 128:batch = train_images[:128]And here’s the next batch:batch = train_images[128:256]And the nth batch:n = 3 batch = train_images[128 * n:128 * (n + 1)]When considering such a batch tensor, the first axis (axis 0) is called the batch axis orbatch dimension. This is a term you’ll frequently encounter when using Keras and otherdeep learning libraries. 2.2.8 Real-world examples of data tensorsLet’s make data tensors more concrete with a few examples similar to what you’llencounter later. The data you’ll manipulate will almost always fall into one of the fol-lowing categories:Vector data—Rank-2 tensors of shape (samples, features), where each sampleis a vector of numerical attributes (“features”)Timeseries data or sequence data—Rank-3 tensors of shape (samples, timesteps,features), where each sample is a sequence (of length timesteps) of featurevectorsImages—Rank-4 tensors of shape (samples, height, width, channels), whereeach sample is a 2D grid of pixels, and each pixel is represented by a vector ofvalues (“channels”)Video—Rank-5 tensors of shape (samples, frames, height, width, channels),where each sample is a sequence (of length frames) of images 2.2.9 Vector dataThis is one of the most common cases. In such a dataset, each single data point can beencoded as a vector, and thus a batch of data will be encoded as a rank-2 tensor (thatis, an array of vectors), where the first axis is the samples axis and the second axis is thefeatures axis. 36CHAPTER 2The mathematical building blocks of neural networks Let’s take a look at two examples:An actuarial dataset of people, where we consider each person’s age, gender,and income. Each person can be characterized as a vector of 3 values, and thusan entire dataset of 100,000 people can be stored in a rank-2 tensor of shape(100000, 3).A dataset of text documents, where we represent each document by the countsof how many times each word appears in it (out of a dictionary of 20,000 com-mon words). Each document can be encoded as a vector of 20,000 values (onecount per word in the dictionary), and thus an entire dataset of 500 documentscan be stored in a tensor of shape (500, 20000). 2.2.10 Timeseries data or sequence dataWhenever time matters in your data (or the notion of sequence order), it makes senseto store it in a rank-3 tensor with an explicit time axis. Each sample can be encoded asa sequence of vectors (a rank-2 tensor), and thus a batch of data will be encoded as arank-3 tensor (see figure 2.3). The time axis is always the second axis (axis of index 1) by convention. Let’s look at afew examples:A dataset of stock prices. Every minute, we store the current price of the stock,the highest price in the past minute, and the lowest price in the past minute.Thus, every minute is encoded as a 3D vector, an entire day of trading isencoded as a matrix of shape (390, 3) (there are 390 minutes in a trading day),and 250 days’ worth of data can be stored in a rank-3 tensor of shape (250,390, 3). Here, each sample would be one day’s worth of data.A dataset of tweets, where we encode each tweet as a sequence of 280 charactersout of an alphabet of 128 unique characters. In this setting, each character canbe encoded as a binary vector of size 128 (an all-zeros vector except for a 1 entryat the index corresponding to the character). Then each tweet can be encodedas a rank-2 tensor of shape (280, 128), and a dataset of 1 million tweets can bestored in a tensor of shape (1000000, 280, 128). FeaturesTimestepsSamplesFigure 2.3 A rank-3 timeseries data tensor 37Data representations for neural networks2.2.11 Image dataImages typically have three dimensions: height, width, and color depth. Althoughgrayscale images (like our MNIST digits) have only a single color channel and couldthus be stored in rank-2 tensors, by convention image tensors are always rank-3, with aone-dimensional color channel for grayscale images. A batch of 128 grayscale imagesof size 256 × 256 could thus be stored in a tensor of shape (128, 256, 256, 1), and abatch of 128 color images could be stored in a tensor of shape (128, 256, 256, 3)(see figure 2.4). There are two conventions for shapes of image tensors: the channels-last convention(which is standard in TensorFlow) and the channels-first convention (which is increas-ingly falling out of favor). The channels- last convention places the color-depth axis at the end: (samples,height, width, color_depth). Meanwhile, the channels-first convention places thecolor depth axis right after the batch axis: (samples, color_depth, height, width).With the channels-first convention, the previous examples would become (128, 1,256, 256) and (128, 3, 256, 256). The Keras API provides support for both formats. 2.2.12 Video dataVideo data is one of the few types of real-world data for which you’ll need rank-5 ten-sors. A video can be understood as a sequence of frames, each frame being a colorimage. Because each frame can be stored in a rank-3 tensor (height, width, color_depth), a sequence of frames can be stored in a rank-4 tensor (frames, height,width, color_depth), and thus a batch of different videos can be stored in a rank-5tensor of shape (samples, frames, height, width, color_depth). For instance, a 60-second, 144 × 256 YouTube video clip sampled at 4 frames persecond would have 240 frames. A batch of four such video clips would be stored in atensor of shape (4, 240, 144, 256, 3). That’s a total of 106,168,320 values! If theColor channelsHeightWidthSamplesFigure 2.4 A rank-4 image data tensor 38CHAPTER 2The mathematical building blocks of neural networksdtype of the tensor was float32, each value would be stored in 32 bits, so the tensorwould represent 405 MB. Heavy! Videos you encounter in real life are much lighter,because they aren’t stored in float32, and they’re typically compressed by a large fac-tor (such as in the MPEG format). 2.3 The gears of neural networks: Tensor operationsMuch as any computer program can be ultimately reduced to a small set of binaryoperations on binary inputs (AND, OR, NOR, and so on), all transformations learnedby deep neural networks can be reduced to a handful of tensor operations (or tensor func-tions) applied to tensors of numeric data. For instance, it’s possible to add tensors,multiply tensors, and so on. In our initial example, we built our model by stacking Dense layers on top of eachother. A Keras layer instance looks like this:keras.layers.Dense(512, activation=\"relu\")This layer can be interpreted as a function, which takes as input a matrix and returnsanother matrix—a new representation for the input tensor. Specifically, the functionis as follows (where W is a matrix and b is a vector, both attributes of the layer):output = relu(dot(input, W) + b)Let’s unpack this. We have three tensor operations here:A dot product (dot) between the input tensor and a tensor named WAn addition (+) between the resulting matrix and a vector bA relu operation: relu(x) is max(x, 0); “relu” stands for “rectified linear unit”NOTEAlthough this section deals entirely with linear algebra expressions,you won’t find any mathematical notation here. I’ve found that mathematicalconcepts can be more readily mastered by programmers with no mathemati-cal background if they’re expressed as short Python snippets instead of math-ematical equations. So we’ll use NumPy and TensorFlow code throughout.2.3.1 Element-wise operationsThe relu o p e r a t i o n a n d a d d i t i o n a r e e l e m e n t - w i s e o p e r a t i o n s : o p e r a t i o n s t h a t a r eapplied independently to each entry in the tensors being considered. This meansthese operations are highly amenable to massively parallel implementations (vectorizedimplementations, a term that comes from the vector processor supercomputer architec-ture from the 1970–90 period). If you want to write a naive Python implementation ofan element-wise operation, you use a for loop, as in this naive implementation of anelement-wise relu operation:def naive_relu(x): assert len(x.shape) == 2 x is a rank-2 NumPy tensor. 39The gears of neural networks: Tensor operations x = x.copy() for i in range(x.shape[0]): for j in range(x.shape[1]): x[i, j] = max(x[i, j], 0) return xYou could do the same for addition:def naive_add(x, y): assert len(x.shape) == 2 assert x.shape == y.shape x = x.copy() for i in range(x.shape[0]): for j in range(x.shape[1]): x[i, j] += y[i, j] return xOn the same principle, you can do element-wise multiplication, subtraction, and so on. In practice, when dealing with NumPy arrays, these operations are available as well-optimized built-in NumPy functions, which themselves delegate the heavy lifting to aBasic Linear Algebra Subprograms (BLAS) implementation. BLAS are low-level,highly parallel, efficient tensor-manipulation routines that are typically implementedin Fortran or C. So, in NumPy, you can do the following element-wise operation, and it will be blaz-ing fast:import numpy as npz = x + y z = np.maximum(z, 0.) Let’s actually time the difference:import time x = np.random.random((20, 100))y = np.random.random((20, 100)) t0 = time.time() for _ in range(1000): z = x + y z = np.maximum(z, 0.) print(\"Took: {0:.2f} s\".format(time.time() - t0))This takes 0.02 s. Meanwhile, the naive version takes a stunning 2.45 s:t0 = time.time() for _ in range(1000): z = naive_add(x, y) z = naive_relu(z) print(\"Took: {0:.2f} s\".format(time.time() - t0))Avoid overwriting the input tensor.x and y are rank-2 NumPy tensors.Avoid overwriting the input tensor. Element-wise additionElement-wise relu 40CHAPTER 2The mathematical building blocks of neural networksLikewise, when running TensorFlow code on a GPU, element-wise operations are exe-cuted via fully vectorized CUDA implementations that can best utilize the highly par-allel GPU chip architecture. 2.3.2 BroadcastingOur earlier naive implementation of naive_add only supports the addition of rank-2tensors with identical shapes. But in the Dense layer introduced earlier, we added arank-2 tensor with a vector. What happens with addition when the shapes of the twotensors being added differ? When possible, and if there’s no ambiguity, the smaller tensor will be broadcast tomatch the shape of the larger tensor. Broadcasting consists of two steps:1Axes (called broadcast axes) are added to the smaller tensor to match the ndim ofthe larger tensor.2The smaller tensor is repeated alongside these new axes to match the full shapeof the larger tensor.Let’s look at a concrete example. Consider X with shape (32, 10) and y with shape(10,):import numpy as npX = np.random.random((32, 10)) y = np.random.random((10,)) First, we add an empty first axis to y, whose shape becomes (1, 10):y = np.expand_dims(y, axis=0) Then, we repeat y 32 times alongside this new axis, so that we end up with a tensor Ywith shape (32, 10), where Y[i, :] == y for i in range(0, 32):Y = np.concatenate([y] * 32, axis=0) At this point, we can proceed to add X and Y, because they have the same shape. In terms of implementation, no new rank-2 tensor is created, because that wouldbe terribly inefficient. The repetition operation is entirely virtual: it happens at thealgorithmic level rather than at the memory level. But thinking of the vector beingrepeated 10 times alongside a new axis is a helpful mental model. Here’s what a naiveimplementation would look like:def naive_add_matrix_and_vector(x, y): assert len(x.shape) == 2 assert len(y.shape) == 1 assert x.shape[1] == y.shape[0] x = x.copy() for i in range(x.shape[0]):X is a random matrix with shape (32, 10). y is a random vector with shape (10,).The shape of y is now (1, 10).Repeat y 32 times along axis 0 to obtain Y, which has shape (32, 10). x is a rank-2 NumPy tensor.y is a NumPy vector.Avoid overwriting the input tensor. 41The gears of neural networks: Tensor operations for j in range(x.shape[1]): x[i, j] += y[j] return xWith broadcasting, you can generally perform element-wise operations that take twoinputs tensors if one tensor has shape (a, b, … n, n + 1, … m) and the other has shape (n,n + 1, … m). The broadcasting will then automatically happen for axes a through n - 1. The following example applies the element-wise maximum operation to two tensorsof different shapes via broadcasting:import numpy as npx = np.random.random((64, 3, 32, 10)) y = np.random.random((32, 10)) z = np.maximum(x, y) 2.3.3 Tensor productThe tensor product, or dot product (not to be confused with an element-wise product, the* operator), is one of the most common, most useful tensor operations. In NumPy, a tensor product is done using the np.dot function (because the math-ematical notation for tensor product is usually a dot):x = np.random.random((32,))y = np.random.random((32,))z = np.dot(x, y)In mathematical notation, you’d note the operation with a dot (•):z=x•yMathematically, what does the dot operation do? Let’s start with the dot product oftwo vectors, x and y. It’s computed as follows:def naive_vector_dot(x, y): assert len(x.shape) == 1 assert len(y.shape) == 1 assert x.shape[0] == y.shape[0] z = 0. for i in range(x.shape[0]): z += x[i] * y[i] return zYou’ll have noticed that the dot product between two vectors is a scalar and that onlyvectors with the same number of elements are compatible for a dot product. You can also take the dot product between a matrix x and a vector y, which returnsa vector where the coefficients are the dot products between y and the rows of x. Youimplement it as follows:x is a random tensor with shape (64, 3, 32, 10).y is a random tensor with shape (32, 10).The output z has shape(64, 3, 32, 10) like x. x and y are NumPy vectors. 42CHAPTER 2The mathematical building blocks of neural networksdef naive_matrix_vector_dot(x, y): assert len(x.shape) == 2 assert len(y.shape) == 1 assert x.shape[1] == y.shape[0] z = np.zeros(x.shape[0]) for i in range(x.shape[0]): for j in range(x.shape[1]): z[i] += x[i, j] * y[j] return zYou could also reuse the code we wrote previously, which highlights the relationshipbetween a matrix-vector product and a vector product:def naive_matrix_vector_dot(x, y): z = np.zeros(x.shape[0]) for i in range(x.shape[0]): z[i] = naive_vector_dot(x[i, :], y) return zNote that as soon as one of the two tensors has an ndim greater than 1, dot is no lon-ger symmetric, which is to say that dot(x, y) isn’t the same as dot(y, x). Of course, a dot product generalizes to tensors with an arbitrary number of axes.The most common applications may be the dot product between two matrices. You cantake the dot product of two matrices x and y (dot(x, y)) if and only if x.shape[1] ==y.shape[0]. The result is a matrix with shape (x.shape[0], y.shape[1]), where thecoefficients are the vector products between the rows of x a n d t h e c o l u m n s o f y.Here’s the naive implementation:def naive_matrix_dot(x, y): assert len(x.shape) == 2 assert len(y.shape) == 2 assert x.shape[1] == y.shape[0] z = np.zeros((x.shape[0], y.shape[1])) for i in range(x.shape[0]): for j in range(y.shape[1]): row_x = x[i, :] column_y = y[:, j] z[i, j] = naive_vector_dot(row_x, column_y) return zTo understand dot-product shape compatibility, it helps to visualize the input and out-put tensors by aligning them as shown in figure 2.5. In the figure, x, y, and z are pictured as rectangles (literal boxes of coefficients).Because the rows of x and the columns of y must have the same size, it follows that thewidth of x must match the height of y. If you go on to develop new machine learningalgorithms, you’ll likely be drawing such diagrams often. x is a NumPy matrix.y is a NumPy vector.The first dimension of x must be the same as the 0th dimension of y!This operation returns a vector of 0s with the same shape as y. x and y are NumPy matrices.The first dimension of x must be the same as the 0th dimension of y!This operation returns a matrix of 0s with a specific shape.Iterates over the rows of x . . .. . . and over the columns of y. 43The gears of neural networks: Tensor operations More generally, you can take the dot product between higher-dimensional tensors,following the same rules for shape compatibility as outlined earlier for the 2D case:(a, b, c, d) • (d,)→(a, b, c)(a, b, c, d) • (d, e)→(a, b, c, e)And so on. 2.3.4 Tensor reshapingA third type of tensor operation that’s essential to understand is tensor reshaping.Although it wasn’t used in the Dense layers in our first neural network example, weused it when we preprocessed the digits data before feeding it into our model:train_images = train_images.reshape((60000, 28 * 28))Reshaping a tensor means rearranging its rows and columns to match a target shape.Naturally, the reshaped tensor has the same total number of coefficients as the initialtensor. Reshaping is best understood via simple examples:>>> x = np.array([[0., 1.], [2., 3.], [4., 5.]])>>> x.shape(3, 2)>>> x = x.reshape((6, 1))>>> xarray([[ 0.], [ 1.], [ 2.], [ 3.], [ 4.], [ 5.]])abx • y = zbx.shape:(a, b)y.shape:(b, c) z.shape:(a, c)Row of xColumn of y z [ i, j ]c Figure 2.5 Matrix dot-product box diagram 44CHAPTER 2The mathematical building blocks of neural networks >>> x = x.reshape((2, 3)) >>> x array([[ 0., 1., 2.], [ 3., 4., 5.]])A special case of reshaping that’s commonly encountered is transposition. Transposing amatrix means exchanging its rows and its columns, so that x[i, :] becomes x[:, i]:>>> x = np.zeros((300, 20)) >>> x = np.transpose(x)>>> x.shape(20, 300)2.3.5 Geometric interpretation of tensor operationsBecause the contents of the tensors manipulated by tensor operations can be inter-preted as coordinates of points in some geometric space, all tensor operations have ageometric interpretation. For instance, let’s consider addition. We’ll start with the fol-lowing vector:A = [0.5, 1]It’s a point in a 2D space (see figure 2.6). It’s common to picture a vector as an arrowlinking the origin to the point, as shown in figure 2.7. Let’s consider a new point, B = [1, 0.25], which we’ll add to the previous one. This isdone geometrically by chaining together the vector arrows, with the resulting locationbeing the vector representing the sum of the previous two vectors (see figure 2.8). Asyou can see, adding a vector B to a vector A represents the action of copying point Ain a new location, whose distance and direction from the original point A is determinedby the vector B. If you apply the same vector addition to a group of points in the plane(an “object”), you would be creating a copy of the entire object in a new location (seeCreates an all-zeros matrix of shape (300, 20) 1 1A[0.5, 1] Figure 2.6 A point in a 2D space1 1A[0.5, 1] Figure 2.7 A point in a 2D space pictured as an arrow 45The gears of neural networks: Tensor operations figure 2.9). Tensor addition thus represents the action of translating an object (movingthe object without distorting it) by a certain amount in a certain direction. In general, elementary geometric operations such as translation, rotation, scaling,skewing, and so on can be expressed as tensor operations. Here are a few examples:Translation: As you just saw, adding a vector to a point will move the point by afixed amount in a fixed direction. Applied to a set of points (such as a 2Dobject), this is called a “translation” (see figure 2.9).Rotation: A counterclockwise rotation of a 2D vector by an angle theta (see fig-ure 2.10) can be achieved via a dot product with a 2 × 2 matrix R = [[cos(theta),-sin(theta)], [sin(theta), cos(theta)]].Figure 2.8 Geometric interpretation of the sum of two vectors1 1A BA + B xyKKVertical factorHorizontal factorHorizontal factorVertical factor+ Figure 2.9 2D translation as a vector addition cos(theta) –sin(theta)sin(theta) cos(theta)xy ThetaKKFigure 2.10 2D rotation (counterclockwise) as a dot product 46CHAPTER 2The mathematical building blocks of neural networksScaling: A vertical and horizontal scaling of the image (see figure 2.11) can beachieved via a dot product with a 2 × 2 matrix S = [[horizontal_factor, 0],[0, vertical_factor]] (note that such a matrix is called a “diagonal matrix,”because it only has non-zero coefficients in its “diagonal,” going from the topleft to the bottom right). Linear transform: A dot product with an arbitrary matrix implements a lineartransform. Note that scaling and rotation, listed previously, are by definition lin-ear transforms.Affine transform: An affine transform (see figure 2.12) is the combination of alinear transform (achieved via a dot product with some matrix) and a transla-tion (achieved via a vector addition). As you have probably recognized, that’sexactly the y = W • x + b computation implemented by the Dense layer! A Denselayer without an activation function is an affine layer. Dense layer with relu a c t i v a t i o n: An important observation about affine trans-forms is that if you apply many of them repeatedly, you still end up with anaffine transform (so you could just have applied that one affine transform inthe first place). Let’s try it with two: affine2(affine1(x)) = W2 • (W1 • x + b1)+ b2 = (W2 • W1) • x + (W2 • b1 + b2). That’s an affine transform where the linearpart is the matrix W2 • W1 and the translation part is the vector W2 • b1 + b2. As aconsequence, a multilayer neural network made entirely of Dense layers without100– 0 . 5xyKKFigure 2.112D scaling as a dot productKW • x + bFigure 2.12 Affine transform in the plane 47The gears of neural networks: Tensor operationsactivations would be equivalent to a single Dense layer. This “deep” neural net-work would just be a linear model in disguise! This is why we need activationfunctions, like relu ( s e e n i n a c t i o n i n f i g u r e 2 . 1 3 ) . T h a n k s t o a c t i v a t i o n f u n c -tions, a chain of Dense layers can be made to implement very complex, non-lineargeometric transformations, resulting in very rich hypothesis spaces for yourdeep neural networks. We’ll cover this idea in more detail in the next chapter. 2.3.6 A geometric interpretation of deep learningYou just learned that neural networks consist entirely of chains of tensor operations,and that these tensor operations are just simple geometric transformations of theinput data. It follows that you can interpret a neural network as a very complex geo-metric transformation in a high-dimensional space, implemented via a series of sim-ple steps. In 3D, the following mental image may prove useful. Imagine two sheets of coloredpaper: one red and one blue. Put one on top of the other. Now crumple themtogether into a small ball. That crumpled paper ball is your input data, and each sheetof paper is a class of data in a classification problem. What a neural network is meantto do is figure out a transformation of the paper ball that would uncrumple it, so as tomake the two classes cleanly separable again (see figure 2.14). With deep learning,this would be implemented as a series of simple transformations of the 3D space, suchas those you could apply on the paper ball with your fingers, one movement at a time. Uncrumpling paper balls is what machine learning is about: finding neat representa-tions for complex, highly folded data manifolds in high-dimensional spaces (a mani-fold is a continuous surface, like our crumpled sheet of paper). At this point, youshould have a pretty good intuition as to why deep learning excels at this: it takes theKrelu(W • x + b)Figure 2.13 Affine transform followed by relu activation Figure 2.14 Uncrumpling a complicated manifold of data 48CHAPTER 2The mathematical building blocks of neural networksapproach of incrementally decomposing a complicated geometric transformationinto a long chain of elementary ones, which is pretty much the strategy a humanwould follow to uncrumple a paper ball. Each layer in a deep network applies a trans-formation that disentangles the data a little, and a deep stack of layers makes tractablean extremely complicated disentanglement process. 2.4 The engine of neural networks: Gradient-based optimizationAs you saw in the previous section, each neural layer from our first model exampletransforms its input data as follows:output = relu(dot(input, W) + b)In this expression, W and b are tensors that are attributes of the layer. They’re calledthe weights or trainable parameters of the layer (the kernel and bias attributes, respec-tively). These weights contain the information learned by the model from exposure totraining data. Initially, these weight matrices are filled with small random values (a step calledrandom initialization). Of course, there’s no reason to expect that relu(dot(input, W)+ b), when W and b are random, will yield any useful representations. The resultingrepresentations are meaningless—but they’re a starting point. What comes next is togradually adjust these weights, based on a feedback signal. This gradual adjustment,also called training, is the learning that machine learning is all about. This happens within what’s called a training loop, which works as follows. Repeatthese steps in a loop, until the loss seems sufficiently low:1Draw a batch of training samples, x, and corresponding targets, y_true.2Run the model on x (a step called the forward pass) to obtain predictions, y_pred.3Compute the loss of the model on the batch, a measure of the mismatch betweeny_pred and y_true.4Update all weights of the model in a way that slightly reduces the loss on thisbatch.You’ll eventually end up with a model that has a very low loss on its training data: a lowmismatch between predictions, y_pred, and expected targets, y_true. The model has“learned” to map its inputs to correct targets. From afar, it may look like magic, butwhen you reduce it to elementary steps, it turns out to be simple. Step 1 sounds easy enough—just I/O code. Steps 2 and 3 are merely the applicationof a handful of tensor operations, so you could implement these steps purely from whatyou learned in the previous section. The difficult part is step 4: updating the model’sweights. Given an individual weight coefficient in the model, how can you computewhether the coefficient should be increased or decreased, and by how much? One naive solution would be to freeze all weights in the model except the one sca-lar coefficient being considered, and try different values for this coefficient. Let’s say 49The engine of neural networks: Gradient-based optimizationthe initial value of the coefficient is 0.3. After the forward pass on a batch of data, theloss of the model on the batch is 0.5. If you change the coefficient’s value to 0.35 andrerun the forward pass, the loss increases to 0.6. But if you lower the coefficient to0.25, the loss falls to 0.4. In this case, it seems that updating the coefficient by –0.05would contribute to minimizing the loss. This would have to be repeated for all coeffi-cients in the model. But such an approach would be horribly inefficient, because you’d need to com-pute two forward passes (which are expensive) for every individual coefficient (ofwhich there are many, usually thousands and sometimes up to millions). Thankfully,there’s a much better approach: gradient descent. Gradient descent is the optimization technique that powers modern neural net-works. Here’s the gist of it. All of the functions used in our models (such as dot or +)transform their input in a smooth and continuous way: if you look at z = x + y, forinstance, a small change in y only results in a small change in z, and if you know thedirection of the change in y, you can infer the direction of the change in z. Mathemat-ically, you’d say these functions are differentiable. If you chain together such functions,the bigger function you obtain is still differentiable. In particular, this applies to thefunction that maps the model’s coefficients to the loss of the model on a batch ofdata: a small change in the model’s coefficients results in a small, predictable changein the loss value. This enables you to use a mathematical operator called the gradientto describe how the loss varies as you move the model’s coefficients in different direc-tions. If you compute this gradient, you can use it to move the coefficients (all at oncein a single update, rather than one at a time) in a direction that decreases the loss. If you already know what differentiable means and what a gradient is, you can skip tosection 2.4.3. Otherwise, the following two sections will help you understand theseconcepts.2.4.1 What’s a derivative?Consider a continuous, smooth function f(x) = y, mapping a number, x, to a newnumber, y. We can use the function in figure 2.15 as an example. Because the function is continuous, a small change in x can only result in a small changein y—that’s the intuition behind continuity. Let’s say you increase x by a small factor,epsilon_x: this results in a small epsilon_y change to y, as shown in figure 2.16.y = f(x)xyFigure 2.15 A continuous, smooth function 50CHAPTER 2The mathematical building blocks of neural networks In addition, because the function is smooth (its curve doesn’t have any abrupt angles),when epsilon_x is small enough, around a certain point p, it’s possible to approxi-mate f as a linear function of slope a, so that epsilon_y becomes a * epsilon_x:f(x + epsilon_x) = y + a * epsilon_xObviously, this linear approximation is valid only when x is close enough to p. T h e s l o p e a is called the derivative o f f i n p. If a i s n e g a t i v e , i t m e a n s a s m a l lincrease in x around p will result in a decrease of f(x) (as shown in figure 2.17), andif a is positive, a small increase in x will result in an increase of f(x). Further, the abso-lute value of a (the magnitude of the derivative) tells you how quickly this increase ordecrease will happen. For every differentiable function f(x) (differentiable means “can be derived”: for exam-ple, smooth, continuous functions can be derived), there exists a derivative functionf'(x), that maps values of x to the slope of the local linear approximation of f inthose points. For instance, the derivative of cos(x) is -sin(x), the derivative of f(x) =a * x is f'(x) = a, and so on. Being able to derive functions is a very powerful tool when it comes to optimization,the task of finding values of x t h a t m i n i m i z e t h e v a l u e o f f(x). If you’re trying toupdate x by a factor epsilon_x in order to minimize f(x), and you know the deriva-tive of f, then your job is done: the derivative completely describes how f(x) evolvesas you change x. If you want to reduce the value of f(x), you just need to move x a lit-tle in the opposite direction from the derivative. y = f(x)epsilon_xepsilon_yxyFigure 2.16 With a continuous function, a small change in x results in a small change in y. y = f(x)xyLocal linearapproximation of , with fslope aFigure 2.17 Derivative of f in p 51The engine of neural networks: Gradient-based optimization2.4.2 Derivative of a tensor operation: The gradientThe function we were just looking at turned a scalar value x into another scalar valuey: you could plot it as a curve in a 2D plane. Now imagine a function that turns a tupleof scalars (x, y) into a scalar value z: that would be a vector operation. You could plotit as a 2D surface in a 3D space (indexed by coordinates x, y, z). Likewise, you canimagine functions that take matrices as inputs, functions that take rank-3 tensors asinputs, etc. The concept of derivation can be applied to any such function, as long as the sur-faces they describe are continuous and smooth. The derivative of a tensor operation(or tensor function) is called a gradient. Gradients are just the generalization of theconcept of derivatives to functions that take tensors as inputs. Remember how, for ascalar function, the derivative represents the local slope of the curve of the function? Inthe same way, the gradient of a tensor function represents the curvature of the multidi-mensional surface described by the function. It characterizes how the output of thefunction varies when its input parameters vary. Let’s look at an example grounded in machine learning. ConsiderAn input vector, x (a sample in a dataset)A matrix, W (the weights of a model)A target, y_true (what the model should learn to associate to x)A loss function, loss (meant to measure the gap between the model’s currentpredictions and y_true)You can use W to compute a target candidate y_pred, and then compute the loss, ormismatch, between the target candidate y_pred and the target y_true:y_pred = dot(W, x) loss_value = loss(y_pred, y_true) Now we’d like to use gradients to figure out how to update W so as to make loss_valuesmaller. How do we do that? Given fixed inputs x and y_true, the preceding operations can be interpreted as afunction mapping values of W (the model’s weights) to loss values:loss_value = f(W) Let’s say the current value of W is W0. Then the derivative of f at the point W0 is a ten-sor grad(loss_value, W0), with the same shape as W, where each coefficientgrad(loss_value, W0)[i, j] indicates the direction and magnitude of the change inloss_value y o u o b s e r v e w h e n m o d i f y i n g W0[i, j]. That tensor grad(loss_value,W0) is the gradient of the function f(W) = loss_value in W0, also called “gradient ofloss_value with respect to W around W0.”We use the model weights, W, to make a prediction for x.We estimate how far off the prediction was.f describes the curve (or high-dimensional surface) formed by loss values when W varies. 52CHAPTER 2The mathematical building blocks of neural networks Concretely, what does grad(loss_value, W0) represent? You saw earlier that the deriva-tive of a function f(x) of a single coefficient can be interpreted as the slope of the curveof f. Likewise, grad(loss_value, W0) can be interpreted as the tensor describing thedirection of steepest ascent o f loss_value = f(W) a r o u n d W0, as well as the slope of thisascent. Each partial derivative describes the slope of f in a specific direction. For this reason, in much the same way that, for a function f(x), you can reducethe value of f(x) by moving x a little in the opposite direction from the derivative,with a function f(W) of a tensor, you can reduce loss_value = f(W) by moving W in theopposite direction from the gradient: for example, W1 = W0 - step * grad(f(W0), W0)(where step is a small scaling factor). That means going against the direction of steep-est ascent of f, which intuitively should put you lower on the curve. Note that the scalingfactor step is needed because grad(loss_value, W0) only approximates the curva-ture when you’re close to W0, so you don’t want to get too far from W0. 2.4.3 Stochastic gradient descentGiven a differentiable function, it’s theoretically possible to find its minimum analyti-cally: it’s known that a function’s minimum is a point where the derivative is 0, so allyou have to do is find all the points where the derivative goes to 0 and check for whichof these points the function has the lowest value. Applied to a neural network, that means finding analytically the combination ofweight values that yields the smallest possible loss function. This can be done by solv-ing the equation grad(f(W), W) = 0 for W. This is a polynomial equation of N variables,where N is the number of coefficients in the model. Although it would be possible tosolve such an equation for N = 2 or N = 3, doing so is intractable for real neural net-works, where the number of parameters is never less than a few thousand and canoften be several tens of millions. Instead, you can use the four-step algorithm outlined at the beginning of thissection: modify the parameters little by little based on the current loss value for arandom batch of data. Because you’re dealing with a differentiable function, youcan compute its gradient, which gives you an efficient way to implement step 4. Ifyou update the weights in the opposite direction from the gradient, the loss will be alittle less every time:1Draw a batch of training samples, x, and corresponding targets, y_true.2Run the model on x to obtain predictions, y_pred (this is called the forward pass).Partial derivativesThe tensor operation grad(f(W), W) ( w h i c h t a k e s a s i n p u t a m a t r i x W) can beexpressed as a combination of scalar functions, grad_ij(f(W), w_ij), each ofwhich would return the derivative of loss_value = f(W) with respect to the coeffi-cient W[i, j] of W, assuming all other coefficients are constant. grad_ij is calledthe partial derivative of f with respect to W[i, j]. 53The engine of neural networks: Gradient-based optimization3Compute the loss of the model on the batch, a measure of the mismatchbetween y_pred and y_true.4Compute the gradient of the loss with regard to the model’s parameters (this iscalled the backward pass).5Move the parameters a little in the opposite direction from the gradient—forexample, W -= learning_rate * gradient—thus reducing the loss on the batcha bit. The learning rate (learning_rate here) would be a scalar factor modulat-ing the “speed” of the gradient descent process.Easy enough! What we just described is called mini-batch stochastic gradient descent(mini-batch SGD). The term stochastic r e f e r s t o t h e f a c t t h a t e a c h b a t c h o f d a t a i sdrawn at random (stochastic is a scientific synonym of random). Figure 2.18 illustrateswhat happens in 1D, when the model has only one parameter and you have only onetraining sample. As you can see, intuitively it’s important to pick a reasonable value for the learning_rate factor. If it’s too small, the descent down the curve will take many iterations, andit could get stuck in a local minimum. If learning_rate is too large, your updates mayend up taking you to completely random locations on the curve. Note that a variant of the mini-batch SGD algorithm would be to draw a single sam-ple and target at each iteration, rather than drawing a batch of data. This would betrue SGD (as opposed to mini-batch SGD). Alternatively, going to the opposite extreme,you could run every step on all d a t a a v a i l a b l e , w h i c h i s c a l l e d batch gradient descent.Each update would then be more accurate, but far more expensive. The efficient com-promise between these two extremes is to use mini-batches of reasonable size. Although figure 2.18 illustrates gradient descent in a 1D parameter space, in prac-tice you’ll use gradient descent in highly dimensional spaces: every weight coefficientin a neural network is a free dimension in the space, and there may be tens of thou-sands or even millions of them. To help you build intuition about loss surfaces, youcan also visualize gradient descent along a 2D loss surface, as shown in figure 2.19. Butyou can’t possibly visualize what the actual process of training a neural network looksLossvalueStartingpoint (t=0)Learning ratet=1t=2t=3ParametervalueFigure 2.18 SGD down a 1D loss curve (one learnable parameter) 54CHAPTER 2The mathematical building blocks of neural networkslike—you can’t represent a 1,000,000-dimensional space in a way that makes sense tohumans. As such, it’s good to keep in mind that the intuitions you develop throughthese low-dimensional representations may not always be accurate in practice. Thishas historically been a source of issues in the world of deep learning research. Additionally, there exist multiple variants of SGD that differ by taking into accountprevious weight updates when computing the next weight update, rather than justlooking at the current value of the gradients. There is, for instance, SGD with momen-tum, as well as Adagrad, RMSprop, and several others. Such variants are known as opti-mization methods or optimizers. In particular, the concept of momentum, which is used inmany of these variants, deserves your attention. Momentum addresses two issues withSGD: convergence speed and local minima. Consider figure 2.20, which shows thecurve of a loss as a function of a model parameter. As you can see, around a certain parameter value, there is a local minimum: aroundthat point, moving left would result in the loss increasing, but so would moving right. Starting point Final point45403530252015105Figure 2.19 Gradient descent down a 2D loss surface (two learnable parameters) Lossvalue ParametervalueLocalminimumGlobalminimumFigure 2.20 A local minimum and a global minimum 55The engine of neural networks: Gradient-based optimizationIf the parameter under consideration were being optimized via SGD with a smalllearning rate, the optimization process could get stuck at the local minimum insteadof making its way to the global minimum. Yo u c a n a v o i d s u c h i s s ue s b y u s i n g m o m e n t u m , w h i c h d r a w s i n s p i r a t i o n f r o mphysics. A useful mental image here is to think of the optimization process as a smallball rolling down the loss curve. If it has enough momentum, the ball won’t getstuck in a ravine and will end up at the global minimum. Momentum is imple-mented by moving the ball at each step based not only on the current slope value(current acceleration) but also on the current velocity (resulting from past accelera-tion). In practice, this means updating the parameter w based not only on the cur-rent gradient value but also on the previous parameter update, such as in this naiveimplementation:past_velocity = 0. momentum = 0.1 while loss > 0.01: w, loss, gradient = get_current_parameters() velocity = past_velocity * momentum - learning_rate * gradient w = w + momentum * velocity - learning_rate * gradient past_velocity = velocity update_parameter(w)2.4.4 Chaining derivatives: The Backpropagation algorithmIn the preceding algorithm, we casually assumed that because a function is differentia-ble, we can easily compute its gradient. But is that true? How can we compute the gra-dient of complex expressions in practice? In the two-layer model we started thechapter with, how can we get the gradient of the loss with regard to the weights?That’s where the Backpropagation algorithm comes in.THE CHAIN RULEBackpropagation is a way to use the derivatives of simple operations (such as addition,relu, or tensor product) to easily compute the gradient of arbitrarily complex combi-nations of these atomic operations. Crucially, a neural network consists of many tensoroperations chained together, each of which has a simple, known derivative. Forinstance, the model defined in listing 2.2 can be expressed as a function parameter-ized by the variables W1, b1, W2, and b2 (belonging to the first and second Dense layersrespectively), involving the atomic operations dot, relu, softmax, and +, as well as ourloss function loss, which are all easily differentiable:loss_value = loss(y_true, softmax(dot(relu(dot(inputs, W1) + b1), W2) + b2))Calculus tells us that such a chain of functions can be derived using the followingidentity, called the chain rule. Consider two functions f and g, as well as the composed function fg such thatfg(x) == f(g(x)):Constant momentum factorOptimization loop 56CHAPTER 2The mathematical building blocks of neural networksdef fg(x): x1 = g(x) y = f(x1) return yThen the chain rule states that grad(y, x) == grad(y, x1) * grad(x1, x). Thisenables you to compute the derivative of fg as long as you know the derivatives of fand g. The chain rule is named as it is because when you add more intermediate func-tions, it starts looking like a chain:def fghj(x): x1 = j(x) x2 = h(x1) x3 = g(x2) y = f(x3) return ygrad(y, x) == (grad(y, x3) * grad(x3, x2) * grad(x2, x1) * grad(x1, x))Applying the chain rule to the computation of thegradient values of a neural network gives rise to analgorithm called backpropagation. Let’s see how thatworks, concretely. AUTOMATIC DIFFERENTIATION WITH COMPUTATION GRAPHSA useful way to think about backpropagation is interms of computation graphs. A computation graph isthe data structure at the heart of TensorFlow and thedeep learning revolution in general. It’s a directedacyclic graph of operations—in our case, tensoroperations. For instance, figure 2.21 shows the graphrepresentation of our first model. C o m p u t a t i o n g r a p h s h a v e b e e n a n e x t r e m e l ysuccessful abstraction in computer science becausethey enable us to treat computation as data: a comput-able expression is encoded as a machine-readabledata structure that can be used as the input or out-put of another program. For instance, you couldimagine a program that receives a computationgraph and returns a new computation graph thatimplements a large-scale distributed version of thesame computation—this would mean that you coulddistribute any computation without having to writethe distribution logic yourself. Or imagine a pro-gram that receives a computation graph and candot+xW1b1 loss_valreludot+W2b2softmaxlossy_trueFigure 2.21 The computation graph representation of our two-layer model 57The engine of neural networks: Gradient-based optimizationautomatically generate the derivative of the expression it represents. It’s much easierto do these things if your computation is expressed as an explicit graph data structurerather than, say, lines of ASCII characters in a .py file. To explain backpropagation clearly, let’s look at a really basic example of a com-putation graph (see figure 2.22). We’ll consider a simplified version of figure 2.21,where we only have one linear layer and where all variables are scalar. We’ll take twoscalar variables w and b, a scalar input x, and apply some operations to them to com-bine them into an output y. Finally, we’ll apply an absolute value error-loss function:loss_val = abs(y_true - y). Since we want to update w and b in a way that will min-imize loss_val, we are interested in computing grad(loss_val, b) and grad(loss_val, w). Let’s set concrete values for the “input nodes” in the graph, that is to say, the inputx, the target y_true, w, and b. We’ll propagate these values to all nodes in thegraph, from top to bottom, until we reach loss_val. This is the forward pass (seefigure 2.23). Now let’s “reverse” the graph: for each edge in the graph going from A to B, we willcreate an opposite edge from B to A, and ask, how much does B vary when A varies?That is to say, what is grad(B, A)? We’ll annotate each inverted edge with this value.This backward graph represents the backward pass (see figure 2.24). *+lossloss_valx1x2x bw y_trueFigure 2.22 A basic example of a computation graph 58CHAPTER 2The mathematical building blocks of neural networks *+lossloss_val = 32 x1 = 6x2 = 7x b1w3 4y_trueFigure 2.23 Running a forward pass *+lossloss_val2 x1x2 grad(loss_val, x2) = 1grad(x2, x1) = 1grad(x1, w) = 2grad(x2, b) = 1x b1w3 4y_trueFigure 2.24 Running a backward pass 59The engine of neural networks: Gradient-based optimizationWe have the following:grad(loss_val, x2) = 1, because as x2 varies by an amount epsilon, loss_val =abs(4 - x2) varies by the same amount.grad(x2, x1) = 1, because as x1 varies by an amount epsilon, x2 = x1 + b = x1 +1 varies by the same amount.grad(x2, b) = 1, because as b varies by an amount epsilon, x2 = x1 + b = 6 + bvaries by the same amount.grad(x1, w) = 2, because as w varies by an amount epsilon, x1 = x * w = 2 * w var-ies by 2 * epsilon.What the chain rule says about this backward graph is that you can obtain the deriva-tive of a node with respect to another node by multiplying the derivatives for each edgealong the path linking the two nodes. For instance, grad(loss_val, w) = grad(loss_val,x2) * grad(x2, x1) * grad(x1, w) (see figure 2.25). By applying the chain rule to our graph, we obtain what we were looking for:grad(loss_val, w) = 1 * 1 * 2 = 2grad(loss_val, b) = 1 * 1 = 1*+abs_diffloss_val2 x1x2 grad(loss_val, x2) = 1grad(x2, x1) = 1grad(x1, w) = 2grad(x2, b) = 1x b1w3 4y_trueFigure 2.25 Path from loss_val to w in the backward graph 60CHAPTER 2The mathematical building blocks of neural networksNOTEIf there are multiple paths linking the two nodes of interest, a and b, inthe backward graph, we would obtain grad(b, a) by summing the contribu-tions of all the paths.And with that, you just saw backpropagation in action! Backpropagation is simply theapplication of the chain rule to a computation graph. There’s nothing more to it.Backpropagation starts with the final loss value and works backward from the top lay-ers to the bottom layers, computing the contribution that each parameter had in theloss value. That’s where the name “backpropagation” comes from: we “back propa-gate” the loss contributions of different nodes in a computation graph. N o w a d a y s p e o p l e i m p l e m e n t n e u r a l n e t w o r k s i n m o d e r n f r a m e w o r k s t h a t a r ecapable of automatic differentiation, such as TensorFlow. Automatic differentiation isimplemented with the kind of computation graph you’ve just seen. Automatic differ-entiation makes it possible to retrieve the gradients of arbitrary compositions of differ-entiable tensor operations without doing any extra work besides writing down theforward pass. When I wrote my first neural networks in C in the 2000s, I had to writemy gradients by hand. Now, thanks to modern automatic differentiation tools, you’llnever have to implement backpropagation yourself. Consider yourself lucky!THE GRADIENT TAPE IN TENSORFLOWThe API through which you can leverage TensorFlow’s powerful automatic differenti-ation capabilities is the GradientTape. It’s a Python scope that will “record” the tensoroperations that run inside it, in the form of a computation graph (sometimes called a“tape”). This graph can then be used to retrieve the gradient of any output withrespect to any variable or set of variables (instances of the tf.Variable class). Atf.Variable is a specific kind of tensor meant to hold mutable state—for instance,the weights of a neural network are always tf.Variable instances.import tensorflow as tfx = tf.Variable(0.) with tf.GradientTape() as tape: y = 2 * x + 3 grad_of_y_wrt_x = tape.gradient(y, x) The GradientTape works with tensor operations:x = tf.Variable(tf.random.uniform((2, 2))) with tf.GradientTape() as tape: y = 2 * x + 3 grad_of_y_wrt_x = tape.gradient(y, x) Instantiate a scalar Variablewith an initial value of 0.Open a GradientTape scope.Inside the scope, apply some tensor operations to our variable.Use the tape to retrieve the gradient of the output y with respect to our variable x.Instantiate a Variable with shape (2, 2) and an initial value of all zeros.grad_of_y_wrt_x is a tensor of shape (2, 2) (like x) describing the curvature of y = 2 * a + 3 around x = [[0, 0], [0, 0]]. 61Looking back at our first exampleIt also works with lists of variables:W = tf.Variable(tf.random.uniform((2, 2)))b = tf.Variable(tf.zeros((2,)))x = tf.random.uniform((2, 2)) with tf.GradientTape() as tape: y = tf.matmul(x, W) + b grad_of_y_wrt_W_and_b = tape.gradient(y, [W, b]) You will learn about the gradient tape in the next chapter. 2.5 Looking back at our first exampleYou’re nearing the end of this chapter, and you should now have a general under-standing of what’s going on behind the scenes in a neural network. What was a mag-ical black box at the start of the chapter has turned into a clearer picture, asillustrated in figure 2.26: the model, composed of layers that are chained together,maps the input data to predictions. The loss function then compares these predic-tions to the targets, producing a loss value: a measure of how well the model’s pre-dictions match what was expected. The optimizer uses this loss value to update themodel’s weights. Let’s go back to the first example in this chapter and review each piece of it in thelight of what you’ve learned since. This was the input data:(train_images, train_labels), (test_images, test_labels) = mnist.load_data()train_images = train_images.reshape((60000, 28 * 28))train_images = train_images.astype(\"float32\") / 255 test_images = test_images.reshape((10000, 28 * 28))test_images = test_images.astype(\"float32\") / 255matmul is how you say “dot product” in TensorFlow.grad_of_y_wrt_W_and_b is a list of two tensors with the same shapes as W and b, respectively. Layer(data transformation)Input XWeightsLayer(data transformation)PredictionsY'WeightupdateTrue targetsYWeightsLoss function OptimizerLoss scoreFigure 2.26 Relationship between the network, layers, loss function, and optimizer 62CHAPTER 2The mathematical building blocks of neural networksNow you understand that the input images are stored in NumPy tensors, which arehere formatted as float32 tensors of shape (60000, 784) (training data) and (10000,784) (test data) respectively. This was our model:model = keras.Sequential([ layers.Dense(512, activation=\"relu\"), layers.Dense(10, activation=\"softmax\")])Now you understand that this model consists of a chain of two Dense layers, that eachlayer applies a few simple tensor operations to the input data, and that these opera-tions involve weight tensors. Weight tensors, which are attributes of the layers, arewhere the knowledge of the model persists. This was the model-compilation step:model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])Now you understand that sparse_categorical_crossentropy is the loss functionthat’s used as a feedback signal for learning the weight tensors, and which the train-ing phase will attempt to minimize. You also know that this reduction of the losshappens via mini-batch stochastic gradient descent. The exact rules governing a spe-cific use of gradient descent are defined by the rmsprop optimizer passed as the firstargument. Finally, this was the training loop:model.fit(train_images, train_labels, epochs=5, batch_size=128)Now you understand what happens when you call fit: the model will start to iterateon the training data in mini-batches of 128 samples, 5 times over (each iteration overall the training data is called an epoch). For each batch, the model will compute thegradient of the loss with regard to the weights (using the Backpropagation algorithm,which derives from the chain rule in calculus) and move the weights in the directionthat will reduce the value of the loss for this batch. After these 5 epochs, the model will have performed 2,345 gradient updates (469per epoch), and the loss of the model will be sufficiently low that the model will becapable of classifying handwritten digits with high accuracy. At this point, you already know most of what there is to know about neural net-works. Let’s prove it by reimplementing a simplified version of that first example“from scratch” in TensorFlow, step by step. 63Looking back at our first example2.5.1 Reimplementing our first example from scratch in TensorFlowWhat better demonstrates full, unambiguous understanding than implementing every-thing from scratch? Of course, what “from scratch” means here is relative: we won’treimplement basic tensor operations, and we won’t implement backpropagation. Butwe’ll go to such a low level that we will barely use any Keras functionality at all. Don’t worry if you don’t understand every little detail in this example just yet. Thenext chapter will dive in more detail into the TensorFlow API. For now, just try to fol-low the gist of what’s going on—the intent of this example is to help crystalize yourunderstanding of the mathematics of deep learning using a concrete implementation.Let’s go!A SIMPLE DENSE CLASSYou’ve learned earlier that the Dense layer implements the following input transfor-mation, where W a n d b a r e m o d e l p a r a m e t e r s , a n d activation i s a n e l e m e n t - w i s efunction (usually relu, but it would be softmax for the last layer):output = activation(dot(W, input) + b)Let’s implement a simple Python class, NaiveDense, that creates two TensorFlowvariables, W a n d b, and exposes a __call__() m e t h o d t h a t a p p l i e s t h e p r e c e d i n gtransformation.import tensorflow as tf class NaiveDense: def __init__(self, input_size, output_size, activation): self.activation = activation w_shape = (input_size, output_size) w_initial_value = tf.random.uniform(w_shape, minval=0, maxval=1e-1) self.W = tf.Variable(w_initial_value) b_shape = (output_size, b_initial_value = tf.zeros(b_shape) self.b = tf.Variable(b_initial_value) def __call__(self, inputs):: return self.activation(tf.matmul(inputs, self.W) + self.b) @property def weights(self): return [self.W, self.b]A SIMPLE SEQUENTIAL CLASSNow, let’s create a NaiveSequential class to chain these layers. It wraps a list of layersand exposes a __call__() m e t h o d t h a t s i m p l y c a l l s t h e u n d e r l y i n g l a y e r s o n t h einputs, in order. It also features a weights property to easily keep track of the layers’parameters.Create a matrix, W, of shape (input_size, output_size), initialized with random values.Create a vector, b, of shape (output_size,), initialized with zeros.Apply the forward pass.Convenience method for retrieving the layer’s weights 64CHAPTER 2The mathematical building blocks of neural networksclass NaiveSequential: def __init__(self, layers): self.layers = layers def __call__(self, inputs): x = inputs for layer in self.layers: x = layer(x) return x @property def weights(self): weights = [] for layer in self.layers: weights += layer.weights return weightsUsing this NaiveDense c l a s s a n d t h i s NaiveSequential class, w e can c reate a moc kKeras model:model = NaiveSequential([ NaiveDense(input_size=28 * 28, output_size=512, activation=tf.nn.relu), NaiveDense(input_size=512, output_size=10, activation=tf.nn.softmax)]) assert len(model.weights) == 4 A BATCH GENERATORNext, we need a way to iterate over the MNIST data in mini-batches. This is easy:import math class BatchGenerator: def __init__(self, images, labels, batch_size=128): assert len(images) == len(labels) self.index = 0 self.images = images self.labels = labels self.batch_size = batch_size self.num_batches = math.ceil(len(images) / batch_size) def next(self): images = self.images[self.index : self.index + self.batch_size] labels = self.labels[self.index : self.index + self.batch_size] self.index += self.batch_size return images, labels2.5.2 Running one training stepThe most difficult part of the process is the “training step”: updating the weights ofthe model after running it on one batch of data. We need to1Compute the predictions of the model for the images in the batch.2Compute the loss value for these predictions, given the actual labels. 65Looking back at our first example3Compute the gradient of the loss with regard to the model’s weights.4Move the weights by a small amount in the direction opposite to the gradient.To compute the gradient, we will use the TensorFlow GradientTape object we intro-duced in section 2.4.4:def one_training_step(model, images_batch, labels_batch): with tf.GradientTape() as tape: predictions = model(images_batch) per_sample_losses = tf.keras.losses.sparse_categorical_crossentropy( labels_batch, predictions) average_loss = tf.reduce_mean(per_sample_losses) gradients = tape.gradient(average_loss, model.weights) update_weights(gradients, model.weights) return average_lossAs you already know, the purpose of the “weight update” step (represented by the pre-ceding update_weights function) is to move the weights by “a bit” in a direction thatwill reduce the loss on this batch. The magnitude of the move is determined by the“learning rate,” typically a small quantity. The simplest way to implement thisupdate_weights function is to subtract gradient * learning_rate from each weight:learning_rate = 1e-3 def update_weights(gradients, weights): for g, w in zip(gradients, weights): w.assign_sub(g * learning_rate) In practice, you would almost never implement a weight update step like this by hand.Instead, you would use an Optimizer instance from Keras, like this:from tensorflow.keras import optimizers optimizer = optimizers.SGD(learning_rate=1e-3) def update_weights(gradients, weights): optimizer.apply_gradients(zip(gradients, weights))Now that our per-batch training step is ready, we can move on to implementing anentire epoch of training. 2.5.3 The full training loopAn epoch of training simply consists of repeating the training step for each batch inthe training data, and the full training loop is simply the repetition of one epoch:def fit(model, images, labels, epochs, batch_size=128): for epoch_counter in range(epochs): print(f\"Epoch {epoch_counter}\")Run the “forwardpass” (computethe model’spredictions undera GradientTapescope).Compute the gradient of the loss withregard to the weights. The output gradientsis a list where each entry corresponds toa weight from the model.weights list.Update the weights using the gradients (we will define this function shortly). assign_sub is the equivalent of -= for TensorFlow variables. 66CHAPTER 2The mathematical building blocks of neural networks batch_generator = BatchGenerator(images, labels) for batch_counter in range(batch_generator.num_batches): images_batch, labels_batch = batch_generator.next() loss = one_training_step(model, images_batch, labels_batch) if batch_counter % 100 == 0: print(f\"loss at batch {batch_counter}: {loss:.2f}\")Let’s test drive it:from tensorflow.keras.datasets import mnist(train_images, train_labels), (test_images, test_labels) = mnist.load_data() train_images = train_images.reshape((60000, 28 * 28))train_images = train_images.astype(\"float32\") / 255 test_images = test_images.reshape((10000, 28 * 28))test_images = test_images.astype(\"float32\") / 255 fit(model, train_images, train_labels, epochs=10, batch_size=128)2.5.4 Evaluating the modelWe can evaluate the model by taking the argmax of its predictions over the test images,and comparing it to the expected labels:predictions = model(test_images)predictions = predictions.numpy() predicted_labels = np.argmax(predictions, axis=1)matches = predicted_labels == test_labelsprint(f\"accuracy: {matches.mean():.2f}\")All done! As you can see, it’s quite a bit of work to do “by hand” what you can do in afew lines of Keras code. But because you’ve gone through these steps, you should nowhave a crystal clear understanding of what goes on inside a neural network when youcall fit(). Having this low-level mental model of what your code is doing behind thescenes will make you better able to leverage the high-level features of the Keras API. SummaryTensors form the foundation of modern machine learning systems. They comein various flavors of dtype, rank, and shape.You can manipulate numerical tensors via tensor operations ( s u c h a s a d d i t i o n ,tensor product, or element-wise multiplication), which can be interpreted asencoding geometric transformations. In general, everything in deep learning isamenable to a geometric interpretation.Deep learning models consist of chains of simple tensor operations, parameter-ized by weights, which are themselves tensors. The weights of a model are whereits “knowledge” is stored.Learning means finding a set of values for the model’s weights that minimizes a lossfunction for a given set of training data samples and their corresponding targets.Calling .numpy() on a TensorFlow tensor converts it to a NumPy tensor. 67SummaryLearning happens by drawing random batches of data samples and their tar-gets, and computing the gradient of the model parameters with respect to theloss on the batch. The model parameters are then moved a bit (the magnitudeof the move is defined by the learning rate) in the opposite direction from thegradient. This is called mini-batch stochastic gradient descent.The entire learning process is made possible by the fact that all tensor operationsin neural networks are differentiable, and thus it’s possible to apply the chain ruleof derivation to find the gradient function mapping the current parameters andcurrent batch of data to a gradient value. This is called backpropagation.Two key concepts you’ll see frequently in future chapters are loss and optimizers.These are the two things you need to define before you begin feeding data intoa model.–T h e loss i s t h e q u a n t i t y y o u ’ l l a t t e m p t t o m i n i m i z e d u r i n g t r a i n i n g , s o i tshould represent a measure of success for the task you’re trying to solve.–T h e optimizer specifies the exact way in which the gradient of the loss will beused to update parameters: for instance, it could be the RMSProp optimizer,SGD with momentum, and so on. 68Introduction to Kerasand TensorFlow This chapter is meant to give you everything you need to start doing deep learningin practice. I’ll give you a quick presentation of Keras (https:/ /keras.io) and Tensor-Flow (https:/ /tensorflow.org), the Python-based deep learning tools that we’ll usethroughout the book. You’ll find out how to set up a deep learning workspace, withTensorFlow, Keras, and GPU support. Finally, building on top of the first contactyou had with Keras and TensorFlow in chapter 2, we’ll review the core componentsof neural networks and how they translate to the Keras and TensorFlow APIs. By the end of this chapter, you’ll be ready to move on to practical, real-worldapplications, which will start with chapter 4.This chapter coversA closer look at TensorFlow, Keras, and their relationshipSetting up a deep learning workspaceAn overview of how core deep learning concepts translate to Keras and TensorFlow 69What’s Keras?3.1 What’s TensorFlow?TensorFlow is a Python-based, free, open source machine learning platform, devel-oped primarily by Google. Much like NumPy, the primary purpose of TensorFlow is toenable engineers and researchers to manipulate mathematical expressions overnumerical tensors. But TensorFlow goes far beyond the scope of NumPy in the follow-ing ways:It can automatically compute the gradient of any differentiable expression (asyou saw in chapter 2), making it highly suitable for machine learning.It can run not only on CPUs, but also on GPUs and TPUs, highly parallel hard-ware accelerators.Computation defined in TensorFlow can be easily distributed across manymachines.TensorFlow programs can be exported to other runtimes, such as C++, Java-Script (for browser-based applications), or TensorFlow Lite (for applicationsrunning on mobile devices or embedded devices), etc. This makes TensorFlowapplications easy to deploy in practical settings.It’s important to keep in mind that TensorFlow is much more than a single library. It’sreally a platform, home to a vast ecosystem of components, some developed by Googleand some developed by third parties. For instance, there’s TF-Agents for reinforce-ment-learning research, TFX for industry-strength machine learning workflow man-agement, TensorFlow Serving for production deployment, and there’s the TensorFlowHub repository of pretrained models. Together, these components cover a very widerange of use cases, from cutting-edge research to large-scale production applications. TensorFlow scales fairly well: for instance, scientists from Oak Ridge National Labhave used it to train a 1.1 exaFLOPS extreme weather forecasting model on the27,000 GPUs of the IBM Summit supercomputer. Likewise, Google has used Tensor-Flow to develop very compute-intensive deep learning applications, such as the chess-playing and Go-playing agent AlphaZero. For your own models, if you have the bud-get, you can realistically hope to scale to around 10 petaFLOPS on a small TPU pod ora large cluster of GPUs rented on Google Cloud or AWS. That would still be around1% of the peak compute power of the top supercomputer in 2019!3.2 What’s Keras?Keras is a deep learning API for Python, built on top of TensorFlow, that provides a con-venient way to define and train any kind of deep learning model. Keras was initiallydeveloped for research, with the aim of enabling fast deep learning experimentation. Through TensorFlow, Keras can run on top of different types of hardware (see fig-ure 3.1)—GPU, TPU, or plain CPU—and can be seamlessly scaled to thousands ofmachines. Keras is known for prioritizing the developer experience. It’s an API for humanbeings, not machines. It follows best practices for reducing cognitive load: it offers 70CHAPTER 3Introduction to Keras and TensorFlow consistent and simple workflows, it minimizes the number of actions required for com-mon use cases, and it provides clear and actionable feedback upon user error. Thismakes Keras easy to learn for a beginner, and highly productive to use for an expert. Keras has well over a million users as of late 2021, ranging from academic research-ers, engineers, and data scientists at both startups and large companies to graduatestudents and hobbyists. Keras is used at Google, Netflix, Uber, CERN, NASA, Yelp,Instacart, Square, and hundreds of startups working on a wide range of problemsacross every industry. Your YouTube recommendations originate from Keras models.The Waymo self-driving cars are developed with Keras models. Keras is also a popularframework on Kaggle, the machine learning competition website, where most deeplearning competitions have been won using Keras. Because Keras has a large and diverse user base, it doesn’t force you to follow a sin-gle “true” way of building and training models. Rather, it enables a wide range of dif-ferent workflows, from the very high level to the very low level, corresponding todifferent user profiles. For instance, you have an array of ways to build models and anarray of ways to train them, each representing a certain trade-off between usability andflexibility. In chapter 5, we’ll review in detail a good fraction of this spectrum of work-flows. You could be using Keras like you would use Scikit-learn—just calling fit() andletting the framework do its thing—or you could be using it like NumPy—taking fullcontrol of every little detail. This means that everything you’re learning now as you’re getting started will stillbe relevant once you’ve become an expert. You can get started easily and then gradu-ally dive into workflows where you’re writing more and more logic from scratch. Youwon’t have to switch to an entirely different framework as you go from student toresearcher, or from data scientist to deep learning engineer. This philosophy is not unlike that of Python itself! Some languages only offer oneway to write programs—for instance, object-oriented programming or functional pro-gramming. Meanwhile, Python is a multiparadigm language: it offers an array of possi-ble usage patterns that all work nicely together. This makes Python suitable to a widerange of very different use cases: system administration, data science, machine learningCPUGPUTPUTensorFlowKerasDeep learning development:layers, models, optimizers, losses,metrics...Tensor manipulation infrastructure:tensors, variables, automaticdiﬀerentiation, distribution...Hardware: executionFigure 3.1 Keras and TensorFlow: TensorFlow is a low-level tensor computing platform, and Keras is a high-level deep learning API 71Setting up a deep learning workspaceengineering, web development . . . or just learning how to program. Likewise, you canthink of Keras as the Python of deep learning: a user-friendly deep learning languagethat offers a variety of workflows to different user profiles. 3.3 Keras and TensorFlow: A brief historyKeras predates TensorFlow by eight months. It was released in March 2015, andTensorFlow was released in November 2015. You may ask, if Keras is built on topof TensorFlow, how it could exist before TensorFlow was released? Keras was originallybuilt on top of Theano, another tensor-manipulation library that provided automaticdifferentiation and GPU support—the earliest of its kind. Theano, developed at theMontréal Institute for Learning Algorithms (MILA) at the Université de Montréal,was in many ways a precursor of TensorFlow. It pioneered the idea of using static com-putation graphs for automatic differentiation and for compiling code to both CPUand GPU. In late 2015, after the release of TensorFlow, Keras was refactored to a multiback-end architecture: it became possible to use Keras with either Theano or TensorFlow,and switching between the two was as easy as changing an environment variable. BySeptember 2016, TensorFlow had reached a level of technical maturity where it becamepossible to make it the default backend option for Keras. In 2017, two new addi-tional backend options were added to Keras: CNTK (developed by Microsoft) andMXNet (developed by Amazon). Nowadays, both Theano and CNTK are out of devel-opment, and MXNet is not widely used outside of Amazon. Keras is back to being asingle-backend API—on top of TensorFlow. Keras and TensorFlow have had a symbiotic relationship for many years. Through-out 2016 and 2017, Keras became well known as the user-friendly way to develop Ten-sorFlow applications, funneling new users into the TensorFlow ecosystem. By late2017, a majority of TensorFlow users were using it through Keras or in combinationwith Keras. In 2018, the TensorFlow leadership picked Keras as TensorFlow’s officialhigh-level API. As a result, the Keras API is front and center in TensorFlow 2.0,released in September 2019—an extensive redesign of TensorFlow and Keras thattakes into account over four years of user feedback and technical progress. By this point, you must be eager to start running Keras and TensorFlow code inpractice. Let’s get you started. 3.4 Setting up a deep learning workspaceBefore you can get started developing deep learning applications, you need to set upyour development environment. It’s highly recommended, although not strictly nec-essary, that you run deep learning code on a modern NVIDIA GPU rather than yourcomputer’s CPU. Some applications—in particular, image processing with convolu-tional networks—will be excruciatingly slow on CPU, even a fast multicore CPU. Andeven for applications that can realistically be run on CPU, you’ll generally see thespeed increase by a factor of 5 or 10 by using a recent GPU. 72CHAPTER 3Introduction to Keras and TensorFlow To do deep learning on a GPU, you have three options:Buy and install a physical NVIDIA GPU on your workstation.Use GPU instances on Google Cloud or AWS EC2.Use the free GPU runtime from Colaboratory, a hosted notebook serviceoffered by Google (for details about what a “notebook” is, see the next section).Colaboratory is the easiest way to get started, as it requires no hardware purchase andno software installation—just open a tab in your browser and start coding. It’s theoption we recommend for running the code examples in this book. However, the freeversion of Colaboratory is only suitable for small workloads. If you want to scale up,you’ll have to use the first or second option. If you don’t already have a GPU that you can use for deep learning (a recent, high-end NVIDIA GPU), then running deep learning experiments in the cloud is a simple,low-cost way for you to move to larger workloads without having to buy any additionalhardware. If you’re developing using Jupyter notebooks, the experience of running inthe cloud is no different from running locally. But if you’re a heavy user of deep learning, this setup isn’t sustainable in the longterm—or even for more than a few months. Cloud instances aren’t cheap: you’d pay$2.48 per hour for a V100 GPU on Google Cloud in mid-2021. Meanwhile, a solidconsumer-class GPU will cost you somewhere between $1,500 and $2,500—a pricethat has been fairly stable over time, even as the specs of these GPUs keep improving.If you’re a heavy user of deep learning, consider setting up a local workstation withone or more GPUs. Additionally, whether you’re running locally or in the cloud, it’s better to be usinga Unix workstation. Although it’s technically possible to run Keras on Windowsdirectly, we don’t recommend it. If you’re a Windows user and you want to do deeplearning on your own workstation, the simplest solution to get everything running isto set up an Ubuntu dual boot on your machine, or to leverage Windows Subsystemfor Linux (WSL), a compatibility layer that enables you to run Linux applicationsfrom Windows. It may seem like a hassle, but it will save you a lot of time and troublein the long run.3.4.1 Jupyter notebooks: The preferred way to run deep learning experimentsJupyter notebooks are a great way to run deep learning experiments—in particular,the many code examples in this book. They’re widely used in the data science andmachine learning communities. A notebook is a file generated by the Jupyter Notebookapp (https:/ /jupyter.org) that you can edit in your browser. It mixes the ability to exe-cute Python code with rich text-editing capabilities for annotating what you’re doing. Anotebook also allows you to break up long experiments into smaller pieces that can beexecuted independently, which makes development interactive and means you don’thave to rerun all of your previous code if something goes wrong late in an experiment. 73Setting up a deep learning workspace I recommend using Jupyter notebooks to get started with Keras, although that isn’ta requirement: you can also run standalone Python scripts or run code from within anIDE such as PyCharm. All the code examples in this book are available as open sourcenotebooks; you can download them from GitHub at github.com/fchollet/deep-learning-with-python-notebooks. 3.4.2 Using ColaboratoryColaboratory (or Colab for short) is a free Jupyter notebook service that requires noinstallation and runs entirely in the cloud. Effectively, it’s a web page that lets youwrite and execute Keras scripts right away. It gives you access to a free (but limited)GPU runtime and even a TPU runtime, so you don’t have to buy your own GPU.Colaboratory is what we recommend for running the code examples in this book.FIRST STEPS WITH COLABORATORYTo get started with Colab, go to https:/ /colab.research.google.com and click the NewNotebook button. You’ll see the standard Notebook interface shown in figure 3.2. You’ll notice two buttons in the toolbar: + Code and + Text. They’re for creating exe-cutable Python code cells and annotation text cells, respectively. After entering codein a code cell, Pressing Shift-Enter will execute it (see figure 3.3). In a text cell, you can use Markdown syntax (see figure 3.4). Pressing Shift-Enteron a text cell will render it. Text cells are useful for giving a readable structure to your notebooks: use them toannotate your code with section titles and long explanation paragraphs or to embedfigures. Notebooks are meant to be a multimedia experience! Figure 3.2 A Colab notebook 74CHAPTER 3Introduction to Keras and TensorFlow INSTALLING PACKAGES WITH PIPThe default Colab environment already comes with TensorFlow and Keras installed,so you can start using it right away without any installation steps required. But if youever need to install something with pip, you can do so by using the following syntax ina code cell (note that the line starts with ! to indicate that it is a shell command ratherthan Python code):!pip install package_nameFigure 3.3 Creating a code cell Figure 3.4 Creating a text cell 75First steps with TensorFlowUSING THE GPU RUNTIMETo use the GPU runtime with Colab, select Runtime > Change Runtime Type in themenu and select GPU for the Hardware Accelerator (see figure 3.5). TensorFlow and Keras will automatically execute on GPU if a GPU is available, sothere’s nothing more you need to do after you’ve selected the GPU runtime. You’ll notice that there’s also a TPU runtime option in that Hardware Acceleratordropdown menu. Unlike the GPU runtime, using the TPU runtime with TensorFlowand Keras does require a bit of manual setup in your code. We’ll cover this in chap-ter 13. For the time being, we recommend that you stick to the GPU runtime to followalong with the code examples in the book. You now have a way to start running Keras code in practice. Next, let’s see how thekey ideas you learned about in chapter 2 translate to Keras and TensorFlow code. 3.5 First steps with TensorFlowAs you saw in the previous chapters, training a neural network revolves around the fol-lowing concepts:First, low-level tensor manipulation—the infrastructure that underlies all mod-ern machine learning. This translates to TensorFlow APIs:–Tensors, including special tensors that store the network’s state (variables)–Tensor operations such as addition, relu, matmulFigure 3.5 Using the GPU runtime with Colab 76CHAPTER 3Introduction to Keras and TensorFlow–Backpropagation, a way to compute the gradient of mathematical expressions(handled in TensorFlow via the GradientTape object)Second, high-level deep learning concepts. This translates to Keras APIs:–Layers, which are combined into a model–A loss function, which defines the feedback signal used for learning–A n optimizer, which determines how learning proceeds–Metrics to evaluate model performance, such as accuracy–A training loop that performs mini-batch stochastic gradient descentIn the previous chapter, you already had a first light contact with some of the corre-sponding TensorFlow and Keras APIs: you’ve briefly used TensorFlow’s Variable class,the matmul operation, and the GradientTape. You’ve instantiated Keras Dense layers,packed them into a Sequential m o d e l , a n d t r a i n e d t h a t m o d e l w i t h t h e fit()method. N o w l e t ’ s t a k e a d e e p e r d i v e i n t o h o w a l l o f t h e s e d i f f e r e n t c o n c e p t s c a n b eapproached in practice using TensorFlow and Keras.3.5.1 Constant tensors and variablesTo do anything in TensorFlow, we’re going to need some tensors. Tensors need to becreated with some initial value. For instance, you could create all-ones or all-zeros ten-sors (see listing 3.1), or tensors of values drawn from a random distribution (see list-ing 3.2).>>> import tensorflow as tf>>> x = tf.ones(shape=(2, 1)) >>> print(x)tf.Tensor([[1.] [1.]], shape=(2, 1), dtype=float32)>>> x = tf.zeros(shape=(2, 1)) >>> print(x)tf.Tensor([[0.] [0.]], shape=(2, 1), dtype=float32)>>> x = tf.random.normal(shape=(3, 1), mean=0., stddev=1.) >>> print(x)tf.Tensor([[-0.14208166] [-0.95319825] [ 1.1096532 ]], shape=(3, 1), dtype=float32)>>> x = tf.random.uniform(shape=(3, 1), minval=0., maxval=1.) >>> print(x)tf.Tensor(Listing 3.1 All-ones or all-zeros tensors Listing 3.2 Random tensorsEquivalent to np.ones(shape=(2, 1))Equivalent to np.zeros(shape=(2, 1)) Tensor of random values drawn from a normal distributionwith mean 0 and standard deviation 1. Equivalent tonp.random.normal(size=(3, 1), loc=0., scale=1.).Tensor of random values drawn from a uniform distribution between 0and 1. Equivalent to np.random.uniform(size=(3, 1), low=0., high=1.). 77First steps with TensorFlow[[0.33779848] [0.06692922] [0.7749394 ]], shape=(3, 1), dtype=float32)A significant difference between NumPy arrays and TensorFlow tensors is that Tensor-Flow tensors aren’t assignable: they’re constant. For instance, in NumPy, you can dothe following.import numpy as npx = np.ones(shape=(2, 2))x[0, 0] = 0.Try to do the same thing in TensorFlow, and you will get an error: “EagerTensor objectdoes not support item assignment.”x = tf.ones(shape=(2, 2))x[0, 0] = 0. To train a model, we’ll need to update its state, which is a set of tensors. If tensorsaren’t assignable, how do we do it? That’s where variables come in. tf.Variable is theclass meant to manage modifiable state in TensorFlow. You’ve already briefly seen it inaction in the training loop implementation at the end of chapter 2. To create a variable, you need to provide some initial value, such as a random tensor.>>> v = tf.Variable(initial_value=tf.random.normal(shape=(3, 1)))>>> print(v)array([[-0.75133973], [-0.4872893 ], [ 1.6626885 ]], dtype=float32)>The state of a variable can be modified via its assign method, as follows.>>> v.assign(tf.ones((3, 1)))array([[1.], [1.], [1.]], dtype=float32)>It also works for a subset of the coefficients. Listing 3.3 NumPy arrays are assignable Listing 3.4 TensorFlow tensors are not assignable Listing 3.5 Creating a TensorFlow variable Listing 3.6 Assigning a value to a TensorFlow variableThis will fail, as a tensor isn’t assignable. 78CHAPTER 3Introduction to Keras and TensorFlow>>> v[0, 0].assign(3.)array([[3.], [1.], [1.]], dtype=float32)>Similarly, assign_add() and assign_sub() are efficient equivalents of += and -=, asshown next. >>> v.assign_add(tf.ones((3, 1)))array([[2.], [2.], [2.]], dtype=float32)>3.5.2 Tensor operations: Doing math in TensorFlowJust like NumPy, TensorFlow offers a large collection of tensor operations to expressmathematical formulas. Here are a few examples.a = tf.ones((2, 2))b = tf.square(a) c = tf.sqrt(a) d = b + c e = tf.matmul(a, b) e *= d Importantly, each of the preceding operations gets executed on the fly: at any point,you can print what the current result is, just like in NumPy. We call this eager execution. 3.5.3 A second look at the GradientTape APISo far, TensorFlow seems to look a lot like NumPy. But here’s something NumPy can’tdo: retrieve the gradient of any differentiable expression with respect to any of itsinputs. Just open a GradientTape scope, apply some computation to one or severalinput tensors, and retrieve the gradient of the result with respect to the inputs.input_var = tf.Variable(initial_value=3.) with tf.GradientTape() as tape: result = tf.square(input_var)gradient = tape.gradient(result, input_var)Listing 3.7 Assigning a value to a subset of a TensorFlow variable Listing 3.8 Using assign_add() Listing 3.9 A few basic math operations Listing 3.10 Using the GradientTapeTake the square.Take the square root.Add two tensors (element-wise).Take the product of two tensors (as discussed in chapter 2).Multiply two tensors(element-wise). 79First steps with TensorFlowThis is most commonly used to retrieve the gradients of the loss of a model withrespect to its weights: gradients = tape.gradient(loss, weights). You saw this inaction in chapter 2. So far, you’ve only seen the case where the input tensors in tape.gradient() wereTensorFlow variables. It’s actually possible for these inputs to be any arbitrary tensor.However, only trainable variables are tracked by default. With a constant tensor, you’dhave to manually mark it as being tracked by calling tape.watch() on it.input_const = tf.constant(3.) with tf.GradientTape() as tape: tape.watch(input_const) result = tf.square(input_const)gradient = tape.gradient(result, input_const)Why is this necessary? Because it would be too expensive to preemptively store theinformation required to compute the gradient of anything with respect to anything.To avoid wasting resources, the tape needs to know what to watch. Trainable variablesare watched by default because computing the gradient of a loss with regard to a list oftrainable variables is the most common use of the gradient tape. The gradient tape is a powerful utility, even capable of computing second-order gra-dients, that is to say, the gradient of a gradient. For instance, the gradient of the posi-tion of an object with regard to time is the speed of that object, and the second-ordergradient is its acceleration. If you measure the position of a falling apple along a vertical axis over time andfind that it verifies position(time) = 4.9 * time ** 2, what is its acceleration? Let’suse two nested gradient tapes to find out.time = tf.Variable(0.) with tf.GradientTape() as outer_tape: with tf.GradientTape() as inner_tape: position = 4.9 * time ** 2 speed = inner_tape.gradient(position, time)acceleration = outer_tape.gradient(speed, time) 3.5.4 An end-to-end example: A linear classifier in pure TensorFlowYou know about tensors, variables, and tensor operations, and you know how to com-pute gradients. That’s enough to build any machine learning model based on gradi-ent descent. And you’re only at chapter 3! In a machine learning job interview, you may be asked to implement a linear classi-fier from scratch in TensorFlow: a very simple task that serves as a filter between candi-dates who have some minimal machine learning background and those who don’t.Listing 3.11 Using GradientTape with constant tensor inputs Listing 3.12 Using nested gradient tapes to compute second-order gradientsWe use the outer tape to compute the gradient of the gradient from the inner tape. Naturally, the answer is 4.9 * 2 = 9.8. 80CHAPTER 3Introduction to Keras and TensorFlowLet’s get you past that filter and use your newfound knowledge of TensorFlow toimplement such a linear classifier. First, let’s come up with some nicely linearly separable synthetic data to work with:two classes of points in a 2D plane. We’ll generate each class of points by drawing theircoordinates from a random distribution with a specific covariance matrix and a spe-cific mean. Intuitively, the covariance matrix describes the shape of the point cloud,and the mean describes its position in the plane (see figure 3.6). We’ll reuse the samecovariance matrix for both point clouds, but we’ll use two different mean values—thepoint clouds will have the same shape, but different positions.num_samples_per_class = 1000 negative_samples = np.random.multivariate_normal( mean=[0, 3], cov=[[1, 0.5],[0.5, 1]], size=num_samples_per_class) positive_samples = np.random.multivariate_normal( mean=[3, 0], cov=[[1, 0.5],[0.5, 1]], size=num_samples_per_class) In the preceding code, negative_samples and positive_samples a r e b o t h a r r a y swith shape (1000, 2). Let’s stack them into a single array with shape (2000, 2).inputs = np.vstack((negative_samples, positive_samples)).astype(np.float32)Let’s generate the corresponding target labels, an array of zeros and ones of shape(2000, 1), where targets[i, 0] is 0 if inputs[i] belongs to class 0 (and inversely).targets = np.vstack((np.zeros((num_samples_per_class, 1), dtype=\"float32\"), np.ones((num_samples_per_class, 1), dtype=\"float32\")))Next, let’s plot our data with Matplotlib.import matplotlib.pyplot as pltplt.scatter(inputs[:, 0], inputs[:, 1], c=targets[:, 0])plt.show()Listing 3.13 Generating two classes of random points in a 2D plane Listing 3.14 Stacking the two classes into an array with shape (2000, 2)Listing 3.15 Generating the corresponding targets (0 and 1)Listing 3.16 Plotting the two point classes (see figure 3.6)Generate the first class of points: 1000 random 2D points. cov=[[1, 0.5],[0.5, 1]] corresponds to an oval-like point cloud oriented from bottom left to top right.Generate the other class of points with a different mean and the same covariance matrix. 81First steps with TensorFlow Now let’s create a linear classifier that can learn to separate these two blobs. A linearclassifier is an affine transformation (prediction = W • input + b) trained to minimizethe square of the difference between predictions and the targets. As you’ll see, it’s actually a much simpler example than the end-to-end example ofa toy two-layer neural network you saw at the end of chapter 2. However, this time youshould be able to understand everything about the code, line by line. Let’s create our variables, W and b, initialized with random values and with zeros,respectively.input_dim = 2output_dim = 1 W=t f . V a r i a b l e ( i n i t i a l _ v a l u e = t f . r a n d o m . u n i f o r m ( s h a p e = ( i n p u t _ d i m ,o u t p u t _ d i m ) ) )b=t f . V a r i a b l e ( i n i t i a l _ v a l u e = t f . z e r o s ( s h a p e = ( o u t p u t _ d i m , ) ) )Here’s our forward pass function.def model(inputs): return tf.matmul(inputs, W) + bBecause our linear classifier operates on 2D inputs, W is really just two scalar coeffi-cients, w1 and w2: W = [[w1], [w2]]. Meanwhile, b is a single scalar coefficient. As such,for a given input point [x, y], its prediction value is prediction = [[w1], [w2]] • [x,y] + b = w1 * x + w2 * y + b.The following listing shows our loss function.Listing 3.17 Creating the linear classifier variables Listing 3.18 The forward pass functionFigure 3.6 Our synthetic data: two classes of random points in the 2D plane The inputs willbe 2D points.The output predictions will be a single score per sample (close to 0 if the sample is predicted to be in class 0, and close to 1 if the sample is predicted to be in class 1). 82CHAPTER 3Introduction to Keras and TensorFlowdef square_loss(targets, predictions): per_sample_losses = tf.square(targets - predictions) return tf.reduce_mean(per_sample_losses) Next is the training step, which receives some training data and updates the weights Wand b so as to minimize the loss on the data.learning_rate = 0.1 def training_step(inputs, targets): with tf.GradientTape() as tape: predictions = model(inputs) loss = square_loss(predictions, targets) grad_loss_wrt_W, grad_loss_wrt_b = tape.gradient(loss, [W, b]) W.assign_sub(grad_loss_wrt_W * learning_rate) b.assign_sub(grad_loss_wrt_b * learning_rate) return lossFor simplicity, we’ll do batch training instead of mini-batch training: we’ll run each trainingstep (gradient computation and weight update) for all the data, rather than iterate overthe data in small batches. On one hand, this means that each training step will takemuch longer to run, since we’ll compute the forward pass and the gradients for 2,000samples at once. On the other hand, each gradient update will be much more effectiveat reducing the loss on the training data, since it will encompass information from alltraining samples instead of, say, only 128 random samples. As a result, we will need manyfewer steps of training, and we should use a larger learning rate than we would typicallyuse for mini-batch training (we’ll use learning_rate = 0.1, defined in listing 3.20).for step in range(40): loss = training_step(inputs, targets) print(f\"Loss at step {step}: {loss:.4f}\")After 40 steps, the training loss seems to have stabilized around 0.025. Let’s plot howour linear model classifies the training data points. Because our targets are zeros andones, a given input point will be classified as “0” if its prediction value is below 0.5, andas “1” if it is above 0.5 (see figure 3.7):predictions = model(inputs)plt.scatter(inputs[:, 0], inputs[:, 1], c=predictions[:, 0] > 0.5)plt.show()Listing 3.19 The mean squared error loss function Listing 3.20 The training step function Listing 3.21 The batch training loopper_sample_losses will be a tensor with the same shape astargets and predictions, containing per-sample loss scores.We need to average these per-sample loss scores into asingle scalar loss value: this is what reduce_mean does. Forward pass, inside a gradient tape scopeRetrieve the gradientof the loss with regardto weights.Update the weights. 83First steps with TensorFlow Recall that the prediction value for a given point [x, y] i s s i m p l y prediction ==[[w1], [w2]] • [x, y] + b == w1 * x + w2 * y + b. Thus, class 0 is defined as w1 * x + w2* y + b < 0.5, and class 1 is defined as w1 * x + w2 * y + b > 0.5. You’ll notice that whatyou’re looking at is really the equation of a line in the 2D plane: w1 * x + w2 * y + b = 0.5.Above the line is class 1, and below the line is class 0. You may be used to seeing lineequations in the format y = a * x + b; in the same format, our line becomes y = - w1 / w2* x + (0.5 - b) / w2. Let’s plot this line (shown in figure 3.8):x = np.linspace(-1, 4, 100) y = - W[0] / W[1] * x + (0.5 - b) / W[1] plt.plot(x, y, \"-r\") plt.scatter(inputs[:, 0], inputs[:, 1], c=predictions[:, 0] > 0.5) Figure 3.7 Our model’s predictions on the training inputs: pretty similar to the training targets Generate 100 regularly spacednumbers between –1 and 4, whichwe will use to plot our line.This is our line’s equation.Plot our line (\"-r\" means “plot it as a red line”).Plot our model’s predictions on the same plot. Figure 3.8 Our model, visualized as a line 84CHAPTER 3Introduction to Keras and TensorFlowThis is really what a linear classifier is all about: finding the parameters of a line (or, inhigher-dimensional spaces, a hyperplane) neatly separating two classes of data. 3.6 Anatomy of a neural network: Understanding core Keras APIsAt this point, you know the basics of TensorFlow, and you can use it to implement atoy model from scratch, such as the batch linear classifier in the previous section, orthe toy neural network at the end of chapter 2. That’s a solid foundation to buildupon. It’s now time to move on to a more productive, more robust path to deep learn-ing: the Keras API.3.6.1 Layers: The building blocks of deep learningThe fundamental data structure in neural networks is the layer, to which you wereintroduced in chapter 2. A layer is a data processing module that takes as input one ormore tensors and that outputs one or more tensors. Some layers are stateless, butmore frequently layers have a state: the layer’s weights, one or several tensors learnedwith stochastic gradient descent, which together contain the network’s knowledge. Different types of layers are appropriate for different tensor formats and differenttypes of data processing. For instance, simple vector data, stored in rank-2 tensors ofshape (samples, features), is often processed by densely connected layers, also calledfully connected or dense layers (the Dense class in Keras). Sequence data, stored in rank-3tensors of shape (samples, timesteps, features), is typically processed by recurrentlayers, such as an LSTM layer, or 1D convolution layers (Conv1D). Image data, stored inrank-4 tensors, is usually processed by 2D convolution layers (Conv2D). You can think of layers as the LEGO bricks of deep learning, a metaphor that ismade explicit by Keras. Building deep learning models in Keras is done by clippingtogether compatible layers to form useful data-transformation pipelines.THE BASE LAYER CLASS IN KERASA simple API should have a single abstraction around which everything is centered. InKeras, that’s the Layer class. Everything in Keras is either a Layer or something thatclosely interacts with a Layer. A Layer is an object that encapsulates some state (weights) and some computation(a forward pass). The weights are typically defined in a build() (although they couldalso be created in the constructor, __init__()), and the computation is defined inthe call() method. In the previous chapter, we implemented a NaiveDense class that contained twoweights W and b and applied the computation output = activation(dot(input, W) +b). This is what the same layer would look like in Keras.from tensorflow import keras class SimpleDense(keras.layers.Layer): Listing 3.22 A Dense layer implemented as a Layer subclassAll Keras layers inherit from the base Layer class. 85Anatomy of a neural network: Understanding core Keras APIs def __init__(self, units, activation=None): super().__init__() self.units = units self.activation = activation def build(self, input_shape): input_dim = input_shape[-1] self.W = self.add_weight(shape=(input_dim, self.units), initializer=\"random_normal\") self.b = self.add_weight(shape=(self.units,), initializer=\"zeros\") def call(self, inputs): y = tf.matmul(inputs, self.W) + self.b if self.activation is not None: y = self.activation(y) return yIn the next section, we’ll cover in detail the purpose of these build() and call()methods. Don’t worry if you don’t understand everything just yet! Once instantiated, a layer like this can be used just like a function, taking as inputa TensorFlow tensor:>>> my_dense = SimpleDense(units=32, activation=tf.nn.relu) >>> input_tensor = tf.ones(shape=(2, 784)) >>> output_tensor = my_dense(input_tensor) >>> print(output_tensor.shape)(2, 32))You’re probably wondering, why did we have to implement call() and build(), sincewe ended up using our layer by plainly calling it, that is to say, by using its __call__()method? It’s because we want to be able to create the state just in time. Let’s see howthat works. AUTOMATIC SHAPE INFERENCE: BUILDING LAYERS ON THE FLYJust like with LEGO bricks, you can only “clip” together layers that are compatible.The notion of layer compatibility here refers specifically to the fact that every layer willonly accept input tensors of a certain shape and will return output tensors of a certainshape. Consider the following example:from tensorflow.keras import layerslayer = layers.Dense(32, activation=\"relu\") This layer will return a tensor where the first dimension has been transformed to be32. It can only be connected to a downstream layer that expects 32-dimensional vec-tors as its input. When using Keras, you don’t have to worry about size compatibility most of thetime, because the layers you add to your models are dynamically built to match theshape of the incoming layer. For instance, suppose you write the following:Weight creation takes place in the build() method.add_weight() is a shortcutmethod for creating weights.It is also possible to createstandalone variables and assignthem as layer attributes, like self.W =tf.Variable(tf.random.uniform(w_shape)).We define theforward passcomputationin the call()method. Instantiate our layer, defined previously.Create some test inputs.Call the layer onthe inputs, justlike a function. A dense layer with 32 output units 86CHAPTER 3Introduction to Keras and TensorFlowfrom tensorflow.keras import models from tensorflow.keras import layersmodel = models.Sequential([ layers.Dense(32, activation=\"relu\"), layers.Dense(32)])The layers didn’t receive any information about the shape of their inputs—instead,they automatically inferred their input shape as being the shape of the first inputsthey see. I n t h e t o y v e r s i o n o f t h e Dense l a y e r w e i m p l e m e n t e d i n c h a p t e r 2 ( w h i c h w enamed NaiveDense), we had to pass the layer’s input size explicitly to the constructorin order to be able to create its weights. That’s not ideal, because it would lead to mod-els that look like this, where each new layer needs to be made aware of the shape ofthe layer before it:model = NaiveSequential([ NaiveDense(input_size=784, output_size=32, activation=\"relu\"), NaiveDense(input_size=32, output_size=64, activation=\"relu\"), NaiveDense(input_size=64, output_size=32, activation=\"relu\"), NaiveDense(input_size=32, output_size=10, activation=\"softmax\")])It would be even worse if the rules used by a layer to produce its output shape arecomplex. For instance, what if our layer returned outputs of shape (batch, input_size * 2 if input_size % 2 == 0 else input_size * 3)? If we were to reimplement our NaiveDense layer as a Keras layer capable of auto-matic shape inference, it would look like the previous SimpleDense layer (see listing3.22), with its build() and call() methods. In SimpleDense, we no longer create weights in the constructor like in the Naive-Dense e x a m p l e ; i n s t e a d , w e c r e a t e t h e m i n a d e d i c a t e d s t a t e - c r e a t i o n m e t h o d ,build(), which receives as an argument the first input shape seen by the layer. Thebuild() m e t h o d i s c a l l e d a u t o m a t i c a l l y t h e f i r s t t i m e t h e l a y e r i s c a l l e d ( v i a i t s__call__() method). In fact, that’s why we defined the computation in a separa tecall() method rather than in the __call__() method directly. The __call__() methodof the base layer schematically looks like this:def __call__(self, inputs): if not self.built: self.build(inputs.shape) self.built = True return self.call(inputs)With automatic shape inference, our previous example becomes simple and neat:model = keras.Sequential([ SimpleDense(32, activation=\"relu\"), SimpleDense(64, activation=\"relu\"), 87Anatomy of a neural network: Understanding core Keras APIs SimpleDense(32, activation=\"relu\"), SimpleDense(10, activation=\"softmax\")])Note that automatic shape inference is not the only thing that the Layer c l a s s ’ s__call__() method handles. It takes care of many more things, in particular routingbetween eager and graph execution (a concept you’ll learn about in chapter 7), andinput masking (which we’ll cover in chapter 11). For now, just remember: whenimplementing your own layers, put the forward pass in the call() method. 3.6.2 From layers to modelsA deep learning model is a graph of layers. In Keras, that’s the Model class. Untilnow, you’ve only seen Sequential models (a subclass of Model), which are simplestacks of layers, mapping a single input to a single output. But as you move forward,you’ll be exposed to a much broader variety of network topologies. These are somecommon ones:Two-branch networksMultihead networksResidual connectionsNetwork topology can get quite involved. For instance, figure 3.9 shows the topologyof the graph of layers of a Transformer, a common architecture designed to processtext data. There are generally two ways of building such models in Keras: you could directlysubclass the Model class, or you could use the Functional API, which lets you do morewith less code. We’ll cover both approaches in chapter 7. The topology of a model defines a hypothesis space. You may remember that in chap-ter 1 we described machine learning as searching for useful representations of someinput data, within a predefined space of possibilities, using guidance from a feedback sig-nal. By choosing a network topology, you constrain your space of possibilities (hypoth-esis space) to a specific series of tensor operations, mapping input data to output data.What you’ll then be searching for is a good set of values for the weight tensorsinvolved in these tensor operations. To learn from data, you have to make assumptions about it. These assumptionsdefine what can be learned. As such, the structure of your hypothesis space—thearchitecture of your model—is extremely important. It encodes the assumptions youmake about your problem, the prior knowledge that the model starts with. Forinstance, if you’re working on a two-class classification problem with a model made ofa single Dense layer with no activation (a pure affine transformation), you are assum-ing that your two classes are linearly separable. Picking the right network architecture is more an art than a science, and althoughthere are some best practices and principles you can rely on, only practice can helpyou become a proper neural-network architect. The next few chapters will both teach 88CHAPTER 3Introduction to Keras and TensorFlow you explicit principles for building neural networks and help you develop intuition asto what works or doesn’t work for specific problems. You’ll build a solid intuitionabout what type of model architectures work for different kinds of problems, how tobuild these networks in practice, how to pick the right learning configuration, andhow to tweak a model until it yields the results you want to see. 3.6.3 The “compile” step: Configuring the learning processOnce the model architecture is defined, you still have to choose three more things:Loss function (objective function)—The quantity that will be minimized duringtraining. It represents a measure of success for the task at hand.LayerNormalizationDense+DenseLayerNormalization+MultiHeadAttentionLayerNormalizationDense+DenseLayerNormalization+MultiHeadAttentionLayerNormalization+MultiHeadAttentionFigure 3.9 The Transformer architecture (covered in chapter 11). There’s a lot going on here. Throughout the next few chapters, you’ll climb your way up to understanding it. 89Anatomy of a neural network: Understanding core Keras APIsOptimizer—Determines how the network will be updated based on the loss func-tion. It implements a specific variant of stochastic gradient descent (SGD).Metrics—The measures of success you want to monitor during training and vali-dation, such as classification accuracy. Unlike the loss, training will not optimizedirectly for these metrics. As such, metrics don’t need to be differentiable.Once you’ve picked your loss, optimizer, and metrics, you can use the built-in compile()and fit() methods to start training your model. Alternatively, you could also writeyour own custom training loops—we’ll cover how to do this in chapter 7. It’s a lotmore work! For now, let’s take a look at compile() and fit(). The compile() method configures the training process—you’ve already been intro-duced to it in your very first neural network example in chapter 2. It takes the argu-ments optimizer, loss, and metrics (a list):model = keras.Sequential([keras.layers.Dense(1)]) model.compile(optimizer=\"rmsprop\", loss=\"mean_squared_error\", metrics=[\"accuracy\"]) In the preceding call to compile(), we passed the optimizer, loss, and metrics asstrings (such as \"rmsprop\"). These strings are actually shortcuts that get converted toPython objects. For instance, \"rmsprop\" b e c o m e s keras.optimizers.RMSprop().Importantly, it’s also possible to specify these arguments as object instances, like this:model.compile(optimizer=keras.optimizers.RMSprop(), loss=keras.losses.MeanSquaredError(), metrics=[keras.metrics.BinaryAccuracy()])This is useful if you want to pass your own custom losses or metrics, or if you want tofurther configure the objects you’re using—for instance, by passing a learning_rateargument to the optimizer:model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-4), loss=my_custom_loss, metrics=[my_custom_metric_1, my_custom_metric_2])In chapter 7, we’ll cover how to create custom losses and metrics. In general, youwon’t have to create your own losses, metrics, or optimizers from scratch, becauseKeras offers a wide range of built-in options that is likely to include what you need: Optimizers:SGD (with or without momentum)RMSprop Adam Define a linear classifier.Specify the optimizer by name: RMSprop (it’s case-insensitive).Specify the loss by name: mean squared error.Specify a list of metrics: inthis case, only accuracy. 90CHAPTER 3Introduction to Keras and TensorFlowAdagrad Etc.Losses:CategoricalCrossentropy SparseCategoricalCrossentropy BinaryCrossentropy MeanSquaredError KLDivergence CosineSimilarity Etc.Metrics:CategoricalAccuracy SparseCategoricalAccuracy BinaryAccuracy AUC Precision Recall Etc.Throughout this book, you’ll see concrete applications of many of these options. 3.6.4 Picking a loss functionChoosing the right loss function for the right problem is extremely important: yournetwork will take any shortcut it can to minimize the loss, so if the objective doesn’tfully correlate with success for the task at hand, your network will end up doingthings you may not have wanted. Imagine a stupid, omnipotent AI trained via SGDwith this poorly chosen objective function: “maximizing the average well-being of allhumans alive.” To make its job easier, this AI might choose to kill all humans except afew and focus on the well-being of the remaining ones—because average well-beingisn’t affected by how many humans are left. That might not be what you intended!Just remember that all neural networks you build will be just as ruthless in loweringtheir loss function—so choose the objective wisely, or you’ll have to face unintendedside effects. Fortunately, when it comes to common problems such as classification, regression,and sequence prediction, there are simple guidelines you can follow to choose thecorrect loss. For instance, you’ll use binary crossentropy for a two-class classificationproblem, categorical crossentropy for a many-class classification problem, and so on.Only when you’re working on truly new research problems will you have to developyour own loss functions. In the next few chapters, we’ll detail explicitly which lossfunctions to choose for a wide range of common tasks. 91Anatomy of a neural network: Understanding core Keras APIs3.6.5 Understanding the fit() methodAfter compile() comes fit(). The fit() method implements the training loop itself.These are its key arguments:The data (inputs and targets) to train on. It will typically be passed either in theform of NumPy arrays or a TensorFlow Dataset object. You’ll learn more aboutthe Dataset API in the next chapters.The number of epochs to train for: how many times the training loop should iter-ate over the data passed.The batch size to use within each epoch of mini-batch gradient descent: thenumber of training examples considered to compute the gradients for oneweight update step.history = model.fit( inputs, targets, epochs=5, batch_size=128 )The call to fit() returns a History object. This object contains a history field, whichis a dict mapping keys such as \"loss\" or specific metric names to the list of their per-epoch values. >>> history.history{\"binary_accuracy\": [0.855, 0.9565, 0.9555, 0.95, 0.951], \"loss\": [0.6573270302042366, 0.07434618508815766, 0.07687718723714351, 0.07412414988875389, 0.07617757616937161]}3.6.6 Monitoring loss and metrics on validation dataThe goal of machine learning is not to obtain models that perform well on the train-ing data, which is easy—all you have to do is follow the gradient. The goal is to obtainmodels that perform well in general, and particularly on data points that the modelhas never encountered before. Just because a model performs well on its training datadoesn’t mean it will perform well on data it has never seen! For instance, it’s possiblethat your model could end up merely memorizing a mapping between your trainingsamples and their targets, which would be useless for the task of predicting targets fordata the model has never seen before. We’ll go over this point in much more detail inchapter 5.Listing 3.23 Calling fit() with NumPy dataThe input examples, as a NumPy arrayThe corresponding training targets, as a NumPy arrayThe training loop will iterate over the data 5 times.The training loop williterate over the data inbatches of 128 examples. 92CHAPTER 3Introduction to Keras and TensorFlow T o k e e p a n e y e o n h o w t h e m o d e l d o e s o n n e w d a t a , i t ’ s s t a n d a r d p r a c t i c e t oreserve a subset of the training data as validation data: you won’t be training the modelon this data, but you will use it to compute a loss value and metrics value. You do thisby using the validation_data argument in fit(). Like the training data, the valida-tion data could be passed as NumPy arrays or as a TensorFlow Dataset object.model = keras.Sequential([keras.layers.Dense(1)])model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=0.1), loss=keras.losses.MeanSquaredError(), metrics=[keras.metrics.BinaryAccuracy()]) indices_permutation = np.random.permutation(len(inputs)) shuffled_inputs = inputs[indices_permutation] shuffled_targets = targets[indices_permutation] num_validation_samples = int(0.3 * len(inputs)) val_inputs = shuffled_inputs[:num_validation_samples] val_targets = shuffled_targets[:num_validation_samples] training_inputs = shuffled_inputs[num_validation_samples:] training_targets = shuffled_targets[num_validation_samples:] model.fit( training_inputs, training_targets, epochs=5, batch_size=16, validation_data=(val_inputs, val_targets) )The value of the loss on the validation data is called the “validation loss,” to distin-guish it from the “training loss.” Note that it’s essential to keep the training data andvalidation data strictly separate: the purpose of validation is to monitor whetherwhat the model is learning is actually useful on new data. If any of the validationdata has been seen by the model during training, your validation loss and metricswill be flawed. Note that if you want to compute the validation loss and metrics after the trainingis complete, you can call the evaluate() method:loss_and_metrics = model.evaluate(val_inputs, val_targets, batch_size=128)evaluate() w i l l i t e r a t e i n b a t c h e s ( o f s i z e batch_size) over the data passed andreturn a list of scalars, where the first entry is the validation loss and the followingentries are the validation metrics. If the model has no metrics, only the validation lossis returned (rather than a list). Listing 3.24 Using the validation_data argumentTo avoid having samples from only one class in the validation data, shuffle the inputs and targets using a random indices permutation.Reserve 30% of the training inputs and targets for validation (we’ll exclude these samples from training and reserve them to compute the validation loss and metrics).Training data, used to update the weights of the modelValidation data, used only to monitor the validation loss and metrics 93Summary3.6.7 Inference: Using a model after trainingOnce you’ve trained your model, you’re going to want to use it to make predictionson new data. This is called inference. To do this, a naive approach would simply be to__call__() the model:predictions = model(new_inputs) However, this will process all inputs in new_inputs at once, which may not be feasibleif you’re looking at a lot of data (in particular, it may require more memory than yourGPU has). A better way to do inference is to use the predict() method. It will iterate over thedata in small batches and return a NumPy array of predictions. And unlike__call__(), it can also process TensorFlow Dataset objects.predictions = model.predict(new_inputs, batch_size=128) For instance, if we use predict() o n s o m e o f o u r v a l i d a t i o n d a t a w i t h t h e l i n e a rmodel we trained earlier, we get scalar scores that correspond to the model’s predic-tion for each input sample:>>> predictions = model.predict(val_inputs, batch_size=128)>>> print(predictions[:10])[[0.3590725 ] [0.82706255] [0.74428225] [0.682058 ] [0.7312616 ] [0.6059811 ] [0.78046083] [0.025846 ] [0.16594526] [0.72068727]]For now, this is all you need to know about Keras models. You are ready to move on tosolving real-world machine learning problems with Keras in the next chapter. SummaryTensorFlow is an industry-strength numerical computing framework that canrun on CPU, GPU, or TPU. It can automatically compute the gradient of anydifferentiable expression, it can be distributed to many devices, and it canexport programs to various external runtimes—even JavaScript.Keras is the standard API for doing deep learning with TensorFlow. It’s whatwe’ll use throughout this book.Key TensorFlow objects include tensors, variables, tensor operations, and thegradient tape.Takes a NumPy array or TensorFlow tensor and returns a TensorFlow tensor Takes a NumPy array or a Dataset and returns a NumPy array 94CHAPTER 3Introduction to Keras and TensorFlowThe central class of Keras is the Layer. A layer encapsulates some weights andsome computation. Layers are assembled into models.Before you start training a model, you need to pick an optimizer, a loss, and somemetrics, which you specify via the model.compile() method.To train a model, you can use the fit() method, which runs mini-batch gradi-ent descent for you. You can also use it to monitor your loss and metrics on val-idation data, a set of inputs that the model doesn’t see during training.Once your model is trained, you use the model.predict() method to generatepredictions on new inputs. 95Getting startedwith neural networks:Classification and regression This chapter is designed to get you started using neural networks to solve real prob-lems. You’ll consolidate the knowledge you gained from chapters 2 and 3, andyou’ll apply what you’ve learned to three new tasks covering the three most com-mon use cases of neural networks—binary classification, multiclass classification,and scalar regression:Classifying movie reviews as positive or negative (binary classification)Classifying news wires by topic (multiclass classification)Estimating the price of a house, given real-estate data (scalar regression)These examples will be your first contact with end-to-end machine learning work-flows: you’ll get introduced to data preprocessing, basic model architecture princi-ples, and model evaluation.This chapter coversYour first examples of real-world machine learning workflowsHandling classification problems over vector dataHandling continuous regression problems over vector data 96CHAPTER 4Getting started with neural networks: Classification and regression By the end of this chapter, you’ll be able to use neural networks to handle simple clas-sification and regression tasks over vector data. You’ll then be ready to start building amore principled, theory-driven understanding of machine learning in chapter 5.Classification and regression glossaryClassification and regression involve many specialized terms. You’ve come acrosssome of them in earlier examples, and you’ll see more of them in future chapters.They have precise, machine learning–specific definitions, and you should be famil-iar with them:Sample or input—One data point that goes into your model.Prediction or output—What comes out of your model.Target—The truth. What your model should ideally have predicted, accordingto an external source of data.Prediction error or loss value—A measure of the distance between yourmodel’s prediction and the target.Classes—A set of possible labels to choose from in a classification problem.For example, when classifying cat and dog pictures, “dog” and “cat” are thetwo classes.Label—A specific instance of a class annotation in a classification problem.For instance, if picture #1234 is annotated as containing the class “dog,”then “dog” is a label of picture #1234.Ground-truth or annotations—All targets for a dataset, typically collected byhumans.Binary classification—A classification task where each input sample shouldbe categorized into two exclusive categories.Multiclass classification—A classification task where each input sampleshould be categorized into more than two categories: for instance, classifyinghandwritten digits.Multilabel classification—A classification task where each input sample canbe assigned multiple labels. For instance, a given image may contain both acat and a dog and should be annotated both with the “cat” label and the“dog” label. The number of labels per image is usually variable.Scalar regression—A task where the target is a continuous scalar value. Pre-dicting house prices is a good example: the different target prices form a con-tinuous space.Vector regression—A task where the target is a set of continuous values: forexample, a continuous vector. If you’re doing regression against multiple val-ues (such as the coordinates of a bounding box in an image), then you’redoing vector regression.Mini-batch or batch—A small set of samples (typically between 8 and 128)that are processed simultaneously by the model. The number of samples isoften a power of 2, to facilitate memory allocation on GPU. When training, amini-batch is used to compute a single gradient-descent update applied tothe weights of the model. 97Classifying movie reviews: A binary classification example4.1 Classifying movie reviews: A binary classification exampleTwo-class classification, or binary classification, is one of the most common kinds ofmachine learning problems. In this example, you’ll learn to classify movie reviews aspositive or negative, based on the text content of the reviews.4.1.1 The IMDB datasetYou’ll work with the IMDB dataset: a set of 50,000 highly polarized reviews from theInternet Movie Database. They’re split into 25,000 reviews for training and 25,000reviews for testing, each set consisting of 50% negative and 50% positive reviews. Just like the MNIST dataset, the IMDB dataset comes packaged with Keras. It hasalready been preprocessed: the reviews (sequences of words) have been turned intosequences of integers, where each integer stands for a specific word in a dictionary.This enables us to focus on model building, training, and evaluation. In chapter 11,you’ll learn how to process raw text input from scratch. The following code will load the dataset (when you run it the first time, about 80MB of data will be downloaded to your machine).from tensorflow.keras.datasets import imdb(train_data, train_labels), (test_data, test_labels) = imdb.load_data( num_words=10000)The argument num_words=10000 m e a n s y o u ’ l l o n l y k e e p t h e t o p 1 0 , 0 0 0 m o s t f r e -quently occurring words in the training data. Rare words will be discarded. This allowsus to work with vector data of manageable size. If we didn’t set this limit, we’d be work-ing with 88,585 unique words in the training data, which is unnecessarily large. Manyof these words only occur in a single sample, and thus can’t be meaningfully used forclassification. The variables train_data and test_data are lists of reviews; each review is a list ofword indices (encoding a sequence of words). train_labels a n d test_labels a r elists of 0s and 1s, where 0 stands for negative and 1 stands for positive:>>> train_data[0][1, 14, 22, 16, ... 178, 32]>>> train_labels[0]1Because we’re restricting ourselves to the top 10,000 most frequent words, no wordindex will exceed 10,000:>>> max([max(sequence) for sequence in train_data])9999For kicks, here’s how you can quickly decode one of these reviews back to English words.Listing 4.1 Loading the IMDB dataset 98CHAPTER 4Getting started with neural networks: Classification and regressionword_index = imdb.get_word_index() reverse_word_index = dict( [(value, key) for (key, value) in word_index.items()])decoded_review = \" \".join( [reverse_word_index.get(i - 3, \"?\") for i in train_data[0]]) 4.1.2 Preparing the dataYou can’t directly feed lists of integers into a neural network. They all have differentlengths, but a neural network expects to process contiguous batches of data. You haveto turn your lists into tensors. There are two ways to do that:Pad your lists so that they all have the same length, turn them into an integertensor of shape (samples, max_length), and start your model with a layer capa-ble of handling such integer tensors (the Embedding layer, which we’ll cover indetail later in the book).Multi-hot encode y o u r l i s t s t o t u r n t h e m i n t o v e c t o r s o f 0 s a n d 1 s . T h i s w o u l dmean, for instance, turning the sequence [8, 5] into a 10,000-dimensional vec-tor that would be all 0s except for indices 8 and 5, which would be 1s. Then youcould use a Dense layer, capable of handling floating-point vector data, as thefirst layer in your model.Let’s go with the latter solution to vectorize the data, which you’ll do manually formaximum clarity.import numpy as np def vectorize_sequences(sequences, dimension=10000): results = np.zeros((len(sequences), dimension)) for i, sequence in enumerate(sequences): for j in sequence: results[i, j] = 1. return resultsx_train = vectorize_sequences(train_data) x_test = vectorize_sequences(test_data)Here’s what the samples look like now:>>> x_train[0]array([ 0., 1., 1., ..., 0., 0., 0.])Listing 4.2 Decoding reviews back to text Listing 4.3 Encoding the integer sequences via multi-hot encodingword_index is a dictionary mapping words to an integer index.Reverses it, mapping integer indices to wordsDecodes the review. Note that the indices are offset by 3 because 0, 1, and 2 are reserved indices for “padding,” “start of sequence,” and “unknown.” Creates an all-zero matrix of shape (len(sequences), dimension)Sets specific indices of results[i] to 1sVectorized training dataVectorized test data 99Classifying movie reviews: A binary classification exampleYou should also vectorize your labels, which is straightforward:y_train = np.asarray(train_labels).astype(\"float32\")y_test = np.asarray(test_labels).astype(\"float32\")Now the data is ready to be fed into a neural network. 4.1.3 Building your modelThe input data is vectors, and the labels are scalars (1s and 0s): this is one of the simplestproblem setups you’ll ever encounter. A type of model that performs well on such a prob-lem is a plain stack of densely connected (Dense) layers with relu activations. There are two key architecture decisions to be made about such a stack of Denselayers:How many layers to useHow many units to choose for each layerIn chapter 5, you’ll learn formal principles to guideyou in making these choices. For the time being,you’ll have to trust me with the following architecturechoices:Two intermediate layers with 16 units eachA third layer that will output the scalar predic-tion regarding the sentiment of the currentreviewFigure 4.1 shows what the model looks like. And thefollowing listing shows the Keras implementation,similar to the MNIST example you saw previously.from tensorflow import keras from tensorflow.keras import layers model = keras.Sequential([ layers.Dense(16, activation=\"relu\"), layers.Dense(16, activation=\"relu\"), layers.Dense(1, activation=\"sigmoid\")])The first argument being passed to each Dense l a y e r i s t h e n u m be r o f units i n t h elayer: the dimensionality of representation space of the layer. You remember fromchapters 2 and 3 that each such Dense layer with a relu activation implements the fol-lowing chain of tensor operations:output = relu(dot(input, W) + b)Listing 4.4 Model definitionDense (units=16)Input(vectorized text) Output(probability)Dense (units=16)Dense (units=1)Figure 4.1 The three-layer model 100CHAPTER 4Getting started with neural networks: Classification and regressionHaving 16 units means the weight matrix W will have shape (input_dimension, 16):the dot product with W will project the input data onto a 16-dimensional representa-tion space (and then you’ll add the bias vector b and apply the relu operation). Youcan intuitively understand the dimensionality of your representation space as “howmuch freedom you’re allowing the model to have when learning internal representa-tions.” Having more units (a higher-dimensional representation space) allows yourmodel to learn more-complex representations, but it makes the model more computa-tionally expensive and may lead to learning unwanted patterns (patterns that willimprove performance on the training data but not on the test data). The intermediate layers use relu as their activation function, and the final layeruses a sigmoid activation so as to output a probability (a score between 0 and 1 indicat-ing how likely the sample is to have the target “1”: how likely the review is to be posi-tive). A relu (rectified linear unit) is a function meant to zero out negative values (seefigure 4.2), whereas a sigmoid “squashes” arbitrary values into the [0, 1] interval (see fig-ure 4.3), outputting something that can be interpreted as a probability. Finally, you need to choose a loss function and an optimizer. Because you’re facing abinary classification problem and the output of your model is a probability (you endyour model with a single-unit layer with a sigmoid activation), it’s best to use thebinary_crossentropy loss. It isn’t the only viable choice: for instance, you could usemean_squared_error. But crossentropy is usually the best choice when you’re dealingFigure 4.2 The rectified linear unit function 101Classifying movie reviews: A binary classification example with models that output probabilities. Crossentropy is a quantity from the field of infor-mation theory that measures the distance between probability distributions or, in thiscase, between the ground-truth distribution and your predictions. As for the choice of the optimizer, we’ll go with rmsprop, which is a usually a gooddefault choice for virtually any problem.What are activation functions, and why are they necessary?Without an activation function like relu (also called a non-linearity), the Dense layerwould consist of two linear operations—a dot product and an addition:output = dot(input, W) + bThe layer could only learn linear transformations (affine transformations) of the inputdata: the hypothesis space of the layer would be the set of all possible linear trans-formations of the input data into a 16-dimensional space. Such a hypothesis spaceis too restricted and wouldn’t benefit from multiple layers of representations,because a deep stack of linear layers would still implement a linear operation: addingmore layers wouldn’t extend the hypothesis space (as you saw in chapter 2).In order to get access to a much richer hypothesis space that will benefit from deeprepresentations, you need a non-linearity, or activation function. relu is the mostpopular activation function in deep learning, but there are many other candidates,which all come with similarly strange names: prelu, elu, and so on.Figure 4.3 The sigmoid function 102CHAPTER 4Getting started with neural networks: Classification and regression Here’s the step where we configure the model with the rmsprop optimizer andthe binary_crossentropy loss function. Note that we’ll also monitor accuracy duringtraining. model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])4.1.4 Validating your approachAs you learned in chapter 3, a deep learning model should never be evaluated on itstraining data—it’s standard practice to use a validation set to monitor the accuracy ofthe model during training. Here, we’ll create a validation set by setting apart 10,000samples from the original training data.x_val = x_train[:10000]partial_x_train = x_train[10000:]y_val = y_train[:10000]partial_y_train = y_train[10000:]We will now train the model for 20 epochs (20 iterations over all samples in the train-ing data) in mini-batches of 512 samples. At the same time, we will monitor loss andaccuracy on the 10,000 samples that we set apart. We do so by passing the validationdata as the validation_data argument.history = model.fit(partial_x_train, partial_y_train, epochs=20, batch_size=512, validation_data=(x_val, y_val))On CPU, this will take less than 2 seconds per epoch—training is over in 20 seconds.At the end of every epoch, there is a slight pause as the model computes its loss andaccuracy on the 10,000 samples of the validation data. Note that the call to model.fit() returns a History object, as you saw in chapter 3.This object has a member history, which is a dictionary containing data about every-thing that happened during training. Let’s look at it:>>> history_dict = history.history>>> history_dict.keys()[u\"accuracy\", u\"loss\", u\"val_accuracy\", u\"val_loss\"]Listing 4.5 Compiling the model Listing 4.6 Setting aside a validation set Listing 4.7 Training your model 103Classifying movie reviews: A binary classification exampleThe dictionary contains four entries: one per metric that was being monitored duringtraining and during validation. In the following two listings, let’s use Matplotlib to plotthe training and validation loss side by side (see figure 4.4), as well as the training andvalidation accuracy (see figure 4.5). Note that your own results may vary slightly due toa different random initialization of your model. Figure 4.4 Training and validation loss Figure 4.5 Training and validation accuracy 104CHAPTER 4Getting started with neural networks: Classification and regressionimport matplotlib.pyplot as plthistory_dict = history.historyloss_values = history_dict[\"loss\"]val_loss_values = history_dict[\"val_loss\"]epochs = range(1, len(loss_values) + 1)plt.plot(epochs, loss_values, \"bo\", label=\"Training loss\")plt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\") plt.title(\"Training and validation loss\")plt.xlabel(\"Epochs\")plt.ylabel(\"Loss\")plt.legend()plt.show()plt.clf()acc = history_dict[\"accuracy\"]val_acc = history_dict[\"val_accuracy\"]plt.plot(epochs, acc, \"bo\", label=\"Training acc\")plt.plot(epochs, val_acc, \"b\", label=\"Validation acc\")plt.title(\"Training and validation accuracy\")plt.xlabel(\"Epochs\")plt.ylabel(\"Accuracy\")plt.legend()plt.show()As you can see, the training loss decreases with every epoch, and the training accuracyincreases with every epoch. That’s what you would expect when running gradient-descent optimization—the quantity you’re trying to minimize should be less withevery iteration. But that isn’t the case for the validation loss and accuracy: they seem topeak at the fourth epoch. This is an example of what we warned against earlier: amodel that performs better on the training data isn’t necessarily a model that willdo better on data it has never seen before. In precise terms, what you’re seeing isoverfitting: after the fourth epoch, you’re overoptimizing on the training data, and youend up learning representations that are specific to the training data and don’t gener-alize to data outside of the training set. In this case, to prevent overfitting, you could stop training after four epochs. Ingeneral, you can use a range of techniques to mitigate overfitting, which we’ll coverin chapter 5. Let’s train a new model from scratch for four epochs and then evaluate it on thetest data.model = keras.Sequential([ layers.Dense(16, activation=\"relu\"), layers.Dense(16, activation=\"relu\"),Listing 4.8 Plotting the training and validation loss Listing 4.9 Plotting the training and validation accuracy Listing 4.10 Retraining a model from scratch\"bo\" is for \"blue dot.\"\"b\" is for\"solid blue line.\"Clears the figure 105Classifying movie reviews: A binary classification example layers.Dense(1, activation=\"sigmoid\")])model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])model.fit(x_train, y_train, epochs=4, batch_size=512)results = model.evaluate(x_test, y_test)The final results are as follows:>>> results[0.2929924130630493, 0.88327999999999995]This fairly naive approach achieves an accuracy of 88%. With state-of-the-artapproaches, you should be able to get close to 95%. 4.1.5 Using a trained model to generate predictions on new dataAfter having trained a model, you’ll want to use it in a practical setting. You can gener-ate the likelihood of reviews being positive by using the predict method, as you’velearned in chapter 3:>>> model.predict(x_test)array([[ 0.98006207] [ 0.99758697] [ 0.99975556] ..., [ 0.82167041] [ 0.02885115] [ 0.65371346]], dtype=float32)As you can see, the model is confident for some samples (0.99 or more, or 0.01 orless) but less confident for others (0.6, 0.4). 4.1.6 Further experimentsThe following experiments will help convince you that the architecture choices you’vemade are all fairly reasonable, although there’s still room for improvement:You used two representation layers before the final classification layer. Try usingone or three representation layers, and see how doing so affects validation andtest accuracy.Try using layers with more units or fewer units: 32 units, 64 units, and so on.Try using the mse loss function instead of binary_crossentropy.Try using the tanh activation (an activation that was popular in the early days ofneural networks) instead of relu.The first number, 0.29, is the test loss, and the second number, 0.88, is the test accuracy. 106CHAPTER 4Getting started with neural networks: Classification and regression4.1.7 Wrapping upHere’s what you should take away from this example:You usually need to do quite a bit of preprocessing on your raw data in order tobe able to feed it—as tensors—into a neural network. Sequences of words canbe encoded as binary vectors, but there are other encoding options too.Stacks of Dense layers with relu activations can solve a wide range of problems(including sentiment classification), and you’ll likely use them frequently.In a binary classification problem (two output classes), your model should endwith a Dense layer with one unit and a sigmoid activation: the output of yourmodel should be a scalar between 0 and 1, encoding a probability.With such a scalar sigmoid output on a binary classification problem, the lossfunction you should use is binary_crossentropy.The rmsprop optimizer is generally a good enough choice, whatever your prob-lem. That’s one less thing for you to worry about.As they get better on their training data, neural networks eventually start over-fitting and end up obtaining increasingly worse results on data they’ve neverseen before. Be sure to always monitor performance on data that is outside ofthe training set. 4.2 Classifying newswires: A multiclass classification exampleIn the previous section, you saw how to classify vector inputs into two mutually exclu-sive classes using a densely connected neural network. But what happens when youhave more than two classes? In this section, we’ll build a model to classify Reuters newswires into 46 mutuallyexclusive topics. Because we have many classes, this problem is an instance of multi-class classification, and because each data point should be classified into only one cate-gory, the problem is more specifically an instance of single-label multiclass classification.If each data point could belong to multiple categories (in this case, topics), we’d befacing a multilabel multiclass classification problem.4.2.1 The Reuters datasetYou’ll work with the Reuters dataset, a set of short newswires and their topics, publishedby Reuters in 1986. It’s a simple, widely used toy dataset for text classification. Thereare 46 different topics; some topics are more represented than others, but each topichas at least 10 examples in the training set. Like IMDB and MNIST, the Reuters dataset comes packaged as part of Keras. Let’stake a look.from tensorflow.keras.datasets import reuters(train_data, train_labels), (test_data, test_labels) = reuters.load_data( num_words=10000)Listing 4.11 Loading the Reuters dataset 107Classifying newswires: A multiclass classification exampleAs with the IMDB dataset, the argument num_words=10000 r e s tr i c ts t h e d a t a t o t h e10,000 most frequently occurring words found in the data. You have 8,982 training examples and 2,246 test examples:>>> len(train_data)8982>>> len(test_data)2246As with the IMDB reviews, each example is a list of integers (word indices):>>> train_data[10][1, 245, 273, 207, 156, 53, 74, 160, 26, 14, 46, 296, 26, 39, 74, 2979,3554, 14, 46, 4689, 4329, 86, 61, 3499, 4795, 14, 61, 451, 4329, 17, 12]Here’s how you can decode it back to words, in case you’re curious.word_index = reuters.get_word_index()reverse_word_index = dict( [(value, key) for (key, value) in word_index.items()])decoded_newswire = \" \".join( [reverse_word_index.get(i - 3, \"?\") for i in train_data[0]]) The label associated with an example is an integer between 0 and 45—a topic index:>>> train_labels[10]34.2.2 Preparing the dataYou can vectorize the data with the exact same code as in the previous example.x_train = vectorize_sequences(train_data)x_test = vectorize_sequences(test_data)To vectorize the labels, there are two possibilities: you can cast the label list as an inte-ger tensor, or you can use one-hot encoding. One-hot encoding is a widely used formatfor categorical data, also called categorical encoding. In this case, one-hot encoding ofthe labels consists of embedding each label as an all-zero vector with a 1 in the place ofthe label index. The following listing shows an example.def to_one_hot(labels, dimension=46): results = np.zeros((len(labels), dimension))Listing 4.12 Decoding newswires back to text Listing 4.13 Encoding the input data Listing 4.14 Encoding the labelsNote that the indices are offset by 3 because 0, 1, and 2 are reservedindices for “padding,” “start of sequence,” and “unknown.” Vectorized training dataVectorized test data 108CHAPTER 4Getting started with neural networks: Classification and regression for i, label in enumerate(labels): results[i, label] = 1. return resultsy_train = to_one_hot(train_labels) y_test = to_one_hot(test_labels)Note that there is a built-in way to do this in Keras:from tensorflow.keras.utils import to_categoricaly_train = to_categorical(train_labels)y_test = to_categorical(test_labels)4.2.3 Building your modelThis topic-classification problem looks similar to the previous movie-review classifica-tion problem: in both cases, we’re trying to classify short snippets of text. But there isa new constraint here: the number of output classes has gone from 2 to 46. Thedimensionality of the output space is much larger. In a stack of Dense layers like those we’ve been using, each layer can only accessinformation present in the output of the previous layer. If one layer drops someinformation relevant to the classification problem, this information can never berecovered by later layers: each layer can potentially become an information bottle-neck. In the previous example, we used 16-dimensional intermediate layers, but a16-dimensional space may be too limited to learn to separate 46 different classes:such small layers may act as information bottlenecks, permanently dropping rele-vant information. For this reason we’ll use larger layers. Let’s go with 64 units.model = keras.Sequential([ layers.Dense(64, activation=\"relu\"), layers.Dense(64, activation=\"relu\"), layers.Dense(46, activation=\"softmax\")])There are two other things you should note about this architecture. First, we end the model with a Dense layer of size 46. This means for each inputsample, the network will output a 46-dimensional vector. Each entry in this vector(each dimension) will encode a different output class. Second, the last layer uses a softmax activation. You saw this pattern in the MNISTexample. It means the model will output a probability distribution over the 46 differentoutput classes—for every input sample, the model will produce a 46-dimensional out-put vector, where output[i] is the probability that the sample belongs to class i. The46 scores will sum to 1. The best loss function to use in this case is categorical_crossentropy. It mea-sures the distance between two probability distributions: here, between the probabilityListing 4.15 Model definitionVectorized training labelsVectorized test labels 109Classifying newswires: A multiclass classification exampledistribution output by the model and the true distribution of the labels. By minimiz-ing the distance between these two distributions, you train the model to output some-thing as close as possible to the true labels. model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])4.2.4 Validating your approachLet’s set apart 1,000 samples in the training data to use as a validation set.x_val = x_train[:1000]partial_x_train = x_train[1000:]y_val = y_train[:1000]partial_y_train = y_train[1000:]Now, let’s train the model for 20 epochs.history = model.fit(partial_x_train,partial_y_train,epochs=20,batch_size=512,validation_data=(x_val, y_val))And finally, let’s display its loss and accuracy curves (see figures 4.6 and 4.7).Listing 4.16 Compiling the model Listing 4.17 Setting aside a validation set Listing 4.18 Training the model Figure 4.6 Training and validation loss 110CHAPTER 4Getting started with neural networks: Classification and regression loss = history.history[\"loss\"]val_loss = history.history[\"val_loss\"]epochs = range(1, len(loss) + 1)plt.plot(epochs, loss, \"bo\", label=\"Training loss\")plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")plt.title(\"Training and validation loss\")plt.xlabel(\"Epochs\")plt.ylabel(\"Loss\")plt.legend()plt.show()plt.clf()acc = history.history[\"accuracy\"]val_acc = history.history[\"val_accuracy\"]plt.plot(epochs, acc, \"bo\", label=\"Training accuracy\")plt.plot(epochs, val_acc, \"b\", label=\"Validation accuracy\")plt.title(\"Training and validation accuracy\")plt.xlabel(\"Epochs\")plt.ylabel(\"Accuracy\")plt.legend()plt.show()The model begins to overfit after nine epochs. Let’s train a new model from scratchfor nine epochs and then evaluate it on the test set.model = keras.Sequential([ layers.Dense(64, activation=\"relu\"),Listing 4.19 Plotting the training and validation loss Listing 4.20 Plotting the training and validation accuracy Listing 4.21 Retraining a model from scratchFigure 4.7 Training and validation accuracy Clears the figure 111Classifying newswires: A multiclass classification example layers.Dense(64, activation=\"relu\"), layers.Dense(46, activation=\"softmax\")])model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])model.fit(x_train, y_train, epochs=9, batch_size=512)results = model.evaluate(x_test, y_test)Here are the final results:>>> results[0.9565213431445807, 0.79697239536954589]This approach reaches an accuracy of ~80%. With a balanced binary classificationproblem, the accuracy reached by a purely random classifier would be 50%. But inthis case, we have 46 classes, and they may not be equally represented. What would bethe accuracy of a random baseline? We could try quickly implementing one to checkthis empirically:>>> import copy>>> test_labels_copy = copy.copy(test_labels)>>> np.random.shuffle(test_labels_copy)>>> hits_array = np.array(test_labels) == np.array(test_labels_copy)>>> hits_array.mean()0.18655387355298308As you can see, a random classifier would score around 19% classification accuracy, sothe results of our model seem pretty good in that light. 4.2.5 Generating predictions on new dataCalling the model’s predict method on new samples returns a class probability distri-bution over all 46 topics for each sample. Let’s generate topic predictions for all of thetest data:predictions = model.predict(x_test)Each entry in “predictions” is a vector of length 46:>>> predictions[0].shape(46,)The coefficients in this vector sum to 1, as they form a probability distribution:>>> np.sum(predictions[0])1.0 112CHAPTER 4Getting started with neural networks: Classification and regressionThe largest entry is the predicted class—the class with the highest probability:>>> np.argmax(predictions[0])44.2.6 A different way to handle the labels and the lossWe mentioned earlier that another way to encode the labels would be to cast them asan integer tensor, like this:y_train = np.array(train_labels)y_test = np.array(test_labels)The only thing this approach would change is the choice of the loss function. The lossfunction used in listing 4.21, categorical_crossentropy, expects the labels to followa categorical encoding. With integer labels, you should use sparse_categorical_crossentropy:model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])This new loss function is still mathematically the same as categorical_crossentropy;it just has a different interface. 4.2.7 The importance of having sufficiently large intermediate layersWe mentioned earlier that because the final outputs are 46-dimensional, you shouldavoid intermediate layers with many fewer than 46 units. Now let’s see what happenswhen we introduce an information bottleneck by having intermediate layers that aresignificantly less than 46-dimensional: for example, 4-dimensional.model = keras.Sequential([ layers.Dense(64, activation=\"relu\"), layers.Dense(4, activation=\"relu\"), layers.Dense(46, activation=\"softmax\")])model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])model.fit(partial_x_train, partial_y_train, epochs=20, batch_size=128, validation_data=(x_val, y_val))The model now peaks at ~71% validation accuracy, an 8% absolute drop. This drop ismostly due to the fact that we’re trying to compress a lot of information (enoughListing 4.22 A model with an information bottleneck 113Predicting house prices: A regression exampleinformation to recover the separation hyperplanes of 46 classes) into an intermediatespace that is too low-dimensional. The model is able to cram most o f t h e n e c e s s a r yinformation into these four-dimensional representations, but not all of it. 4.2.8 Further experimentsLike in the previous example, I encourage you to try out the following experiments totrain your intuition about the kind of configuration decisions you have to make withsuch models:Try using larger or smaller layers: 32 units, 128 units, and so on.You used two intermediate layers before the final softmax classification layer.Now try using a single intermediate layer, or three intermediate layers.4.2.9 Wrapping upHere’s what you should take away from this example:If you’re trying to classify data points among N classes, your model should endwith a Dense layer of size N.In a single-label, multiclass classification problem, your model should end witha softmax activation so that it will output a probability distribution over the Noutput classes.Categorical crossentropy is almost always the loss function you should use forsuch problems. It minimizes the distance between the probability distributionsoutput by the model and the true distribution of the targets.There are two ways to handle labels in multiclass classification:– Encoding the labels via categorical encoding (also known as one-hot encod-ing) and using categorical_crossentropy as a loss function– Encoding the labels as integers and using the sparse_categorical_cross-entropy loss functionIf you need to classify data into a large number of categories, you should avoidcreating information bottlenecks in your model due to intermediate layers thatare too small. 4.3 Predicting house prices: A regression exampleThe two previous examples were considered classification problems, where the goalwas to predict a single discrete label of an input data point. Another common type ofmachine learning problem is regression, which consists of predicting a continuousvalue instead of a discrete label: for instance, predicting the temperature tomorrow,given meteorological data or predicting the time that a software project will take tocomplete, given its specifications.NOTEDon’t confuse regression and the logistic regression algorithm. Confusingly,logistic regression isn’t a regression algorithm—it’s a classification algorithm. 114CHAPTER 4Getting started with neural networks: Classification and regression4.3.1 The Boston housing price datasetIn this section, we’ll attempt to predict the median price of homes in a given Bostonsuburb in the mid-1970s, given data points about the suburb at the time, such as thecrime rate, the local property tax rate, and so on. The dataset we’ll use has an interest-ing difference from the two previous examples. It has relatively few data points: only506, split between 404 training samples and 102 test samples. And each feature in theinput data (for example, the crime rate) has a different scale. For instance, some val-ues are proportions, which take values between 0 and 1, others take values between 1and 12, others between 0 and 100, and so on.from tensorflow.keras.datasets import boston_housing(train_data, train_targets), (test_data, test_targets) = ( boston_housing.load_data())Let’s look at the data:>>> train_data.shape(404, 13)>>> test_data.shape(102, 13)As you can see, we have 404 training samples and 102 test samples, each with 13numerical features, such as per capita crime rate, average number of rooms per dwell-ing, accessibility to highways, and so on. The targets are the median values of owner-occupied homes, in thousands of dollars:>>> train_targets[1 5 . 2 , 4 2 . 3 , 5 0 .. . . 1 9 . 4 , 1 9 . 4 , 2 9 . 1 ]The prices are typically between $10,000 and $50,000. If that sounds cheap, remem-ber that this was the mid-1970s, and these prices aren’t adjusted for inflation. 4.3.2 Preparing the dataIt would be problematic to feed into a neural network values that all take wildly differ-ent ranges. The model might be able to automatically adapt to such heterogeneousdata, but it would definitely make learning more difficult. A widespread best practicefor dealing with such data is to do feature-wise normalization: for each feature in theinput data (a column in the input data matrix), we subtract the mean of the featureand divide by the standard deviation, so that the feature is centered around 0 and hasa unit standard deviation. This is easily done in NumPy.mean = train_data.mean(axis=0)train_data -= meanListing 4.23 Loading the Boston housing dataset Listing 4.24 Normalizing the data 115Predicting house prices: A regression examplestd = train_data.std(axis=0)train_data /= stdtest_data -= meantest_data /= stdNote that the quantities used for normalizing the test data are computed using thetraining data. You should never use any quantity computed on the test data in yourworkflow, even for something as simple as data normalization. 4.3.3 Building your modelBecause so few samples are available, we’ll use a very small model with two intermedi-ate layers, each with 64 units. In general, the less training data you have, the worseoverfitting will be, and using a small model is one way to mitigate overfitting.def build_model(): model = keras.Sequential([ layers.Dense(64, activation=\"relu\"), layers.Dense(64, activation=\"relu\"), layers.Dense(1) ]) model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"]) return modelThe model ends with a single unit and no activation (it will be a linear layer). This is atypical setup for scalar regression (a regression where you’re trying to predict a singlecontinuous value). Applying an activation function would constrain the range the out-put can take; for instance, if you applied a sigmoid activation function to the last layer,the model could only learn to predict values between 0 and 1. Here, because the lastlayer is purely linear, the model is free to learn to predict values in any range. Note that we compile the model with the mse loss function—mean squared error, thesquare of the difference between the predictions and the targets. This is a widely usedloss function for regression problems. We’re also monitoring a new metric during training: mean absolute error (MAE). It’s theabsolute value of the difference between the predictions and the targets. For instance, anMAE of 0.5 on this problem would mean your predictions are off by $500 on average. 4.3.4 Validating your approach using K-fold validationTo evaluate our model while we keep adjusting its parameters (such as the number ofepochs used for training), we could split the data into a training set and a validationset, as we did in the previous examples. But because we have so few data points, thevalidation set would end up being very small (for instance, about 100 examples). As aconsequence, the validation scores might change a lot depending on which datapoints we chose for validation and which we chose for training: the validation scoresListing 4.25 Model definitionBecause we need to instantiate the same model multiple times, we use a function to construct it. 116CHAPTER 4Getting started with neural networks: Classification and regressionmight have a high variance with regard to the validation split. This would prevent usfrom reliably evaluating our model. The best practice in such situations is to use K-fold cross-validation (see figure 4.8). It consists of splitting the available data into K partitions (typically K = 4 or 5), instanti-ating K identical models, and training each one on K – 1 partitions while evaluatingon the remaining partition. The validation score for the model used is then the aver-age of the K validation scores obtained. In terms of code, this is straightforward.k = 4 num_val_samples = len(train_data) // knum_epochs = 100 all_scores = [] for i in range(k): print(f\"Processing fold #{i}\") val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples] val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples] partial_train_data = np.concatenate( [train_data[:i * num_val_samples], train_data[(i + 1) * num_val_samples:]], axis=0) partial_train_targets = np.concatenate( [train_targets[:i * num_val_samples], train_targets[(i + 1) * num_val_samples:]], axis=0) model = build_model() model.fit(partial_train_data, partial_train_targets, epochs=num_epochs, batch_size=16, verbose=0) val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)all_scores.append(val_mae)Listing 4.26 K-fold validationData split into 3 partitionsValidationTrainingTrainingValidationscore #1Fold 1TrainingValidationTrainingValidationscore #2Final score:averageFold 2TrainingTrainingValidationValidationscore #3Fold 3Figure 4.8 K-fold cross-validation with K=3 Prepares thevalidation data: datafrom partition #kPrepares the training data: data from all other partitionsBuilds the Keras model (already compiled)Trains the model (in silent mode, verbose = 0)Evaluates the model onthe validation data 117Predicting house prices: A regression exampleRunning this with num_epochs = 100 yields the following results:>>> all_scores[2.112449, 3.0801501, 2.6483836, 2.4275346]>>> np.mean(all_scores)2.5671294The different runs do indeed show rather different validation scores, from 2.1 to 3.1.The average (2.6) is a much more reliable metric than any single score—that’s theentire point of K-fold cross-validation. In this case, we’re off by $2,600 on average,which is significant considering that the prices range from $10,000 to $50,000. Let’s try training the model a bit longer: 500 epochs. To keep a record of how wellthe model does at each epoch, we’ll modify the training loop to save the per-epochvalidation score log for each fold.num_epochs = 500 all_mae_histories = [] for i in range(k): print(f\"Processing fold #{i}\") val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples] val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples] partial_train_data = np.concatenate( [train_data[:i * num_val_samples], train_data[(i + 1) * num_val_samples:]], axis=0) partial_train_targets = np.concatenate( [train_targets[:i * num_val_samples], train_targets[(i + 1) * num_val_samples:]], axis=0) model = build_model() history = model.fit(partial_train_data, partial_train_targets, validation_data=(val_data, val_targets), epochs=num_epochs, batch_size=16, verbose=0) mae_history = history.history[\"val_mae\"] all_mae_histories.append(mae_history)We can then compute the average of the per-epoch MAE scores for all folds.average_mae_history = [ np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]Let’s plot this; see figure 4.9.plt.plot(range(1, len(average_mae_history) + 1), average_mae_history)plt.xlabel(\"Epochs\")plt.ylabel(\"Validation MAE\")plt.show()Listing 4.27 Saving the validation logs at each fold Listing 4.28 Building the history of successive mean K-fold validation scoresListing 4.29 Plotting validation scoresPrepares thevalidation data: datafrom partition #kPrepares the training data: data from all other partitionsBuilds the Keras model (already compiled)Trains the model (in silent mode, verbose=0) 118CHAPTER 4Getting started with neural networks: Classification and regression It may be a little difficult to read the plot, due to a scaling issue: the validation MAEfor the first few epochs is dramatically higher than the values that follow. Let’s omitthe first 10 data points, which are on a different scale than the rest of the curve.truncated_mae_history = average_mae_history[10:]plt.plot(range(1, len(truncated_mae_history) + 1), truncated_mae_history)plt.xlabel(\"Epochs\")plt.ylabel(\"Validation MAE\")plt.show()As you can see in figure 4.10, validation MAE stops improving significantly after120–140 epochs (this number includes the 10 epochs we omitted). Past that point,we start overfitting. Once you’re finished tuning other parameters of the model (in addition to thenumber of epochs, you could also adjust the size of the intermediate layers), you cantrain a final production model on all of the training data, with the best parameters,and then look at its performance on the test data.model = build_model()model.fit(train_data, train_targets,epochs=130,b a t c h _ s i z e =16,v e r b o s e =0)test_mse_score, test_mae_score = model.evaluate(test_data, test_targets)Listing 4.30 Plotting validation scores, excluding the first 10 data points Listing 4.31 Training the final modelFigure 4.9 Validation MAE by epoch Gets a fresh, compiled modelTrains it on the entirety of the data 119Predicting house prices: A regression example Here’s the final result:>>> test_mae_score2.4642276763916016We’re still off by a bit under $2,500. It’s an improvement! Just like with the two previ-ous tasks, you can try varying the number of layers in the model, or the number ofunits per layer, to see if you can squeeze out a lower test error. 4.3.5 Generating predictions on new dataWhen calling predict() on our binary classification model, we retrieved a scalar scorebetween 0 and 1 for each input sample. With our multiclass classification model, weretrieved a probability distribution over all classes for each sample. Now, with this sca-lar regression model, predict() returns the model’s guess for the sample’s price inthousands of dollars:>>> predictions = model.predict(test_data)>>> predictions[0]array([9.990133], dtype=float32)The first house in the test set is predicted to have a price of about $10,000. 4.3.6 Wrapping upHere’s what you should take away from this scalar regression example:Regression is done using different loss functions than we used for classification.Mean squared error (MSE) is a loss function commonly used for regression.Figure 4.10 Validation MAE by epoch, excluding the first 10 data points 120CHAPTER 4Getting started with neural networks: Classification and regressionSimilarly, evaluation metrics to be used for regression differ from those used forclassification; naturally, the concept of accuracy doesn’t apply for regression. Acommon regression metric is mean absolute error (MAE).When features in the input data have values in different ranges, each featureshould be scaled independently as a preprocessing step.When there is little data available, using K-fold validation is a great way to reli-ably evaluate a model.When little training data is available, it’s preferable to use a small model with fewintermediate layers (typically only one or two), in order to avoid severe overfitting. SummaryThe three most common kinds of machine learning tasks on vector data arebinary classification, multiclass classification, and scalar regression.–T h e “ W r a p p i n g u p ” s e c t i o n s e a r l i e r i n t h e c h a p t e r s u m m a r i z e t h e i m p o r t a n tpoints you’ve learned regarding each task.– Regression uses different loss functions and different evaluation metricsthan classification.You’ll usually need to preprocess raw data before feeding it into a neural network.When your data has features with different ranges, scale each feature inde-pendently as part of preprocessing.As training progresses, neural networks eventually begin to overfit and obtainworse results on never-before-seen data.If you don’t have much training data, use a small model with only one or twointermediate layers, to avoid severe overfitting.If your data is divided into many categories, you may cause information bottle-necks if you make the intermediate layers too small.When you’re working with little data, K-fold validation can help reliably evalu-ate your model. 121Fundamentalsof machine learning After the three practical examples in chapter 4, you should be starting to feel famil-iar with how to approach classification and regression problems using neural net-works, and you’ve witnessed the central problem of machine learning: overfitting.This chapter will formalize some of your new intuition about machine learning intoa solid conceptual framework, highlighting the importance of accurate model eval-uation and the balance between training and generalization.5.1 Generalization: The goal of machine learningIn the three examples presented in chapter 4—predicting movie reviews, topic clas-sification, and house-price regression—we split the data into a training set, a valida-tion set, and a test set. The reason not to evaluate the models on the same data theyThis chapter coversUnderstanding the tension between generalization and optimization, the fundamental issue in machine learningEvaluation methods for machine learning modelsBest practices to improve model fittingBest practices to achieve better generalization 122CHAPTER 5Fundamentals of machine learningwere trained on quickly became evident: after just a few epochs, performance onnever-before-seen data started diverging from performance on the training data,which always improves as training progresses. The models started to overfit. Overfittinghappens in every machine learning problem. The fundamental issue in machine learning is the tension between optimizationand generalization. Optimization refers to the process of adjusting a model to get thebest performance possible on the training data (the learning i n machine learning),whereas generalization r ef e r s t o h o w we l l t h e t r a i n e d mo d e l p er fo r m s o n d a ta i t h a snever seen before. The goal of the game is to get good generalization, of course, butyou don’t control generalization; you can only fit the model to its training data. If youdo that too well, overfitting kicks in and generalization suffers. But what causes overfitting? How can we achieve good generalization?5.1.1 Underfitting and overfittingFor the models you saw in the previous chapter, performance on the held-out valida-tion data started improving as training went on and then inevitably peaked after awhile. This pattern (illustrated in figure 5.1) is universal. You’ll see it with any modeltype and any dataset. At the beginning of training, optimization and generalization are correlated: thelower the loss on training data, the lower the loss on test data. While this is happening,your model is said to be underfit: there is still progress to be made; the network hasn’tyet modeled all relevant patterns in the training data. But after a certain number ofiterations on the training data, generalization stops improving, validation metrics stalland then begin to degrade: the model is starting to overfit. That is, it’s beginning tolearn patterns that are specific to the training data but that are misleading or irrele-vant when it comes to new data.Lossvalue Training timeUnderﬁttingOverﬁttingRobust ﬁtTraining curveValidation curve Figure 5.1 Canonical overfitting behavior 123Generalization: The goal of machine learning O v e r f i t t i n g i s p a r t i c u l a r l y l i k e l y t o o c c u r w h e n y o u r d a t a i s n o i s y , i f i t i n v o l v e suncertainty, or if it includes rare features. Let’s look at concrete examples.NOISY TRAINING DATAIn real-world datasets, it’s fairly common for some inputs to be invalid. Perhaps aMNIST digit could be an all-black image, for instance, or something like figure 5.2. What are these? I don’t know either. But they’re all part of the MNIST training set.What’s even worse, however, is having perfectly valid inputs that end up mislabeled,like those in figure 5.3. If a model goes out of its way to incorporate such outliers, its generalization perfor-mance will degrade, as shown in figure 5.4. For instance, a 4 that looks very close tothe mislabeled 4 in figure 5.3 may end up getting classified as a 9. Figure 5.2 Some pretty weird MNIST training samples Figure 5.3 Mislabeled MNIST training samples 124CHAPTER 5Fundamentals of machine learning AMBIGUOUS FEATURESNot all data noise comes from inaccuracies—even perfectly clean and neatly labeleddata can be noisy when the problem involves uncertainty and ambiguity. In classifica-tion tasks, it is often the case that some regions of the input feature space are associ-ated with multiple classes at the same time. Let’s say you’re developing a model thattakes an image of a banana and predicts whether the banana is unripe, ripe, or rotten.These categories have no objective boundaries, so the same picture might be classifiedas either unripe or ripe by different human labelers. Similarly, many problems involverandomness. You could use atmospheric pressure data to predict whether it will raintomorrow, but the exact same measurements may be followed sometimes by rain andsometimes by a clear sky, with some probability. A m o d e l c o u l d o v e r f i t t o s u c h p r o b a b i l i s t i c d a t a b y b e i n g t o o c o n f i d e n t a b o u tambiguous regions of the feature space, like in figure 5.5. A more robust fit wouldignore individual data points and look at the bigger picture. Figure 5.4 Dealing with outliers: robust fit vs. overfitting Area of uncertainty Figure 5.5 Robust fit vs. overfitting giving an ambiguous area of the feature space 125Generalization: The goal of machine learningRARE FEATURES AND SPURIOUS CORRELATIONSIf you’ve only ever seen two orange tabby cats in your life, and they both happened tobe terribly antisocial, you might infer that orange tabby cats are generally likely to beantisocial. That’s overfitting: if you had been exposed to a wider variety of cats, includ-ing more orange ones, you’d have learned that cat color is not well correlated withcharacter. Likewise, machine learning models trained on datasets that include rare featurevalues are highly susceptible to overfitting. In a sentiment classification task, if theword “cherimoya” (a fruit native to the Andes) only appears in one text in the train-ing data, and this text happens to be negative in sentiment, a poorly regularizedmodel might put a very high weight on this word and always classify new texts thatmention cherimoyas as negative, whereas, objectively, there’s nothing negative aboutthe cherimoya.1 Importantly, a feature value doesn’t need to occur only a couple of times to lead tospurious correlations. Consider a word that occurs in 100 samples in your trainingdata and that’s associated with a positive sentiment 54% of the time and with a nega-tive sentiment 46% of the time. That difference may well be a complete statisticalfluke, yet your model is likely to learn to leverage that feature for its classification task.This is one of the most common sources of overfitting. Here’s a striking example. Take MNIST. Create a new training set by concatenating784 white noise dimensions to the existing 784 dimensions of the data, so half of thedata is now noise. For comparison, also create an equivalent dataset by concatenating784 all-zeros dimensions. Our concatenation of meaningless features does not at allaffect the information content of the data: we’re only adding something. Human clas-sification accuracy wouldn’t be affected by these transformations at all.from tensorflow.keras.datasets import mnist import numpy as np (train_images, train_labels), _ = mnist.load_data()train_images = train_images.reshape((60000, 28 * 28))train_images = train_images.astype(\"float32\") / 255 train_images_with_noise_channels = np.concatenate( [train_images, np.random.random((len(train_images), 784))], axis=1) train_images_with_zeros_channels = np.concatenate( [train_images, np.zeros((len(train_images), 784))], axis=1)Now, let’s train the model from chapter 2 on both of these training sets. 1Mark Twain even called it “the most delicious fruit known to men.”Listing 5.1 Adding white noise channels or all-zeros channels to MNIST 126CHAPTER 5Fundamentals of machine learningfrom tensorflow import keras from tensorflow.keras import layers def get_model(): model = keras.Sequential([ layers.Dense(512, activation=\"relu\"), layers.Dense(10, activation=\"softmax\") ]) model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]) return model model = get_model()history_noise = model.fit( train_images_with_noise_channels, train_labels, epochs=10, batch_size=128, validation_split=0.2) model = get_model()history_zeros = model.fit( train_images_with_zeros_channels, train_labels, epochs=10, batch_size=128, validation_split=0.2)Let’s compare how the validation accuracy of each model evolves over time.import matplotlib.pyplot as pltval_acc_noise = history_noise.history[\"val_accuracy\"]val_acc_zeros = history_zeros.history[\"val_accuracy\"]epochs = range(1, 11)plt.plot(epochs, val_acc_noise, \"b-\", label=\"Validation accuracy with noise channels\")plt.plot(epochs, val_acc_zeros, \"b--\", label=\"Validation accuracy with zeros channels\")plt.title(\"Effect of noise channels on validation accuracy\")plt.xlabel(\"Epochs\")plt.ylabel(\"Accuracy\")plt.legend()Despite the data holding the same information in both cases, the validation accuracyof the model trained with noise channels ends up about one percentage point lower(see figure 5.6)—purely through the influence of spurious correlations. The morenoise channels you add, the further accuracy will degrade. Noisy features inevitably lead to overfitting. As such, in cases where you aren’t surewhether the features you have are informative or distracting, it’s common to do featureListing 5.2 Training the same model on MNIST data with noise channels or all-zero channels Listing 5.3 Plotting a validation accuracy comparison 127Generalization: The goal of machine learning selection before training. Restricting the IMDB data to the top 10,000 most commonwords was a crude form of feature selection, for instance. The typical way to do fea-ture selection is to compute some usefulness score for each feature available—ameasure of how informative the feature is with respect to the task, such as themutual information between the feature and the labels—and only keep features thatare above some threshold. Doing this would filter out the white noise channels inthe preceding example. 5.1.2 The nature of generalization in deep learningA remarkable fact about deep learning models is that they can be trained to fit any-thing, as long as they have enough representational power. Don’t believe me? Try shuffling the MNIST labels and train a model on that. Eventhough there is no relationship whatsoever between the inputs and the shuffledlabels, the training loss goes down just fine, even with a relatively small model. Natu-rally, the validation loss does not improve at all over time, since there is no possibilityof generalization in this setting.(train_images, train_labels), _ = mnist.load_data()train_images = train_images.reshape((60000, 28 * 28))train_images = train_images.astype(\"float32\") / 255 random_train_labels = train_labels[:]np.random.shuffle(random_train_labels) model = keras.Sequential([ layers.Dense(512, activation=\"relu\"),Listing 5.4 Fitting an MNIST model with randomly shuffled labelsFigure 5.6 Effect of noise channels on validation accuracy 128CHAPTER 5Fundamentals of machine learning layers.Dense(10, activation=\"softmax\")])model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])model.fit(train_images, random_train_labels, epochs=100, batch_size=128, validation_split=0.2)In fact, you don’t even need to do this with MNIST data—you could just generatewhite noise inputs and random labels. You could fit a model on that, too, as long as ithas enough parameters. It would just end up memorizing specific inputs, much like aPython dictionary. If this is the case, then how come deep learning models generalize at all? Shouldn’tthey just learn an ad hoc mapping between training inputs and targets, like a fancydict? What expectation can we have that this mapping will work for new inputs? As it turns out, the nature of generalization in deep learning has rather little to dowith deep learning models themselves, and much to do with the structure of informa-tion in the real world. Let’s take a look at what’s really going on here.THE MANIFOLD HYPOTHESISThe input to an MNIST classifier (before preprocessing) is a 28 × 28 array of integersbetween 0 and 255. The total number of possible input values is thus 256 to the powerof 784—much greater than the number of atoms in the universe. However, very few ofthese inputs would look like valid MNIST samples: actual handwritten digits onlyoccupy a tiny subspace of the parent space of all possible 28 × 28 uint8 arrays. What’smore, this subspace isn’t just a set of points sprinkled at random in the parent space: itis highly structured. First, the subspace of valid handwritten digits is continuous: if you take a sampleand modify it a little, it will still be recognizable as the same handwritten digit. Fur-ther, all samples in the valid subspace are connected by smooth paths that run throughthe subspace. This means that if you take two random MNIST digits A and B, thereexists a sequence of “intermediate” images that morph A into B, such that two consec-utive digits are very close to each other (see figure 5.7). Perhaps there will be a fewambiguous shapes close to the boundary between two classes, but even these shapeswould still look very digit-like. In technical terms, you would say that handwritten digits form a manifold withinthe space of possible 28 × 28 uint8 arrays. That’s a big word, but the concept is prettyintuitive. A “manifold” is a lower-dimensional subspace of some parent space that islocally similar to a linear (Euclidian) space. For instance, a smooth curve in the planeis a 1D manifold within a 2D space, because for every point of the curve, you can drawa tangent (the curve can be approximated by a line at every point). A smooth surfacewithin a 3D space is a 2D manifold. And so on. 129Generalization: The goal of machine learning More generally, the manifold hypothesis posits that all natural data lies on a low-dimen-sional manifold within the high-dimensional space where it is encoded. That’s a prettystrong statement about the structure of information in the universe. As far as we know,it’s accurate, and it’s the reason why deep learning works. It’s true for MNIST digits,but also for human faces, tree morphology, the sounds of the human voice, and evennatural language. The manifold hypothesis implies thatMachine learning models only have to fit relatively simple, low-dimensional,highly structured subspaces within their potential input space (latent mani-folds).Within one of these manifolds, it’s always possible to interpolate between twoinputs, that is to say, morph one into another via a continuous path along whichall points fall on the manifold.The ability to interpolate between samples is the key to understanding generalizationin deep learning. INTERPOLATION AS A SOURCE OF GENERALIZATIONIf you work with data points that can be interpolated, you can start making sense ofpoints you’ve never seen before by relating them to other points that lie close on themanifold. In other words, you can make sense of the totality of the space using only asample of the space. You can use interpolation to fill in the blanks. Note that interpolation on the latent manifold is different from linear interpola-tion in the parent space, as illustrated in figure 5.8. For instance, the average of pixelsbetween two MNIST digits is usually not a valid digit. C r u c i a l l y , w h i l e d e e p l e a r n i n g a c h i e v e s g e n e r a l i z a t i o n v i a i n t e r p o l a t i o n o n alearned approximation of the data manifold, it would be a mistake to assume thatinterpolation is all there is to generalization. It’s the tip of the iceberg. Interpolationcan only help you make sense of things that are very close to what you’ve seen before:Figure 5.7 Different MNIST digits gradually morphing into one another, showing that the space of handwritten digits forms a “manifold.” This image was generated using code from chapter 12. 130CHAPTER 5Fundamentals of machine learning it enables local generalization. But remarkably, humans deal with extreme novelty all thetime, and they do just fine. You don’t need to be trained in advance on countlessexamples of every situation you’ll ever have to encounter. Every single one of yourdays is different from any day you’ve experienced before, and different from any dayexperienced by anyone since the dawn of humanity. You can switch between spendinga week in NYC, a week in Shanghai, and a week in Bangalore without requiring thou-sands of lifetimes of learning and rehearsal for each city. Humans are capable of extreme generalization, which is enabled by cognitive mecha-nisms other than interpolation: abstraction, symbolic models of the world, reasoning,logic, common sense, innate priors about the world—what we generally call reason, asopposed to intuition and pattern recognition. The latter are largely interpolative innature, but the former isn’t. Both are essential to intelligence. We’ll talk more aboutthis in chapter 14. WHY DEEP LEARNING WORKSRemember the crumpled paper ball metaphor from chapter 2? A sheet of paper rep-resents a 2D manifold within 3D space (see figure 5.9). A deep learning model is atool for uncrumpling paper balls, that is, for disentangling latent manifolds. A deep learning model is basically a very high-dimensional curve—a curve that issmooth and continuous (with additional constraints on its structure, originating frommodel architecture priors), since it needs to be differentiable. And that curve is fittedto data points via gradient descent, smoothly and incrementally. By its very nature,deep learning is about taking a big, complex curve—a manifold—and incrementallyadjusting its parameters until it fits some training data points.Manifold interpolation(intermediate pointon the latent manifold)Linear interpolation(average in the encoding space) Figure 5.8 Difference between linear interpolation and interpolation on the latent manifold. Every point on the latent manifold of digits is a valid digit, but the average of two digits usually isn’t. Figure 5.9 Uncrumpling a complicated manifold of data 131Generalization: The goal of machine learning The curve involves enough parameters that it could fit anything—indeed, if you letyour model train for long enough, it will effectively end up purely memorizing itstraining data and won’t generalize at all. However, the data you’re fitting to isn’t madeof isolated points sparsely distributed across the underlying space. Your data forms ahighly structured, low-dimensional manifold within the input space—that’s the mani-fold hypothesis. And because fitting your model curve to this data happens graduallyand smoothly over time as gradient descent progresses, there will be an intermediatepoint during training at which the model roughly approximates the natural manifoldof the data, as you can see in figure 5.10. Moving along the curve learned by the model at that point will come close to movingalong the actual latent manifold of the data—as such, the model will be capable ofmaking sense of never-before-seen inputs via interpolation between training inputs. Besides the trivial fact that they have sufficient representational power, there are afew properties of deep learning models that make them particularly well-suited tolearning latent manifolds:Deep learning models implement a smooth, continuous mapping from theirinputs to their outputs. It has to be smooth and continuous because it mustbe differentiable, by necessity (you couldn’t do gradient descent otherwise).Before training:the model startswith a random initial state.Final state: the modeloverﬁts the training data,reaching perfect training loss.Beginning of training:the model graduallymoves toward a better ﬁt. Test time: performanceof robustly ﬁt modelon new data pointsFurther training: a robustﬁt is achieved, transitively,in the process of morphingthe model from its initialstate to its ﬁnal state. Test time: performanceof overﬁt modelon new data points Figure 5.10 Going from a random model to an overfit model, and achieving a robust fit as an intermediate state 132CHAPTER 5Fundamentals of machine learningThis smoothness helps approximate latent manifolds, which follow the sameproperties.Deep learning models tend to be structured in a way that mirrors the “shape” ofthe information in their training data (via architecture priors). This is particu-larly the case for image-processing models (discussed in chapters 8 and 9) andsequence-processing models (chapter 10). More generally, deep neural net-works structure their learned representations in a hierarchical and modularway, which echoes the way natural data is organized. TRAINING DATA IS PARAMOUNTWhile deep learning is indeed well suited to manifold learning, the power to general-ize is more a consequence of the natural structure of your data than a consequence ofany property of your model. You’ll only be able to generalize if your data forms a man-ifold where points can be interpolated. The more informative and the less noisy yourfeatures are, the better you will be able to generalize, since your input space will besimpler and better structured. Data curation and feature engineering are essential togeneralization. Further, because deep learning is curve fitting, for a model to perform well it needsto be trained on a dense sampling of its input space. A “dense sampling” in this contextmeans that the training data should densely cover the entirety of the input datamanifold (see figure 5.11). This is especially true near decision boundaries. With asufficiently dense sampling, it becomes possible to make sense of new inputs by inter-polating between past training inputs without having to use common sense, abstractreasoning, or external knowledge about the world—all things that machine learningmodels have no access to.Original latent spaceSparse sampling: themodel learned doesn’tmatch the latentspace and leads toincorrect interpolation.Dense sampling:the model learnedapproximates thelatent space well,and interpolationleads to generalization. Figure 5.11 A dense sampling of the input space is necessary in order to learn a model capable of accurate generalization. 133Evaluating machine learning modelsAs such, you should always keep in mind that the best way to improve a deep learningmodel is to train it on more data or better data (of course, adding overly noisy or inac-curate data will harm generalization). A denser coverage of the input data manifoldwill yield a model that generalizes better. You should never expect a deep learningmodel to perform anything more than crude interpolation between its training sam-ples, and thus you should do everything you can to make interpolation as easy as pos-sible. The only thing you will find in a deep learning model is what you put into it: thepriors encoded in its architecture and the data it was trained on. When getting more data isn’t possible, the next best solution is to modulate thequantity of information that your model is allowed to store, or to add constraints onthe smoothness of the model curve. If a network can only afford to memorize a smallnumber of patterns, or very regular patterns, the optimization process will force it tofocus on the most prominent patterns, which have a better chance of generalizingwell. The process of fighting overfitting this way is called regularization. We’ll reviewregularization techniques in depth in section 5.4.4. Before you can start tweaking your model to help it generalize better, you’ll need away to assess how your model is currently doing. In the following section, you’ll learnhow you can monitor generalization during model development: model evaluation. 5.2 Evaluating machine learning modelsYou can only control what you can observe. Since your goal is to develop models thatcan successfully generalize to new data, it’s essential to be able to reliably measure thegeneralization power of your model. In this section, I’ll formally introduce the differ-ent ways you can evaluate machine learning models. You’ve already seen most of themin action in the previous chapter.5.2.1 Training, validation, and test setsEvaluating a model always boils down to splitting the available data into three sets:training, validation, and test. You train on the training data and evaluate your modelon the validation data. Once your model is ready for prime time, you test it one finaltime on the test data, which is meant to be as similar as possible to production data.Then you can deploy the model in production. You may ask, why not have two sets: a training set and a test set? You’d train on thetraining data and evaluate on the test data. Much simpler! The reason is that developing a model always involves tuning its configuration: forexample, choosing the number of layers or the size of the layers (called the hyperpa-rameters of the model, to distinguish them from the parameters, which are the network’sweights). You do this tuning by using as a feedback signal the performance of themodel on the validation data. In essence, this tuning is a form of learning: a search fora good configuration in some parameter space. As a result, tuning the configurationof the model based on its performance on the validation set can quickly result in over-fitting to the validation set, even though your model is never directly trained on it. 134CHAPTER 5Fundamentals of machine learning Central to this phenomenon is the notion of information leaks. Every time you tunea hyperparameter of your model based on the model’s performance on the validationset, some information about the validation data leaks into the model. If you do thisonly once, for one parameter, then very few bits of information will leak, and your val-idation set will remain reliable for evaluating the model. But if you repeat this manytimes—running one experiment, evaluating on the validation set, and modifying yourmodel as a result—then you’ll leak an increasingly significant amount of informationabout the validation set into the model. At the end of the day, you’ll end up with a model that performs artificially well onthe validation data, because that’s what you optimized it for. You care about perfor-mance on completely new data, not on the validation data, so you need to use a com-pletely different, never-before-seen dataset to evaluate the model: the test dataset.Your model shouldn’t have had access to any information about the test set, even indi-rectly. If anything about the model has been tuned based on test set performance,then your measure of generalization will be flawed. Splitting your data into training, validation, and test sets may seem straightforward,but there are a few advanced ways to do it that can come in handy when little data isavailable. Let’s review three classic evaluation recipes: simple holdout validation, K-foldvalidation, and iterated K-fold validation with shuffling. We’ll also talk about the useof common-sense baselines to check that your training is going somewhere.SIMPLE HOLDOUT VALIDATIONSet apart some fraction of your data as your test set. Train on the remaining data, andevaluate on the test set. As you saw in the previous sections, in order to prevent infor-mation leaks, you shouldn’t tune your model based on the test set, and therefore youshould also reserve a validation set. Schematically, holdout validation looks like figure 5.12. Listing 5.5 shows a simpleimplementation. num_validation_samples =10000 np.random.shuffle(data) Listing 5.5 Holdout validation (note that labels are omitted for simplicity)Training setTotal available labeled data Train on this Evaluateon thisHeld-outvalidationsetFigure 5.12 Simple holdout validation splitShuffling the data is usually appropriate. 135Evaluating machine learning modelsvalidation_data = data[:num_validation_samples] training_data = data[num_validation_samples:] model = get_model() model.fit(training_data, ...) validation_score = model.evaluate(validation_data, ...) ... model = get_model() model.fit(np.concatenate([training_data, validation_data]), ...) test_score = model.evaluate(test_data, ...) This is the simplest evaluation protocol, and it suffers from one flaw: if little data isavailable, then your validation and test sets may contain too few samples to be statisti-cally representative of the data at hand. This is easy to recognize: if different randomshuffling rounds of the data before splitting end up yielding very different measuresof model performance, then you’re having this issue. K-fold validation and iteratedK-fold validation are two ways to address this, as discussed next. K-FOLD VALIDATIONWith this approach, you split your data into K partitions of equal size. For each parti-tion i, train a model on the remaining K - 1 partitions, and evaluate it on partition i.Your final score is then the averages of the K scores obtained. This method is helpfulwhen the performance of your model shows significant variance based on your train-test split. Like holdout validation, this method doesn’t exempt you from using a dis-tinct validation set for model calibration. Schematically, K-fold cross-validation looks like figure 5.13. Listing 5.6 shows a sim-ple implementation. Defines thevalidationsetDefines the training setTrains a model on the training data, and evaluates it on the validation dataAt this point you can tune your model, retrain it, evaluate it, tune it again.Once you’ve tuned your hyperparameters, it’s common to train your final model from scratch on all non-test data available. Data split into 3 partitionsValidationTrainingTrainingValidationscore #1Fold 1TrainingValidationTrainingValidationscore #2Final score:averageFold 2TrainingTrainingValidationValidationscore #3Fold 3Figure 5.13 K-fold cross-validation with K=3 136CHAPTER 5Fundamentals of machine learningk=3 num_validation_samples = len(data) // knp.random.shuffle(data)validation_scores = [] forfoldinrange(k):validation_data = data[num_validation_samples * fold: num_validation_samples * (fold +1)] training_data = np.concatenate( data[:num_validation_samples * fold], data[num_validation_samples * (fold +1):]) model = get_model() model.fit(training_data, ...)validation_score = model.evaluate(validation_data, ...)validation_scores.append(validation_score)validation_score = np.average(validation_scores) model = get_model() model.fit(data, ...) test_score = model.evaluate(test_data, ...) ITERATED K-FOLD VALIDATION WITH SHUFFLINGThis one is for situations in which you have relatively little data available and you needto evaluate your model as precisely as possible. I’ve found it to be extremely helpful inKaggle competitions. It consists of applying K-fold validation multiple times, shufflingthe data every time before splitting it K w a y s . T h e f i n a l s c o r e i s t h e a v e r a g e o f t h escores obtained at each run of K-fold validation. Note that you end up training andevaluating P * K models (where P is the number of iterations you use), which can bevery expensive. 5.2.2 Beating a common-sense baselineBesides the different evaluation protocols you have available, one last thing youshould know about is the use of common-sense baselines. T r a i n i n g a d e e p l e a r n i n g m o d e l i s a b i t l i k e p r e s s i n g a b u t t o n t h a t l a u n c h e s arocket in a parallel world. You can’t hear it or see it. You can’t observe the manifoldlearning process—it’s happening in a space with thousands of dimensions, and even ifyou projected it to 3D, you couldn’t interpret it. The only feedback you have is yourvalidation metrics—like an altitude meter on your invisible rocket. It’s particularly important to be able to tell whether you’re getting off the groundat all. What was the altitude you started at? Your model seems to have an accuracy of15%—is that any good? Before you start working with a dataset, you should always picka trivial baseline that you’ll try to beat. If you cross that threshold, you’ll know you’redoing something right: your model is actually using the information in the input datato make predictions that generalize, and you can keep going. This baseline could beListing 5.6 K-fold cross-validation (note that labels are omitted for simplicity)Selects the validation-data partition Uses the remainder of the data as training data. Note that the + operator represents list concatenation, not summation.Creates a brand-new instance of the model (untrained)Validation score: average of the validation scores of the k foldsTrains the final model on all non-test data available 137Evaluating machine learning modelsthe performance of a random classifier, or the performance of the simplest non-machine learning technique you can imagine. For instance, in the MNIST digit-classification example, a simple baseline would bea validation accuracy greater than 0.1 (random classifier); in the IMDB example, itwould be a validation accuracy greater than 0.5. In the Reuters example, it would bearound 0.18-0.19, due to class imbalance. If you have a binary classification problemwhere 90% of samples belong to class A and 10% belong to class B, then a classifierthat always predicts A already achieves 0.9 in validation accuracy, and you’ll need to dobetter than that. Having a common-sense baseline you can refer to is essential when you’re gettingstarted on a problem no one has solved before. If you can’t beat a trivial solution, yourmodel is worthless—perhaps you’re using the wrong model, or perhaps the problemyou’re tackling can’t even be approached with machine learning in the first place.Time to go back to the drawing board. 5.2.3 Things to keep in mind about model evaluationKeep an eye out for the following when you’re choosing an evaluation protocol:Data representativeness—You want both your training set and test set to be rep-resentative of the data at hand. For instance, if you’re trying to classify imagesof digits, and you’re starting from an array of samples where the samples areordered by their class, taking the first 80% of the array as your training setand the remaining 20% as your test set will result in your training set contain-ing only classes 0–7, whereas your test set will contain only classes 8–9. Thisseems like a ridiculous mistake, but it’s surprisingly common. For this reason,you usually should randomly shuffle your data before splitting it into trainingand test sets.The arrow of time—If you’re trying to predict the future given the past (for exam-ple, tomorrow’s weather, stock movements, and so on), you should not ran-domly shuffle your data before splitting it, because doing so will create atemporal leak: your model will effectively be trained on data from the future. Insuch situations, you should always make sure all data in your test set is posteriorto the data in the training set.Redundancy in your data—If some data points in your data appear twice (fairlycommon with real-world data), then shuffling the data and splitting it into atraining set and a validation set will result in redundancy between the trainingand validation sets. In effect, you’ll be testing on part of your training data,which is the worst thing you can do! Make sure your training set and validationset are disjoint.Having a reliable way to evaluate the performance of your model is how you’ll be ableto monitor the tension at the heart of machine learning—between optimization andgeneralization, underfitting and overfitting. 138CHAPTER 5Fundamentals of machine learning5.3 Improving model fitTo achieve the perfect fit, you must first overfit. Since you don’t know in advancewhere the boundary lies, you must cross it to find it. Thus, your initial goal as you startworking on a problem is to achieve a model that shows some generalization powerand that is able to overfit. Once you have such a model, you’ll focus on refining gener-alization by fighting overfitting. There are three common problems you’ll encounter at this stage:Training doesn’t get started: your training loss doesn’t go down over time.Training gets started just fine, but your model doesn’t meaningfully generalize:you can’t beat the common-sense baseline you set.Training and validation loss both go down over time, and you can beat yourbaseline, but you don’t seem to be able to overfit, which indicates you’re stillunderfitting.Let’s see how you can address these issues to achieve the first big milestone of amachine learning project: getting a model that has some generalization power (it canbeat a trivial baseline) and that is able to overfit.5.3.1 Tuning key gradient descent parametersSometimes training doesn’t get started, or it stalls too early. Your loss is stuck. This isalways something you can overcome: remember that you can fit a model to randomdata. Even if nothing about your problem makes sense, you should still be able to trainsomething—if only by memorizing the training data. When this happens, it’s always a problem with the configuration of the gradientdescent process: your choice of optimizer, the distribution of initial values in theweights of your model, your learning rate, or your batch size. All these parameters areinterdependent, and as such it is usually sufficient to tune the learning rate and thebatch size while keeping the rest of the parameters constant. Let’s look at a concrete example: let’s train the MNIST model from chapter 2 withan inappropriately large learning rate of value 1.(train_images, train_labels), _ = mnist.load_data()train_images = train_images.reshape((60000, 28 * 28))train_images = train_images.astype(\"float32\") / 255 model = keras.Sequential([ layers.Dense(512, activation=\"relu\"), layers.Dense(10, activation=\"softmax\")])model.compile(optimizer=keras.optimizers.RMSprop(1.), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])model.fit(train_images, train_labels, epochs=10,Listing 5.7 Training an MNIST model with an incorrectly high learning rate 139Improving model fit batch_size=128, validation_split=0.2)The model quickly reaches a training and validation accuracy in the 30%–40% range,but cannot get past that. Let’s try to lower the learning rate to a more reasonable valueof 1e-2.model = keras.Sequential([ layers.Dense(512, activation=\"relu\"), layers.Dense(10, activation=\"softmax\")])model.compile(optimizer=keras.optimizers.RMSprop(1e-2), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])model.fit(train_images, train_labels, epochs=10, batch_size=128, validation_split=0.2)The model is now able to train. If you find yourself in a similar situation, tryLowering or increasing the learning rate. A learning rate that is too high maylead to updates that vastly overshoot a proper fit, like in the preceding example,and a learning rate that is too low may make training so slow that it appearsto stall.Increasing the batch size. A batch with more samples will lead to gradients thatare more informative and less noisy (lower variance).You will, eventually, find a configuration that gets training started. 5.3.2 Leveraging better architecture priorsYou have a model that fits, but for some reason your validation metrics aren’t improv-ing at all. They remain no better than what a random classifier would achieve: yourmodel trains but doesn’t generalize. What’s going on? This is perhaps the worst machine learning situation you can find yourself in. Itindicates that something is fundamentally wrong with your approach, and it may not be easyto tell what. Here are some tips. First, it may be that the input data you’re using simply doesn’t contain sufficientinformation to predict your targets: the problem as formulated is not solvable. This iswhat happened earlier when we tried to fit an MNIST model where the labels wereshuffled: the model would train just fine, but validation accuracy would stay stuck at10%, because it was plainly impossible to generalize with such a dataset. It may also be that the kind of model you’re using is not suited for the problem athand. For instance, in chapter 10, you’ll see an example of a timeseries predictionListing 5.8 The same model with a more appropriate learning rate 140CHAPTER 5Fundamentals of machine learningproblem where a densely connected architecture isn’t able to beat a trivial baseline,whereas a more appropriate recurrent architecture does manage to generalize well.Using a model that makes the right assumptions about the problem is essential toachieve generalization: you should leverage the right architecture priors. In the following chapters, you’ll learn about the best architectures to use for a vari-ety of data modalities—images, text, timeseries, and so on. In general, you shouldalways make sure to read up on architecture best practices for the kind of task you’reattacking—chances are you’re not the first person to attempt it. 5.3.3 Increasing model capacityIf you manage to get to a model that fits, where validation metrics are going down,and that seems to achieve at least some level of generalization power, congratulations:you’re almost there. Next, you need to get your model to start overfitting. Consider the following small model—a simple logistic regression—trained on MNISTpixels.model = keras.Sequential([layers.Dense(10, activation=\"softmax\")])model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])history_small_model = model.fit( train_images, train_labels, epochs=20, batch_size=128, validation_split=0.2)You get loss curves that look like figure 5.14:import matplotlib.pyplot as pltval_loss = history_small_model.history[\"val_loss\"]epochs = range(1, 21)plt.plot(epochs, val_loss, \"b--\", label=\"Validation loss\")plt.title(\"Effect of insufficient model capacity on validation loss\")plt.xlabel(\"Epochs\")plt.ylabel(\"Loss\")plt.legend()Validation metrics seem to stall, or to improve very slowly, instead of peaking andreversing course. The validation loss goes to 0.26 and just stays there. You can fit, butyou can’t clearly overfit, even after many iterations over the training data. You’re likelyto encounter similar curves often in your career. Remember that it should always be possible to overfit. Much like the problemwhere the training loss doesn’t go down, this is an issue that can always be solved. IfListing 5.9 A simple logistic regression on MNIST 141Improving model fit you can’t seem to be able to overfit, it’s likely a problem with the representationalpower of your model: you’re going to need a bigger model, one with more capacity,that is to say, one able to store more information. You can increase representationalpower by adding more layers, using bigger layers (layers with more parameters), orusing kinds of layers that are more appropriate for the problem at hand (betterarchitecture priors). Let’s try training a bigger model, one with two intermediate layers with 96 unitseach:model = keras.Sequential([ layers.Dense(96, activation=\"relu\"), layers.Dense(96, activation=\"relu\"), layers.Dense(10, activation=\"softmax\"),])model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])history_large_model = model.fit( train_images, train_labels, epochs=20, batch_size=128, validation_split=0.2)The validation curve now looks exactly like it should: the model fits fast and startsoverfitting after 8 epochs (see figure 5.15). Figure 5.14 Effect of insufficient model capacity on loss curves 142CHAPTER 5Fundamentals of machine learning 5.4 Improving generalizationOnce your model has shown itself to have some generalization power and to be ableto overfit, it’s time to switch your focus to maximizing generalization.5.4.1 Dataset curationYou’ve already learned that generalization in deep learning originates from the latentstructure of your data. If your data makes it possible to smoothly interpolate betweensamples, you will be able to train a deep learning model that generalizes. If your prob-lem is overly noisy or fundamentally discrete, like, say, list sorting, deep learning willnot help you. Deep learning is curve fitting, not magic. As such, it is essential that you make sure that you’re working with an appropriatedataset. Spending more effort and money on data collection almost always yields a muchgreater return on investment than spending the same on developing a better model.Make sure you have enough data. Remember that you need a dense sampling ofthe input-cross-output space. More data will yield a better model. Sometimes,problems that seem impossible at first become solvable with a larger dataset.Minimize labeling errors—visualize your inputs to check for anomalies, andproofread your labels.Clean your data and deal with missing values (we’ll cover this in the next chapter).If you have many features and you aren’t sure which ones are actually useful, dofeature selection.A particularly important way to improve the generalization potential of your data isfeature engineering. For most machine learning problems, feature engineering is a keyingredient for success. Let’s take a look. Figure 5.15 Validation loss for a model with appropriate capacity 143Improving generalization5.4.2 Feature engineeringFeature engineering is the process of using your own knowledge about the data and aboutthe machine learning algorithm at hand (in this case, a neural network) to make thealgorithm work better by applying hardcoded (non-learned) transformations to thedata before it goes into the model. In many cases, it isn’t reasonable to expect amachine learning model to be able to learn from completely arbitrary data. The dataneeds to be presented to the model in a way that will make the model’s job easier. Let’s look at an intuitive example. Suppose you’re trying to develop a model thatcan take as input an image of a clock and can output the time of day (see figure 5.16). If you choose to use the raw pixels of the image as input data, you have a difficultmachine learning problem on your hands. You’ll need a convolutional neural net-work to solve it, and you’ll have to expend quite a bit of computational resources totrain the network. But if you already understand the problem at a high level (you understand howhumans read time on a clock face), you can come up with much better input featuresfor a machine learning algorithm: for instance, it’s easy to write a five-line Pythonscript to follow the black pixels of the clock hands and output the (x, y) coordinatesof the tip of each hand. Then a simple machine learning algorithm can learn to asso-ciate these coordinates with the appropriate time of day. You can go even further: do a coordinate change, and express the (x, y) coordi-nates as polar coordinates with regard to the center of the image. Your input willbecome the angle theta of each clock hand. At this point, your features are makingthe problem so easy that no machine learning is required; a simple rounding opera-tion and dictionary lookup are enough to recover the approximate time of day. That’s the essence of feature engineering: making a problem easier by expressingit in a simpler way. Make the latent manifold smoother, simpler, better organized.Doing so usually requires understanding the problem in depth.Raw data:pixel gridBetterfeatures:clock hands’coordinates{x1: 0.7,y1: 0.7}{x2: 0.5,y2: 0.0}{x1: 0.0,y2: 1.0}{x2: -0.38,y2: 0.32}Even betterfeatures:angles ofclock handstheta1: 45theta2: 0theta1: 90theta2: 140Figure 5.16 Feature engineering for reading the time on a clock 144CHAPTER 5Fundamentals of machine learning Before deep learning, feature engineering used to be the most important part ofthe machine learning workflow, because classical shallow algorithms didn’t havehypothesis spaces rich enough to learn useful features by themselves. The way you pre-sented the data to the algorithm was absolutely critical to its success. For instance,before convolutional neural networks became successful on the MNIST digit-classifi-cation problem, solutions were typically based on hardcoded features such as thenumber of loops in a digit image, the height of each digit in an image, a histogram ofpixel values, and so on. Fortunately, modern deep learning removes the need for most feature engineer-ing, because neural networks are capable of automatically extracting useful featuresfrom raw data. Does this mean you don’t have to worry about feature engineering aslong as you’re using deep neural networks? No, for two reasons:Good features still allow you to solve problems more elegantly while using fewerresources. For instance, it would be ridiculous to solve the problem of reading aclock face using a convolutional neural network.Good features let you solve a problem with far less data. The ability of deeplearning models to learn features on their own relies on having lots of trainingdata available; if you have only a few samples, the information value in their fea-tures becomes critical. 5.4.3 Using early stoppingIn deep learning, we always use models that are vastly overparameterized: they haveway more degrees of freedom than the minimum necessary to fit to the latent mani-fold of the data. This overparameterization is not an issue, because you never fully fit adeep learning model. Such a fit wouldn’t generalize at all. You will always interrupt train-ing long before you’ve reached the minimum possible training loss. Finding the exact point during training where you’ve reached the most generaliz-able fit—the exact boundary between an underfit curve and an overfit curve—is oneof the most effective things you can do to improve generalization. In the examples in the previous chapter, we would start by training our models forlonger than needed to figure out the number of epochs that yielded the best valida-tion metrics, and then we would retrain a new model for exactly that number ofepochs. This is pretty standard, but it requires you to do redundant work, which cansometimes be expensive. Naturally, you could just save your model at the end of eachepoch, and once you’ve found the best epoch, reuse the closest saved model you have.In Keras, it’s typical to do this with an EarlyStopping callback, which will interrupttraining as soon as validation metrics have stopped improving, while remembering thebest known model state. You’ll learn to use callbacks in chapter 7. 145Improving generalization5.4.4 Regularizing your modelRegularization techniques are a set of best practices that actively impede the model’s abil-ity to fit perfectly to the training data, with the goal of making the model perform bet-ter during validation. This is called “regularizing” the model, because it tends to makethe model simpler, more “regular,” its curve smoother, more “generic”; thus it is lessspecific to the training set and better able to generalize by more closely approximat-ing the latent manifold of the data. Keep in mind that regularizing a model is a process that should always be guidedby an accurate evaluation procedure. You will only achieve generalization if you canmeasure it. Let’s review some of the most common regularization techniques and apply themin practice to improve the movie-classification model from chapter 4.REDUCING THE NETWORK’S SIZEYou’ve already learned that a model that is too small will not overfit. The simplest wayto mitigate overfitting is to reduce the size of the model (the number of learnableparameters in the model, determined by the number of layers and the number ofunits per layer). If the model has limited memorization resources, it won’t be able tosimply memorize its training data; thus, in order to minimize its loss, it will have toresort to learning compressed representations that have predictive power regardingthe targets—precisely the type of representations we’re interested in. At the sametime, keep in mind that you should use models that have enough parameters that theydon’t underfit: your model shouldn’t be starved for memorization resources. There isa compromise to be found between too much capacity and not enough capacity. Unfortunately, there is no magical formula to determine the right number of layersor the right size for each layer. You must evaluate an array of different architectures (onyour validation set, not on your test set, of course) in order to find the correct modelsize for your data. The general workflow for finding an appropriate model size is to startwith relatively few layers and parameters, and increase the size of the layers or add newlayers until you see diminishing returns with regard to validation loss. Let’s try this on the movie-review classification model. The following listing showsour original model.from tensorflow.keras.datasets import imdb(train_data, train_labels), _ = imdb.load_data(num_words=10000) def vectorize_sequences(sequences, dimension=10000): results = np.zeros((len(sequences), dimension)) for i, sequence in enumerate(sequences): results[i, sequence] = 1. return resultstrain_data = vectorize_sequences(train_data) Listing 5.10 Original model 146CHAPTER 5Fundamentals of machine learningmodel = keras.Sequential([ layers.Dense(16, activation=\"relu\"), layers.Dense(16, activation=\"relu\"), layers.Dense(1, activation=\"sigmoid\")])model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])history_original = model.fit(train_data, train_labels, epochs=20, batch_size=512, validation_split=0.4)Now let’s try to replace it with this smaller model.model = keras.Sequential([ layers.Dense(4, activation=\"relu\"), layers.Dense(4, activation=\"relu\"), layers.Dense(1, activation=\"sigmoid\")])model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])history_smaller_model = model.fit( train_data, train_labels, epochs=20, batch_size=512, validation_split=0.4)Figure 5.17 shows a comparison of the validation losses of the original model and thesmaller model.Listing 5.11 Version of the model with lower capacity Figure 5.17 Original model vs. smaller model on IMDB review classification 147Improving generalizationAs you can see, the smaller model starts overfitting later than the reference model(after six epochs rather than four), and its performance degrades more slowly once itstarts overfitting. Now, let’s add to our benchmark a model that has much more capacity—far morethan the problem warrants. While it is standard to work with models that are signifi-cantly overparameterized for what they’re trying to learn, there can definitely be sucha thing as too much memorization capacity. You’ll know your model is too large if itstarts overfitting right away and if its validation loss curve looks choppy with high-variance (although choppy validation metrics could also be a symptom of using anunreliable validation process, such as a validation split that’s too small).model = keras.Sequential([ layers.Dense(512, activation=\"relu\"), layers.Dense(512, activation=\"relu\"), layers.Dense(1, activation=\"sigmoid\")])model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])history_larger_model = model.fit( train_data, train_labels, epochs=20, batch_size=512, validation_split=0.4)Figure 5.18 shows how the bigger model fares compared with the reference model.Listing 5.12 Version of the model with higher capacity Figure 5.18 Original model vs. much larger model on IMDB review classification 148CHAPTER 5Fundamentals of machine learningThe bigger model starts overfitting almost immediately, after just one epoch, and itoverfits much more severely. Its validation loss is also noisier. It gets training loss nearzero very quickly. The more capacity the model has, the more quickly it can model thetraining data (resulting in a low training loss), but the more susceptible it is to overfit-ting (resulting in a large difference between the training and validation loss). ADDING WEIGHT REGULARIZATIONYou may be familiar with the principle of Occam’s razor: given two explanations forsomething, the explanation most likely to be correct is the simplest one—the one thatmakes fewer assumptions. This idea also applies to the models learned by neural net-works: given some training data and a network architecture, multiple sets of weightvalues (multiple models) could explain the data. Simpler models are less likely to over-fit than complex ones. A simple model in this context is a model where the distribution of parameter valueshas less entropy (or a model with fewer parameters, as you saw in the previous sec-tion). Thus, a common way to mitigate overfitting is to put constraints on the com-plexity of a model by forcing its weights to take only small values, which makes thedistribution of weight values more regular. This is called weight regularization, and it’sdone by adding to the loss function of the model a cost associated with having largeweights. This cost comes in two flavors:L1 regularization—The cost added is proportional to the absolute value of theweight coefficients (the L1 norm of the weights).L2 regularization—The cost added is proportional to the square of the value of theweight coefficients (the L2 norm of the weights). L2 regularization is also calledweight decay in the context of neural networks. Don’t let the different name con-fuse you: weight decay is mathematically the same as L2 regularization.In Keras, weight regularization is added by passing weight regularizer instances to layersas keyword arguments. Let’s add L2 weight regularization to our initial movie-reviewclassification model.from tensorflow.keras import regularizersmodel = keras.Sequential([ layers.Dense(16, kernel_regularizer=regularizers.l2(0.002), activation=\"relu\"), layers.Dense(16, kernel_regularizer=regularizers.l2(0.002), activation=\"relu\"), layers.Dense(1, activation=\"sigmoid\")])model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])Listing 5.13 Adding L2 weight regularization to the model 149Improving generalizationhistory_l2_reg = model.fit( train_data, train_labels, epochs=20, batch_size=512, validation_split=0.4)In the preceding listing, l2(0.002) means every coefficient in the weight matrix ofthe layer will add 0.002 * weight_coefficient_value ** 2 to the total loss of themodel. Note that because this penalty is only added at training time, the loss for thismodel will be much higher at training than at test time. Figure 5.19 shows the impact of the L2 regularization penalty. As you can see, themodel with L2 regularization has become much more resistant to overfitting than thereference model, even though both models have the same number of parameters. As an alternative to L2 regularization, you can use one of the following Keras weightregularizers.fromtensorflow.kerasimportregularizersregularizers.l1(0.001) regularizers.l1_l2(l1=0.001,l 2 =0.001) Note that weight regularization is more typically used for smaller deep learning mod-els. Large deep learning models tend to be so overparameterized that imposing con-straints on weight values hasn’t much impact on model capacity and generalization. Inthese cases, a different regularization technique is preferred: dropout. Listing 5.14 Different weight regularizers available in KerasFigure 5.19 Effect of L2 weight regularization on validation loss L1 regularizationSimultaneous L1 and L2 regularization 150CHAPTER 5Fundamentals of machine learningADDING DROPOUTDropout i s o n e o f t h e m o s t e f f e c t i v e a n d m o s t c o m m o n l y u s e d r e g u l a r i z a t i o n t e c h -niques for neural networks; it was developed by Geoff Hinton and his students at theUniversity of Toronto. Dropout, applied to a layer, consists of randomly dropping out(setting to zero) a number of output features of the layer during training. Let’s say agiven layer would normally return a vector [0.2, 0.5, 1.3, 0.8, 1.1] for a giveninput sample during training. After applying dropout, this vector will have a few zeroentries distributed at random: for example, [0, 0.5, 1.3, 0, 1.1]. The dropout rateis the fraction of the features that are zeroed out; it’s usually set between 0.2 and 0.5.At test time, no units are dropped out; instead, the layer’s output values are scaleddown by a factor equal to the dropout rate, to balance for the fact that more units areactive than at training time. C o n s i d e r a N u m P y m a t r i x c o n t a i n i n g t h e o u t p u t o f a l a y e r , layer_output, ofshape (batch_size, features). At training time, we zero out at random a fraction ofthe values in the matrix:layer_output *= np.random.randint(0,h i g h =2,s i z e = l a y e r _ o u t p u t . s h a p e ) At test time, we scale down the output by the dropout rate. Here, we scale by 0.5(because we previously dropped half the units):layer_output *=0.5 Note that this process can be implemented by doing both operations at training timeand leaving the output unchanged at test time, which is often the way it’s imple-mented in practice (see figure 5.20):layer_output *= np.random.randint(0,h i g h =2,s i z e = l a y e r _ o u t p u t . s h a p e ) layer_output /=0.5 This technique may seem strange and arbitrary. Why would this help reduce overfit-ting? Hinton says he was inspired by, among other things, a fraud-prevention mecha-nism used by banks. In his own words, “I went to my bank. The tellers kept changingand I asked one of them why. He said he didn’t know but they got moved around a lot.At training time, drops out 50% of the units in the outputAt test timeAt training timeNote that we’re scaling up rather than scaling down in this case.0.3* 20.60.20.70.20.11.90.51.50.00.31.00.00.31.20.00.050%dropout0.60.00.70.20.11.90.01.50.00.30.00.00.30.00.0Figure 5.20 Dropout applied to an activation matrix at training time, with rescaling happening during training. At test time the activation matrix is unchanged. 151Improving generalizationI figured it must be because it would require cooperation between employees to suc-cessfully defraud the bank. This made me realize that randomly removing a differentsubset of neurons on each example would prevent conspiracies and thus reduce over-fitting.” The core idea is that introducing noise in the output values of a layer canbreak up happenstance patterns that aren’t significant (what Hinton refers to as con-spiracies), which the model will start memorizing if no noise is present. In Keras, you can introduce dropout in a model via the Dropout layer, which isapplied to the output of the layer right before it. Let’s add two Dropout layers in theIMDB model to see how well they do at reducing overfitting.model = keras.Sequential([ layers.Dense(16, activation=\"relu\"), layers.Dropout(0.5), layers.Dense(16, activation=\"relu\"), layers.Dropout(0.5), layers.Dense(1, activation=\"sigmoid\")])model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])history_dropout = model.fit( train_data, train_labels, epochs=20, batch_size=512, validation_split=0.4)Figure 5.21 shows a plot of the results. This is a clear improvement over the referencemodel—it also seems to be working much better than L2 regularization, since the low-est validation loss reached has improved.Listing 5.15 Adding dropout to the IMDB model Figure 5.21 Effect of dropout on validation loss 152CHAPTER 5Fundamentals of machine learningTo recap, these are the most common ways to maximize generalization and preventoverfitting in neural networks:Get more training data, or better training data.Develop better features.Reduce the capacity of the model.Add weight regularization (for smaller models).Add dropout. SummaryThe purpose of a machine learning model is to generalize: to perform accuratelyon never-before-seen inputs. It’s harder than it seems.A deep neural network achieves generalization by learning a parametric modelthat can successfully interpolate between training samples—such a model can besaid to have learned the “latent manifold” of the training data. This is why deeplearning models can only make sense of inputs that are very close to whatthey’ve seen during training.The fundamental problem in machine learning is the tension between optimizationand generalization: to attain generalization, you must first achieve a good fit tothe training data, but improving your model’s fit to the training data will inevi-tably start hurting generalization after a while. Every single deep learning bestpractice deals with managing this tension.The ability of deep learning models to generalize comes from the fact that theymanage to learn to approximate the latent manifold of their data, and can thusmake sense of new inputs via interpolation.It’s essential to be able to accurately evaluate the generalization power of yourmodel while you’re developing it. You have at your disposal an array of evalua-tion methods, from simple holdout validation to K-fold cross-validation anditerated K-fold cross-validation with shuffling. Remember to always keep a com-pletely separate test set for final model evaluation, since information leaks fromyour validation data to your model may have occurred.When you start working on a model, your goal is first to achieve a model thathas some generalization power and that can overfit. Best practices for doingthis include tuning your learning rate and batch size, leveraging better architec-ture priors, increasing model capacity, or simply training longer.As your model starts overfitting, your goal switches to improving generalizationthrough model regularization. You can reduce your model’s capacity, add dropoutor weight regularization, and use early stopping. And naturally, a larger or bet-ter dataset is always the number one way to help a model generalize. 153The universal workflowof machine learning Our previous examples have assumed that we already had a labeled dataset to startfrom, and that we could immediately start training a model. In the real world, thisis often not the case. You don’t start from a dataset, you start from a problem. I m a g i n e t h a t y o u ’ r e s t a r t i n g y o u r o w n m a c h i n e l e a r n i n g c o n s u l t i n g s h o p . Y o uincorporate, you put up a fancy website, you notify your network. The projects startrolling in:A personalized photo search engine for a picture-sharing social network—type in “wedding” and retrieve all the pictures you took at weddings, withoutany manual tagging needed.Flagging spam and offensive text content among the posts of a buddingchat app.Building a music recommendation system for users of an online radio.Detecting credit card fraud for an e-commerce website.This chapter coversSteps for framing a machine learning problemSteps for developing a working modelSteps for deploying your model in production and maintaining it 154CHAPTER 6The universal workflow of machine learningPredicting display ad click-through rate to decide which ad to serve to a givenuser at a given time.Flagging anomalous cookies on the conveyor belt of a cookie-manufacturing line.Using satellite images to predict the location of as-yet unknown archeological sites. It would be very convenient if you could import the correct dataset from keras.data-sets a n d s t a r t f i t t i n g s o m e d e e p l ea rn i n g m o d e l s . U n f o r t u na t e l y , i n t h e r e a l w o r l dyou’ll have to start from scratch. In this chapter, you’ll learn about a universal step-by-step blueprint that you canuse to approach and solve any machine learning problem, like those in the previouslist. This template will bring together and consolidate everything you’ve learned inchapters 4 and 5, and will give you the wider context that should anchor what you’lllearn in the next chapters. The universal workflow of machine learning is broadly structured in three parts:1Define the task—Understand the problem domain and the business logic under-lying what the customer asked for. Collect a dataset, understand what the datarepresents, and choose how you will measure success on the task.2Develop a model—Prepare your data so that it can be processed by a machinelearning model, select a model evaluation protocol and a simple baseline tobeat, train a first model that has generalization power and that can overfit, andthen regularize and tune your model until you achieve the best possible gener-alization performance.3Deploy the model—Present your work to stakeholders, ship the model to a webserver, a mobile app, a web page, or an embedded device, monitor the model’sNote on ethicsYou may sometimes be offered ethically dubious projects, such as “building an AIthat rates the trustworthiness of someone from a picture of their face.” First of all,the validity of the project is in doubt: it isn’t clear why trustworthiness would bereflected on someone’s face. Second, such a task opens the door to all kinds of eth-ical problems. Collecting a dataset for this task would amount to recording the biasesand prejudices of the people who label the pictures. The models you would train onsuch data would merely encode these same biases into a black-box algorithm thatwould give them a thin veneer of legitimacy. In a largely tech-illiterate society likeours, “the AI algorithm said this person cannot be trusted” strangely appears to carrymore weight and objectivity than “John Smith said this person cannot be trusted,”despite the former being a learned approximation of the latter. Your model would belaundering and operationalizing at scale the worst aspects of human judgement, withnegative effects on the lives of real people.Technology is never neutral. If your work has any impact on the world, this impact hasa moral direction: technical choices are also ethical choices. Always be deliberateabout the values you want your work to support. 155Define the taskperformance in the wild, and start collecting the data you’ll need to build thenext-generation model.Let’s dive in.6.1 Define the taskYou can’t do good work without a deep understanding of the context of what you’redoing. Why is your customer trying to solve this particular problem? What value willthey derive from the solution—how will your model be used, and how will it fit intoyour customer’s business processes? What kind of data is available, or could be col-lected? What kind of machine learning task can be mapped to the business problem?6.1.1 Frame the problemFraming a machine learning problem usually involves many detailed discussions withstakeholders. Here are the questions that should be on the top of your mind:What will your input data be? What are you trying to predict? You can onlylearn to predict something if you have training data available: for example,you can only learn to classify the sentiment of movie reviews if you have bothmovie reviews and sentiment annotations available. As such, data availabilityis usually the limiting factor at this stage. In many cases, you will have toresort to collecting and annotating new datasets yourself (which we’ll coverin the next section).What type of machine learning task are you facing? Is it binary classification?Multiclass classification? Scalar regression? Vector regression? Multiclass, multi-label classification? Image segmentation? Ranking? Something else, like cluster-ing, generation, or reinforcement learning? In some cases, it may be thatmachine learning isn’t even the best way to make sense of the data, and youshould use something else, such as plain old-school statistical analysis.–T h e p h o t o s e a r c h e n g i n e p r o j e c t i s a m u l t i c l a s s , m u l t i l a b e l c l a s s i f i c a t i o n t a s k .–T h e s p a m d e t e c t i o n p r o j e c t i s a b i n a r y c l a s s i f i c a t i o n t a s k . I f y o u s e t “ o f f e n s i v econtent” as a separate class, it’s a three-way classification task.–T h e m u s i c r e c o m m e n d a t i o n e n g i n e t u r n s o u t t o b e b e t t e r h a n d l e d n o t v i adeep learning, but via matrix factorization (collaborative filtering).–T h e c r e d i t c a r d f r a u d d e t e c t i o n p r o j e c t i s a b i n a r y c l a s s i f i c a t i o n t a s k .–T h e c l i c k - t h r o u g h - r a t e p r e d i c t i o n p r o j e c t i s a s c a l a r r e g r e s s i o n t a s k .–A n o m a l o u s c o o k i e d e t e c t i o n i s a b i n a r y c l a s s i f i c a t i o n t a s k , b u t i t w i l l a l s orequire an object detection model as a first stage in order to correctly cropout the cookies in raw images. Note that the set of machine learning tech-niques known as “anomaly detection” would not be a good fit in this setting!–T h e p r o j e c t f o r f i n d i n g n e w a r c h e o l o g i c a l s i t e s f r o m s a t e l l i t e i m a g e s i s a nimage-similarity ranking task: you need to retrieve new images that look themost like known archeological sites. 156CHAPTER 6The universal workflow of machine learningWhat do existing solutions look like? Perhaps your customer already has ahandcrafted algorithm that handles spam filtering or credit card fraud detec-tion, with lots of nested if statements. Perhaps a human is currently in chargeof manually handling the process under consideration—monitoring the con-veyor belt at the cookie plant and manually removing the bad cookies, or craft-ing playlists of song recommendations to be sent out to users who liked aspecific artist. You should make sure you understand what systems are already inplace and how they work.Are there particular constraints you will need to deal with? For example, youcould find out that the app for which you’re building a spam detection system isstrictly end-to-end encrypted, so that the spam detection model will have to liveon the end user’s phone and must be trained on an external dataset. Perhapsthe cookie-filtering model has such latency constraints that it will need to runon an embedded device at the factory rather than on a remote server. Youshould understand the full context in which your work will fit.Once you’ve done your research, you should know what your inputs will be, what yourtargets will be, and what broad type of machine learning task the problem maps to. Beaware of the hypotheses you’re making at this stage:You hypothesize that your targets can be predicted given your inputs.You hypothesize that the data that’s available (or that you will soon collect) issufficiently informative to learn the relationship between inputs and targets.Until you have a working model, these are merely hypotheses, waiting to be validatedor invalidated. Not all problems can be solved with machine learning; just becauseyou’ve assembled examples of inputs X and targets Y doesn’t mean X contains enoughinformation to predict Y. For instance, if you’re trying to predict the movements of astock on the stock market given its recent price history, you’re unlikely to succeed,because price history doesn’t contain much predictive information. 6.1.2 Collect a datasetOnce you understand the nature of the task and you know what your inputs and tar-gets are going to be, it’s time for data collection—the most arduous, time-consuming,and costly part of most machine learning projects.The photo search engine project requires you to first select the set of labels youwant to classify—you settle on 10,000 common image categories. Then youneed to manually tag hundreds of thousands of your past user-uploaded imageswith labels from this set.For the chat app’s spam detection project, because user chats are end-to-endencrypted, you cannot use their contents for training a model. You need to gainaccess to a separate dataset of tens of thousands of unfiltered social mediaposts, and manually tag them as spam, offensive, or acceptable. 157Define the taskFor the music recommendation engine, you can just use the “likes” of yourusers. No new data needs to be collected. Likewise for the click-through-rateprediction project: you have an extensive record of click-through rate for yourpast ads, going back years.For the cookie-flagging model, you will need to install cameras above the con-veyor belts to collect tens of thousands of images, and then someone will needto manually label these images. The people who know how to do this currentlywork at the cookie factory, but it doesn’t seem too difficult. You should be ableto train people to do it.The satellite imagery project will require a team of archeologists to collect adatabase of existing sites of interest, and for each site you will need to find exist-ing satellite images taken in different weather conditions. To get a good model,you’re going to need thousands of different sites.You learned in chapter 5 that a model’s ability to generalize comes almost entirelyfrom the properties of the data it is trained on—the number of data points you have,the reliability of your labels, the quality of your features. A good dataset is an asset wor-thy of care and investment. If you get an extra 50 hours to spend on a project, chancesare that the most effective way to allocate them is to collect more data rather thansearch for incremental modeling improvements. The point that data matters more than algorithms was most famously made in a2009 paper by Google researchers titled “The Unreasonable Effectiveness of Data”(the title is a riff on the well-known 1960 article “The Unreasonable Effectiveness ofMathematics in the Natural Sciences” by Eugene Wigner). This was before deep learn-ing was popular, but, remarkably, the rise of deep learning has only made the impor-tance of data greater. I f y o u ’ re d o i n g s up e rv i s e d l ea r n i n g , t h en o n ce y o u’ v e c o l l e ct e d i n p ut s (s u ch a simages) you’re going to need annotations for them (such as tags for those images)—the targets you will train your model to predict. Sometimes, annotations can be retrievedautomatically, such as those for the music recommendation task or the click-through-rate prediction task. But often you have to annotate your data by hand. This is a labor-heavy process.INVESTING IN DATA ANNOTATION INFRASTRUCTUREYour data annotation process will determine the quality of your targets, which in turndetermine the quality of your model. Carefully consider the options you have available:Should you annotate the data yourself?Should you use a crowdsourcing platform like Mechanical Turk to collect labels?Should you use the services of a specialized data-labeling company?Outsourcing can potentially save you time and money, but it takes away control. Usingsomething like Mechanical Turk is likely to be inexpensive and to scale well, but yourannotations may end up being quite noisy. 158CHAPTER 6The universal workflow of machine learning To pick the best option, consider the constraints you’re working with:Do the data labelers need to be subject matter experts, or could anyone anno-tate the data? The labels for a cat-versus-dog image classification problem canbe selected by anyone, but those for a dog breed classification task require spe-cialized knowledge. Meanwhile, annotating CT scans of bone fractures prettymuch requires a medical degree.If annotating the data requires specialized knowledge, can you train people todo it? If not, how can you get access to relevant experts?Do you, yourself, understand the way experts come up with the annotations? Ifyou don’t, you will have to treat your dataset as a black box, and you won’t be ableto perform manual feature engineering—this isn’t critical, but it can be limiting.If you decide to label your data in-house, ask yourself what software you will use to recordannotations. You may well need to develop that software yourself. Productive data anno-tation software will save you a lot of time, so it’s worth investing in it early in a project. BEWARE OF NON-REPRESENTATIVE DATAMachine learning models can only make sense of inputs that are similar to whatthey’ve seen before. As such, it’s critical that the data used for training should be repre-sentative of the production data. This concern should be the foundation of all yourdata collection work. Suppose you’re developing an app where users can take pictures of a plate of food tofind out the name of the dish. You train a model using pictures from an image-sharingsocial network that’s popular with foodies. Come deployment time, feedback fromangry users starts rolling in: your app gets the answer wrong 8 times out of 10. What’sgoing on? Your accuracy on the test set was well over 90%! A quick look at user-uploadeddata reveals that mobile picture uploads of random dishes from random restaurantstaken with random smartphones look nothing like the professional-quality, well-lit,appetizing pictures you trained the model on: your training data wasn’t representative of theproduction data. That’s a cardinal sin—welcome to machine learning hell. If possible, collect data directly from the environment where your model will beused. A movie review sentiment classification model should be used on new IMDBreviews, not on Yelp restaurant reviews, nor on Twitter status updates. If you want torate the sentiment of a tweet, start by collecting and annotating actual tweets from asimilar set of users as those you’re expecting in production. If it’s not possible to trainon production data, then make sure you fully understand how your training and pro-duction data differ, and that you are actively correcting for these differences. A related phenomenon you should be aware of is concept drift. You’ll encounterconcept drift in almost all real-world problems, especially those that deal with user-generated data. Concept drift occurs when the properties of the production datachange over time, causing model accuracy to gradually decay. A music recommenda-tion engine trained in the year 2013 may not be very effective today. Likewise, theIMDB dataset you worked with was collected in 2011, and a model trained on it would 159Define the tasklikely not perform as well on reviews from 2020 compared to reviews from 2012, asvocabulary, expressions, and movie genres evolve over time. Concept drift is particu-larly acute in adversarial contexts like credit card fraud detection, where fraud pat-terns change practically every day. Dealing with fast concept drift requires constantdata collection, annotation, and model retraining. Keep in mind that machine learning can only be used to memorize patterns thatare present in your training data. You can only recognize what you’ve seen before.Using machine learning trained on past data to predict the future is making theassumption that the future will behave like the past. That often isn’t the case.The problem of sampling biasA particularly insidious and common case of non-representative data is samplingbias. Sampling bias occurs when your data collection process interacts with whatyou are trying to predict, resulting in biased measurements. A famous historicalexample occurred in the 1948 US presidential election. On election night, the Chi-cago Tribune printed the headline “DEWEY DEFEATS TRUMAN.” The next morning,Truman emerged as the winner. The editor of the Tribune had trusted the results ofa phone survey—but phone users in 1948 were not a random, representative sam-ple of the voting population. They were more likely to be richer, conservative, andto vote for Dewey, the Republican candidate. Nowadays, every phone survey takes sampling bias into account. That doesn’t meanthat sampling bias is a thing of the past in political polling—far from it. But unlike in1948, pollsters are aware of it and take steps to correct it. “DEWEY DEFEATS TRUMAN”: A famous example of sampling bias 160CHAPTER 6The universal workflow of machine learning6.1.3 Understand your dataIt’s pretty bad practice to treat a dataset as a black box. Before you start training mod-els, you should explore and visualize your data to gain insights about what makes itpredictive, which will inform feature engineering and screen for potential issues.If your data includes images or natural language text, take a look at a few sam-ples (and their labels) directly.If your data contains numerical features, it’s a good idea to plot the histogramof feature values to get a feel for the range of values taken and the frequency ofdifferent values.If your data includes location information, plot it on a map. Do any clear pat-terns emerge?Are some samples missing values for some features? If so, you’ll need to dealwith this when you prepare the data (we’ll cover how to do this in the nextsection).If your task is a classification problem, print the number of instances of eachclass in your data. Are the classes roughly equally represented? If not, you willneed to account for this imbalance.Check for target leaking: the presence of features in your data that provide infor-mation about the targets and which may not be available in production. Ifyou’re training a model on medical records to predict whether someone will betreated for cancer in the future, and the records include the feature “this per-son has been diagnosed with cancer,” then your targets are being artificiallyleaked into your data. Always ask yourself, is every feature in your data some-thing that will be available in the same form in production?6.1.4 Choose a measure of successTo control something, you need to be able to observe it. To achieve success on a proj-ect, you must first define what you mean by success. Accuracy? Precision and recall?Customer retention rate? Your metric for success will guide all of the technical choicesyou make throughout the project. It should directly align with your higher-level goals,such as the business success of your customer. For balanced classification problems, where every class is equally likely, accuracyand the area under a receiver operating characteristic (ROC) curve, abbreviated as ROCAUC, are common metrics. For class-imbalanced problems, ranking problems, ormultilabel classification, you can use precision and recall, as well as a weighted form ofaccuracy or ROC AUC. And it isn’t uncommon to have to define your own custommetric by which to measure success. To get a sense of the diversity of machine learningsuccess metrics and how they relate to different problem domains, it’s helpful tobrowse the data science competitions on Kaggle (https:/ / kaggle.com); they showcase awide range of problems and evaluation metrics. 161Develop a model6.2 Develop a modelOnce you know how you will measure your progress, you can get started with modeldevelopment. Most tutorials and research projects assume that this is the only step—skipping problem definition and dataset collection, which are assumed already done,and skipping model deployment and maintenance, which are assumed to be handledby someone else. In fact, model development is only one step in the machine learningworkflow, and if you ask me, it’s not the most difficult one. The hardest things inmachine learning are framing problems and collecting, annotating, and cleaningdata. So cheer up—what comes next will be easy in comparison!6.2.1 Prepare the dataAs you’ve learned before, deep learning models typically don’t ingest raw data. Datapreprocessing aims at making the raw data at hand more amenable to neural net-works. This includes vectorization, normalization, or handling missing values. Manypreprocessing techniques are domain-specific (for example, specific to text data orimage data); we’ll cover those in the following chapters as we encounter them inpractical examples. For now, we’ll review the basics that are common to all datadomains.VECTORIZATIONAll inputs and targets in a neural network must typically be tensors of floating-pointdata (or, in specific cases, tensors of integers or strings). Whatever data you need toprocess—sound, images, text—you must first turn into tensors, a step called data vec-torization. For instance, in the two previous text-classification examples in chapter 4,we started with text represented as lists of integers (standing for sequences of words),and we used one-hot encoding to turn them into a tensor of float32 d a t a . I n t h eexamples of classifying digits and predicting house prices, the data came in vectorizedform, so we were able to skip this step. VALUE NORMALIZATIONIn the MNIST digit-classification example from chapter 2, we started with image dataencoded as integers in the 0–255 range, encoding grayscale values. Before we fed thisdata into our network, we had to cast it to float32 and divide by 255 so we’d end upwith floating-point values in the 0–1 range. Similarly, when predicting house prices, westarted with features that took a variety of ranges—some features had small floating-point values, and others had fairly large integer values. Before we fed this data intoour network, we had to normalize each feature independently so that it had a stan-dard deviation of 1 and a mean of 0. In general, it isn’t safe to feed into a neural network data that takes relativelylarge values (for example, multi-digit integers, which are much larger than the ini-tial values taken by the weights of a network) or data that is heterogeneous (forexample, data where one feature is in the range 0–1 and another is in the range100–200). Doing so can trigger large gradient updates that will prevent the network 162CHAPTER 6The universal workflow of machine learningfrom converging. To make learning easier for your network, your data should havethe following characteristics:Take small values—Typically, most values should be in the 0–1 range.Be homogenous—All features should take values in roughly the same range.Additionally, the following stricter normalization practice is common and can help,although it isn’t always necessary (for example, we didn’t do this in the digit-classifica-tion example):Normalize each feature independently to have a mean of 0.Normalize each feature independently to have a standard deviation of 1.This is easy to do with NumPy arrays:x- =x . m e a n ( a x i s =0) x/ =x . s t d ( a x i s =0)HANDLING MISSING VALUESYou may sometimes have missing values in your data. For instance, in the house-priceexample, the first feature (the column of index 0 in the data) was the per capita crimerate. What if this feature wasn’t available for all samples? You’d then have missing val-ues in the training or test data. You could just discard the feature entirely, but you don’t necessarily have to.If the feature is categorical, it’s safe to create a new category that means “thevalue is missing.” The model will automatically learn what this implies withrespect to the targets.If the feature is numerical, avoid inputting an arbitrary value like \"0\", becauseit may create a discontinuity in the latent space formed by your features, mak-ing it harder for a model trained on it to generalize. Instead, consider replac-ing the missing value with the average or median value for the feature in thedataset. You could also train a model to predict the feature value given the val-ues of other features.Note that if you’re expecting missing categorial features in the test data, but the networkwas trained on data without any missing values, the network won’t have learned toignore missing values! In this situation, you should artificially generate training sampleswith missing entries: copy some training samples several times, and drop some of thecategorical features that you expect are likely to be missing in the test data. 6.2.2 Choose an evaluation protocolAs you learned in the previous chapter, the purpose of a model is to achieve general-ization, and every modeling decision you will make throughout the model develop-ment process will be guided by validation metrics that seek to measure generalizationperformance. The goal of your validation protocol is to accurately estimate what yourAssuming x is a 2D data matrix of shape (samples, features) 163Develop a modelsuccess metric of choice (such as accuracy) will be on actual production data. The reli-ability of that process is critical to building a useful model. In chapter 5, we reviewed three common evaluation protocols:Maintaining a holdout validation set—This is the way to go when you have plentyof data.Doing K-fold cross-validation—This is the right choice when you have too few sam-ples for holdout validation to be reliable.Doing iterated K-fold validation—This is for performing highly accurate modelevaluation when little data is available.Pick one of these. In most cases, the first will work well enough. As you learned,though, always be mindful of the representativity of your validation set, and be carefulnot to have redundant samples between your training set and your validation set. 6.2.3 Beat a baselineAs you start working on the model itself, your initial goal is to achieve statistical power,as you saw in chapter 5: that is, to develop a small model that is capable of beating asimple baseline. At this stage, these are the three most important things you should focus on:Feature engineering—Filter out uninformative features (feature selection) and useyour knowledge of the problem to develop new features that are likely to be useful.Selecting the correct architecture priors—What type of model architecture will youuse? A densely connected network, a convnet, a recurrent neural network, aTransformer? Is deep learning even a good approach for the task, or should youuse something else?Selecting a good-enough training configuration—What loss function should you use?What batch size and learning rate?Picking the right loss functionIt’s often not possible to directly optimize for the metric that measures success ona problem. Sometimes there is no easy way to turn a metric into a loss function; lossfunctions, after all, need to be computable given only a mini-batch of data (ideally, aloss function should be computable for as little as a single data point) and must bedifferentiable (otherwise, you can’t use backpropagation to train your network). Forinstance, the widely used classification metric ROC AUC can’t be directly optimized.Hence, in classification tasks, it’s common to optimize for a proxy metric of ROC AUC,such as crossentropy. In general, you can hope that the lower the crossentropy gets,the higher the ROC AUC will be.The following table can help you choose a last-layer activation and a loss function fora few common problem types. 164CHAPTER 6The universal workflow of machine learning For most problems, there are existing templates you can start from. You’re not thefirst person to try to build a spam detector, a music recommendation engine, or animage classifier. Make sure you research prior art to identify the feature engineeringtechniques and model architectures that are most likely to perform well on your task. Note that it’s not always possible to achieve statistical power. If you can’t beat a sim-ple baseline after trying multiple reasonable architectures, it may be that the answerto the question you’re asking isn’t present in the input data. Remember that you’remaking two hypotheses:You hypothesize that your outputs can be predicted given your inputs.You hypothesize that the available data is sufficiently informative to learn therelationship between inputs and outputs.It may well be that these hypotheses are false, in which case you must go back to thedrawing board. 6.2.4 Scale up: Develop a model that overfitsOnce you’ve obtained a model that has statistical power, the question becomes, is yourmodel sufficiently powerful? Does it have enough layers and parameters to properlymodel the problem at hand? For instance, a logistic regression model has statisticalpower on MNIST but wouldn’t be sufficient to solve the problem well. Remember thatthe universal tension in machine learning is between optimization and generalization.The ideal model is one that stands right at the border between underfitting and over-fitting, between undercapacity and overcapacity. To figure out where this border lies,first you must cross it. To figure out how big a model you’ll need, you must develop a model that overfits.This is fairly easy, as you learned in chapter 5:1Add layers.2Make the layers bigger.3Train for more epochs.Always monitor the training loss and validation loss, as well as the training and valida-tion values for any metrics you care about. When you see that the model’s perfor-mance on the validation data begins to degrade, you’ve achieved overfitting. (continued)Choosing the right last-layer activation and loss function for your modelProblem typeLast-layer activationLoss functionBinary classificationsigmoid binary_crossentropyMulticlass, single-label classificationsoftmax categorical_crossentropyMulticlass, multilabel classificationsigmoid binary_crossentropy 165Deploy the model6.2.5 Regularize and tune your modelOnce you’ve achieved statistical power and you’re able to overfit, you know you’re on theright path. At this point, your goal becomes to maximize generalization performance. This phase will take the most time: you’ll repeatedly modify your model, train it,evaluate on your validation data (not the test data at this point), modify it again, andrepeat, until the model is as good as it can get. Here are some things you should try:Try different architectures; add or remove layers.Add dropout.If your model is small, add L1 or L2 regularization.Try different hyperparameters (such as the number of units per layer or thelearning rate of the optimizer) to find the optimal configuration.Optionally, iterate on data curation or feature engineering: collect and anno-tate more data, develop better features, or remove features that don’t seem tobe informative.It’s possible to automate a large chunk of this work by using automated hyperparametertuning software, such as KerasTuner. We’ll cover this in chapter 13. Be mindful of the following: Every time you use feedback from your validation pro-cess to tune your model, you leak information about the validation process into themodel. Repeated just a few times, this is innocuous; done systematically over manyiterations, it will eventually cause your model to overfit to the validation process (eventhough no model is directly trained on any of the validation data). This makes theevaluation process less reliable. Once you’ve developed a satisfactory model configuration, you can train yourfinal production model on all the available data (training and validation) and evalu-ate it one last time on the test set. If it turns out that performance on the test set issignificantly worse than the performance measured on the validation data, this maymean either that your validation procedure wasn’t reliable after all, or that youbegan overfitting to the validation data while tuning the parameters of the model.In this case, you may want to switch to a more reliable evaluation protocol (such asiterated K-fold validation). 6.3 Deploy the modelYour model has successfully cleared its final evaluation on the test set—it’s ready to bedeployed and to begin its productive life.6.3.1 Explain your work to stakeholders and set expectationsSuccess and customer trust are about consistently meeting or exceeding people’sexpectations. The actual system you deliver is only half of that picture; the other halfis setting appropriate expectations before launch. The expectations of non-specialists towards AI systems are often unrealistic. Forexample, they might expect that the system “understands” its task and is capable of 166CHAPTER 6The universal workflow of machine learningexercising human-like common sense in the context of the task. To address this, youshould consider showing some examples of the failure modes o f y o u r m o d e l ( f o rinstance, show what incorrectly classified samples look like, especially those for whichthe misclassification seems surprising). They might also expect human-level performance, especially for processes that werepreviously handled by people. Most machine learning models, because they are (imper-fectly) trained to approximate human-generated labels, do not nearly get there. Youshould clearly convey model performance expectations. Avoid using abstract statementslike “The model has 98% accuracy” (which most people mentally round up to 100%),and prefer talking, for instance, about false negative rates and false positive rates. Youcould say, “With these settings, the fraud detection model would have a 5% false nega-tive rate and a 2.5% false positive rate. Every day, an average of 200 valid transactionswould be flagged as fraudulent and sent for manual review, and an average of 14 fraudu-lent transactions would be missed. An average of 266 fraudulent transactions would becorrectly caught.” Clearly relate the model’s performance metrics to business goals. You should also make sure to discuss with stakeholders the choice of key launchparameters—for instance, the probability threshold at which a transaction should beflagged (different thresholds will produce different false negative and false positiverates). Such decisions involve trade-offs that can only be handled with a deep under-standing of the business context. 6.3.2 Ship an inference modelA machine learning project doesn’t end when you arrive at a Colab notebook that cansave a trained model. You rarely put in production the exact same Python modelobject that you manipulated during training. First, you may want to export your model to something other than Python:Your production environment may not support Python at all—for instance, ifit’s a mobile app or an embedded system.If the rest of the app isn’t in Python (it could be in JavaScript, C++, etc.), the useof Python to serve a model may induce significant overhead.Second, since your production model will only be used to output predictions (a phasecalled inference), rather than for training, you have room to perform various optimiza-tions that can make the model faster and reduce its memory footprint. Let’s take a quick look at the different model deployment options you have available.DEPLOYING A MODEL AS A REST APIThis is perhaps the common way to turn a model into a product: install TensorFlow ona server or cloud instance, and query the model’s predictions via a REST API. Youcould build your own serving app using something like Flask (or any other Pythonweb development library), or use TensorFlow’s own library for shipping models asAPIs, called TensorFlow Serving (www.tensorflow.org/tfx/guide/serving). With Tensor-Flow Serving, you can deploy a Keras model in minutes. 167Deploy the model You should use this deployment setup whenThe application that will consume the model’s prediction will have reliableaccess to the internet (obviously). For instance, if your application is a mobileapp, serving predictions from a remote API means that the application won’t beusable in airplane mode or in a low-connectivity environment.The application does not have strict latency requirements: the request, infer-ence, and answer round trip will typically take around 500 ms.The input data sent for inference is not highly sensitive: the data will need tobe available on the server in a decrypted form, since it will need to be seen bythe model (but note that you should use SSL encryption for the HTTP requestand answer).For instance, the image search engine project, the music recommender system, thecredit card fraud detection project, and the satellite imagery project are all good fitsfor serving via a REST API. A n i m p o r t a n t q u e s t i o n w h e n d e p l o y i n g a m o d e l a s a R E S T A P I i s w h e t h e r y o uwant to host the code on your own, or whether you want to use a fully managed third-party cloud service. For instance, Cloud AI Platform, a Google product, lets you simplyupload your TensorFlow model to Google Cloud Storage (GCS), and it gives you anAPI endpoint to query it. It takes care of many practical details such as batching pre-dictions, load balancing, and scaling. DEPLOYING A MODEL ON A DEVICESometimes, you may need your model to live on the same device that runs the applica-tion that uses it—maybe a smartphone, an embedded ARM CPU on a robot, or amicrocontroller on a tiny device. You may have seen a camera capable of automati-cally detecting people and faces in the scenes you pointed it at: that was probably asmall deep learning model running directly on the camera. You should use this setup whenYour model has strict latency constraints or needs to run in a low-connectivityenvironment. If you’re building an immersive augmented reality application,querying a remote server is not a viable option.Your model can be made sufficiently small that it can run under the memory andpower constraints of the target device. You can use the TensorFlow Model Opti-mization Toolkit to help with this (www.tensorflow.org/model_optimization).Getting the highest possible accuracy isn’t mission critical for your task. Thereis always a trade-off between runtime efficiency and accuracy, so memory andpower constraints often require you to ship a model that isn’t quite as good asthe best model you could run on a large GPU.The input data is strictly sensitive and thus shouldn’t be decryptable on aremote server. 168CHAPTER 6The universal workflow of machine learningOur spam detection model will need to run on the end user’s smartphone as part ofthe chat app, because messages are end-to-end encrypted and thus cannot be read bya remotely hosted model. Likewise, the bad-cookie detection model has strict latencyconstraints and will need to run at the factory. Thankfully, in this case, we don’t haveany power or space constraints, so we can actually run the model on a GPU. To deploy a Keras model on a smartphone or embedded device, your go-to solutionis TensorFlow Lite (www.tensorflow.org/lite). It’s a framework for efficient on-devicedeep learning inference that runs on Android and iOS smartphones, as well as ARM64-based computers, Raspberry Pi, or certain microcontrollers. It includes a converter thatcan straightforwardly turn your Keras model into the TensorFlow Lite format. DEPLOYING A MODEL IN THE BROWSERDeep learning is often used in browser-based or desktop-based JavaScript applications.While it is usually possible to have the application query a remote model via a RESTAPI, there can be key advantages in having the model run directly in the browser, onthe user’s computer (utilizing GPU resources if they’re available). Use this setup whenYou want to offload compute to the end user, which can dramatically reduceserver costs.The input data needs to stay on the end user’s computer or phone. Forinstance, in our spam detection project, the web version and the desktop ver-sion of the chat app (implemented as a cross-platform app written in Java-Script) should use a locally run model.Your application has strict latency constraints. While a model running on the enduser’s laptop or smartphone is likely to be slower than one running on a largeGPU on your own server, you don’t have the extra 100 ms of network round trip.You need your app to keep working without connectivity, after the model hasbeen downloaded and cached.You should only go with this option if your model is small enough that it won’t hog theCPU, GPU, or RAM of your user’s laptop or smartphone. In addition, since the entiremodel will be downloaded to the user’s device, you should make sure that nothingabout the model needs to stay confidential. Be mindful of the fact that, given a traineddeep learning model, it is usually possible to recover some information about the train-ing data: better not to make your trained model public if it was trained on sensitive data. To deploy a model in JavaScript, the TensorFlow ecosystem includes TensorFlow.js(www.tensorflow.org/js), a JavaScript library for deep learning that implementsalmost all of the Keras API (originally developed under the working name WebKeras)as well as many lower-level TensorFlow APIs. You can easily import a saved Kerasmodel into TensorFlow.js to query it as part of your browser-based JavaScript app oryour desktop Electron app. 169Deploy the modelINFERENCE MODEL OPTIMIZATIONOptimizing your model for inference is especially important when deploying in anenvironment with strict constraints on available power and memory (smartphonesand embedded devices) or for applications with low latency requirements. You shouldalways seek to optimize your model before importing into TensorFlow.js or exportingit to TensorFlow Lite. There are two popular optimization techniques you can apply:Weight pruning—Not every coefficient in a weight tensor contributes equally tothe predictions. It’s possible to considerably lower the number of parametersin the layers of your model by only keeping the most significant ones. Thisreduces the memory and compute footprint of your model, at a small cost inperformance metrics. By deciding how much pruning you want to apply, youare in control of the trade-off between size and accuracy.Weight quantization—Deep learning models are trained with single-precisionfloating-point (float32) weights. However, it’s possible to quantize weights to8-bit signed integers (int8) to get an inference-only model that’s a quarter thesize but remains near the accuracy of the original model.The TensorFlow ecosystem includes a weight pruning and quantization toolkit (www.tensorflow.org/model_optimization) that is deeply integrated with the Keras API. 6.3.3 Monitor your model in the wildYou’ve exported an inference model, you’ve integrated it into your application, andyou’ve done a dry run on production data—the model behaved exactly as you expected.You’ve written unit tests as well as logging and status-monitoring code—perfect. Now it’stime to press the big red button and deploy to production. Even this is not the end. Once you’ve deployed a model, you need to keep moni-toring its behavior, its performance on new data, its interaction with the rest of theapplication, and its eventual impact on business metrics.Is user engagement in your online radio up or down after deploying the newmusic recommender system? Has the average ad click-through rate increasedafter switching to the new click-through-rate prediction model? Consider usingrandomized A/B testing to isolate the impact of the model itself from otherchanges: a subset of cases should go through the new model, while anothercontrol subset should stick to the old process. Once sufficiently many cases havebeen processed, the difference in outcomes between the two is likely attribut-able to the model.If possible, do a regular manual audit of the model’s predictions on productiondata. It’s generally possible to reuse the same infrastructure as for data annotation:send some fraction of the production data to be manually annotated, and com-pare the model’s predictions to the new annotations. For instance, you shoulddefinitely do this for the image search engine and the bad-cookie flagging system. 170CHAPTER 6The universal workflow of machine learningWhen manual audits are impossible, consider alternative evaluation avenuessuch as user surveys (for example, in the case of the spam and offensive-contentflagging system). 6.3.4 Maintain your modelLastly, no model lasts forever. You’ve already learned about concept drift: over time, thecharacteristics of your production data will change, gradually degrading the perfor-mance and relevance of your model. The lifespan of your music recommender systemwill be counted in weeks. For the credit card fraud detection systems, it will be days. Acouple of years in the best case for the image search engine. As soon as your model has launched, you should be getting ready to train the nextgeneration that will replace it. As such,Watch out for changes in the production data. Are new features becoming avail-able? Should you expand or otherwise edit the label set?Keep collecting and annotating data, and keep improving your annotationpipeline over time. In particular, you should pay special attention to collectingsamples that seem to be difficult for your current model to classify—such sam-ples are the most likely to help improve performance.This concludes the universal workflow of machine learning—that’s a lot of things tokeep in mind. It takes time and experience to become an expert, but don’t worry,you’re already a lot wiser than you were a few chapters ago. You are now familiar withthe big picture—the entire spectrum of what machine learning projects entail. Whilemost of this book will focus on model development, you’re now aware that it’s onlyone part of the entire workflow. Always keep in mind the big picture!SummaryWhen you take on a new machine learning project, first define the problem athand:–U n d e r s t a n d t h e b r o a d e r c o n t e x t o f w h a t y o u ’ r e s e t t i n g o u t t o d o — w h a t ’ s t h eend goal and what are the constraints?–C o l l e c t a n d a n n o t a t e a d a t a s e t ; m a k e s u r e y o u u n d e r s t a n d y o u r d a t a i n d e p t h .–C h o o s e h o w y o u ’ l l m e a s u r e s u c c e s s f o r y o u r p r o b l e m — w h a t m e t r i c s w i l l y o umonitor on your validation data?Once you understand the problem and you have an appropriate dataset, developa model:–P r e p a r e y o u r d a t a .–P i c k y o u r e v a l u a t i o n p r o t o c o l : h o l d o u t v a l i d a t i o n ? K - f o l d v a l i d a t i o n ? W h i c hportion of the data should you use for validation?–A c h i e v e s t a t i s t i c a l p o w e r : b e a t a s i m p l e b a s e l i n e .–S c a l e u p : d e v e l o p a m o d e l t h a t c a n o v e r f i t . 171Summary–R e g u l a r i z e y o u r m o d e l a n d t u n e i t s h y p e r p a r a m e t e r s , b a s e d o n p e r f o r m a n c eon the validation data. A lot of machine learning research tends to focus onlyon this step, but keep the big picture in mind.When your model is ready and yields good performance on the test data, it’stime for deployment:–F i r s t , m a k e s u r e y o u s e t a p p r o p r i a t e e x p e c t a t i o n s w i t h s t a k e h o l d e r s .–O p t i m i z e a f i n a l m o d e l f o r i n f e r e n c e , a n d s h i p a m o d e l t o t h e d e p l o y m e n tenvironment of choice—web server, mobile, browser, embedded device, etc.– Monitor your model’s performance in production, and keep collecting dataso you can develop the next generation of the model. 172Working with Keras:A deep dive You’ve now got some experience with Keras—you’re familiar with the Sequentialmodel, Dense l a y e r s , a n d b u i l t - i n A P I s f o r t r a i n i n g , e v a l u a t i o n , a n d i n f e r e n c e —compile(), fit(), evaluate(), and predict(). You even learned in chapter 3 howto inherit from the Layer class to create custom layers, and how to use the Tensor-Flow GradientTape to implement a step-by-step training loop. In the coming chapters, we’ll dig into computer vision, timeseries forecast-ing, natural language processing, and generative deep learning. These complexapplications will require much more than a Sequential a r c h i t e c t u r e a n d t h edefault fit() loop. So let’s first turn you into a Keras expert! In this chapter,you’ll get a complete overview of the key ways to work with Keras APIs: everythingThis chapter coversCreating Keras models with the Sequential class, the Functional API, and model subclassingUsing built-in Keras training and evaluation loopsUsing Keras callbacks to customize trainingUsing TensorBoard to monitor training and evaluation metricsWriting training and evaluation loops from scratch 173Different ways to build Keras modelsyou’re going to need to handle the advanced deep learning use cases you’ll encoun-ter next.7.1 A spectrum of workflowsThe design of the Keras API is guided by the principle of progressive disclosure of complex-ity: make it easy to get started, yet make it possible to handle high-complexity usecases, only requiring incremental learning at each step. Simple use cases should beeasy and approachable, and arbitrarily advanced workflows should be possible: no mat-ter how niche and complex the thing you want to do, there should be a clear path toit. A path that builds upon the various things you’ve learned from simpler workflows.This means that you can grow from beginner to expert and still use the same tools—only in different ways. As such, there’s not a single “true” way of using Keras. Rather, Keras offers a spec-trum of workflows, from the very simple to the very flexible. There are different ways tobuild Keras models, and different ways to train them, answering different needs.Because all these workflows are based on shared APIs, such as Layer and Model, com-ponents from any workflow can be used in any other workflow—they can all talk toeach other. 7.2 Different ways to build Keras modelsThere are three APIs for building models in Keras (see figure 7.1):The Sequential model, the most approachable API—it’s basically a Python list. Assuch, it’s limited to simple stacks of layers.The Functional API, which focuses on graph-like model architectures. It rep-resents a nice mid-point between usability and flexibility, and as such, it’s themost commonly used model-building API.Model subclassing, a low-level option where you write everything yourself fromscratch. This is ideal if you want full control over every little thing. However, youwon’t get access to many built-in Keras features, and you will be more at risk ofmaking mistakes.Sequential API+ built-in layersFunctional API+ built-in layersFunctional API+ custom layers+ custom metrics+ custom losses+ ...Subclassing:write everythingyourself from scratchNovice users,simple modelsEngineers withstandard usecasesEngineers withniche use casesrequiring bespokesolutionsResearchersFigure 7.1 Progressive disclosure of complexity for model building 174CHAPTER 7Working with Keras: A deep dive7.2.1 The Sequential modelThe simplest way to build a Keras model is to use the Sequential model, which youalready know about.from tensorflow import keras from tensorflow.keras import layers model = keras.Sequential([ layers.Dense(64, activation=\"relu\"), layers.Dense(10, activation=\"softmax\")])Note that it’s possible to build the same model incrementally via the add() method,which is similar to the append() method of a Python list.model = keras.Sequential()model.add(layers.Dense(64, activation=\"relu\"))model.add(layers.Dense(10, activation=\"softmax\"))You saw in chapter 4 that layers only get built (which is to say, create their weights) whenthey are called for the first time. That’s because the shape of the layers' weights dependson the shape of their input: until the input shape is known, they can’t be created. As such, the preceding Sequential model does not have any weights (listing 7.3)until you actually call it on some data, or call its build() method with an input shape(listing 7.4).>>> model.weights ValueError: Weights for model sequential_1 have not yet been created.>>> model.build(input_shape=(None, 3)) >>> model.weights [<tf.Variable \"dense_2/kernel:0\" shape=(3, 64) dtype=float32, ... >, <tf.Variable \"dense_2/bias:0\" shape=(64,) dtype=float32, ... > <tf.Variable \"dense_3/kernel:0\" shape=(64, 10) dtype=float32, ... >, <tf.Variable \"dense_3/bias:0\" shape=(10,) dtype=float32, ... >]After the model is built, you can display its contents via the summary() method, whichcomes in handy for debugging.Listing 7.1 The Sequential class Listing 7.2 Incrementally building a Sequential model Listing 7.3 Models that aren’t yet built have no weightsListing 7.4 Calling a model for the first time to build itAt that point, the model isn’t built yet. Builds the model—now the model will expect samples of shape (3,). The None in the input shape signals that the batch size could be anything.Now you can retrieve the model’s weights. 175Different ways to build Keras models>>> model.summary()Model: \"sequential_1\" _________________________________________________________________Layer (type) Output Shape Param # =================================================================dense_2 (Dense) (None, 64) 256 _________________________________________________________________dense_3 (Dense) (None, 10) 650 =================================================================Total params: 906 Trainable params: 906 Non-trainable params: 0 _________________________________________________________________As you can see, this model happens to be named “sequential_1.” You can give namesto everything in Keras—every model, every layer.>>> model = keras.Sequential(name=\"my_example_model\")>>> model.add(layers.Dense(64, activation=\"relu\", name=\"my_first_layer\"))>>> model.add(layers.Dense(10, activation=\"softmax\", name=\"my_last_layer\"))>>> model.build((None, 3))>>> model.summary()Model: \"my_example_model\" _________________________________________________________________Layer (type) Output Shape Param # =================================================================my_first_layer (Dense) (None, 64) 256 _________________________________________________________________my_last_layer (Dense) (None, 10) 650 =================================================================Total params: 906 Trainable params: 906 Non-trainable params: 0 _________________________________________________________________When building a Sequential model incrementally, it’s useful to be able to print a sum-mary of what the current model looks like after you add each layer. But you can’t printa summary until the model is built! There’s actually a way to have your Sequentialbuilt on the fly: just declare the shape of the model’s inputs in advance. You can dothis via the Input class.model = keras.Sequential()model.add(keras.Input(shape=(3,))) model.add(layers.Dense(64, activation=\"relu\"))Listing 7.5 The summary() method Listing 7.6 Naming models and layers with the name argument Listing 7.7 Specifying the input shape of your model in advanceUse Input to declare the shape of the inputs. Note that the shape argument must be the shape of each sample, not the shape of one batch. 176CHAPTER 7Working with Keras: A deep diveNow you can use summary() to follow how the output shape of your model changes asyou add more layers:>>> model.summary()Model: \"sequential_2\" _________________________________________________________________Layer (type) Output Shape Param # =================================================================dense_4 (Dense) (None, 64) 256 =================================================================Total params: 256 Trainable params: 256 Non-trainable params: 0 _________________________________________________________________>>> model.add(layers.Dense(10, activation=\"softmax\"))>>> model.summary()Model: \"sequential_2\" _________________________________________________________________Layer (type) Output Shape Param # =================================================================dense_4 (Dense) (None, 64) 256 _________________________________________________________________dense_5 (Dense) (None, 10) 650 =================================================================Total params: 906 Trainable params: 906 Non-trainable params: 0 _________________________________________________________________This is a pretty common debugging workflow when dealing with layers that transformtheir inputs in complex ways, such as the convolutional layers you’ll learn about inchapter 8. 7.2.2 The Functional APIThe Sequential model is easy to use, but its applicability is extremely limited: it canonly express models with a single input and a single output, applying one layer afterthe other in a sequential fashion. In practice, it’s pretty common to encounter modelswith multiple inputs (say, an image and its metadata), multiple outputs (differentthings you want to predict about the data), or a nonlinear topology. In such cases, you’d build your model using the Functional API. This is what mostKeras models you’ll encounter in the wild use. It’s fun and powerful—it feels like play-ing with LEGO bricks.A SIMPLE EXAMPLELet’s start with something simple: the stack of two layers we used in the previous sec-tion. Its Functional API version looks like the following listing. 177Different ways to build Keras modelsinputs = keras.Input(shape=(3,), name=\"my_input\")features = layers.Dense(64, activation=\"relu\")(inputs)outputs = layers.Dense(10, activation=\"softmax\")(features)model = keras.Model(inputs=inputs, outputs=outputs)Let’s go over this step by step. We started by declaring an Input (note that you can also give names to these inputobjects, like everything else):inputs = keras.Input(shape=(3,), name=\"my_input\")This inputs object holds information about the shape and dtype of the data that themodel will process:>>> inputs.shape(None, 3) >>> inputs.dtype float32We call such an object a symbolic tensor. It doesn’t contain any actual data, but itencodes the specifications of the actual tensors of data that the model will see whenyou use it. It stands for future tensors of data. Next, we created a layer and called it on the input:features = layers.Dense(64, activation=\"relu\")(inputs)All Keras layers can be called both on real tensors of data and on these symbolic ten-sors. In the latter case, they return a new symbolic tensor, with updated shape anddtype information:>>> features.shape(None, 64)After obtaining the final outputs, we instantiated the model by specifying its inputsand outputs in the Model constructor:outputs = layers.Dense(10, activation=\"softmax\")(features)model = keras.Model(inputs=inputs, outputs=outputs)Here’s the summary of our model:>>> model.summary()Model: \"functional_1\" _________________________________________________________________Layer (type) Output Shape Param # =================================================================my_input (InputLayer) [(None, 3)] 0 _________________________________________________________________Listing 7.8 A simple Functional model with two Dense layers The model will process batches where each sample has shape (3,). The number of samples per batch is variable (indicated by the None batch size).These batches will have dtype float32. 178CHAPTER 7Working with Keras: A deep divedense_6 (Dense) (None, 64) 256 _________________________________________________________________dense_7 (Dense) (None, 10) 650 =================================================================Total params: 906 Trainable params: 906 Non-trainable params: 0 _________________________________________________________________MULTI-INPUT, MULTI-OUTPUT MODELSUnlike this toy model, most deep learning models don’t look like lists—they look likegraphs. They may, for instance, have multiple inputs or multiple outputs. It’s for thiskind of model that the Functional API really shines. Let’s say you’re building a system to rank customer support tickets by priority androute them to the appropriate department. Your model has three inputs:The title of the ticket (text input)The text body of the ticket (text input)Any tags added by the user (categorical input, assumed here to be one-hotencoded)We can encode the text inputs as arrays of ones and zeros of size vocabulary_size(see chapter 11 for detailed information about text encoding techniques). Your model also has two outputs:The priority score of the ticket, a scalar between 0 and 1 (sigmoid output)The department that should handle the ticket (a softmax over the set of depart-ments)You can build this model in a few lines with the Functional API.vocabulary_size = 10000 num_tags = 100 num_departments = 4 title = keras.Input(shape=(vocabulary_size,), name=\"title\") text_body = keras.Input(shape=(vocabulary_size,), name=\"text_body\") tags = keras.Input(shape=(num_tags,), name=\"tags\") features = layers.Concatenate()([title, text_body, tags]) features = layers.Dense(64, activation=\"relu\")(features) priority = layers.Dense(1, activation=\"sigmoid\", name=\"priority\")(features) department = layers.Dense( num_departments, activation=\"softmax\", name=\"department\")(features) model = keras.Model(inputs=[title, text_body, tags], outputs=[priority, department]) Listing 7.9 A multi-input, multi-output Functional modelDefinemodelinputs.Combine input features intoa single tensor, features, byconcatenating them. Apply an intermediatelayer to recombine inputfeatures into richerrepresentations.Definemodeloutputs.Create the model by specifying its inputs and outputs. 179Different ways to build Keras modelsThe Functional API is a simple, LEGO-like, yet very flexible way to define arbitrarygraphs of layers like these. TRAINING A MULTI-INPUT, MULTI-OUTPUT MODELYou can train your model in much the same way as you would train a Sequentialmodel, by calling fit() with lists of input and output data. These lists of data shouldbe in the same order as the inputs you passed to the Model constructor.import numpy as np num_samples = 1280 title_data = np.random.randint(0, 2, size=(num_samples, vocabulary_size)) text_body_data = np.random.randint(0, 2, size=(num_samples, vocabulary_size))tags_data = np.random.randint(0, 2, size=(num_samples, num_tags)) priority_data = np.random.random(size=(num_samples, 1)) department_data = np.random.randint(0, 2, size=(num_samples, num_departments))model.compile(optimizer=\"rmsprop\", loss=[\"mean_squared_error\", \"categorical_crossentropy\"], metrics=[[\"mean_absolute_error\"], [\"accuracy\"]])model.fit([title_data, text_body_data, tags_data], [priority_data, department_data], epochs=1)model.evaluate([title_data, text_body_data, tags_data], [priority_data, department_data])priority_preds, department_preds = model.predict( [title_data, text_body_data, tags_data])If you don’t want to rely on input order (for instance, because you have many inputsor outputs), you can also leverage the names you gave to the Input objects and theoutput layers, and pass data via dictionaries. model.compile(optimizer=\"rmsprop\", loss={\"priority\": \"mean_squared_error\", \"department\": \"categorical_crossentropy\"}, metrics={\"priority\": [\"mean_absolute_error\"], \"department\": [\"accuracy\"]})model.fit({\"title\": title_data, \"text_body\": text_body_data, \"tags\": tags_data}, {\"priority\": priority_data, \"department\": department_data}, epochs=1)model.evaluate({\"title\": title_data, \"text_body\": text_body_data, \"tags\": tags_data}, {\"priority\": priority_data, \"department\": department_data})priority_preds, department_preds = model.predict( {\"title\": title_data, \"text_body\": text_body_data, \"tags\": tags_data})Listing 7.10 Training a model by providing lists of input and target arrays Listing 7.11 Training a model by providing dicts of input and target arraysDummyinputdataDummytarget data 180CHAPTER 7Working with Keras: A deep diveTHE POWER OF THE FUNCTIONAL API: ACCESS TO LAYER CONNECTIVITYA Functional model is an explicit graph data structure. This makes it possible toinspect how layers are connected and reuse previous graph nodes (which are layeroutputs) as part of new models. It also nicely fits the “mental model” that most research-ers use when thinking about a deep neural network: a graph of layers. This enablestwo important use cases: model visualization and feature extraction. Let’s visualize the connectivity of the model we just defined (the topology of themodel). You can plot a Functional model as a graph with the plot_model() utility (seefigure 7.2).keras.utils.plot_model(model, \"ticket_classifier.png\") You can add to this plot the input and output shapes of each layer in the model, whichcan be helpful during debugging (see figure 7.3).keras.utils.plot_model( model, \"ticket_classifier_with_shape_info.png\", show_shapes=True)Figure 7.2 Plot generated by plot_model() on our ticket classifier model Figure 7.3 Model plot with shape information added 181Different ways to build Keras modelsThe “None” in the tensor shapes represents the batch size: this model allows batchesof any size. Access to layer connectivity also means that you can inspect and reuse individualnodes (layer calls) in the graph. The model.layers model property provides the listof layers that make up the model, and for each layer you can query layer.input andlayer.output.>>> model.layers[<tensorflow.python.keras.engine.input_layer.InputLayer at 0x7fa963f9d358>, <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7fa963f9d2e8>, <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7fa963f9d470>, <tensorflow.python.keras.layers.merge.Concatenate at 0x7fa963f9d860>, <tensorflow.python.keras.layers.core.Dense at 0x7fa964074390>, <tensorflow.python.keras.layers.core.Dense at 0x7fa963f9d898>, <tensorflow.python.keras.layers.core.Dense at 0x7fa963f95470>]>>> model.layers[3].input[<tf.Tensor \"title:0\" shape=(None, 10000) dtype=float32>, <tf.Tensor \"text_body:0\" shape=(None, 10000) dtype=float32>, <tf.Tensor \"tags:0\" shape=(None, 100) dtype=float32>]>>> model.layers[3].output<tf.Tensor \"concatenate/concat:0\" shape=(None, 20100) dtype=float32>This enables you to do feature extraction, creating models that reuse intermediate fea-tures from another model. Let’s say you want to add another output to the previous model—you want to esti-mate how long a given issue ticket will take to resolve, a kind of difficulty rating. Youcould do this via a classification layer over three categories: “quick,” “medium,” and“difficult.” You don’t need to recreate and retrain a model from scratch. You can startfrom the intermediate features of your previous model, since you have access to them,like this.features = model.layers[4].output difficulty = layers.Dense(3, activation=\"softmax\", name=\"difficulty\")(features) new_model = keras.Model( inputs=[title, text_body, tags], outputs=[priority, department, difficulty])Let’s plot our new model (see figure 7.4):keras.utils.plot_model( new_model, \"updated_ticket_classifier.png\", show_shapes=True)Listing 7.12 Retrieving the inputs or outputs of a layer in a Functional model Listing 7.13 Creating a new model by reusing intermediate layer outputslayers[4] is our intermediate Dense layer 182CHAPTER 7Working with Keras: A deep dive 7.2.3 Subclassing the Model classThe last model-building pattern you should know about is the most advanced one:Model subclassing. You learned in chapter 3 how to subclass the Layer class to createcustom layers. Subclassing Model is pretty similar:In the __init__() method, define the layers the model will use.In the call() method, define the forward pass of the model, reusing the layerspreviously created.Instantiate your subclass, and call it on data to create its weights.REWRITING OUR PREVIOUS EXAMPLE AS A SUBCLASSED MODELLet’s take a look at a simple example: we will reimplement the customer support ticketmanagement model using a Model subclass.class CustomerTicketModel(keras.Model): def __init__(self, num_departments): super().__init__() self.concat_layer = layers.Concatenate() self.mixing_layer = layers.Dense(64, activation=\"relu\") self.priority_scorer = layers.Dense(1, activation=\"sigmoid\") self.department_classifier = layers.Dense( num_departments, activation=\"softmax\") def call(self, inputs): title = inputs[\"title\"] text_body = inputs[\"text_body\"] tags = inputs[\"tags\"] features = self.concat_layer([title, text_body, tags]) features = self.mixing_layer(features)Listing 7.14 A simple subclassed modelFigure 7.4 Plot of our new model Don’t forget to call the super() constructor!Define sublayers in the constructor.Define the forward pass in the call() method. 183Different ways to build Keras models priority = self.priority_scorer(features) department = self.department_classifier(features) return priority, departmentOnce you’ve defined the model, you can instantiate it. Note that it will only create itsweights the first time you call it on some data, much like Layer subclasses:model = CustomerTicketModel(num_departments=4)priority, department = model( {\"title\": title_data, \"text_body\": text_body_data, \"tags\": tags_data})So far, everything looks very similar to Layer subclassing, a workflow you encounteredin chapter 3. What, then, is the difference between a Layer subclass and a Model sub-class? It’s simple: a “layer” is a building block you use to create models, and a “model”is the top-level object that you will actually train, export for inference, etc. In short, aModel has fit(), evaluate(), and predict() methods. Layers don’t. Other than that,the two classes are virtually identical. (Another difference is that you can save a modelto a file on disk, which we will cover in a few sections.) You can compile and train a Model subclass just like a Sequential or Functionalmodel:model.compile(optimizer=\"rmsprop\", loss=[\"mean_squared_error\", \"categorical_crossentropy\"], metrics=[[\"mean_absolute_error\"], [\"accuracy\"]]) model.fit({\"title\": title_data, \"text_body\": text_body_data, \"tags\": tags_data}, [priority_data, department_data], epochs=1)model.evaluate({\"title\": title_data, \"text_body\": text_body_data, \"tags\": tags_data}, [priority_data, department_data])priority_preds, department_preds = model.predict({\"title\": title_data, \"text_body\": text_body_data, \"tags\": tags_data})The Model subclassing workflow is the most flexible way to build a model. It enablesyou to build models that cannot be expressed as directed acyclic graphs of layers—imagine, for instance, a model where the call() method uses layers inside a for loop,or even calls them recursively. Anything is possible—you’re in charge. BEWARE: WHAT SUBCLASSED MODELS DON’T SUPPORTThis freedom comes at a cost: with subclassed models, you are responsible for more ofthe model logic, which means your potential error surface is much larger. As a result,you will have more debugging work to do. You are developing a new Python object,not just snapping together LEGO bricks.The structure of what you pass as the loss andmetrics arguments must match exactly what getsreturned by call()—here, a list of two elements. The structure of the input data must match exactly what is expected by the call() method—here, a dict with keys title, text_body, and tags.The structure of the target data must match exactly what is returned by the call() method—here, a list of two elements. 184CHAPTER 7Working with Keras: A deep dive Functional and subclassed models are also substantially different in nature. A Func-tional model is an explicit data structure—a graph of layers, which you can view, inspect,and modify. A subclassed model is a piece of bytecode—a Python class with a call()method that contains raw code. This is the source of the subclassing workflow’s flexibil-ity—you can code up whatever functionality you like—but it introduces new limitations. For instance, because the way layers are connected to each other is hidden insidethe body of the call() m e t h o d , y o u c a n n o t a c c e s s t h a t i n f o r m a t i o n . C a l l i n g sum-mary() will not display layer connectivity, and you cannot plot the model topology viaplot_model(). Likewise, if you have a subclassed model, you cannot access the nodesof the graph of layers to do feature extraction because there is simply no graph. Oncethe model is instantiated, its forward pass becomes a complete black box. 7.2.4 Mixing and matching different componentsCrucially, choosing one of these patterns—the Sequential model, the Functional API,or Model subclassing—does not lock you out of the others. All models in the Keras APIcan smoothly interoperate with each other, whether they’re Sequential models, Func-tional models, or subclassed models written from scratch. They’re all part of the samespectrum of workflows. For instance, you can use a subclassed layer or model in a Functional model.class Classifier(keras.Model): def __init__(self, num_classes=2): super().__init__() if num_classes == 2: num_units = 1 activation = \"sigmoid\" else: num_units = num_classes activation = \"softmax\" self.dense = layers.Dense(num_units, activation=activation) def call(self, inputs): return self.dense(inputs) inputs = keras.Input(shape=(3,))features = layers.Dense(64, activation=\"relu\")(inputs)outputs = Classifier(num_classes=10)(features)model = keras.Model(inputs=inputs, outputs=outputs)Inversely, you can use a Functional model as part of a subclassed layer or model. inputs = keras.Input(shape=(64,))outputs = layers.Dense(1, activation=\"sigmoid\")(inputs)binary_classifier = keras.Model(inputs=inputs, outputs=outputs)Listing 7.15 Creating a Functional model that includes a subclassed model Listing 7.16 Creating a subclassed model that includes a Functional model 185Using built-in training and evaluation loopsclass MyModel(keras.Model): def __init__(self, num_classes=2): super().__init__() self.dense = layers.Dense(64, activation=\"relu\") self.classifier = binary_classifier def call(self, inputs): features = self.dense(inputs) return self.classifier(features) model = MyModel()7.2.5 Remember: Use the right tool for the jobYou’ve learned about the spectrum of workflows for building Keras models, from thesimplest workflow, the Sequential model, to the most advanced one, model subclass-ing. When should you use one over the other? Each one has its pros and cons—pickthe one most suitable for the job at hand. In general, the Functional API provides you with a pretty good trade-off betweenease of use and flexibility. It also gives you direct access to layer connectivity, which isvery powerful for use cases such as model plotting or feature extraction. If you can usethe Functional API—that is, if your model can be expressed as a directed acyclic graphof layers—I recommend using it over model subclassing. G o i n g f o r w a r d , a l l e x a m p l e s i n t h i s b o o k w i l l u s e t h e F u n c t i o n a l A P I , s i m p l ybecause all the models we will work with are expressible as graphs of layers. We will,however, make frequent use of subclassed layers. In general, using Functional modelsthat include subclassed layers provides the best of both worlds: high development flex-ibility while retaining the advantages of the Functional API. 7.3 Using built-in training and evaluation loopsThe principle of progressive disclosure of complexity—access to a spectrum of work-flows that go from dead easy to arbitrarily flexible, one step at a time—also applies tomodel training. Keras provides you with different workflows for training models. Theycan be as simple as calling fit() on your data, or as advanced as writing a new train-ing algorithm from scratch. You are already familiar with the compile(), fit(), evaluate(), predict() work-flow. As a reminder, take a look at the following listing.from tensorflow.keras.datasets import mnist def get_mnist_model(): inputs = keras.Input(shape=(28 * 28,)) features = layers.Dense(512, activation=\"relu\")(inputs) features = layers.Dropout(0.5)(features) outputs = layers.Dense(10, activation=\"softmax\")(features)Listing 7.17 The standard workflow: compile(), fit(), evaluate(), predict()Create a model (we factor this into a separate function so as to reuse it later). 186CHAPTER 7Working with Keras: A deep dive model = keras.Model(inputs, outputs) return model (images, labels), (test_images, test_labels) = mnist.load_data() images = images.reshape((60000, 28 * 28)).astype(\"float32\") / 255 test_images = test_images.reshape((10000, 28 * 28)).astype(\"float32\") / 255 train_images, val_images = images[10000:], images[:10000]train_labels, val_labels = labels[10000:], labels[:10000] model = get_mnist_model()model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]) model.fit(train_images, train_labels, epochs=3, validation_data=(val_images, val_labels)) test_metrics = model.evaluate(test_images, test_labels) predictions = model.predict(test_images) There are a couple of ways you can customize this simple workflow:Provide your own custom metrics.Pass callbacks to the fit() method to schedule actions to be taken at specificpoints during training.Let’s take a look at these.7.3.1 Writing your own metricsMetrics are key to measuring the performance of your model—in particular, to mea-suring the difference between its performance on the training data and its perfor-mance on the test data. Commonly used metrics for classification and regression arealready part of the built-in keras.metrics module, and most of the time that’s whatyou will use. But if you’re doing anything out of the ordinary, you will need to be ableto write your own metrics. It’s simple! A Keras metric is a subclass of the keras.metrics.Metric class. Like layers, a met-ric has an internal state stored in TensorFlow variables. Unlike layers, these variablesaren’t updated via backpropagation, so you have to write the state-update logic your-self, which happens in the update_state() method. For example, here’s a simple custom metric that measures the root mean squarederror (RMSE).import tensorflow as tf class RootMeanSquaredError(keras.metrics.Metric): Listing 7.18 Implementing a custom metric by subclassing the Metric classLoad your data, reservingsome for validation.Compile the model by specifying its optimizer, the loss function to minimize, and the metrics to monitor.Use fit() to train the model, optionally providing validation data to monitor performance on unseen data.Use evaluate() to compute the loss and metrics on new data.Use predict() to computeclassification probabilitieson new data. Subclass the Metric class. 187Using built-in training and evaluation loops def __init__(self, name=\"rmse\", **kwargs): super().__init__(name=name, **kwargs) self.mse_sum = self.add_weight(name=\"mse_sum\", initializer=\"zeros\") self.total_samples = self.add_weight( name=\"total_samples\", initializer=\"zeros\", dtype=\"int32\") def update_state(self, y_true, y_pred, sample_weight=None): y_true = tf.one_hot(y_true, depth=tf.shape(y_pred)[1]) mse = tf.reduce_sum(tf.square(y_true - y_pred)) self.mse_sum.assign_add(mse) num_samples = tf.shape(y_pred)[0] self.total_samples.assign_add(num_samples)You use the result() method to return the current value of the metric: def result(self): return tf.sqrt(self.mse_sum / tf.cast(self.total_samples, tf.float32))Meanwhile, you also need to expose a way to reset the metric state without having toreinstantiate it—this enables the same metric objects to be used across differentepochs of training or across both training and evaluation. You do this with thereset_state() method: def reset_state(self): self.mse_sum.assign(0.) self.total_samples.assign(0)Custom metrics can be used just like built-in ones. Let’s test-drive our own metric:model = get_mnist_model()model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\", RootMeanSquaredError()])model.fit(train_images, train_labels, epochs=3, validation_data=(val_images, val_labels))test_metrics = model.evaluate(test_images, test_labels)You can now see the fit() progress bar displaying the RMSE of your model. 7.3.2 Using callbacksLaunching a training run on a large dataset for tens of epochs using model.fit() canbe a bit like launching a paper airplane: past the initial impulse, you don’t have anycontrol over its trajectory or its landing spot. If you want to avoid bad outcomes (andthus wasted paper airplanes), it’s smarter to use, not a paper plane, but a drone thatcan sense its environment, send data back to its operator, and automatically makeDefine the statevariables in theconstructor. Likefor layers, youhave access tothe add_weight()method.Implement the state update logic in update_state(). The y_true argumentis the targets (or labels) for one batch, while y_pred represents thecorresponding predictions from the model. You can ignore thesample_weight argument—we won’t use it here.To match ourMNIST model, weexpect categoricalpredictions andinteger labels. 188CHAPTER 7Working with Keras: A deep divesteering decisions based on its current state. The Keras callbacks A P I w i l l h e l p y o utransform your call to model.fit() from a paper airplane into a smart, autonomousdrone that can self-introspect and dynamically take action. A ca llback is an object (a class instance implementing specific methods) that ispassed to the model in the call to fit() and that is called by the model at variouspoints during training. It has access to all the available data about the state of themodel and its performance, and it can take action: interrupt training, save a model,load a different weight set, or otherwise alter the state of the model. Here are some examples of ways you can use callbacks:Model checkpointing—Saving the current state of the model at different pointsduring training.Early stopping—Interrupting training when the validation loss is no longerimproving (and of course, saving the best model obtained during training).Dynamically adjusting the value of certain parameters during training—Such as thelearning rate of the optimizer.Logging training and validation metrics during training, or visualizing the representa-tions learned by the model as they’re updated—The fit() progress bar that you’refamiliar with is in fact a callback!The keras.callbacks module includes a number of built-in callbacks (this is not anexhaustive list):keras.callbacks.ModelCheckpointkeras.callbacks.EarlyStoppingkeras.callbacks.LearningRateSchedulerkeras.callbacks.ReduceLROnPlateaukeras.callbacks.CSVLoggerLet’s review two of them to give you an idea of how to use them: EarlyStopping andModelCheckpoint.THE EARLYSTOPPING AND MODELCHECKPOINT CALLBACKSWhen you’re training a model, there are many things you can’t predict from the start.In particular, you can’t tell how many epochs will be needed to get to an optimal vali-dation loss. Our examples so far have adopted the strategy of training for enoughepochs that you begin overfitting, using the first run to figure out the proper numberof epochs to train for, and then finally launching a new training run from scratchusing this optimal number. Of course, this approach is wasteful. A much better way tohandle this is to stop training when you measure that the validation loss is no longerimproving. This can be achieved using the EarlyStopping callback. The EarlyStopping callback interrupts training once a target metric being moni-tored has stopped improving for a fixed number of epochs. For instance, this callbackallows you to interrupt training as soon as you start overfitting, thus avoiding having toretrain your model for a smaller number of epochs. This callback is typically used in 189Using built-in training and evaluation loopscombination with ModelCheckpoint, which lets you continually save the model duringtraining (and, optionally, save only the current best model so far: the version of themodel that achieved the best performance at the end of an epoch).callbacks_list = [ keras.callbacks.EarlyStopping( monitor=\"val_accuracy\", patience=2, ), keras.callbacks.ModelCheckpoint( filepath=\"checkpoint_path.keras\", monitor=\"val_loss\", save_best_only=True, )]model = get_mnist_model()model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]) model.fit(train_images, train_labels, epochs=10, callbacks=callbacks_list, validation_data=(val_images, val_labels)) Note that you can always save models manually after training as well—just callmodel.save('my_checkpoint_path'). To reload the model you’ve saved, just use model = keras.models.load_model(\"checkpoint_path.keras\")7.3.3 Writing your own callbacksIf you need to take a specific action during training that isn’t covered by one of thebuilt-in callbacks, you can write your own callback. Callbacks are implemented by sub-classing the keras.callbacks.Callback class. You can then implement any numberof the following transparently named methods, which are called at various pointsduring training:on_epoch_begin(epoch, logs) on_epoch_end(epoch, logs) on_batch_begin(batch, logs) on_batch_end(batch, logs) on_train_begin(logs) on_train_end(logs) Listing 7.19 Using the callbacks argument in the fit() methodCallbacks are passed to the model via thecallbacks argument in fit(), which takes a list ofcallbacks. You can pass any number of callbacks.Interrupts training when improvement stopsMonitors the model’s validation accuracyInterrupts training when accuracy has stopped improving for two epochsSaves thecurrentweights afterevery epochPath to thedestinationmodel fileThese two arguments mean you won’t overwrite the model file unless val_loss has improved, which allows you to keep the best model seen during training.You monitor accuracy, so it should be part of the model’s metrics.Note that because the callback will monitor validation loss and validation accuracy, you need to pass validation_data to the call to fit(). Called at the startof every epochCalled at the end of every epochCalled right before processing each batchCalled right after processing each batchCalled at the start of trainingCalled at the endof training 190CHAPTER 7Working with Keras: A deep diveThese methods are all called with a logs argument, which is a dictionary containinginformation about the previous batch, epoch, or training run—training and valida-tion metrics, and so on. The on_epoch_* a n d on_batch_* m e t h o d s a l s o t a k e t h eepoch or batch index as their first argument (an integer). Here’s a simple example that saves a list of per-batch loss values during trainingand saves a graph of these values at the end of each epoch.from matplotlib import pyplot as plt class LossHistory(keras.callbacks.Callback): def on_train_begin(self, logs): self.per_batch_losses = [] def on_batch_end(self, batch, logs): self.per_batch_losses.append(logs.get(\"loss\")) def on_epoch_end(self, epoch, logs): plt.clf() plt.plot(range(len(self.per_batch_losses)), self.per_batch_losses, label=\"Training loss for each batch\") plt.xlabel(f\"Batch (epoch {epoch})\") plt.ylabel(\"Loss\") plt.legend() plt.savefig(f\"plot_at_epoch_{epoch}\") self.per_batch_losses = []Let’s test-drive it:model = get_mnist_model()model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])model.fit(train_images, train_labels, epochs=10, callbacks=[LossHistory()], validation_data=(val_images, val_labels))We get plots that look like figure 7.5. 7.3.4 Monitoring and visualization with TensorBoardTo do good research or develop good models, you need rich, frequent feedback aboutwhat’s going on inside your models during your experiments. That’s the point of run-ning experiments: to get information about how well a model performs—as muchinformation as possible. Making progress is an iterative process, a loop—you start withan idea and express it as an experiment, attempting to validate or invalidate your idea.You run this experiment and process the information it generates. This inspires yournext idea. The more iterations of this loop you’re able to run, the more refined andListing 7.20 Creating a custom callback by subclassing the Callback class 191Using built-in training and evaluation loops powerful your ideas become. Keras helps you go from idea to experiment in the leastpossible time, and fast GPUs can help you get from experiment to result as quickly aspossible. But what about processing the experiment’s results? That’s where Tensor-Board comes in (see figure 7.6). TensorBoard (www.tensorflow.org/tensorboard) is a browser-based application thatyou can run locally. It’s the best way to monitor everything that goes on inside yourmodel during training. With TensorBoard, you canVisually monitor metrics during trainingVisualize your model architectureVisualize histograms of activations and gradientsExplore embeddings in 3DIf you’re monitoring more information than just the model’s final loss, you candevelop a clearer vision of what the model does and doesn’t do, and you can makeprogress more quickly.Figure 7.5 The output of our custom history plotting callback IdeaVisualizationframework:TensorBoardDeep learningframework:KerasGPUs, TPUsResults ExperimentFigure 7.6 The loop of progress 192CHAPTER 7Working with Keras: A deep dive The easiest way to use TensorBoard with a Keras model and the fit() method is touse the keras.callbacks.TensorBoard callback. In the simplest case, just specify where you want the callback to write logs, andyou’re good to go:model = get_mnist_model()model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]) tensorboard = keras.callbacks.TensorBoard( log_dir=\"/full_path_to_your_log_dir\",)model.fit(train_images, train_labels, epochs=10, validation_data=(val_images, val_labels), callbacks=[tensorboard])Once the model starts running, it will write logs at the target location. If you are run-ning your Python script on a local machine, you can then launch the local Tensor-Board server using the following command (note that the tensorboard e x e c u t a b l eshould be already available if you have installed TensorFlow via pip; if not, you caninstall TensorBoard manually via pip install tensorboard):tensorboard --logdir /full_path_to_your_log_dirYou can then navigate to the URL that the command returns in order to access theTensorBoard interface. If you are running your script in a Colab notebook, you can run an embedded Ten-sorBoard instance as part of your notebook, using the following commands:%load_ext tensorboard%tensorboard --logdir /full_path_to_your_log_dirIn the TensorBoard interface, you will be able to monitor live graphs of your trainingand evaluation metrics (see figure 7.7). 7.4 Writing your own training and evaluation loopsThe fit() workflow strikes a nice balance between ease of use and flexibility. It’s whatyou will use most of the time. However, it isn’t meant to support everything a deeplearning researcher may want to do, even with custom metrics, custom losses, and cus-tom callbacks. After all, the built-in fit() workflow is solely focused on supervised learning: a setupwhere there are known targets (also called labels or annotations) associated with yourinput data, and where you compute your loss as a function of these targets and themodel’s predictions. However, not every form of machine learning falls into this 193Writing your own training and evaluation loops category. There are other setups where no explicit targets are present, such as genera-tive learning (which we will discuss in chapter 12), self-supervised learning (where targetsare obtained from the inputs), and reinforcement learning (where learning is driven byoccasional “rewards,” much like training a dog). Even if you’re doing regular super-vised learning, as a researcher, you may want to add some novel bells and whistles thatrequire low-level flexibility. Whenever you find yourself in a situation where the built-in fit() is not enough,you will need to write your own custom training logic. You already saw simple exam-ples of low-level training loops in chapters 2 and 3. As a reminder, the contents of atypical training loop look like this:1Run the forward pass (compute the model’s output) inside a gradient tape toobtain a loss value for the current batch of data.2Retrieve the gradients of the loss with regard to the model’s weights.3Update the model’s weights so as to lower the loss value on the current batchof data. Figure 7.7 TensorBoard can be used for easy monitoring of training and evaluation metrics. 194CHAPTER 7Working with Keras: A deep diveThese steps are repeated for as many batches as necessary. This is essentially whatfit() does under the hood. In this section, you will learn to reimplement fit() fromscratch, which will give you all the knowledge you need to write any training algorithmyou may come up with. Let’s go over the details.7.4.1 Training versus inferenceIn the low-level training loop examples you’ve seen so far, step 1 (the forward pass)was done via predictions = model(inputs), and step 2 (retrieving the gradientscomputed by the gradient tape) was done via gradients = tape.gradient(loss,model.weights). In the general case, there are actually two subtleties you need to takeinto account. Some Keras layers, such as the Dropout layer, have different behaviors during trainingand during inference (when you use them to generate predictions). Such layers exposea training B o o l e a n a r g u m e n t i n t h e i r call() m e t ho d . C a l li ng dropout(inputs,training=True) w i l l d r o p s o m e a c t i v a t i o n e n t r i e s , w h i l e c a l l i n g dropout(inputs,training=False) does nothing. By extension, Functional and Sequential models alsoexpose this training argument in their call() methods. Remember to pass training=True when you call a Keras model during the forward pass! Our forward pass thusbecomes predictions = model(inputs, training=True). I n a d d i t i o n , n o t e t h a t w h e n y o u r e t r i e v e t h e g r a d i e n t s o f t h e w e i g h t s o f y o u rmodel, you should not use tape.gradients(loss, model.weights), but rather tape.gradients(loss, model.trainable_weights). Indeed, layers and models own twokinds of weights:Trainable weights—These are meant to be updated via backpropagation to mini-mize the loss of the model, such as the kernel and bias of a Dense layer.Non-trainable weights—These are meant to be updated during the forward passby the layers that own them. For instance, if you wanted a custom layer to keepa counter of how many batches it has processed so far, that information wouldbe stored in a non-trainable weight, and at each batch, your layer would incre-ment the counter by one.Among Keras built-in layers, the only layer that features non-trainable weights is theBatchNormalization layer, which we will discuss in chapter 9. The BatchNormalizationlayer needs non-trainable weights in order to track information about the mean andstandard deviation of the data that passes through it, so as to perform an onlineapproximation of feature normalization (a concept you learned about in chapter 6). Taking into account these two details, a supervised-learning training step ends uplooking like this:def train_step(inputs, targets): with tf.GradientTape() as tape: predictions = model(inputs, training=True) loss = loss_fn(targets, predictions) 195Writing your own training and evaluation loops gradients = tape.gradients(loss, model.trainable_weights) optimizer.apply_gradients(zip(model.trainable_weights, gradients))7.4.2 Low-level usage of metricsIn a low-level training loop, you will probably want to leverage Keras metrics (whethercustom ones or the built-in ones). You’ve already learned about the metrics API: sim-ply call update_state(y_true, y_pred) for each batch of targets and predictions, andthen use result() to query the current metric value:metric = keras.metrics.SparseCategoricalAccuracy()targets = [0, 1, 2]predictions = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]metric.update_state(targets, predictions)current_result = metric.result()print(f\"result: {current_result:.2f}\")You may also need to track the average of a scalar value, such as the model’s loss. Youcan do this via the keras.metrics.Mean metric:values = [0, 1, 2, 3, 4]mean_tracker = keras.metrics.Mean() for value in values: mean_tracker.update_state(value) print(f\"Mean of values: {mean_tracker.result():.2f}\")Remember to use metric.reset_state() when you want to reset the current results(at the start of a training epoch or at the start of evaluation). 7.4.3 A complete training and evaluation loopLet’s combine the forward pass, backward pass, and metrics tracking into a fit()-liketraining step function that takes a batch of data and targets and returns the logs thatwould get displayed by the fit() progress bar.model = get_mnist_model()loss_fn = keras.losses.SparseCategoricalCrossentropy() optimizer = keras.optimizers.RMSprop() metrics = [keras.metrics.SparseCategoricalAccuracy()] loss_tracking_metric = keras.metrics.Mean() def train_step(inputs, targets): with tf.GradientTape() as tape: predictions = model(inputs, training=True) loss = loss_fn(targets, predictions) gradients = tape.gradient(loss, model.trainable_weights) optimizer.apply_gradients(zip(gradients, model.trainable_weights)) Listing 7.21 Writing a step-by-step training loop: the training step functionPrepare the lossfunction.Prepare the optimizer.Prepare the list of metrics to monitor.Prepare a Mean metric tracker to keep track of the loss average.Run the forward pass. Note that we pass training=True.Run the backward pass. Note thatwe use model.trainable_weights. 196CHAPTER 7Working with Keras: A deep dive logs = {} for metric in metrics: metric.update_state(targets, predictions) logs[metric.name] = metric.result() loss_tracking_metric.update_state(loss) logs[\"loss\"] = loss_tracking_metric.result() return logs We will need to reset the state of our metrics at the start of each epoch and before run-ning evaluation. Here’s a utility function to do it.def reset_metrics(): for metric in metrics: metric.reset_state() loss_tracking_metric.reset_state()We can now lay out our complete training loop. Note that we use a tf.data.Datasetobject to turn our NumPy data into an iterator that iterates over the data in batches ofsize 32.training_dataset = tf.data.Dataset.from_tensor_slices( (train_images, train_labels))training_dataset = training_dataset.batch(32)epochs = 3 for epoch in range(epochs): reset_metrics() for inputs_batch, targets_batch in training_dataset: logs = train_step(inputs_batch, targets_batch) print(f\"Results at the end of epoch {epoch}\") for key, value in logs.items(): print(f\"...{key}: {value:.4f}\")And here’s the evaluation loop: a simple for loop that repeatedly calls a test_step()function, which processes a single batch of data. The test_step() function is just a sub-set of the logic of train_step(). It omits the code that deals with updating the weightsof the model—that is to say, everything involving the GradientTape and the optimizer.def test_step(inputs, targets): predictions = model(inputs, training=False) loss = loss_fn(targets, predictions) logs = {} for metric in metrics: metric.update_state(targets, predictions) logs[\"val_\" + metric.name] = metric.result()Listing 7.22 Writing a step-by-step training loop: resetting the metrics Listing 7.23 Writing a step-by-step training loop: the loop itself Listing 7.24 Writing a step-by-step evaluation loopKeep track of metrics.Keep track of the loss average.Return the current values of the metrics and the loss. Note that we pass training=False. 197Writing your own training and evaluation loops loss_tracking_metric.update_state(loss) logs[\"val_loss\"] = loss_tracking_metric.result() return logs val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_labels))val_dataset = val_dataset.batch(32)reset_metrics() for inputs_batch, targets_batch in val_dataset: logs = test_step(inputs_batch, targets_batch) print(\"Evaluation results:\") for key, value in logs.items(): print(f\"...{key}: {value:.4f}\")Congrats—you’ve just reimplemented fit() a n d evaluate()! Or almost: fit()and evaluate() support many more features, including large-scale distributed com-putation, which requires a bit more work. It also includes several key performanceoptimizations. Let’s take a look at one of these optimizations: TensorFlow function compilation. 7.4.4 Make it fast with tf.functionYou may have noticed that your custom loops are running significantly slower than thebuilt-in fit() and evaluate(), despite implementing essentially the same logic.That’s because, by default, TensorFlow code is executed line by line, eagerly, much likeNumPy code or regular Python code. Eager execution makes it easier to debug yourcode, but it is far from optimal from a performance standpoint. It’s more performant to compile your TensorFlow code into a computation graph thatcan be globally optimized in a way that code interpreted line by line cannot. The syn-tax to do this is very simple: just add a @tf.function to any function you want to com-pile before executing, as shown in the following listing.@tf.function def test_step(inputs, targets): predictions = model(inputs, training=False) loss = loss_fn(targets, predictions) logs = {} for metric in metrics: metric.update_state(targets, predictions) logs[\"val_\" + metric.name] = metric.result() loss_tracking_metric.update_state(loss) logs[\"val_loss\"] = loss_tracking_metric.result() return logs val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_labels))val_dataset = val_dataset.batch(32)reset_metrics() Listing 7.25 Adding a @tf.function decorator to our evaluation-step functionThis is the only line that changed. 198CHAPTER 7Working with Keras: A deep divefor inputs_batch, targets_batch in val_dataset: logs = test_step(inputs_batch, targets_batch) print(\"Evaluation results:\") for key, value in logs.items(): print(f\"...{key}: {value:.4f}\")On the Colab CPU, we go from taking 1.80 s to run the evaluation loop to only 0.8 s.Much faster! Remember, while you are debugging your code, prefer running it eagerly, withoutany @tf.function decorator. It’s easier to track bugs this way. Once your code is work-ing and you want to make it fast, add a @tf.function decorator to your training stepand your evaluation step—or any other performance-critical function. 7.4.5 Leveraging fit() with a custom training loopIn the previous sections, we were writing our own training loop entirely from scratch.Doing so provides you with the most flexibility, but you end up writing a lot of codewhile simultaneously missing out on many convenient features of fit(), such as call-backs or built-in support for distributed training. What if you need a custom training algorithm, but you still want to leverage thepower of the built-in Keras training logic? There’s actually a middle ground betweenfit() and a training loop written from scratch: y ou can provide a custom trainingstep function and let the framework do the rest. You can do this by overriding the train_step() method of the Model class. This isthe function that is called by fit() for every batch of data. You will then be able to callfit() as usual, and it will be running your own learning algorithm under the hood. Here’s a simple example:We create a new class that subclasses keras.Model.We override the method train_step(self, data). Its contents are nearly iden-tical to what we used in the previous section. It returns a dictionary mappingmetric names (including the loss) to their current values.We implement a metrics p r o p e r t y t h a t t r a c k s t h e m o d e l ’ s Metric i n s t a n c e s .This enables the model to automatically call reset_state() o n t h e m o d e l ’ smetrics at the start of each epoch and at the start of a call to evaluate(), so youdon’t have to do it by hand.loss_fn = keras.losses.SparseCategoricalCrossentropy()loss_tracker = keras.metrics.Mean(name=\"loss\") class CustomModel(keras.Model): def train_step(self, data): inputs, targets = data with tf.GradientTape() as tape: predictions = self(inputs, training=True) loss = loss_fn(targets, predictions)Listing 7.26 Implementing a custom training step to use with fit()This metric object will be used to track the average of per-batch losses during training and evaluation.We override the train_step method.We use self(inputs, training=True) instead of model(inputs, training=True), since our model is the class itself. 199Writing your own training and evaluation loops gradients = tape.gradient(loss, model.trainable_weights) optimizer.apply_gradients(zip(gradients, model.trainable_weights)) loss_tracker.update_state(loss) return {\"loss\": loss_tracker.result()} @property def metrics(self): return [loss_tracker] We can now instantiate our custom model, compile it (we only pass the optimizer, sincethe loss is already defined outside of the model), and train it using fit() as usual:inputs = keras.Input(shape=(28 * 28,))features = layers.Dense(512, activation=\"relu\")(inputs)features = layers.Dropout(0.5)(features)outputs = layers.Dense(10, activation=\"softmax\")(features)model = CustomModel(inputs, outputs) model.compile(optimizer=keras.optimizers.RMSprop())model.fit(train_images, train_labels, epochs=3)There are a couple of points to note:This pattern does not prevent you from building models with the FunctionalAPI. You can do this whether you’re building Sequential models, FunctionalAPI models, or subclassed models.You don’t need to use a @tf.function decorator when you override train_step—the framework does it for you.Now, what about metrics, and what about configuring the loss via compile()? Afteryou’ve called compile(), you get access to the following:self.compiled_loss—The loss function you passed to compile().self.compiled_metrics—A wrapper for the list of metrics you passed, whichallows you to call self.compiled_metrics.update_state() t o u p d a t e a l l o fyour metrics at once.self.metrics—The actual list of metrics you passed to compile(). Note that italso includes a metric that tracks the loss, similar to what we did manually withour loss_tracking_metric earlier.We can thus writeclass CustomModel(keras.Model): def train_step(self, data): inputs, targets = data with tf.GradientTape() as tape: predictions = self(inputs, training=True) loss = self.compiled_loss(targets, predictions) gradients = tape.gradient(loss, model.trainable_weights)We update the loss tracker metric that tracks the average of the loss.We return the average loss so far by querying the loss tracker metric.Any metric you would like to reset across epochs should be listed here. Compute the loss via self.compiled_loss. 200CHAPTER 7Working with Keras: A deep dive optimizer.apply_gradients(zip(gradients, model.trainable_weights)) self.compiled_metrics.update_state(targets, predictions) return {m.name: m.result() for m in self.metrics} Let’s try it:inputs = keras.Input(shape=(28 * 28,))features = layers.Dense(512, activation=\"relu\")(inputs)features = layers.Dropout(0.5)(features)outputs = layers.Dense(10, activation=\"softmax\")(features)model = CustomModel(inputs, outputs) model.compile(optimizer=keras.optimizers.RMSprop(), loss=keras.losses.SparseCategoricalCrossentropy(), metrics=[keras.metrics.SparseCategoricalAccuracy()])model.fit(train_images, train_labels, epochs=3)That was a lot of information, but you now know enough to use Keras to do almostanything. SummaryKeras offers a spectrum of different workflows, based on the principle of progres-sive disclosure of complexity. They all smoothly inter-operate together.You can build models via the Sequential class, via the Functional API, or by sub-classing the Model class. Most of the time, you’ll be using the Functional API.The simplest way to train and evaluate a model is via the default fit() a n devaluate() methods.Keras callbacks provide a simple way to monitor models during your call tofit() and automatically take action based on the state of the model.You can also fully take control of what fit() d o e s b y o v e r r i d i n g t h e train_step() method.Beyond fit(), you can also write your own training loops entirely from scratch.This is useful for researchers implementing brand-new training algorithms.Update the model’s metrics via self.compiled_metrics.Return a dict mapping metricnames to their current value. 201Introductionto deep learningfor computer vision Computer vision is the earliest and biggest success story of deep learning. Everyday, you’re interacting with deep vision models—via Google Photos, Google imagesearch, YouTube, video filters in camera apps, OCR software, and many more.These models are also at the heart of cutting-edge research in autonomous driving,robotics, AI-assisted medical diagnosis, autonomous retail checkout systems, andeven autonomous farming. Computer vision is the problem domain that led to the initial rise of deep learn-ing between 2011 and 2015. A type of deep learning model called convolutionalneural networks s t a r t e d g e t t i n g r e m a r k a b l y g o o d r e s u l t s o n i m a g e c l a s s i f i c a t i o ncompetitions around that time, first with Dan Ciresan winning two niche competi-tions (the ICDAR 2011 Chinese character recognition competition and the IJCNNThis chapter coversUnderstanding convolutional neural networks (convnets)Using data augmentation to mitigate overfittingUsing a pretrained convnet to do feature extractionFine-tuning a pretrained convnet 202CHAPTER 8Introduction to deep learning for computer vision2011 German traffic signs recognition competition), and then more notably in fall2012 with Hinton’s group winning the high-profile ImageNet large-scale visual recog-nition challenge. Many more promising results quickly started bubbling up in othercomputer vision tasks. Interestingly, these early successes weren’t quite enough to make deep learningmainstream at the time—it took a few years. The computer vision research commu-nity had spent many years investing in methods other than neural networks, and itwasn’t quite ready to give up on them just because there was a new kid on the block.In 2013 and 2014, deep learning still faced intense skepticism from many seniorcomputer vision researchers. It was only in 2016 that it finally became dominant. Iremember exhorting an ex-professor of mine, in February 2014, to pivot to deeplearning. “It’s the next big thing!” I would say. “Well, maybe it’s just a fad,” hereplied. By 2016, his entire lab was doing deep learning. There’s no stopping anidea whose time has come. This chapter introduces convolutional neural networks, also known as convnets, thetype of deep learning model that is now used almost universally in computer visionapplications. You’ll learn to apply convnets to image-classification problems—in par-ticular those involving small training datasets, which are the most common use case ifyou aren’t a large tech company.8.1 Introduction to convnetsWe’re about to dive into the theory of what convnets are and why they have been sosuccessful at computer vision tasks. But first, let’s take a practical look at a simple conv-net example that classifies MNIST digits, a task we performed in chapter 2 using adensely connected network (our test accuracy then was 97.8%). Even though theconvnet will be basic, its accuracy will blow our densely connected model from chap-ter 2 out of the water. The following listing shows what a basic convnet looks like. It’s a stack of Conv2Dand MaxPooling2D layers. You’ll see in a minute exactly what they do. We’ll build themodel using the Functional API, which we introduced in the previous chapter.from tensorflow import keras from tensorflow.keras import layersinputs = keras.Input(shape=(28, 28, 1))x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(inputs)x = layers.MaxPooling2D(pool_size=2)(x)x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)x = layers.MaxPooling2D(pool_size=2)(x)x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)x = layers.Flatten()(x)outputs = layers.Dense(10, activation=\"softmax\")(x)model = keras.Model(inputs=inputs, outputs=outputs)Listing 8.1 Instantiating a small convnet 203Introduction to convnetsImportantly, a convnet takes as input tensors of shape (image_height, image_width,image_channels), not including the batch dimension. In this case, we’ll configure theconvnet to process inputs of size (28, 28, 1), which is the format of MNIST images. Let’s display the architecture of our convnet.>>> model.summary()Model: \"model\" _________________________________________________________________Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) [(None, 28, 28, 1)] 0 _________________________________________________________________conv2d (Conv2D) (None, 26, 26, 32) 320 _________________________________________________________________max_pooling2d (MaxPooling2D) (None, 13, 13, 32) 0 _________________________________________________________________conv2d_1 (Conv2D) (None, 11, 11, 64) 18496 _________________________________________________________________max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64) 0 _________________________________________________________________conv2d_2 (Conv2D) (None, 3, 3, 128) 73856 _________________________________________________________________flatten (Flatten) (None, 1152) 0 _________________________________________________________________dense (Dense) (None, 10) 11530 =================================================================Total params: 104,202 Trainable params: 104,202 Non-trainable params: 0 _________________________________________________________________You can see that the output of every Conv2D and MaxPooling2D layer is a rank-3 tensorof shape (height, width, channels). The width and height dimensions tend toshrink as you go deeper in the model. The number of channels is controlled by thefirst argument passed to the Conv2D layers (32, 64, or 128). After the last Conv2D layer, we end up with an output of shape (3, 3, 128)—a 3 × 3feature map of 128 channels. The next step is to feed this output into a densely con-nected classifier like those you’re already familiar with: a stack of Dense layers. Theseclassifiers process vectors, which are 1D, whereas the current output is a rank-3 tensor.To bridge the gap, we flatten the 3D outputs to 1D with a Flatten layer before addingthe Dense layers. Finally, we do 10-way classification, so our last layer has 10 outputs and a softmaxactivation. Now, let’s train the convnet on the MNIST digits. We’ll reuse a lot of the code fromthe MNIST example in chapter 2. Because we’re doing 10-way classification with asoftmax output, we’ll use the categorical crossentropy loss, and because our labels areintegers, we’ll use the sparse version, sparse_categorical_crossentropy.Listing 8.2 Displaying the model’s summary 204CHAPTER 8Introduction to deep learning for computer visionfrom tensorflow.keras.datasets import mnist (train_images, train_labels), (test_images, test_labels) = mnist.load_data()train_images = train_images.reshape((60000, 28, 28, 1))train_images = train_images.astype(\"float32\") / 255 test_images = test_images.reshape((10000, 28, 28, 1))test_images = test_images.astype(\"float32\") / 255 model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])model.fit(train_images, train_labels, epochs=5, batch_size=64)Let’s evaluate the model on the test data.>>> test_loss, test_acc = model.evaluate(test_images, test_labels)>>> print(f\"Test accuracy: {test_acc:.3f}\")Test accuracy: 0.991 Whereas the densely connected model from chapter 2 had a test accuracy of 97.8%,the basic convnet has a test accuracy of 99.1%: we decreased the error rate by about60% (relative). Not bad! But why does this simple convnet work so well, compared to a densely connectedmodel? To answer this, let’s dive into what the Conv2D and MaxPooling2D layers do.8.1.1 The convolution operationThe fundamental difference between a densely connected layer and a convolutionlayer is this: Dense layers learn global patterns in their input feature space (for exam-ple, for a MNIST digit, patterns involving all pixels), whereas convolution layers learnlocal patterns—in the case of images, patterns found in small 2D windows of theinputs (see figure 8.1). In the previous example, these windows were all 3 × 3.Listing 8.3 Training the convnet on MNIST images Listing 8.4 Evaluating the convnet Figure 8.1 Images can be broken into local patterns such as edges, textures, and so on. 205Introduction to convnetsThis key characteristic gives convnets two interesting properties:The patterns they learn are translation-invariant. After learning a certain pattern inthe lower-right corner of a picture, a convnet can recognize it anywhere: forexample, in the upper-left corner. A densely connected model would have tolearn the pattern anew if it appeared at a new location. This makes convnetsdata-efficient when processing images (because the visual world is fundamentallytranslation-invariant): they need fewer training samples to learn representationsthat have generalization power.They can learn spatial hierarchies of patterns. A first convolution layer will learnsmall local patterns such as edges, a second convolution layer will learn largerpatterns made of the features of the first layers, and so on (see figure 8.2). Thisallows convnets to efficiently learn increasingly complex and abstract visual con-cepts, because the visual world is fundamentally spatially hierarchical. Convolutions operate over rank-3 tensors called feature maps, with two spatial axes(height and width) as well as a depth axis (also called the channels axis). For an RGBimage, the dimension of the depth axis is 3, because the image has three color chan-nels: red, green, and blue. For a black-and-white picture, like the MNIST digits, thedepth is 1 (levels of gray). The convolution operation extracts patches from its inputfeature map and applies the same transformation to all of these patches, producingan output feature map. This output feature map is still a rank-3 tensor: it has a width and“cat” Figure 8.2 The visual world forms a spatial hierarchy of visual modules: elementary lines or textures combine into simple objects such as eyes or ears, which combine into high-level concepts such as “cat.” 206CHAPTER 8Introduction to deep learning for computer visiona height. Its depth can be arbitrary, because the output depth is a parameter of thelayer, and the different channels in that depth axis no longer stand for specific colorsas in RGB input; rather, they stand for filters. Filters encode specific aspects of theinput data: at a high level, a single filter could encode the concept “presence of a facein the input,” for instance. In the MNIST example, the first convolution layer takes a feature map of size (28,28, 1) and outputs a feature map of size (26, 26, 32): it computes 32 filters over itsinput. Each of these 32 output channels contains a 26 × 26 grid of values, which is aresponse map of the filter over the input, indicating the response of that filter pattern atdifferent locations in the input (see figure 8.3). That is what the term feature map means: every dimension in the depth axis is a feature(or filter), and the rank-2 tensor output[:, :, n] is the 2D spatial map of the responseof this filter over the input. Convolutions are defined by two key parameters:Size of the patches extracted from the inputs—These are typically 3 × 3 or 5 × 5. In theexample, they were 3 × 3, which is a common choice.Depth of the output feature map—This is the number of filters computed by the con-volution. The example started with a depth of 32 and ended with a depth of 64.In Keras Conv2D layers, these parameters are the first arguments passed to the layer:Conv2D(output_depth, (window_height, window_width)). A convolution works by sliding these windows of size 3 × 3 or 5 × 5 over the 3Dinput feature map, stopping at every possible location, and extracting the 3D patch ofsurrounding features (shape (window_height, window_width, input_depth)). Eachsuch 3D patch is then transformed into a 1D vector of shape (output_depth,), which isdone via a tensor product with a learned weight matrix, called the convolution kernel—the same kernel is reused across every patch. All of these vectors (one per patch) arethen spatially reassembled into a 3D output map of shape (height, width, output_depth). Every spatial location in the output feature map corresponds to the samelocation in the input feature map (for example, the lower-right corner of the outputcontains information about the lower-right corner of the input). For instance, withResponse map,quantifying the presenceof the ﬁlter’s pattern atdiﬀerent locations Original inputSingle ﬁlter Figure 8.3 The concept of a response map: a 2D map of the presence of a pattern at different locations in an input 207Introduction to convnets3× 3 w i n d o w s , t h e v e c t o r output[i, j, :] comes from the 3D patch input[i-1:i+1,j-1:j+1, :]. The full process is detailed in figure 8.4. Note that the output width and height may differ from the input width and height fortwo reasons:Border effects, which can be countered by padding the input feature mapThe use of strides, which I’ll define in a secondLet’s take a deeper look at these notions.UNDERSTANDING BORDER EFFECTS AND PADDINGConsider a 5 × 5 feature map (25 tiles total). There are only 9 tiles around which youcan center a 3 × 3 window, forming a 3 × 3 grid (see figure 8.5). Hence, the output fea-ture map will be 3 × 3. It shrinks a little: by exactly two tiles alongside each dimension,in this case. You can see this border effect in action in the earlier example: you startwith 28 × 28 inputs, which become 26 × 26 after the first convolution layer. If you want to get an output feature map with the same spatial dimensions as theinput, you can use padding. Padding consists of adding an appropriate number of rowsHeightInput feature map Output feature map3 × 3 input patchesTransformed patchesWidthInputdepth Dot productwith kernelOutputdepth OutputdepthFigure 8.4 How convolution works 208CHAPTER 8Introduction to deep learning for computer vision and columns on each side of the input feature map so as to make it possible to fit cen-ter convolution windows around every input tile. For a 3 × 3 window, you add one col-umn on the right, one column on the left, one row at the top, and one row at thebottom. For a 5 × 5 window, you add two rows (see figure 8.6). In Conv2D layers, padding is configurable via the padding argument, which takes twovalues: \"valid\", which means no padding (only valid window locations will be used),and \"same\", which means “pad in such a way as to have an output with the same widthand height as the input.” The padding argument defaults to \"valid\". UNDERSTANDING CONVOLUTION STRIDESThe other factor that can influence output size is the notion of strides. Our descriptionof convolution so far has assumed that the center tiles of the convolution windows areall contiguous. But the distance between two successive windows is a parameter of theFigure 8.5 Valid locations of 3 × 3 patches in a 5 × 5 input feature map etc.Figure 8.6 Padding a 5 × 5 input in order to be able to extract 25 3 × 3 patches 209Introduction to convnetsconvolution, called its stride, which defaults to 1. It’s possible to have strided convolu-tions: convolutions with a stride higher than 1. In figure 8.7, you can see the patchesextracted by a 3 × 3 convolution with stride 2 over a 5 × 5 input (without padding). Using stride 2 means the width and height of the feature map are downsampled by afactor of 2 (in addition to any changes induced by border effects). Strided convolu-tions are rarely used in classification models, but they come in handy for some types ofmodels, as you will see in the next chapter. In classification models, instead of strides, we tend to use the max-pooling operationto downsample feature maps, which you saw in action in our first convnet example.Let’s look at it in more depth. 8.1.2 The max-pooling operationIn the convnet example, you may have noticed that the size of the feature maps ishalved after every MaxPooling2D layer. For instance, before the first MaxPooling2D lay-ers, the feature map is 26 × 26, but the max-pooling operation halves it to 13 × 13.That’s the role of max pooling: to aggressively downsample feature maps, much likestrided convolutions. Max pooling consists of extracting windows from the input feature maps andoutputting the max value of each channel. It’s conceptually similar to convolution,except that instead of transforming local patches via a learned linear transforma-tion (the convolution kernel), they’re transformed via a hardcoded max t e n s o roperation. A big difference from convolution is that max pooling is usually donewith 2 × 2 windows and stride 2, in order to downsample the feature maps by a fac-tor of 2. On the other hand, convolution is typically done with 3 × 3 windows and nostride (stride 1).11234234Figure 8.7 3 × 3 convolution patches with 2 × 2 strides 210CHAPTER 8Introduction to deep learning for computer vision Why downsample feature maps this way? Why not remove the max-pooling layersand keep fairly large feature maps all the way up? Let’s look at this option. Our modelwould then look like the following listing.inputs = keras.Input(shape=(28, 28, 1))x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(inputs)x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)x = layers.Flatten()(x)outputs = layers.Dense(10, activation=\"softmax\")(x)model_no_max_pool = keras.Model(inputs=inputs, outputs=outputs)Here’s a summary of the model:>>> model_no_max_pool.summary()Model: \"model_1\" _________________________________________________________________Layer (type) Output Shape Param # =================================================================input_2 (InputLayer) [(None, 28, 28, 1)] 0 _________________________________________________________________conv2d_3 (Conv2D) (None, 26, 26, 32) 320 _________________________________________________________________conv2d_4 (Conv2D) (None, 24, 24, 64) 18496 _________________________________________________________________conv2d_5 (Conv2D) (None, 22, 22, 128) 73856 _________________________________________________________________flatten_1 (Flatten) (None, 61952) 0 _________________________________________________________________dense_1 (Dense) (None, 10) 619530 =================================================================Total params: 712,202 Trainable params: 712,202 Non-trainable params: 0 _________________________________________________________________What’s wrong with this setup? Two things:It isn’t conducive to learning a spatial hierarchy of features. The 3 × 3 windowsin the third layer will only contain information coming from 7 × 7 windows inthe initial input. The high-level patterns learned by the convnet will still be verysmall with regard to the initial input, which may not be enough to learn to clas-sify digits (try recognizing a digit by only looking at it through windows that are7 × 7 pixels!). We need the features from the last convolution layer to containinformation about the totality of the input.The final feature map has 22 × 22 × 128 = 61,952 total coefficients per sample.This is huge. When you flatten it to stick a Dense layer of size 10 on top, thatlayer would have over half a million parameters. This is far too large for such asmall model and would result in intense overfitting.Listing 8.5 An incorrectly structured convnet missing its max-pooling layers 211Training a convnet from scratch on a small datasetIn short, the reason to use downsampling is to reduce the number of feature-mapcoefficients to process, as well as to induce spatial-filter hierarchies by making succes-sive convolution layers look at increasingly large windows (in terms of the fraction ofthe original input they cover). Note that max pooling isn’t the only way you can achieve such downsampling. Asyou already know, you can also use strides in the prior convolution layer. And you canuse average pooling instead of max pooling, where each local input patch is trans-formed by taking the average value of each channel over the patch, rather than themax. But max pooling tends to work better than these alternative solutions. The rea-son is that features tend to encode the spatial presence of some pattern or conceptover the different tiles of the feature map (hence the term feature map), and it’s moreinformative to look at the maximal presence of different features than at their averagepresence. The most reasonable subsampling strategy is to first produce dense maps offeatures (via unstrided convolutions) and then look at the maximal activation of thefeatures over small patches, rather than looking at sparser windows of the inputs (viastrided convolutions) or averaging input patches, which could cause you to miss ordilute feature-presence information. At this point, you should understand the basics of convnets—feature maps, convo-lution, and max pooling—and you should know how to build a small convnet to solvea toy problem such as MNIST digits classification. Now let’s move on to more useful,practical applications. 8.2 Training a convnet from scratch on a small datasetHaving to train an image-classification model using very little data is a common situ-ation, which you’ll likely encounter in practice if you ever do computer vision in aprofessional context. A “few” samples can mean anywhere from a few hundred to afew tens of thousands of images. As a practical example, we’ll focus on classifyingimages as dogs or cats in a dataset containing 5,000 pictures of cats and dogs (2,500cats, 2,500 dogs). We’ll use 2,000 pictures for training, 1,000 for validation, and2,000 for testing. In this section, we’ll review one basic strategy to tackle this problem: training a newmodel from scratch using what little data you have. We’ll start by naively training asmall convnet on the 2,000 training samples, without any regularization, to set a base-line for what can be achieved. This will get us to a classification accuracy of about70%. At that point, the main issue will be overfitting. Then we’ll introduce data aug-mentation, a powerful technique for mitigating overfitting in computer vision. By usingdata augmentation, we’ll improve the model to reach an accuracy of 80–85%. In the next section, we’ll review two more essential techniques for applying deeplearning to small datasets: feature extraction with a pretrained model (which will get us toan accuracy of 97.5%) and fine-tuning a pretrained model (which will get us to a final accu-racy of 98.5%). Together, these three strategies—training a small model from scratch,doing feature extraction using a pretrained model, and fine-tuning a pretrained 212CHAPTER 8Introduction to deep learning for computer visionmodel—will constitute your future toolbox for tackling the problem of performingimage classification with small datasets.8.2.1 The relevance of deep learning for small-data problemsWhat qualifies as “enough samples” to train a model is relative—relative to the sizeand depth of the model you’re trying to train, for starters. It isn’t possible to train aconvnet to solve a complex problem with just a few tens of samples, but a few hundredcan potentially suffice if the model is small and well regularized and the task is simple.Because convnets learn local, translation-invariant features, they’re highly data-efficienton perceptual problems. Training a convnet from scratch on a very small image data-set will yield reasonable results despite a relative lack of data, without the need for anycustom feature engineering. You’ll see this in action in this section. What’s more, deep learning models are by nature highly repurposable: you cantake, say, an image-classification or speech-to-text model trained on a large-scale data-set and reuse it on a significantly different problem with only minor changes. Specifi-cally, in the case of computer vision, many pretrained models (usually trained on theImageNet dataset) are now publicly available for download and can be used to boot-strap powerful vision models out of very little data. This is one of the greateststrengths of deep learning: feature reuse. You’ll explore this in the next section. Let’s start by getting our hands on the data. 8.2.2 Downloading the dataThe Dogs vs. Cats dataset that we will use isn’t packaged with Keras. It was made avail-able by Kaggle as part of a computer vision competition in late 2013, back when conv-nets weren’t mainstream. You can download the original dataset from www.kaggle.com/c/dogs-vs-cats/data (you’ll need to create a Kaggle account if you don’t alreadyhave one—don’t worry, the process is painless). You can also use the Kaggle API todownload the dataset in Colab (see the “Downloading a Kaggle dataset in GoogleColaboratory” sidebar).Downloading a Kaggle dataset in Google ColaboratoryKaggle makes available an easy-to-use API to programmatically download Kaggle-hosted datasets. You can use it to download the Dogs vs. Cats dataset to a Colabnotebook, for instance. This API is available as the kaggle package, which is prein-stalled on Colab. Downloading this dataset is as easy as running the following com-mand in a Colab cell:!kaggle competitions download -c dogs-vs-catsHowever, access to the API is restricted to Kaggle users, so in order to run the pre-ceding command, you first need to authenticate yourself. The kaggle package willlook for your login credentials in a JSON file located at ~/.kaggle/kaggle.json. Let’screate this file. 213Training a convnet from scratch on a small dataset The pictures in our dataset are medium-resolution color JPEGs. Figure 8.8 showssome examples. Unsurprisingly, the original dogs-versus-cats Kaggle competition, all the way backin 2013, was won by entrants who used convnets. The best entries achieved up to 95%accuracy. In this example, we will get fairly close to this accuracy (in the next section),even though we will train our models on less than 10% of the data that was available tothe competitors. This dataset contains 25,000 images of dogs and cats (12,500 from each class) andis 543 MB (compressed). After downloading and uncompressing the data, we’ll createa new dataset containing three subsets: a training set with 1,000 samples of each class,First, you need to create a Kaggle API key and download it to your local machine. Justnavigate to the Kaggle website in a web browser, log in, and go to the My Accountpage. In your account settings, you’ll find an API section. Clicking the Create New APIToken button will generate a kaggle.json key file and will download it to your machine.Second, go to your Colab notebook, and upload the API’s key JSON file to your Colabsession by running the following code in a notebook cell:from google.colab import filesfiles.upload()When you run this cell, you will see a Choose Files button appear. Click it and selectthe kaggle.json file you just downloaded. This uploads the file to the local Colab run-time.Finally, create a ~/.kaggle folder (mkdir ~/.kaggle), and copy the key file to it(cp kaggle.json ~/.kaggle/). As a security best practice, you should also makesure that the file is only readable by the current user, yourself (chmod 600):!mkdir ~/.kaggle!cp kaggle.json ~/.kaggle/!chmod 600 ~/.kaggle/kaggle.jsonYou can now download the data we’re about to use:!kaggle competitions download -c dogs-vs-catsThe first time you try to download the data, you may get a “403 Forbidden” error.That’s because you need to accept the terms associated with the dataset before youdownload it—you’ll have to go to www.kaggle.com/c/dogs-vs-cats/rules ( w h i l elogged into your Kaggle account) and click the I Understand and Accept button. Youonly need to do this once.Finally, the training data is a compressed file named train.zip. Make sure you uncom-press it (unzip) silently (-qq):!unzip -qq train.zip 214CHAPTER 8Introduction to deep learning for computer vision a validation set with 500 samples of each class, and a test set with 1,000 samples of eachclass. Why do this? Because many of the image datasets you’ll encounter in yourcareer only contain a few thousand samples, not tens of thousands. Having more dataavailable would make the problem easier, so it’s good practice to learn with a smalldataset. The subsampled dataset we will work with will have the following directory structure:cats_vs_dogs_small/...train/......cat/ ......dog/ ...validation/......cat/ ......dog/ ...test/......cat/ ......dog/ Let’s make it happen in a couple calls to shutil.import os, shutil, pathlib original_dir = pathlib.Path(\"train\") new_base_dir = pathlib.Path(\"cats_vs_dogs_small\") Listing 8.6 Copying images to training, validation, and test directoriesFigure 8.8 Samples from the Dogs vs. Cats dataset. Sizes weren’t modified: the samples come in different sizes, colors, backgrounds, etc. Contains 1,000 cat imagesContains 1,000 dog imagesContains 500 cat imagesContains 500 dog imagesContains 1,000 cat imagesContains 1,000 dog imagesPath to the directory where the original dataset was uncompressedDirectory where we will store our smaller dataset 215Training a convnet from scratch on a small datasetdef make_subset(subset_name, start_index, end_index): for category in (\"cat\", \"dog\"): dir = new_base_dir / subset_name / category os.makedirs(dir) fnames = [f\"{category}.{i}.jpg\" for i in range(start_index, end_index)] for fname in fnames: shutil.copyfile(src=original_dir / fname, dst=dir / fname) make_subset(\"train\", start_index=0, end_index=1000) make_subset(\"validation\", start_index=1000, end_index=1500) make_subset(\"test\", start_index=1500, end_index=2500) We now have 2,000 training images, 1,000 validation images, and 2,000 test images.Each split contains the same number of samples from each class: this is a balancedbinary-classification problem, which means classification accuracy will be an appropri-ate measure of success. 8.2.3 Building the modelWe will reuse the same general model structure you saw in the first example: the conv-net will be a stack of alternated Conv2D (with relu activation) and MaxPooling2D layers. But because we’re dealing with bigger images and a more complex problem, we’llmake our model larger, accordingly: it will have two more Conv2D and MaxPooling2Dstages. This serves both to augment the capacity of the model and to further reduce thesize of the feature maps so they aren’t overly large when we reach the Flatten layer.Here, because we start from inputs of size 180 pixels × 180 pixels (a somewhat arbitrarychoice), we end up with feature maps of size 7 × 7 just before the Flatten layer.NOTEThe depth of the feature maps progressively increases in the model(from 32 to 256), whereas the size of the feature maps decreases (from 180 ×180 to 7 × 7). This is a pattern you’ll see in almost all convnets.Because we’re looking at a binary-classification problem, we’ll end the model with asingle unit (a Dense layer of size 1) and a sigmoid activation. This unit will encode theprobability that the model is looking at one class or the other. One last small difference: we will start the model with a Rescaling layer, which willrescale image inputs (whose values are originally in the [0, 255] range) to the [0, 1] range.from tensorflow import keras from tensorflow.keras import layers Listing 8.7 Instantiating a small convnet for dogs vs. cats classificationUtility function to copy cat (and dog) images from index start_index to index end_index to the subdirectory new_base_dir/{subset_name}/cat (and /dog). The \"subset_name\" will be either \"train\", \"validation\", or \"test\".Create the training subset with the first 1,000 images of each category.Create the validation subset with the next 500 images of each category.Create the test subset with the next 1,000 images of each category. 216CHAPTER 8Introduction to deep learning for computer visioninputs = keras.Input(shape=(180, 180, 3)) x = layers.Rescaling(1./255)(inputs) x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)x = layers.MaxPooling2D(pool_size=2)(x)x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)x = layers.MaxPooling2D(pool_size=2)(x)x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)x = layers.MaxPooling2D(pool_size=2)(x)x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)x = layers.MaxPooling2D(pool_size=2)(x)x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)x = layers.Flatten()(x)outputs = layers.Dense(1, activation=\"sigmoid\")(x)model = keras.Model(inputs=inputs, outputs=outputs)Let’s look at how the dimensions of the feature maps change with every successivelayer:>>> model.summary()Model: \"model_2\" _________________________________________________________________Layer (type) Output Shape Param # =================================================================input_3 (InputLayer) [(None, 180, 180, 3)] 0 _________________________________________________________________rescaling (Rescaling) (None, 180, 180, 3) 0 _________________________________________________________________conv2d_6 (Conv2D) (None, 178, 178, 32) 896 _________________________________________________________________max_pooling2d_2 (MaxPooling2 (None, 89, 89, 32) 0 _________________________________________________________________conv2d_7 (Conv2D) (None, 87, 87, 64) 18496 _________________________________________________________________max_pooling2d_3 (MaxPooling2 (None, 43, 43, 64) 0 _________________________________________________________________conv2d_8 (Conv2D) (None, 41, 41, 128) 73856 _________________________________________________________________max_pooling2d_4 (MaxPooling2 (None, 20, 20, 128) 0 _________________________________________________________________conv2d_9 (Conv2D) (None, 18, 18, 256) 295168 _________________________________________________________________max_pooling2d_5 (MaxPooling2 (None, 9, 9, 256) 0 _________________________________________________________________conv2d_10 (Conv2D) (None, 7, 7, 256) 590080 _________________________________________________________________flatten_2 (Flatten) (None, 12544) 0 _________________________________________________________________dense_2 (Dense) (None, 1) 12545 =================================================================Total params: 991,041 Trainable params: 991,041 Non-trainable params: 0 _________________________________________________________________The modelexpectsRGB imagesof size180 × 180.Rescale inputs to the [0, 1] range by dividing them by 255. 217Training a convnet from scratch on a small datasetFor the compilation step, we’ll go with the RMSprop optimizer, as usual. Because weended the model with a single sigmoid unit, we’ll use binary crossentropy as the loss(as a reminder, check out table 6.1 in chapter 6 for a cheat sheet on which loss func-tion to use in various situations). model.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"accuracy\"])8.2.4 Data preprocessingAs you know by now, data should be formatted into appropriately preprocessed floating-point tensors before being fed into the model. Currently, the data sits on a drive asJPEG files, so the steps for getting it into the model are roughly as follows:1Read the picture files.2Decode the JPEG content to RGB grids of pixels.3Convert these into floating-point tensors.4Resize them to a shared size (we’ll use 180 × 180).5Pack them into batches (we’ll use batches of 32 images).It may seem a bit daunting, but fortunately Keras has utilities to take care of these stepsautomatically. In particular, Keras features the utility function image_dataset_from_directory(), which lets you quickly set up a data pipeline that can automatically turnimage files on disk into batches of preprocessed tensors. This is what we’ll use here. Calling image_dataset_from_directory(directory) will first list the subdirecto-ries of directory and assume each one contains images from one of our classes. It willthen index the image files in each subdirectory. Finally, it will create and return atf.data.Dataset object configured to read these files, shuffle them, decode them totensors, resize them to a shared size, and pack them into batches.from tensorflow.keras.utils import image_dataset_from_directory train_dataset = image_dataset_from_directory( new_base_dir / \"train\", image_size=(180, 180), batch_size=32)validation_dataset = image_dataset_from_directory( new_base_dir / \"validation\", image_size=(180, 180), batch_size=32)test_dataset = image_dataset_from_directory( new_base_dir / \"test\", image_size=(180, 180), batch_size=32)Listing 8.8 Configuring the model for training Listing 8.9 Using image_dataset_from_directory to read images 218CHAPTER 8Introduction to deep learning for computer visionUnderstanding TensorFlow Dataset objectsTensorFlow makes available the tf.data API to create efficient input pipelines formachine learning models. Its core class is tf.data.Dataset.A Dataset object is an iterator: you can use it in a for loop. It will typically returnbatches of input data and labels. You can pass a Dataset object directly to the fit()method of a Keras model.The Dataset class handles many key features that would otherwise be cumbersometo implement yourself—in particular, asynchronous data prefetching (preprocessingthe next batch of data while the previous one is being handled by the model, whichkeeps execution flowing without interruptions).The Dataset class also exposes a functional-style API for modifying datasets. Here’sa quick example: let’s create a Dataset instance from a NumPy array of random num-bers. We’ll consider 1,000 samples, where each sample is a vector of size 16:import numpy as np import tensorflow as tfrandom_numbers = np.random.normal(size=(1000, 16))dataset = tf.data.Dataset.from_tensor_slices(random_numbers) At first, our dataset just yields single samples:>>> for i, element in enumerate(dataset):>>> print(element.shape)>>> if i >= 2:>>> break(16,)(16,)(16,)We can use the .batch() method to batch the data:>>> batched_dataset = dataset.batch(32)>>> for i, element in enumerate(batched_dataset):>>> print(element.shape)>>> if i >= 2:>>> break(32, 16)(32, 16)(32, 16)More broadly, we have access to a range of useful dataset methods, such as.shuffle(buffer_size)—Shuffles elements within a buffer.prefetch(buffer_size)—Prefetches a buffer of elements in GPU memoryto achieve better device utilization..map(callable)—Applies an arbitrary transformation to each element of thedataset (the function callable, which expects to take as input a single ele-ment yielded by the dataset).The from_tensor_slices() class method can beused to create a Dataset from a NumPy array,or a tuple or dict of NumPy arrays. 219Training a convnet from scratch on a small dataset Let’s look at the output of one of these Dataset objects: it yields batches of 180 × 180RGB images (shape (32, 180, 180, 3)) and integer labels (shape (32,)). There are32 samples in each batch (the batch size).>>> for data_batch, labels_batch in train_dataset:>>> print(\"data batch shape:\", data_batch.shape)>>> print(\"labels batch shape:\", labels_batch.shape)>>> breakdata batch shape: (32, 180, 180, 3)labels batch shape: (32,)Let’s fit the model on our dataset. We’ll use the validation_data argument in fit()to monitor validation metrics on a separate Dataset object. Note that we’ll also use a ModelCheckpoint callback to save the model after eachepoch. We’ll configure it with the path specifying where to save the file, as well as thearguments save_best_only=True and monitor=\"val_loss\": they tell the callback toonly save a new file (overwriting any previous one) when the current value of theval_loss metric is lower than at any previous time during training. This guaranteesthat your saved file will always contain the state of the model corresponding to its best-performing training epoch, in terms of its performance on the validation data. As aresult, we won’t have to retrain a new model for a lower number of epochs if we startoverfitting: we can just reload our saved file.callbacks = [ keras.callbacks.ModelCheckpoint( filepath=\"convnet_from_scratch.keras\", save_best_only=True, monitor=\"val_loss\")]The .map() method, in particular, is one that you will use often. Here’s an example.We’ll use it to reshape the elements in our toy dataset from shape (16,) to shape(4, 4):>>> reshaped_dataset = dataset.map(lambda x: tf.reshape(x, (4, 4)))>>> for i, element in enumerate(reshaped_dataset):>>> print(element.shape)>>> if i >= 2:>>> break(4, 4)(4, 4)(4, 4)You’re about to see more map() action in this chapter.Listing 8.10 Displaying the shapes of the data and labels yielded by the Dataset Listing 8.11 Fitting the model using a Dataset 220CHAPTER 8Introduction to deep learning for computer visionhistory = model.fit( train_dataset, epochs=30, validation_data=validation_dataset, callbacks=callbacks)Let’s plot the loss and accuracy of the model over the training and validation dataduring training (see figure 8.9).import matplotlib.pyplot as pltaccuracy = history.history[\"accuracy\"]val_accuracy = history.history[\"val_accuracy\"]loss = history.history[\"loss\"]val_loss = history.history[\"val_loss\"]epochs = range(1, len(accuracy) + 1)plt.plot(epochs, accuracy, \"bo\", label=\"Training accuracy\")plt.plot(epochs, val_accuracy, \"b\", label=\"Validation accuracy\")plt.title(\"Training and validation accuracy\")plt.legend()plt.figure()plt.plot(epochs, loss, \"bo\", label=\"Training loss\")plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")plt.title(\"Training and validation loss\")plt.legend()plt.show() These plots are characteristic of overfitting. The training accuracy increases linearlyover time, until it reaches nearly 100%, whereas the validation accuracy peaks at 75%.The validation loss reaches its minimum after only ten epochs and then stalls, whereasthe training loss keeps decreasing linearly as training proceeds. Let’s check the test accuracy. We’ll reload the model from its saved file to evaluateit as it was before it started overfitting.Listing 8.12 Displaying curves of loss and accuracy during training Figure 8.9 Training and validation metrics for a simple convnet 221Training a convnet from scratch on a small datasettest_model = keras.models.load_model(\"convnet_from_scratch.keras\")test_loss, test_acc = test_model.evaluate(test_dataset) print(f\"Test accuracy: {test_acc:.3f}\")We get a test accuracy of 69.5%. (Due to the randomness of neural network initializa-tions, you may get numbers within one percentage point of that.) B e c a u s e w e h a v e r e l a t i v e l y f e w t r a i n i n g s a m p l e s ( 2 , 0 0 0 ) , o v e r f i t t i n g w i l l b e o u rnumber one concern. You already know about a number of techniques that can helpmitigate overfitting, such as dropout and weight decay (L2 regularization). We’re nowgoing to work with a new one, specific to computer vision and used almost universallywhen processing images with deep learning models: data augmentation. 8.2.5 Using data augmentationOverfitting is caused by having too few samples to learn from, rendering you unableto train a model that can generalize to new data. Given infinite data, your modelwould be exposed to every possible aspect of the data distribution at hand: you wouldnever overfit. Data augmentation takes the approach of generating more training datafrom existing training samples by augmenting t h e s a m p l e s v i a a n u m b e r o f r a n d o mtransformations that yield believable-looking images. The goal is that, at training time,your model will never see the exact same picture twice. This helps expose the modelto more aspects of the data so it can generalize better. In Keras, this can be done by adding a number of data augmentation layers at thestart of your model. Let’s get started with an example: the following Sequential modelchains several random image transformations. In our model, we’d include it rightbefore the Rescaling layer.data_augmentation = keras.Sequential( [ layers.RandomFlip(\"horizontal\"), layers.RandomRotation(0.1), layers.RandomZoom(0.2), ])These are just a few of the layers available (for more, see the Keras documentation).Let’s quickly go over this code:RandomFlip(\"horizontal\")—Applies horizontal flipping to a random 50% ofthe images that go through itRandomRotation(0.1)—Rotates the input images by a random value in the range[–10%, +10%] (these are fractions of a full circle—in degrees, the range wouldbe [–36 degrees, +36 degrees])Listing 8.13 Evaluating the model on the test set Listing 8.14 Define a data augmentation stage to add to an image model 222CHAPTER 8Introduction to deep learning for computer visionRandomZoom(0.2)—Zooms in or out of the image by a random factor in therange [-20%, +20%]Let’s look at the augmented images (see figure 8.10).plt.figure(figsize=(10, 10)) for images, _ in train_dataset.take(1): for i in range(9): augmented_images = data_augmentation(images) ax = plt.subplot(3, 3, i + 1) plt.imshow(augmented_images[0].numpy().astype(\"uint8\")) plt.axis(\"off\") If we train a new model using this data-augmentation configuration, the model willnever see the same input twice. But the inputs it sees are still heavily intercorrelatedListing 8.15 Displaying some randomly augmented training imagesWe can use take(N) to only sample N batches from the dataset. This is equivalent to inserting a break in the loop after the Nth batch.Apply theaugmentationstage to thebatch ofimages.Display the first image in the output batch.For each of the nine iterations, this is adifferent augmentation of the same image. Figure 8.10 Generating variations of a very good boy via random data augmentation 223Training a convnet from scratch on a small datasetbecause they come from a small number of original images—we can’t produce newinformation; we can only remix existing information. As such, this may not be enoughto completely get rid of overfitting. To further fight overfitting, we’ll also add a Dropoutlayer to our model right before the densely connected classifier. One last thing you should know about random image augmentation layers: justlike Dropout, they’re inactive during inference (when we call predict() or evaluate()).During evaluation, our model will behave just the same as when it did not includedata augmentation and dropout.inputs = keras.Input(shape=(180, 180, 3))x = data_augmentation(inputs)x = layers.Rescaling(1./255)(x)x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)x = layers.MaxPooling2D(pool_size=2)(x)x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)x = layers.MaxPooling2D(pool_size=2)(x)x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)x = layers.MaxPooling2D(pool_size=2)(x)x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)x = layers.MaxPooling2D(pool_size=2)(x)x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)x = layers.Flatten()(x)x = layers.Dropout(0.5)(x)outputs = layers.Dense(1, activation=\"sigmoid\")(x)model = keras.Model(inputs=inputs, outputs=outputs) model.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"accuracy\"])Let’s train the model using data augmentation and dropout. Because we expect over-fitting to occur much later during training, we will train for three times as manyepochs—one hundred.callbacks = [ keras.callbacks.ModelCheckpoint( filepath=\"convnet_from_scratch_with_augmentation.keras\", save_best_only=True, monitor=\"val_loss\")]history = model.fit( train_dataset, epochs=100, validation_data=validation_dataset, callbacks=callbacks)Let’s plot the results again: see figure 8.11. Thanks to data augmentation and drop-out, we start overfitting much later, around epochs 60–70 (compared to epoch 10 forListing 8.16 Defining a new convnet that includes image augmentation and dropout Listing 8.17 Training the regularized convnet 224CHAPTER 8Introduction to deep learning for computer vision the original model). The validation accuracy ends up consistently in the 80–85% range—a big improvement over our first try. Let’s check the test accuracy.test_model = keras.models.load_model( \"convnet_from_scratch_with_augmentation.keras\")test_loss, test_acc = test_model.evaluate(test_dataset)print(f\"Test accuracy: {test_acc:.3f}\")We get a test accuracy of 83.5%. It’s starting to look good! If you’re using Colab, makesure you download the saved file (convnet_from_scratch_with_augmentation.keras),as we will use it for some experiments in the next chapter. B y f u r t h e r t u n i n g t h e m o d e l ’ s c o n f i g u r a t i on (such as the number of fi lters perconvolution layer, or the number of layers in the model), we might be able to get aneven better accuracy, likely up to 90%. But it would prove difficult to go any higherjust by training our own convnet from scratch, because we have so little data to workwith. As a next step to improve our accuracy on this problem, we’ll have to use a pre-trained model, which is the focus of the next two sections. 8.3 Leveraging a pretrained modelA common and highly effective approach to deep learning on small image datasets isto use a pretrained model. A pretrained model is a model that was previously trained ona large dataset, typically on a large-scale image-classification task. If this original data-set is large enough and general enough, the spatial hierarchy of features learned bythe pretrained model can effectively act as a generic model of the visual world, andhence, its features can prove useful for many different computer vision problems,even though these new problems may involve completely different classes than thoseof the original task. For instance, you might train a model on ImageNet (where classesListing 8.18 Evaluating the model on the test setFigure 8.11 Training and validation metrics with data augmentation 225Leveraging a pretrained modelare mostly animals and everyday objects) and then repurpose this trained model forsomething as remote as identifying furniture items in images. Such portability oflearned features across different problems is a key advantage of deep learning com-pared to many older, shallow learning approaches, and it makes deep learning veryeffective for small-data problems. In this case, let’s consider a large convnet trained on the ImageNet dataset (1.4million labeled images and 1,000 different classes). ImageNet contains many animalclasses, including different species of cats and dogs, and you can thus expect it to per-form well on the dogs-versus-cats classification problem. W e ’ l l u s e t h e V G G 1 6 a r c h i t e c t u r e , d e v e l o p e d b y K a r e n S i m o n y a n a n d A n d r e wZisserman in 2014.1 Although it’s an older model, far from the current state of the artand somewhat heavier than many other recent models, I chose it because its architec-ture is similar to what you’re already familiar with, and it’s easy to understand withoutintroducing any new concepts. This may be your first encounter with one of thesecutesy model names—VGG, ResNet, Inception, Xception, and so on; you’ll get usedto them because they will come up frequently if you keep doing deep learning forcomputer vision. T h e r e a r e t w o w a y s t o u s e a p r e t r a i n e d m o d e l : feature extraction a n d fine-tuning.We’ll cover both of them. Let’s start with feature extraction.8.3.1 Feature extraction with a pretrained modelFeature extraction consists of using the representations learned by a previouslytrained model to extract interesting features from new samples. These features arethen run through a new classifier, which is trained from scratch. As you saw previously, convnets used for image classification comprise two parts:they start with a series of pooling and convolution layers, and they end with a denselyconnected classifier. The first part is called the convolutional base of the model. In thecase of convnets, feature extraction consists of taking the convolutional base of a pre-viously trained network, running the new data through it, and training a new classifieron top of the output (see figure 8.12). W h y o n l y r e u s e t h e c o n v o l u t i o n a l b a s e ? C o u l d w e r e u s e t h e d e n s e l y c o n n e c t e dclassifier as well? In general, doing so should be avoided. The reason is that the repre-sentations learned by the convolutional base are likely to be more generic and, there-fore, more reusable: the feature maps of a convnet are presence maps of genericconcepts over a picture, which are likely to be useful regardless of the computer visionproblem at hand. But the representations learned by the classifier will necessarily bespecific to the set of classes on which the model was trained—they will only containinformation about the presence probability of this or that class in the entire picture.Additionally, representations found in densely connected layers no longer contain any1Karen Simonyan and Andrew Zisserman, “Very Deep Convolutional Networks for Large-Scale Image Recogni-tion,” arXiv (2014), https:/ /arxiv.org/abs/1409.1556. 226CHAPTER 8Introduction to deep learning for computer vision information about where objects are located in the input image; these layers get rid ofthe notion of space, whereas the object location is still described by convolutional fea-ture maps. For problems where object location matters, densely connected featuresare largely useless. Note that the level of generality (and therefore reusability) of the representationsextracted by specific convolution layers depends on the depth of the layer in themodel. Layers that come earlier in the model extract local, highly generic featuremaps (such as visual edges, colors, and textures), whereas layers that are higher upextract more-abstract concepts (such as “cat ear” or “dog eye”). So if your new datasetdiffers a lot from the dataset on which the original model was trained, you may be bet-ter off using only the first few layers of the model to do feature extraction, rather thanusing the entire convolutional base. I n t h i s c a s e , b e c a u s e t h e I m a g e N e t c l a s s s e t c o n t a i n s m u l t i p l e d o g a n d c a tclasses, it’s likely to be beneficial to reuse the information contained in the denselyconnected layers of the original model. But we’ll choose not to, in order to coverthe more general case where the class set of the new problem doesn’t overlap theclass set of the original model. Let’s put this into practice by using the convolu-tional base of the VGG16 network, trained on ImageNet, to extract interesting fea-tures from cat and dog images, and then train a dogs-versus-cats classifier on top ofthese features. The VGG16 model, among others, comes prepackaged with Keras. You can importit from the keras.applications module. Many other image-classification models (allpretrained on the ImageNet dataset) are available as part of keras.applications:Prediction InputTrainedclassiﬁerTrainedconvolutionalbasePrediction InputTrainedclassiﬁerTrainedconvolutionalbasePrediction InputNew classiﬁer(randomly initialized)Trainedconvolutionalbase(frozen) Figure 8.12 Swapping classifiers while keeping the same convolutional base 227Leveraging a pretrained modelXceptionResNetMobileNetEfficientNetDenseNetetc.Let’s instantiate the VGG16 model.conv_base = keras.applications.vgg16.VGG16( weights=\"imagenet\", include_top=False, input_shape=(180, 180, 3))We pass three arguments to the constructor:weights specifies the weight checkpoint from which to initialize the model.include_top refers to including (or not) the densely connected classifier ontop of the network. By default, this densely connected classifier corresponds tothe 1,000 classes from ImageNet. Because we intend to use our own denselyconnected classifier (with only two classes: cat a n d dog), we don’t need toinclude it.input_shape is the shape of the image tensors that we’ll feed to the network.This argument is purely optional: if we don’t pass it, the network will be able toprocess inputs of any size. Here we pass it so that we can visualize (in the follow-ing summary) how the size of the feature maps shrinks with each new convolu-tion and pooling layer.Here’s the detail of the architecture of the VGG16 convolutional base. It’s similar tothe simple convnets you’re already familiar with:>>> conv_base.summary()Model: \"vgg16\" _________________________________________________________________Layer (type) Output Shape Param # =================================================================input_19 (InputLayer) [(None, 180, 180, 3)] 0 _________________________________________________________________block1_conv1 (Conv2D) (None, 180, 180, 64) 1792 _________________________________________________________________block1_conv2 (Conv2D) (None, 180, 180, 64) 36928 _________________________________________________________________block1_pool (MaxPooling2D) (None, 90, 90, 64) 0 _________________________________________________________________block2_conv1 (Conv2D) (None, 90, 90, 128) 73856 _________________________________________________________________Listing 8.19 Instantiating the VGG16 convolutional base 228CHAPTER 8Introduction to deep learning for computer visionblock2_conv2 (Conv2D) (None, 90, 90, 128) 147584 _________________________________________________________________block2_pool (MaxPooling2D) (None, 45, 45, 128) 0 _________________________________________________________________block3_conv1 (Conv2D) (None, 45, 45, 256) 295168 _________________________________________________________________block3_conv2 (Conv2D) (None, 45, 45, 256) 590080 _________________________________________________________________block3_conv3 (Conv2D) (None, 45, 45, 256) 590080 _________________________________________________________________block3_pool (MaxPooling2D) (None, 22, 22, 256) 0 _________________________________________________________________block4_conv1 (Conv2D) (None, 22, 22, 512) 1180160 _________________________________________________________________block4_conv2 (Conv2D) (None, 22, 22, 512) 2359808 _________________________________________________________________block4_conv3 (Conv2D) (None, 22, 22, 512) 2359808 _________________________________________________________________block4_pool (MaxPooling2D) (None, 11, 11, 512) 0 _________________________________________________________________block5_conv1 (Conv2D) (None, 11, 11, 512) 2359808 _________________________________________________________________block5_conv2 (Conv2D) (None, 11, 11, 512) 2359808 _________________________________________________________________block5_conv3 (Conv2D) (None, 11, 11, 512) 2359808 _________________________________________________________________block5_pool (MaxPooling2D) (None, 5, 5, 512) 0 =================================================================Total params: 14,714,688 Trainable params: 14,714,688 Non-trainable params: 0 _________________________________________________________________The final feature map has shape (5, 5, 512). That’s the feature map on top of whichwe’ll stick a densely connected classifier. At this point, there are two ways we could proceed:Run the convolutional base over our dataset, record its output to a NumPy arrayon disk, and then use this data as input to a standalone, densely connected clas-sifier similar to those you saw in chapter 4 of this book. This solution is fast andcheap to run, because it only requires running the convolutional base once forevery input image, and the convolutional base is by far the most expensive partof the pipeline. But for the same reason, this technique won’t allow us to usedata augmentation.Extend the model we have (conv_base) by adding Dense layers on top, and runthe whole thing from end to end on the input data. This will allow us to usedata augmentation, because every input image goes through the convolutionalbase every time it’s seen by the model. But for the same reason, this technique isfar more expensive than the first. 229Leveraging a pretrained modelWe’ll cover both techniques. Let’s walk through the code required to set up the firstone: recording the output of conv_base on our data and using these outputs as inputsto a new model.FAST FEATURE EXTRACTION WITHOUT DATA AUGMENTATIONWe’ll start by extracting features as NumPy arrays by calling the predict() method ofthe conv_base model on our training, validation, and testing datasets. Let’s iterate over our datasets to extract the VGG16 features.import numpy as np def get_features_and_labels(dataset): all_features = [] all_labels = [] for images, labels in dataset: preprocessed_images = keras.applications.vgg16.preprocess_input(images) features = conv_base.predict(preprocessed_images) all_features.append(features) all_labels.append(labels) return np.concatenate(all_features), np.concatenate(all_labels) train_features, train_labels = get_features_and_labels(train_dataset)val_features, val_labels = get_features_and_labels(validation_dataset)test_features, test_labels = get_features_and_labels(test_dataset)Importantly, predict() only expects images, not labels, but our current dataset yieldsbatches that contain both images and their labels. Moreover, the VGG16 model expectsinputs that are preprocessed with the function keras.applications.vgg16.prepro-cess_input, which scales pixel values to an appropriate range. The extracted features are currently of shape (samples, 5, 5, 512):>>> train_features.shape(2000, 5, 5, 512)At this point, we can define our densely connected classifier (note the use of dropoutfor regularization) and train it on the data and labels that we just recorded.inputs = keras.Input(shape=(5, 5, 512))x = layers.Flatten()(inputs) x = layers.Dense(256)(x)x = layers.Dropout(0.5)(x)outputs = layers.Dense(1, activation=\"sigmoid\")(x)model = keras.Model(inputs, outputs)Listing 8.20 Extracting the VGG16 features and corresponding labels Listing 8.21 Defining and training the densely connected classifierNote the use of the Flatten layer before passing the features to a Dense layer. 230CHAPTER 8Introduction to deep learning for computer visionmodel.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"accuracy\"]) callbacks = [ keras.callbacks.ModelCheckpoint( filepath=\"feature_extraction.keras\", save_best_only=True, monitor=\"val_loss\")]history = model.fit( train_features, train_labels, epochs=20, validation_data=(val_features, val_labels), callbacks=callbacks)Training is very fast because we only have to deal with two Dense layers—an epochtakes less than one second even on CPU. Let’s look at the loss and accuracy curves during training (see figure 8.13). import matplotlib.pyplot as pltacc = history.history[\"accuracy\"]val_acc = history.history[\"val_accuracy\"]loss = history.history[\"loss\"]val_loss = history.history[\"val_loss\"]epochs = range(1, len(acc) + 1)plt.plot(epochs, acc, \"bo\", label=\"Training accuracy\")plt.plot(epochs, val_acc, \"b\", label=\"Validation accuracy\")plt.title(\"Training and validation accuracy\")plt.legend()plt.figure()plt.plot(epochs, loss, \"bo\", label=\"Training loss\")Listing 8.22 Plotting the resultsFigure 8.13 Training and validation metrics for plain feature extraction 231Leveraging a pretrained modelplt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")plt.title(\"Training and validation loss\")plt.legend()plt.show()We reach a validation accuracy of about 97%—much better than we achieved in theprevious section with the small model trained from scratch. This is a bit of an unfaircomparison, however, because ImageNet contains many dog and cat instances, whichmeans that our pretrained model already has the exact knowledge required for thetask at hand. This won’t always be the case when you use pretrained features. H o w e v e r , t h e p l o t s a l s o i n d i c a t e t h a t w e ’ r e o v e r f i t t i n g a l m o s t f r o m t h e s t a r t —despite using dropout with a fairly large rate. That’s because this technique doesn’tuse data augmentation, which is essential for preventing overfitting with small imagedatasets. FEATURE EXTRACTION TOGETHER WITH DATA AUGMENTATIONNow let’s review the second technique I mentioned for doing feature extraction,which is much slower and more expensive, but which allows us to use data augmenta-tion during training: creating a model that chains the conv_base with a new denseclassifier, and training it end to end on the inputs. In order to do this, we will first freeze the convolutional base. Freezing a layer or set oflayers means preventing their weights from being updated during training. If we don’tdo this, the representations that were previously learned by the convolutional base willbe modified during training. Because the Dense layers on top are randomly initialized,very large weight updates would be propagated through the network, effectivelydestroying the representations previously learned. In Keras, we freeze a layer or model by setting its trainable attribute to False.conv_base = keras.applications.vgg16.VGG16( weights=\"imagenet\", include_top=False)conv_base.trainable = FalseSetting trainable to False empties the list of trainable weights of the layer or model.>>> conv_base.trainable = True>>> print(\"This is the number of trainable weights \" \"before freezing the conv base:\", len(conv_base.trainable_weights))This is the number of trainable weights before freezing the conv base: 26 >>> conv_base.trainable = False>>> print(\"This is the number of trainable weights \" \"after freezing the conv base:\", len(conv_base.trainable_weights))This is the number of trainable weights after freezing the conv base: 0 Listing 8.23 Instantiating and freezing the VGG16 convolutional base Listing 8.24 Printing the list of trainable weights before and after freezing 232CHAPTER 8Introduction to deep learning for computer visionNow we can create a new model that chains together1A data augmentation stage2Our frozen convolutional base3A dense classifierdata_augmentation = keras.Sequential( [ layers.RandomFlip(\"horizontal\"), layers.RandomRotation(0.1), layers.RandomZoom(0.2), ]) inputs = keras.Input(shape=(180, 180, 3))x = data_augmentation(inputs) x = keras.applications.vgg16.preprocess_input(x) x = conv_base(x)x = layers.Flatten()(x)x = layers.Dense(256)(x)x = layers.Dropout(0.5)(x)outputs = layers.Dense(1, activation=\"sigmoid\")(x)model = keras.Model(inputs, outputs)model.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"accuracy\"])With this setup, only the weights from the two Dense l a y e r s t h a t w e a d d e d w i l l b etrained. That’s a total of four weight tensors: two per layer (the main weight matrixand the bias vector). Note that in order for these changes to take effect, you must firstcompile the model. If you ever modify weight trainability after compilation, youshould then recompile the model, or these changes will be ignored. Let’s train our model. Thanks to data augmentation, it will take much longer forthe model to start overfitting, so we can train for more epochs—let’s do 50.NOTEThis technique is expensive enough that you should only attempt it ifyou have access to a GPU (such as the free GPU available in Colab)—it’sintractable on CPU. If you can’t run your code on GPU, then the previoustechnique is the way to go.callbacks = [ keras.callbacks.ModelCheckpoint( filepath=\"feature_extraction_with_data_augmentation.keras\", save_best_only=True, monitor=\"val_loss\")]Listing 8.25 Adding a data augmentation stage and a classifier to the convolutional base Apply data augmentation.Apply input value scaling. 233Leveraging a pretrained modelhistory = model.fit( train_dataset, epochs=50, validation_data=validation_dataset, callbacks=callbacks)Let’s plot the results again (see figure 8.14). As you can see, we reach a validationaccuracy of over 98%. This is a strong improvement over the previous model. Let’s check the test accuracy.test_model = keras.models.load_model( \"feature_extraction_with_data_augmentation.keras\")test_loss, test_acc = test_model.evaluate(test_dataset)print(f\"Test accuracy: {test_acc:.3f}\")We get a test accuracy of 97.5%. This is only a modest improvement compared to theprevious test accuracy, which is a bit disappointing given the strong results on the vali-dation data. A model’s accuracy always depends on the set of samples you evaluate iton! Some sample sets may be more difficult than others, and strong results on one setwon’t necessarily fully translate to all other sets. Listing 8.26 Evaluating the model on the test setFigure 8.14 Training and validation metrics for feature extraction with data augmentation 234CHAPTER 8Introduction to deep learning for computer vision8.3.2 Fine-tuning a pretrained modelAnother widely used technique for model reuse,complementary to feature extraction, is fine-tuning(see figure 8.15). Fine-tuning consists of unfreezinga few of the top layers of a frozen model base usedfor feature extraction, and jointly training both thenewly added part of the model (in this case, thefully connected classifier) and these top layers. Thisis called fine-tuning b e c a u s e i t s l i g h t l y a d j u s t s t h emore abstract representations of the model beingreused in order to make them more relevant for theproblem at hand. I s t a t e d e a r l i e r t h a t i t ’ s n e c e s s a r y t o f r e e z e t h econvolution base of VGG16 in order to be able totrain a randomly initialized classifier on top. For thesame reason, it’s only possible to fine-tune the toplayers of the convolutional base once the classifier ontop has already been trained. If the classifier isn’talready trained, the error signal propagatingthrough the network during training will be toolarge, and the representations previously learned bythe layers being fine-tuned will be destroyed. Thusthe steps for fine-tuning a network are as follows:1Add our custom network on top of analready-trained base network.2Freeze the base network.3Train the part we added.4Unfreeze some layers in the base network.(Note that you should not unfreeze “batchnormalization” layers, which are not relevanthere since there are no such layers in VGG16.Batch normalization and its impact on fine-tuning is explained in the next chapter.)5Jointly train both these layers and the part weadded.You already completed the first three steps whendoing feature extraction. Let’s proceed with step 4:we’ll unfreeze our conv_base a n d t h e n f r e e z e i n d i -vidual layers inside it. DenseDenseFlattenMaxPooling2DConvolution2DConvolution2DConvolution2DMaxPooling2DConvolution2DConvolution2DConvolution2DMaxPooling2DConvolution2DConvolution2DConvolution2DMaxPooling2DConvolution2DConvolution2DMaxPooling2DConvolution2DConvolution2DConv block 1:frozenConv block 2:frozenConv block 3:frozen Conv block 4:frozen We ﬁne-tuneconv block 5.We ﬁne-tuneour own fullyconnectedclassiﬁer.Figure 8.15 Fine-tuning the last convolutional block of the VGG16 network 235Leveraging a pretrained model As a reminder, this is what our convolutional base looks like:>>> conv_base.summary()Model: \"vgg16\" _________________________________________________________________Layer (type) Output Shape Param # =================================================================input_19 (InputLayer) [(None, 180, 180, 3)] 0 _________________________________________________________________block1_conv1 (Conv2D) (None, 180, 180, 64) 1792 _________________________________________________________________block1_conv2 (Conv2D) (None, 180, 180, 64) 36928 _________________________________________________________________block1_pool (MaxPooling2D) (None, 90, 90, 64) 0 _________________________________________________________________block2_conv1 (Conv2D) (None, 90, 90, 128) 73856 _________________________________________________________________ block2_conv2 (Conv2D) (None, 90, 90, 128) 147584 _________________________________________________________________block2_pool (MaxPooling2D) (None, 45, 45, 128) 0 _________________________________________________________________block3_conv1 (Conv2D) (None, 45, 45, 256) 295168 _________________________________________________________________block3_conv2 (Conv2D) (None, 45, 45, 256) 590080 _________________________________________________________________block3_conv3 (Conv2D) (None, 45, 45, 256) 590080 _________________________________________________________________block3_pool (MaxPooling2D) (None, 22, 22, 256) 0 _________________________________________________________________block4_conv1 (Conv2D) (None, 22, 22, 512) 1180160 _________________________________________________________________block4_conv2 (Conv2D) (None, 22, 22, 512) 2359808 _________________________________________________________________block4_conv3 (Conv2D) (None, 22, 22, 512) 2359808 _________________________________________________________________block4_pool (MaxPooling2D) (None, 11, 11, 512) 0 _________________________________________________________________block5_conv1 (Conv2D) (None, 11, 11, 512) 2359808 _________________________________________________________________block5_conv2 (Conv2D) (None, 11, 11, 512) 2359808 _________________________________________________________________block5_conv3 (Conv2D) (None, 11, 11, 512) 2359808 _________________________________________________________________block5_pool (MaxPooling2D) (None, 5, 5, 512) 0 =================================================================Total params: 14,714,688 Trainable params: 14,714,688 Non-trainable params: 0 _________________________________________________________________We’ll fine-tune the last three convolutional layers, which means all layers up to block4_pool should be frozen, and the layers block5_conv1, block5_conv2, and block5_conv3should be trainable. 236CHAPTER 8Introduction to deep learning for computer vision Why not fine-tune more layers? Why not fine-tune the entire convolutional base?You could. But you need to consider the following:Earlier layers in the convolutional base encode more generic, reusable features,whereas layers higher up encode more specialized features. It’s more useful tofine-tune the more specialized features, because these are the ones that needto be repurposed on your new problem. There would be fast-decreasing returnsin fine-tuning lower layers.The more parameters you’re training, the more you’re at risk of overfitting.The convolutional base has 15 million parameters, so it would be risky toattempt to train it on your small dataset.Thus, in this situation, it’s a good strategy to fine-tune only the top two or three layersin the convolutional base. Let’s set this up, starting from where we left off in the previ-ous example.conv_base.trainable = Truefor layer in conv_base.layers[:-4]: layer.trainable = FalseNow we can begin fine-tuning the model. We’ll do this with the RMSprop optimizer,using a very low learning rate. The reason for using a low learning rate is that we want tolimit the magnitude of the modifications we make to the representations of the threelayers we’re fine-tuning. Updates that are too large may harm these representations.model.compile(loss=\"binary_crossentropy\", optimizer=keras.optimizers.RMSprop(learning_rate=1e-5), metrics=[\"accuracy\"]) callbacks = [ keras.callbacks.ModelCheckpoint( filepath=\"fine_tuning.keras\", save_best_only=True, monitor=\"val_loss\")]history = model.fit( train_dataset, epochs=30, validation_data=validation_dataset, callbacks=callbacks)We can finally evaluate this model on the test data:model = keras.models.load_model(\"fine_tuning.keras\")test_loss, test_acc = model.evaluate(test_dataset) print(f\"Test accuracy: {test_acc:.3f}\")Listing 8.27 Freezing all layers until the fourth from the last Listing 8.28 Fine-tuning the model 237SummaryHere, we get a test accuracy of 98.5% (again, your own results may be within one per-centage point). In the original Kaggle competition around this dataset, this wouldhave been one of the top results. It’s not quite a fair comparison, however, since weused pretrained features that already contained prior knowledge about cats and dogs,which competitors couldn’t use at the time. On the positive side, by leveraging modern deep learning techniques, we managedto reach this result using only a small fraction of the training data that was availablefor the competition (about 10%). There is a huge difference between being able totrain on 20,000 samples compared to 2,000 samples! Now you have a solid set of tools for dealing with image-classification problems—inparticular, with small datasets. SummaryConvnets are the best type of machine learning models for computer visiontasks. It’s possible to train one from scratch even on a very small dataset, withdecent results.Convnets work by learning a hierarchy of modular patterns and concepts torepresent the visual world.On a small dataset, overfitting will be the main issue. Data augmentation is apowerful way to fight overfitting when you’re working with image data.It’s easy to reuse an existing convnet on a new dataset via feature extraction.This is a valuable technique for working with small image datasets.As a complement to feature extraction, you can use fine-tuning, which adapts toa new problem some of the representations previously learned by an existingmodel. This pushes performance a bit further. 238Advanced deep learningfor computer vision The previous chapter gave you a first introduction to deep learning for computervision via simple models (stacks of Conv2D and MaxPooling2D layers) and a simpleuse case (binary image classification). But there’s more to computer vision thanimage classification! This chapter dives deeper into more diverse applications andadvanced best practices.9.1 Three essential computer vision tasksSo far, we’ve focused on image classification models: an image goes in, a labelcomes out. “This image likely contains a cat; this other one likely contains a dog.”But image classification is only one of several possible applications of deep learningThis chapter coversThe different branches of computer vision: image classification, image segmentation, object detectionModern convnet architecture patterns: residual connections, batch normalization, depthwise separable convolutionsTechniques for visualizing and interpreting what convnets learn 239Three essential computer vision tasksin computer vision. In general, there are three essential computer vision tasks youneed to know about:Image classification—Where the goal is to assign one or more labels to an image.It may be either single-label classification (an image can only be in one cate-gory, excluding the others), or multi-label classification (tagging all categoriesthat an image belongs to, as seen in figure 9.1). For example, when you searchfor a keyword on the Google Photos app, behind the scenes you’re querying avery large multilabel classification model—one with over 20,000 different classes,trained on millions of images.Image segmentation—Where the goal is to “segment” or “partition” an image intodifferent areas, with each area usually representing a category (as seen in fig-ure 9.1). For instance, when Zoom or Google Meet diplays a custom back-ground behind you in a video call, it’s using an image segmentation model totell your face apart from what’s behind it, at pixel precision.Object detection—Where the goal is to draw rectangles (called bounding boxes)around objects of interest in an image, and associate each rectangle with a class.A self-driving car could use an object-detection model to monitor cars, pedestri-ans, and signs in view of its cameras, for instance. Figure 9.1 The three main computer vision tasks: classification, segmentation, detection 240CHAPTER 9Advanced deep learning for computer visionDeep learning for computer vision also encompasses a number of somewhat moreniche tasks besides these three, such as image similarity scoring (estimating how visu-ally similar two images are), keypoint detection (pinpointing attributes of interest inan image, such as facial features), pose estimation, 3D mesh estimation, and so on.But to start with, image classification, image segmentation, and object detection formthe foundation that every machine learning engineer should be familiar with. Mostcomputer vision applications boil down to one of these three. You’ve seen image classification in action in the previous chapter. Next, let’s diveinto image segmentation. It’s a very useful and versatile technique, and you can straight-forwardly approach it with what you’ve already learned so far. Note that we won’t cover object detection, because it would be too specialized andtoo complicated for an introductory book. However, you can check out the RetinaNetexample on keras.io, which shows how to build and train an object detection modelfrom scratch in Keras in around 450 lines of code (https:/ /keras.io/examples/vision/retinanet/). 9.2 An image segmentation exampleImage segmentation with deep learning is about using a model to assign a class toeach pixel in an image, thus segmenting t h e i m a g e i n t o d i f f e r e n t z o n e s ( s u c h a s“background” and “foreground,” or “road,” “car,” and “sidewalk”). This general cat-egory of techniques can be used to power a considerable variety of valuable applica-tions in image and video editing, autonomous driving, robotics, medical imaging,and so on. There are two different flavors of image segmentation that you should know about:Semantic segmentation, where each pixel is independently classified into a seman-tic category, like “cat.” If there are two cats in the image, the corresponding pix-els are all mapped to the same generic “cat” category (see figure 9.2).Instance segmentation, which seeks not only to classify image pixels by category,but also to parse out individual object instances. In an image with two cats in it,instance segmentation would treat “cat 1” and “cat 2” as two separate classes ofpixels (see figure 9.2).In this example, we’ll focus on semantic segmentation: we’ll be looking once again atimages of cats and dogs, and this time we’ll learn how to tell apart the main subjectand its background. W e ’ l l w o r k w i t h t h e O x f o r d - I I I T P e t s d a t a s e t (www.robots.ox.ac.uk/~vgg/data/pets/), which contains 7,390 pictures of various breeds of cats and dogs, together withforeground-background segmentation masks for each picture. A segmentation mask isthe image-segmentation equivalent of a label: it’s an image the same size as the inputimage, with a single color channel where each integer value corresponds to the class 241An image segmentation example of the corresponding pixel in the input image. In our case, the pixels of our segmen-tation masks can take one of three integer values:1 (foreground)2 (background)3 (contour)Let’s start by downloading and uncompressing our dataset, using the wget and tarshell utilities:!wget http:/ /www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz!wget http:/ /www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz!tar -xf images.tar.gz!tar -xf annotations.tar.gzThe input pictures are stored as JPG files in the images/ folder (such as images/Abys-sinian_1.jpg), and the corresponding segmentation mask is stored as a PNG file withthe same name in the annotations/trimaps/ folder (such as annotations/trimaps/Abyssinian_1.png). Let’s prepare the list of input file paths, as well as the list of the correspondingmask file paths:import os input_dir = \"images/\" target_dir = \"annotations/trimaps/\" input_img_paths = sorted( [os.path.join(input_dir, fname) for fname in os.listdir(input_dir) if fname.endswith(\".jpg\")])Figure 9.2 Semantic segmentation vs. instance segmentation 242CHAPTER 9Advanced deep learning for computer visiontarget_paths = sorted( [os.path.join(target_dir, fname) for fname in os.listdir(target_dir) if fname.endswith(\".png\") and not fname.startswith(\".\")])Now, what does one of these inputs and its mask look like? Let’s take a quick look.Here’s a sample image (see figure 9.3):import matplotlib.pyplot as plt from tensorflow.keras.utils import load_img, img_to_array plt.axis(\"off\")plt.imshow(load_img(input_img_paths[9])) And here is its corresponding target (see figure 9.4):def display_target(target_array): normalized_array = (target_array.astype(\"uint8\") - 1) * 127 plt.axis(\"off\") plt.imshow(normalized_array[:, :, 0]) img = img_to_array(load_img(target_paths[9], color_mode=\"grayscale\")) display_target(img)Display input image number 9. Figure 9.3 An example image The original labels are 1, 2, and 3. We subtract 1 so that thelabels range from 0 to 2, and then we multiply by 127 so thatthe labels become 0 (black), 127 (gray), 254 (near-white).We use color_mode=\"grayscale\" sothat the image we load is treated ashaving a single color channel. 243An image segmentation example Next, let’s load our inputs and targets into two NumPy arrays, and let’s split the arraysinto a training and a validation set. Since the dataset is very small, we can just loadeverything into memory:import numpy as np import random img_size = (200, 200) num_imgs = len(input_img_paths) random.Random(1337).shuffle(input_img_paths) random.Random(1337).shuffle(target_paths) def path_to_input_image(path): return img_to_array(load_img(path, target_size=img_size)) def path_to_target(path): img = img_to_array( load_img(path, target_size=img_size, color_mode=\"grayscale\")) img = img.astype(\"uint8\") - 1 return img input_imgs = np.zeros((num_imgs,) + img_size + (3,), dtype=\"float32\") targets = np.zeros((num_imgs,) + img_size + (1,), dtype=\"uint8\") for i in range(num_imgs): input_imgs[i] = path_to_input_image(input_img_paths[i]) targets[i] = path_to_target(target_paths[i]) num_val_samples = 1000 train_input_imgs = input_imgs[:-num_val_samples] train_targets = targets[:-num_val_samples] val_input_imgs = input_imgs[-num_val_samples:] val_targets = targets[-num_val_samples:] Figure 9.4 The corresponding target mask We resize everything to 200 × 200.Total number of samples in the dataShuffle the file paths (they were originally sorted by breed). We use the same seed (1337) in both statements to ensure that the input paths and target paths stay in the same order.Subtract 1 so that our labels become 0, 1, and 2.Load all images in the input_imgsfloat32 array and their masks in thetargets uint8 array (same order). Theinputs have three channels (RBG values)and the targets have a single channel(which contains integer labels).Reserve1,000samples forvalidation.Split thedata into atraining and avalidation set. 244CHAPTER 9Advanced deep learning for computer visionNow it’s time to define our model:from tensorflow import keras from tensorflow.keras import layers def get_model(img_size, num_classes): inputs = keras.Input(shape=img_size + (3,)) x = layers.Rescaling(1./255)(inputs) x = layers.Conv2D(64, 3, strides=2, activation=\"relu\", padding=\"same\")(x) x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x) x = layers.Conv2D(128, 3, strides=2, activation=\"relu\", padding=\"same\")(x) x = layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\")(x) x = layers.Conv2D(256, 3, strides=2, padding=\"same\", activation=\"relu\")(x) x = layers.Conv2D(256, 3, activation=\"relu\", padding=\"same\")(x) x = layers.Conv2DTranspose(256, 3, activation=\"relu\", padding=\"same\")(x) x = layers.Conv2DTranspose( 256, 3, activation=\"relu\", padding=\"same\", strides=2)(x) x = layers.Conv2DTranspose(128, 3, activation=\"relu\", padding=\"same\")(x) x = layers.Conv2DTranspose( 128, 3, activation=\"relu\", padding=\"same\", strides=2)(x) x = layers.Conv2DTranspose(64, 3, activation=\"relu\", padding=\"same\")(x) x = layers.Conv2DTranspose( 64, 3, activation=\"relu\", padding=\"same\", strides=2)(x) outputs = layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(x) model = keras.Model(inputs, outputs) return model model = get_model(img_size=img_size, num_classes=3)model.summary()Here’s the output of the model.summary() call:Model: \"model\" _________________________________________________________________Layer (type) Output Shape Param # =================================================================input_1 (InputLayer) [(None, 200, 200, 3)] 0 _________________________________________________________________rescaling (Rescaling) (None, 200, 200, 3) 0 _________________________________________________________________conv2d (Conv2D) (None, 100, 100, 64) 1792 _________________________________________________________________conv2d_1 (Conv2D) (None, 100, 100, 64) 36928 _________________________________________________________________conv2d_2 (Conv2D) (None, 50, 50, 128) 73856 _________________________________________________________________conv2d_3 (Conv2D) (None, 50, 50, 128) 147584 _________________________________________________________________Don’t forget torescale inputimages to the[0-1] range.Note how we usepadding=\"same\"everywhere to avoidthe influence of borderpadding on featuremap size. We end the modelwith a per-pixel three-waysoftmax to classify eachoutput pixel into one ofour three categories. 245An image segmentation exampleconv2d_4 (Conv2D) (None, 25, 25, 256) 295168 _________________________________________________________________conv2d_5 (Conv2D) (None, 25, 25, 256) 590080 _________________________________________________________________conv2d_transpose (Conv2DTran (None, 25, 25, 256) 590080 _________________________________________________________________conv2d_transpose_1 (Conv2DTr (None, 50, 50, 256) 590080 _________________________________________________________________conv2d_transpose_2 (Conv2DTr (None, 50, 50, 128) 295040 _________________________________________________________________conv2d_transpose_3 (Conv2DTr (None, 100, 100, 128) 147584 _________________________________________________________________conv2d_transpose_4 (Conv2DTr (None, 100, 100, 64) 73792 _________________________________________________________________conv2d_transpose_5 (Conv2DTr (None, 200, 200, 64) 36928 _________________________________________________________________conv2d_6 (Conv2D) (None, 200, 200, 3) 1731 =================================================================Total params: 2,880,643 Trainable params: 2,880,643 Non-trainable params: 0 _________________________________________________________________The first half of the model closely resembles the kind of convnet you’d use for imageclassification: a stack of Conv2D layers, with gradually increasing filter sizes. We down-sample our images three times by a factor of two each, ending up with activations of size(25, 25, 256). The purpose of this first half is to encode the images into smaller featuremaps, where each spatial location (or pixel) contains information about a large spatialchunk of the original image. You can understand it as a kind of compression. One important difference between the first half of this model and the classifica-tion models you’ve seen before is the way we do downsampling: in the classificationconvnets from the last chapter, we used MaxPooling2D layers to downsample featuremaps. Here, we downsample by adding strides to every other convolution layer (if youdon’t remember the details of how convolution strides work, see “Understanding con-volution strides” in section 8.1.1). We do this because, in the case of image segmenta-tion, we care a lot about the spatial location of information in the image, since we needto produce per-pixel target masks as output of the model. When you do 2 × 2 maxpooling, you are completely destroying location information within each pooling win-dow: you return one scalar value per window, with zero knowledge of which of thefour locations in the windows the value came from. So while max pooling layers per-form well for classification tasks, they would hurt us quite a bit for a segmentationtask. Meanwhile, strided convolutions do a better job at downsampling feature mapswhile retaining location information. Throughout this book, you’ll notice that wetend to use strides instead of max pooling in any model that cares about feature loca-tion, such as the generative models in chapter 12. The second half of the model is a stack of Conv2DTranspose layers. What are those?Well, the output of the first half of the model is a feature map of shape (25, 25, 256), 246CHAPTER 9Advanced deep learning for computer visionbut we want our final output to have the same shape as the target masks, (200, 200,3). Therefore, we need to apply a kind of inverse of the transformations we’ve appliedso far—something that will upsample the feature maps instead of downsampling them.That’s the purpose of the Conv2DTranspose layer: you can think of it as a kind of convolu-tion layer that learns to upsample. If you have an input of shape (100, 100, 64), and yourun it through the layer Conv2D(128, 3, strides=2, padding=\"same\"), you get anoutput of shape (50, 50, 128). If you run this output through the layer Conv2D-Transpose(64, 3, strides=2, padding=\"same\"), you get back an output of shape (100,100, 64), the same as the original. So after compressing our inputs into feature maps ofshape (25, 25, 256) via a stack of Conv2D layers, we can simply apply the correspondingsequence of Conv2DTranspose layers to get back to images of shape (200, 200, 3). We can now compile and fit our model:model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\") callbacks = [ keras.callbacks.ModelCheckpoint(\"oxford_segmentation.keras\", save_best_only=True)] history = model.fit(train_input_imgs, train_targets, epochs=50, callbacks=callbacks, batch_size=64, validation_data=(val_input_imgs, val_targets))Let’s display our training and validation loss (see figure 9.5):epochs = range(1, len(history.history[\"loss\"]) + 1)loss = history.history[\"loss\"] Figure 9.5 Displaying training and validation loss curves 247An image segmentation exampleval_loss = history.history[\"val_loss\"]plt.figure()plt.plot(epochs, loss, \"bo\", label=\"Training loss\")plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")plt.title(\"Training and validation loss\")plt.legend()You can see that we start overfitting midway, around epoch 25. Let’s reload our bestperforming model according to the validation loss, and demonstrate how to use it topredict a segmentation mask (see figure 9.6):from tensorflow.keras.utils import array_to_img model = keras.models.load_model(\"oxford_segmentation.keras\") i = 4 test_image = val_input_imgs[i]plt.axis(\"off\")plt.imshow(array_to_img(test_image)) mask = model.predict(np.expand_dims(test_image, 0))[0] def display_mask(pred): mask = np.argmax(pred, axis=-1) mask *= 127 plt.axis(\"off\") plt.imshow(mask) display_mask(mask)Utility to display a model’s prediction Figure 9.6 A test image and its predicted segmentation mask 248CHAPTER 9Advanced deep learning for computer visionThere are a couple of small artifacts in our predicted mask, caused by geometric shapesin the foreground and background. Nevertheless, our model appears to work nicely. B y t h i s p o i n t , t h r o u g h o u t c h a p t e r 8 a n d t h e b e g i n n i n g o f c h a p t e r 9 , y o u ’ v elearned the basics of how to perform image classification and image segmentation:you can already accomplish a lot with what you know. However, the convnets thatexperienced engineers develop to solve real-world problems aren’t quite as simple asthose we’ve been using in our demonstrations so far. You’re still lacking the essentialmental models and thought processes that enable experts to make quick and accuratedecisions about how to put together state-of-the-art models. To bridge that gap, youneed to learn about architecture patterns. Let’s dive in. 9.3 Modern convnet architecture patternsA model’s “architecture” is the sum of the choices that went into creating it: which lay-ers to use, how to configure them, and in what arrangement to connect them. Thesechoices define the hypothesis space of your model: the space of possible functions thatgradient descent can search over, parameterized by the model’s weights. Like featureengineering, a good hypothesis space encodes prior knowledge that you have about theproblem at hand and its solution. For instance, using convolution layers means that youknow in advance that the relevant patterns present in your input images are translation-invariant. In order to effectively learn from data, you need to make assumptions aboutwhat you’re looking for. Model architecture is often the difference between success and failure. If you makeinappropriate architecture choices, your model may be stuck with suboptimal metrics,and no amount of training data will save it. Inversely, a good model architecture willaccelerate learning and will enable your model to make efficient use of the training dataavailable, reducing the need for large datasets. A good model architecture is one thatreduces the size of the search space or otherwise makes it easier to converge to a good point of thesearch space. Just like feature engineering and data curation, model architecture is allabout making the problem simpler for gradient descent to solve. And remember that gradi-ent descent is a pretty stupid search process, so it needs all the help it can get. Model architecture is more an art than a science. Experienced machine learningengineers are able to intuitively cobble together high-performing models on their firsttry, while beginners often struggle to create a model that trains at all. The keywordhere is intuitively: no one can give you a clear explanation of what works and whatdoesn’t. Experts rely on pattern-matching, an ability that they acquire through exten-sive practical experience. You’ll develop your own intuition throughout this book.However, it’s not all about intuition either—there isn’t much in the way of actual sci-ence, but as in any engineering discipline, there are best practices. I n t h e f o l l o w i n g s e c t i o n s , w e ’ l l r e v i e w a f e w e s s e n t i a l c o n v n e t a r c h i t e c t u r e b e s tpractices: in particular, residual connections, batch normalization, and separable convolu-tions. Once you master how to use them, you will be able to build highly effectiveimage models. We will apply them to our cat vs. dog classification problem. 249Modern convnet architecture patterns Let’s start from the bird’s-eye view: the modularity-hierarchy-reuse (MHR) formulafor system architecture.9.3.1 Modularity, hierarchy, and reuseIf you want to make a complex system simpler, there’s a universal recipe you canapply: just structure your amorphous soup of complexity into modules, organize themodules into a hierarchy, and start reusing t h e s a m e m o d u l e s i n m u l t i p l e p l a c e s a sappropriate (“reuse” is another word for abstraction in this context). That’s the MHRformula (modularity-hierarchy-reuse), and it underlies system architecture acrosspretty much every domain where the term “architecture” is used. It’s at the heart ofthe organization of any system of meaningful complexity, whether it’s a cathedral,your own body, the US Navy, or the Keras codebase (see figure 9.7). If you’re a software engineer, you’re already keenly familiar with these principles: aneffective codebase is one that is modular, hierarchical, and where you don’t reimple-ment the same thing twice, but instead rely on reusable classes and functions. If youFigure 9.7 Complex systems follow a hierarchical structure and are organized into distinct modules, which are reused multiple times (such as your four limbs, which are all variants of the same blueprint, or your 20 “fingers”). 250CHAPTER 9Advanced deep learning for computer visionfactor your code by following these principles, you could say you’re doing “softwarearchitecture.” Deep learning itself is simply the application of this recipe to continuous optimiza-tion via gradient descent: you take a classic optimization technique (gradient descentover a continuous function space), and you structure the search space into modules(layers), organized into a deep hierarchy (often just a stack, the simplest kind of hier-archy), where you reuse whatever you can (for instance, convolutions are all aboutreusing the same information in different spatial locations). Likewise, deep learning model architecture is primarily about making clever use ofmodularity, hierarchy, and reuse. You’ll notice that all popular convnet architecturesare not only structured into layers, they’re structured into repeated groups of layers(called “blocks” or “modules”). For instance, the popular VGG16 architecture we usedin the previous chapter is structured into repeated “conv, conv, max pooling” blocks(see figure 9.8). F u r t h e r , m o s t c o n v n e t s o f t e n f e atu re py ramid-l ik e str uctur es (feature hierarchies).Recall, for example, the progression in the number of convolution filters we used inthe first convnet we built in the previous chapter: 32, 64, 128. The number of filtersgrows with layer depth, while the size of the feature maps shrinks accordingly. You’llnotice the same pattern in the blocks of the VGG16 model (see figure 9.8). 224 × 224 × 3 28 × 28 × 51214 × 14 × 5127 × 7 × 5121 × 1 × 4096Convolution+ReLUMax poolingFully connected+ReLUSoftmax1 × 1 × 1000112 × 112 × 12856 × 56 × 256224 × 224 × 64 Figure 9.8 The VGG16 architecture: note the repeated layer blocks and the pyramid-like structure of the feature maps 251Modern convnet architecture patternsDeeper hierarchies are intrinsically good because they encourage feature reuse, andtherefore abstraction. In general, a deep stack of narrow layers performs better than ashallow stack of large layers. However, there’s a limit to how deep you can stack layers,due to the problem of vanishing gradients. This leads us to our first essential modelarchitecture pattern: residual connections. 9.3.2 Residual connectionsYou probably know about the game of Telephone, also called Chinese whispers in theUK and téléphone arabe in France, where an initial message is whispered in the ear of aplayer, who then whispers it in the ear of the next player, and so on. The final messageends up bearing little resemblance to its original version. It’s a fun metaphor for thecumulative errors that occur in sequential transmission over a noisy channel. As it happens, backpropagation in a sequential deep learning model is pretty simi-lar to the game of Telephone. You’ve got a chain of functions, like this one:y = f4(f3(f2(f1(x))))On the importance of ablation studies in deep learning researchDeep learning architectures are often more evolved than designed—they were devel-oped by repeatedly trying things and selecting what seemed to work. Much like in bio-logical systems, if you take any complicated experimental deep learning setup,chances are you can remove a few modules (or replace some trained features withrandom ones) with no loss of performance.This is made worse by the incentives that deep learning researchers face: by makinga system more complex than necessary, they can make it appear more interesting ormore novel, and thus increase their chances of getting a paper through the peer-review process. If you read lots of deep learning papers, you will notice that they’reoften optimized for peer review in both style and content in ways that actively hurtclarity of explanation and reliability of results. For instance, mathematics in deeplearning papers is rarely used for clearly formalizing concepts or deriving non-obviousresults—rather, it gets leveraged as a signal of seriousness, like an expensive suit ona salesman.The goal of research shouldn’t be merely to publish, but to generate reliable knowl-edge. Crucially, understanding causality in your system is the most straightforwardway to generate reliable knowledge. And there’s a very low-effort way to look into cau-sality: ablation studies. Ablation studies consist of systematically trying to removeparts of a system—making it simpler—to identify where its performance actuallycomes from. If you find that X + Y + Z gives you good results, also try X, Y, Z, X + Y,X + Z, and Y + Z, and see what happens.If you become a deep learning researcher, cut through the noise in the research pro-cess: do ablation studies for your models. Always ask, “Could there be a simplerexplanation? Is this added complexity really necessary? Why?” 252CHAPTER 9Advanced deep learning for computer visionThe name of the game is to adjust the parameters of each function in the chain basedon the error recorded on the output of f4 (the loss of the model). To adjust f1, you’llneed to percolate error information through f2, f3, and f4. However, each successivefunction in the chain introduces some amount of noise. If your function chain is toodeep, this noise starts overwhelming gradient information, and backpropagationstops working. Your model won’t train at all. This is the vanishing gradients problem. The fix is simple: just force each function in the chain to be nondestructive—toretain a noiseless version of the information contained in the previous input. The eas-iest way to implement this is to use a residual connection. It’s dead easy: just add theinput of a layer or block of layers back to its output (see figure 9.9). The residual con-nection acts as an information shortcut a r o u n d d e s t r u c t i v e o r n o i s y b l o c k s ( s u c h a sblocks that contain relu activations or dropout layers), enabling error gradient infor-mation from early layers to propagate noiselessly through a deep network. This tech-nique was introduced in 2015 with the ResNet family of models (developed by He et al.at Microsoft).1 In practice, you’d implement a residual connection as follows.x=. . . residual = x x=b l o c k ( x ) x=a d d ( [ x ,r e s i d u a l ] ) 1Kaiming He et al., “Deep Residual Learning for Image Recognition,” Conference on Computer Vision and Pat-tern Recognition (2015), https:/ /arxiv.org/abs/1512.03385.Listing 9.1 A residual connection in pseudocodeOutputResidualconnection+BlockInput Figure 9.9 A residual connection around a processing blockSome inputtensorSave a pointer to the original input. This is called the residual.This computation block can potentially be destructive or noisy, and that’s fine.Add the original input to the layer’soutput: the final output will thusalways preserve full informationabout the original input. 253Modern convnet architecture patternsNote that adding the input back to the output of a block implies that the outputshould have the same shape as the input. However, this is not the case if your blockincludes convolutional layers with an increased number of filters, or a max poolinglayer. In such cases, use a 1 × 1 Conv2D layer with no activation to linearly project theresidual to the desired output shape (see listing 9.2). You’d typically use padding=\"same\" in the convolution layers in your target block so as to avoid spatial downsam-pling due to padding, and you’d use strides in the residual projection to match anydownsampling caused by a max pooling layer (see listing 9.3).from tensorflow import keras from tensorflow.keras import layers inputs = keras.Input(shape=(32, 32, 3))x = layers.Conv2D(32, 3, activation=\"relu\")(inputs)residual = x x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x) residual = layers.Conv2D(64, 1)(residual) x=l a y e r s . a d d ( [ x ,r e s i d u a l ] ) inputs = keras.Input(shape=(32, 32, 3))x = layers.Conv2D(32, 3, activation=\"relu\")(inputs)residual = x x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x) x = layers.MaxPooling2D(2, padding=\"same\")(x) residual = layers.Conv2D(64, 1, strides=2)(residual) x=l a y e r s . a d d ( [ x ,r e s i d u a l ] ) To make these ideas more concrete, here’s an example of a simple convnet structuredinto a series of blocks, each made of two convolution layers and one optional maxpooling layer, with a residual connection around each block:inputs = keras.Input(shape=(32, 32, 3))x = layers.Rescaling(1./255)(inputs) def residual_block(x, filters, pooling=False): residual = x x = layers.Conv2D(filters, 3, activation=\"relu\", padding=\"same\")(x) x = layers.Conv2D(filters, 3, activation=\"relu\", padding=\"same\")(x)Listing 9.2 Residual block where the number of filters changes Listing 9.3 Case where the target block includes a max pooling layerSetaside theresidual.This is the layer around which we createa residual connection: it increases thenumber of output filers from 32 to 64.Note that we use padding=\"same\"to avoid downsamplingdue to padding.The residual only had 32 filters, so we use a 1 × 1 Conv2D to project it to the correct shape.Now the block output and theresidual have the same shapeand can be added.Setaside theresidual.This is the block of two layers around whichwe create a residual connection: it includes a2 × 2 max pooling layer. Note that we usepadding=\"same\" in both the convolutionlayer and the max pooling layer to avoiddownsampling due to padding.We use strides=2 in the residual projection to match the downsampling created by the max pooling layer.Now the block output and the residual have the same shape and can be added. Utility function to apply a convolutional block with a residual connection, with an option to add max pooling 254CHAPTER 9Advanced deep learning for computer vision if pooling: x = layers.MaxPooling2D(2, padding=\"same\")(x) residual = layers.Conv2D(filters, 1, strides=2)(residual) elif filters != residual.shape[-1]: residual = layers.Conv2D(filters, 1)(residual) x = layers.add([x, residual]) return x x = residual_block(x, filters=32, pooling=True) x = residual_block(x, filters=64, pooling=True) x=r e s i d u a l _ b l o c k ( x ,f i l t e r s = 1 2 8 ,p o o l i n g =False) x = layers.GlobalAveragePooling2D()(x)outputs = layers.Dense(1, activation=\"sigmoid\")(x)model = keras.Model(inputs=inputs, outputs=outputs)model.summary()This is the model summary we get:Model: \"model\" __________________________________________________________________________________________________Layer (type) Output Shape Param # Connected to ==================================================================================================input_1 (InputLayer) [(None, 32, 32, 3)] 0 __________________________________________________________________________________________________rescaling (Rescaling) (None, 32, 32, 3) 0 input_1[0][0] __________________________________________________________________________________________________conv2d (Conv2D) (None, 32, 32, 32) 896 rescaling[0][0] __________________________________________________________________________________________________conv2d_1 (Conv2D) (None, 32, 32, 32) 9248 conv2d[0][0]__________________________________________________________________________________________________max_pooling2d (MaxPooling2D) (None, 16, 16, 32) 0 conv2d_1[0][0]__________________________________________________________________________________________________conv2d_2 (Conv2D) (None, 16, 16, 32) 128 rescaling[0][0]__________________________________________________________________________________________________add (Add) (None, 16, 16, 32) 0 max_pooling2d[0][0] conv2d_2[0][0]__________________________________________________________________________________________________conv2d_3 (Conv2D) (None, 16, 16, 64) 18496 add[0][0]__________________________________________________________________________________________________conv2d_4 (Conv2D) (None, 16, 16, 64) 36928 conv2d_3[0][0]__________________________________________________________________________________________________max_pooling2d_1 (MaxPooling2D) (None, 8, 8, 64) 0 conv2d_4[0][0]__________________________________________________________________________________________________conv2d_5 (Conv2D) (None, 8, 8, 64) 2112 add[0][0]__________________________________________________________________________________________________add_1 (Add) (None, 8, 8, 64) 0 max_pooling2d_1[0][0] conv2d_5[0][0]__________________________________________________________________________________________________conv2d_6 (Conv2D) (None, 8, 8, 128) 73856 add_1[0][0]__________________________________________________________________________________________________conv2d_7 (Conv2D) (None, 8, 8, 128) 147584 conv2d_6[0][0]__________________________________________________________________________________________________If we use max pooling, we add a strided convolution to project the residual to the expected shape.If we don’t use max pooling, we only project the residual if the number of channels has changed.FirstblockSecond block; note the increasing filter count in each block.The last block doesn’t need a maxpooling layer, since we will applyglobal average pooling right after it. 255Modern convnet architecture patternsconv2d_8 (Conv2D) (None, 8, 8, 128) 8320 add_1[0][0]__________________________________________________________________________________________________add_2 (Add) (None, 8, 8, 128) 0 conv2d_7[0][0] conv2d_8[0][0]__________________________________________________________________________________________________global_average_pooling2d (Globa (None, 128) 0 add_2[0][0]__________________________________________________________________________________________________dense (Dense) (None, 1) 129 global_average_pooling2d[0][0]==================================================================================================Total params: 297,697 Trainable params: 297,697 Non-trainable params: 0 __________________________________________________________________________________________________With residual connections, you can build networks of arbitrary depth, without havingto worry about vanishing gradients. Now let’s move on to the next essential convnet architecture pattern: batch normal-ization. 9.3.3 Batch normalizationNormalization is a broad category of methods that seek to make different samples seenby a machine learning model more similar to each other, which helps the model learnand generalize well to new data. The most common form of data normalization is oneyou’ve already seen several times in this book: centering the data on zero by subtract-ing the mean from the data, and giving the data a unit standard deviation by dividingthe data by its standard deviation. In effect, this makes the assumption that the datafollows a normal (or Gaussian) distribution and makes sure this distribution is cen-tered and scaled to unit variance:normalized_data = (data - np.mean(data, axis=...)) / np.std(data, axis=...)Previous examples in this book normalized data before feeding it into models. Butdata normalization may be of interest after every transformation operated by the net-work: even if the data entering a Dense or Conv2D network has a 0 mean and unit vari-ance, there’s no reason to expect a priori that this will be the case for the data comingout. Could normalizing intermediate activations help? B a t c h n o r m a l i z a t i o n d o e s j u s t t h a t . I t ’ s a t y p e o f l a y e r (BatchNormalization i nKeras) introduced in 2015 by Ioffe and Szegedy;2 it can adaptively normalize data evenas the mean and variance change over time during training. During training, it usesthe mean and variance of the current batch of data to normalize samples, and duringinference (when a big enough batch of representative data may not be available), ituses an exponential moving average of the batch-wise mean and variance of the dataseen during training.2Sergey Ioffe and Christian Szegedy, “Batch Normalization: Accelerating Deep Network Training by ReducingInternal Covariate Shift,” Proceedings of the 32nd International Conference on Machine Learning (2015), https:/ /arxiv.org/abs/1502.03167. 256CHAPTER 9Advanced deep learning for computer vision Although the original paper stated that batch normalization operates by “reducinginternal covariate shift,” no one really knows for sure why batch normalization helps.There are various hypotheses, but no certitudes. You’ll find that this is true of manythings in deep learning—deep learning is not an exact science, but a set of ever-changing, empirically derived engineering best practices, woven together by unreli-able narratives. You will sometimes feel like the book you have in hand tells you how todo something but doesn’t quite satisfactorily say why it works: that’s because we knowthe how but we don’t know the why. Whenever a reliable explanation is available, Imake sure to mention it. Batch normalization isn’t one of those cases. In practice, the main effect of batch normalization appears to be that it helps withgradient propagation—much like residual connections—and thus allows for deepernetworks. Some very deep networks can only be trained if they include multipleBatchNormalization l a y e r s . F o r i n s t a n c e , b a t c h n o r m a l i z a t i o n i s u s e d l i b e r a l l y i nmany of the advanced convnet architectures that come packaged with Keras, such asResNet50, EfficientNet, and Xception. The BatchNormalization layer can be used after any layer—Dense, Conv2D, etc.:x = ...x = layers.Conv2D(32, 3, use_bias=False)(x) x=l a y e r s . B a t c h N o r m a l i z a t i o n ( ) ( x )NOTEBoth Dense and Conv2D involve a bias vector, a learned variable whosepurpose is to make the layer affine rather than purely linear. For instance,Conv2D returns, schematically, y = conv(x, kernel) + bias, and Dense returnsy = dot(x, kernel) + bias. Because the normalization step will take care ofcentering the layer’s output on zero, the bias vector is no longer neededwhen using BatchNormalization, and the layer can be created without it viathe option use_bias=False. This makes the layer slightly leaner.Importantly, I would generally recommend placing the previous layer’s activation afterthe batch normalization layer (although this is still a subject of debate). So instead ofdoing what is shown in listing 9.4, you would do what’s shown in listing 9.5.x = layers.Conv2D(32, 3, activation=\"relu\")(x)x = layers.BatchNormalization()(x)x = layers.Conv2D(32, 3, use_bias=False)(x) x = layers.BatchNormalization()(x)x = layers.Activation(\"relu\")(x) Listing 9.4 How not to use batch normalizationListing 9.5 How to use batch normalization: the activation comes lastBecause the output of the Conv2D layer gets normalized, the layer doesn’t need its own bias vector. Note the lack of activation here.We place the activation after theBatchNormalization layer. 257Modern convnet architecture patternsThe intuitive reason for this approach is that batch normalization will center yourinputs on zero, while your relu activation uses zero as a pivot for keeping or droppingactivated channels: doing normalization before the activation maximizes the utiliza-tion of the relu. That said, this ordering best practice is not exactly critical, so if youdo convolution, then activation, and then batch normalization, your model will stilltrain, and you won’t necessarily see worse results. Now let’s take a look at the last architecture pattern in our series: depthwise separableconvolutions. 9.3.4 Depthwise separable convolutionsWhat if I told you that there’s a layer you can use as a drop-in replacement for Conv2Dthat will make your model smaller (fewer trainable weight parameters) and leaner(fewer floating-point operations) and cause it to perform a few percentage points bet-ter on its task? That is precisely what the depthwise separable convolution layer does (Sep-arableConv2D in Keras). This layer performs a spatial convolution on each channel ofits input, independently, before mixing output channels via a pointwise convolution(a 1 × 1 convolution), as shown in figure 9.10.On batch normalization and fine-tuningBatch normalization has many quirks. One of the main ones relates to fine-tuning:when fine-tuning a model that includes BatchNormalization layers, I recommendleaving these layers frozen (set their trainable attribute to False). Otherwise theywill keep updating their internal mean and variance, which can interfere with the verysmall updates applied to the surrounding Conv2D layers. 1 × 1 conv(pointwise conv)Depthwise convolution:independent spatialconvs per channelConcatenate Split channels3 × 3 conv3 × 3 conv3 × 3 conv3 × 3 conv Figure 9.10 Depthwise separable convolution: a depthwise convolution followed by a pointwise convolution 258CHAPTER 9Advanced deep learning for computer visionThis is equivalent to separating the learning of spatial features and the learning ofchannel-wise features. In much the same way that convolution relies on the assump-tion that the patterns in images are not tied to specific locations, depthwise separableconvolution relies on the assumption that spatial locations in intermediate activationsare highly correlated, but different channels are highly independent. Because this assumptionis generally true for the image representations learned by deep neural networks, itserves as a useful prior that helps the model make more efficient use of its trainingdata. A model with stronger priors about the structure of the information it will haveto process is a better model—as long as the priors are accurate. D e p t h w i s e s e p a r a b l e c o n v o l u t i o n r e q u i r e s s i g n i f i c a n t l y f e w e r p a r a m e t e r s a n dinvolves fewer computations compared to regular convolution, while having compara-ble representational power. It results in smaller models that converge faster and areless prone to overfitting. These advantages become especially important when you’retraining small models from scratch on limited data. When it comes to larger-scale models, depthwise separable convolutions are thebasis of the Xception architecture, a high-performing convnet that comes packagedwith Keras. You can read more about the theoretical grounding for depthwise separa-ble convolutions and Xception in the paper “Xception: Deep Learning with Depth-wise Separable Convolutions.”3 3François Chollet, “Xception: Deep Learning with Depthwise Separable Convolutions,” Conference on Com-puter Vision and Pattern Recognition (2017), https:/ /arxiv.org/abs/1610.02357.The co-evolution of hardware, software, and algorithmsConsider a regular convolution operation with a 3 × 3 window, 64 input channels, and64 output channels. It uses 3*3*64*64 = 36,864 trainable parameters, and whenyou apply it to an image, it runs a number of floating-point operations that is propor-tional to this parameter count. Meanwhile, consider an equivalent depthwise separa-ble convolution: it only involves 3*3*64 + 64*64 = 4,672 trainable parameters, andproportionally fewer floating-point operations. This efficiency improvement onlyincreases as the number of filters or the size of the convolution windows gets larger.As a result, you would expect depthwise separable convolutions to be dramaticallyfaster, right? Hold on. This would be true if you were writing simple CUDA or C imple-mentations of these algorithms—in fact, you do see a meaningful speedup when run-ning on CPU, where the underlying implementation is parallelized C. But in practice,you’re probably using a GPU, and what you’re executing on it is far from a “simple”CUDA implementation: it’s a cuDNN kernel, a piece of code that has been extraordi-narily optimized, down to each machine instruction. It certainly makes sense tospend a lot of effort optimizing this code, since cuDNN convolutions on NVIDIA hard-ware are responsible for many exaFLOPS of computation every day. But a side effectof this extreme micro-optimization is that alternative approaches have little chanceto compete on performance—even approaches that have significant intrinsic advan-tages, like depthwise separable convolutions. 259Modern convnet architecture patterns 9.3.5 Putting it together: A mini Xception-like modelAs a reminder, here are the convnet architecture principles you’ve learned so far:Your model should be organized into repeated blocks of layers, usually made ofmultiple convolution layers and a max pooling layer.The number of filters in your layers should increase as the size of the spatial fea-ture maps decreases.Deep and narrow is better than broad and shallow.Introducing residual connections around blocks of layers helps you traindeeper networks.It can be beneficial to introduce batch normalization layers after your convolu-tion layers.It can be beneficial to replace Conv2D layers with SeparableConv2D l a y e r s ,which are more parameter-efficient.Let’s bring these ideas together into a single model. Its architecture will resemble asmaller version of Xception, and we’ll apply it to the dogs vs. cats task from the lastchapter. For data loading and model training, we’ll simply reuse the setup we used insection 8.2.5, but we’ll replace the model definition with the following convnet:Despite repeated requests to NVIDIA, depthwise separable convolutions have notbenefited from nearly the same level of software and hardware optimization as regu-lar convolutions, and as a result they remain only about as fast as regular convolu-tions, even though they’re using quadratically fewer parameters and floating-pointoperations. Note, though, that using depthwise separable convolutions remains agood idea even if it does not result in a speedup: their lower parameter count meansthat you are less at risk of overfitting, and their assumption that channels should beuncorrelated leads to faster model convergence and more robust representations.What is a slight inconvenience in this case can become an impassable wall in othersituations: because the entire hardware and software ecosystem of deep learninghas been micro-optimized for a very specific set of algorithms (in particular, convnetstrained via backpropagation), there’s an extremely high cost to steering away from thebeaten path. If you were to experiment with alternative algorithms, such as gradient-free optimization or spiking neural networks, the first few parallel C++ or CUDAimplementations you’d come up with would be orders of magnitude slower than agood old convnet, no matter how clever and efficient your ideas were. Convincingother researchers to adopt your method would be a tough sell, even if it were justplain better.You could say that modern deep learning is the product of a co-evolution processbetween hardware, software, and algorithms: the availability of NVIDIA GPUs andCUDA led to the early success of backpropagation-trained convnets, which led NVIDIAto optimize its hardware and software for these algorithms, which in turn led to con-solidation of the research community behind these methods. At this point, figuringout a different path would require a multi-year re-engineering of the entire ecosystem. 260CHAPTER 9Advanced deep learning for computer visioninputs = keras.Input(shape=(180, 180, 3))x = data_augmentation(inputs) x = layers.Rescaling(1./255)(x) x = layers.Conv2D(filters=32, kernel_size=5, use_bias=False)(x) for size in [32, 64, 128, 256, 512]: residual = x x = layers.BatchNormalization()(x) x = layers.Activation(\"relu\")(x) x = layers.SeparableConv2D(size, 3, padding=\"same\", use_bias=False)(x) x = layers.BatchNormalization()(x) x = layers.Activation(\"relu\")(x) x = layers.SeparableConv2D(size, 3, padding=\"same\", use_bias=False)(x) x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x) residual = layers.Conv2D( size, 1, strides=2, padding=\"same\", use_bias=False)(residual) x = layers.add([x, residual]) x = layers.GlobalAveragePooling2D()(x) x = layers.Dropout(0.5)(x) outputs = layers.Dense(1, activation=\"sigmoid\")(x)model = keras.Model(inputs=inputs, outputs=outputs) This convnet has a trainable parameter count of 721,857, slightly lower than the991,041 trainable parameters of the original model, but still in the same ballpark. Fig-ure 9.11 shows its training and validation curves.We use the same data augmentation configuration as before.Don’tforgetinputrescaling! Note that the assumption that underliesseparable convolution, “feature channels arelargely independent,” does not hold for RGBimages! Red, green, and blue color channelsare actually highly correlated in naturalimages. As such, the first layer in our modelis a regular Conv2D layer. We’ll start usingSeparableConv2D afterwards.We apply a series of convolutional blocks with increasing feature depth. Each block consists of two batch-normalized depthwise separable convolution layers and a max pooling layer, with a residual connection around the entire block.In the original model, we used a Flatten layer before the Dense layer. Here, we go with a GlobalAveragePooling2D layer.Like in the original model, we add a dropout layer for regularization. Figure 9.11 Training and validation metrics with an Xception-like architecture 261Interpreting what convnets learnYou’ll find that our new model achieves a test accuracy of 90.8%, compared to 83.5%for the naive model in the last chapter. As you can see, following architecture bestpractices does have an immediate, sizable impact on model performance! At this point, if you want to further improve performance, you should start system-atically tuning the hyperparameters of your architecture—a topic we’ll cover in detailin chapter 13. We haven’t gone through this step here, so the configuration of the pre-ceding model is purely based on the best practices we discussed, plus, when it comesto gauging model size, a small amount of intuition. Note that these architecture best practices are relevant to computer vision in gen-eral, not just image classification. For example, Xception is used as the standard convo-lutional base in DeepLabV3, a popular state-of-the-art image segmentation solution.4 This concludes our introduction to essential convnet architecture best practices.With these principles in hand, you’ll be able to develop higher-performing modelsacross a wide range of computer vision tasks. You’re now well on your way to becom-ing a proficient computer vision practitioner. To further deepen your expertise,there’s one last important topic we need to cover: interpreting how a model arrives atits predictions. 9.4 Interpreting what convnets learnA fundamental problem when building a computer vision application is that of inter-pretability: why did your classifier think a particular image contained a fridge, when allyou can see is a truck? This is especially relevant to use cases where deep learning isused to complement human expertise, such as in medical imaging use cases. We willend this chapter by getting you familiar with a range of different techniques for visual-izing what convnets learn and understanding the decisions they make. It’s often said that deep learning models are “black boxes”: they learn representa-tions that are difficult to extract and present in a human-readable form. Although thisis partially true for certain types of deep learning models, it’s definitely not true forconvnets. The representations learned by convnets are highly amenable to visualiza-tion, in large part because they’re representations of visual concepts. Since 2013, a widearray of techniques has been developed for visualizing and interpreting these repre-sentations. We won’t survey all of them, but we’ll cover three of the most accessibleand useful ones:Visualizing intermediate convnet outputs (intermediate activations)—Useful for under-standing how successive convnet layers transform their input, and for getting afirst idea of the meaning of individual convnet filtersVisualizing convnet filters—Useful for understanding precisely what visual patternor concept each filter in a convnet is receptive to4Liang-Chieh Chen et al., “Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmen-tation,” ECCV (2018), https:/ /arxiv.org/abs/1802.02611. 262CHAPTER 9Advanced deep learning for computer visionVisualizing heatmaps of class activation in an image—Useful for understandingwhich parts of an image were identified as belonging to a given class, thus allow-ing you to localize objects in imagesFor the first method—activation visualization—we’ll use the small convnet that wetrained from scratch on the dogs-versus-cats classification problem in section 8.2. Forthe next two methods, we’ll use a pretrained Xception model.9.4.1 Visualizing intermediate activationsVisualizing intermediate activations consists of displaying the values returned by variousconvolution and pooling layers in a model, given a certain input (the output of a layer isoften called its activation, the output of the activation function). This gives a view intohow an input is decomposed into the different filters learned by the network. We wantto visualize feature maps with three dimensions: width, height, and depth (channels).Each channel encodes relatively independent features, so the proper way to visualizethese feature maps is by independently plotting the contents of every channel as a 2Dimage. Let’s start by loading the model that you saved in section 8.2:>>> from tensorflow import keras>>> model = keras.models.load_model( \"convnet_from_scratch_with_augmentation.keras\")>>> model.summary()Model: \"model_1\" _________________________________________________________________Layer (type) Output Shape Param # =================================================================input_2 (InputLayer) [(None, 180, 180, 3)] 0 _________________________________________________________________sequential (Sequential) (None, 180, 180, 3) 0 _________________________________________________________________ rescaling_1 (Rescaling) (None, 180, 180, 3) 0 _________________________________________________________________conv2d_5 (Conv2D) (None, 178, 178, 32) 896 _________________________________________________________________max_pooling2d_4 (MaxPooling2 (None, 89, 89, 32) 0 _________________________________________________________________conv2d_6 (Conv2D) (None, 87, 87, 64) 18496 _________________________________________________________________max_pooling2d_5 (MaxPooling2 (None, 43, 43, 64) 0 _________________________________________________________________conv2d_7 (Conv2D) (None, 41, 41, 128) 73856 _________________________________________________________________max_pooling2d_6 (MaxPooling2 (None, 20, 20, 128) 0 _________________________________________________________________conv2d_8 (Conv2D) (None, 18, 18, 256) 295168 _________________________________________________________________max_pooling2d_7 (MaxPooling2 (None, 9, 9, 256) 0 _________________________________________________________________ 263Interpreting what convnets learnconv2d_9 (Conv2D) (None, 7, 7, 256) 590080 _________________________________________________________________flatten_1 (Flatten) (None, 12544) 0 _________________________________________________________________dropout (Dropout) (None, 12544) 0 _________________________________________________________________dense_1 (Dense) (None, 1) 12545 =================================================================Total params: 991,041 Trainable params: 991,041 Non-trainable params: 0 _________________________________________________________________Next, we’ll get an input image—a picture of a cat, not part of the images the networkwas trained on.from tensorflow import keras import numpy as np img_path = keras.utils.get_file( fname=\"cat.jpg\", origin=\"https://img-datasets.s3.amazonaws.com/cat.jpg\") def get_img_array(img_path, target_size): img = keras.utils.load_img( img_path, target_size=target_size) array = keras.utils.img_to_array(img) array = np.expand_dims(array, axis=0) return array img_tensor = get_img_array(img_path, target_size=(180, 180))Let’s display the picture (see figure 9.12).import matplotlib.pyplot as pltplt.axis(\"off\")plt.imshow(img_tensor[0].astype(\"uint8\"))plt.show()In order to extract the feature maps we want to look at, we’ll create a Keras model thattakes batches of images as input, and that outputs the activations of all convolutionand pooling layers. Listing 9.6 Preprocessing a single image Listing 9.7 Displaying the test pictureDownload a test image.Open the image file and resize it.Turn the image into a float32 NumPy array of shape (180, 180, 3).Add a dimension to transform the array into a “batch” of a single sample. Its shape is now (1, 180, 180, 3). 264CHAPTER 9Advanced deep learning for computer vision from tensorflow.keras import layers layer_outputs = []layer_names = [] for layer in model.layers: if isinstance(layer, (layers.Conv2D, layers.MaxPooling2D)): layer_outputs.append(layer.output) layer_names.append(layer.name) activation_model = keras.Model(inputs=model.input, outputs=layer_outputs) When fed an image input, this model returns the values of the layer activations in theoriginal model, as a list. This is the first time you’ve encountered a multi-outputmodel in this book in practice since you learned about them in chapter 7; until now,the models you’ve seen have had exactly one input and one output. This one has oneinput and nine outputs: one output per layer activation.activations = activation_model.predict(img_tensor) For instance, this is the activation of the first convolution layer for the cat image input:>>> first_layer_activation = activations[0]>>> print(first_layer_activation.shape)(1, 178, 178, 32)Listing 9.8 Instantiating a model that returns layer activations Listing 9.9 Using the model to compute layer activationsFigure 9.12 The test cat picture Extract the outputs of allConv2D and MaxPooling2Dlayers and put them in a list.Save the layer names for later.Create a model that will return theseoutputs, given the model input. Return a list of nine NumPy arrays:one array per layer activation. 265Interpreting what convnets learnIt’s a 178 × 178 feature map with 32 channels. Let’s try plotting the fifth channel of theactivation of the first layer of the original model (see figure 9.13).import matplotlib.pyplot as pltplt.matshow(first_layer_activation[0, :, :, 5], cmap=\"viridis\") This channel appears to encode a diagonal edge detector—but note that your ownchannels may vary, because the specific filters learned by convolution layers aren’tdeterministic. Now, let’s plot a complete visualization of all the activations in the network (see fig-ure 9.14). We’ll extract and plot every channel in each of the layer activations, andwe’ll stack the results in one big grid, with channels stacked side by side.images_per_row = 16 for layer_name, layer_activation in zip(layer_names, activations): n_features = layer_activation.shape[-1] size = layer_activation.shape[1] n_cols = n_features // images_per_row display_grid = np.zeros(((size + 1) * n_cols - 1, images_per_row * (size + 1) - 1)) for col in range(n_cols): for row in range(images_per_row): channel_index = col * images_per_row + row channel_image = layer_activation[0, :, :, channel_index].copy() Listing 9.10 Visualizing the fifth channel Listing 9.11 Visualizing every channel in every intermediate activationFigure 9.13 Fifth channel of the activation of the first layer on the test cat picture Iterate over the activations (and thenames of the corresponding layers).The layer activation has shape (1, size, size, n_features).Prepare an empty grid for displaying all the channels in this activation.This is a single channel (or feature). 266CHAPTER 9Advanced deep learning for computer visionif channel_image.sum() != 0: channel_image -= channel_image.mean() channel_image /= channel_image.std() channel_image *= 64 channel_image += 128 channel_image = np.clip(channel_image, 0, 255).astype(\"uint8\") display_grid[ col * (size + 1): (col + 1) * size + col, row * (size + 1) : (row + 1) * size + row] = channel_image scale = 1. / size plt.figure(figsize=(scale * display_grid.shape[1], scale * display_grid.shape[0])) plt.title(layer_name) plt.grid(False) plt.axis(\"off\") plt.imshow(display_grid, aspect=\"auto\", cmap=\"viridis\") Normalize channelvalues within the[0, 255] range. All-zero channels arekept at zero.Place thechannelmatrix in theempty gridwe prepared.Display the grid for the layer. Figure 9.14 Every channel of every layer activation on the test cat picture 267Interpreting what convnets learnThere are a few things to note here:The first layer acts as a collection of various edge detectors. At that stage, theactivations retain almost all of the information present in the initial picture.As you go deeper, the activations become increasingly abstract and less visuallyinterpretable. They begin to encode higher-level concepts such as “cat ear” and“cat eye.” Deeper presentations carry increasingly less information about thevisual contents of the image, and increasingly more information related to theclass of the image.The sparsity of the activations increases with the depth of the layer: in the firstlayer, almost all filters are activated by the input image, but in the following lay-ers, more and more filters are blank. This means the pattern encoded by the fil-ter isn’t found in the input image.We have just evidenced an important universal characteristic of the representationslearned by deep neural networks: the features extracted by a layer become increas-ingly abstract with the depth of the layer. The activations of higher layers carry lessand less information about the specific input being seen, and more and more infor-mation about the target (in this case, the class of the image: cat or dog). A deep neu-ral network effectively acts as an information distillation pipeline, with raw data going in(in this case, RGB pictures) and being repeatedly transformed so that irrelevant infor-mation is filtered out (for example, the specific visual appearance of the image), anduseful information is magnified and refined (for example, the class of the image). This is analogous to the way humans and animals perceive the world: after observ-ing a scene for a few seconds, a human can remember which abstract objects werepresent in it (bicycle, tree) but can’t remember the specific appearance of theseobjects. In fact, if you tried to draw a generic bicycle from memory, chances are youcouldn’t get it even remotely right, even though you’ve seen thousands of bicycles inyour lifetime (see, for example, figure 9.15). Try it right now: this effect is absolutelyreal. Your brain has learned to completely abstract its visual input—to transform it Figure 9.15 Left: attempts to draw a bicycle from memory. Right: what a schematic bicycle should look like. 268CHAPTER 9Advanced deep learning for computer visioninto high-level visual concepts while filtering out irrelevant visual details—making ittremendously difficult to remember how things around you look. 9.4.2 Visualizing convnet filtersAnother easy way to inspect the filters learned by convnets is to display the visual pat-tern that each filter is meant to respond to. This can be done with gradient ascent ininput space: applying gradient descent to the value of the input image of a convnet so asto maximize the response of a specific filter, starting from a blank input image. Theresulting input image will be one that the chosen filter is maximally responsive to. Let’s try this with the filters of the Xception model, pretrained on ImageNet. Theprocess is simple: we’ll build a loss function that maximizes the value of a given filterin a given convolution layer, and then we’ll use stochastic gradient descent to adjustthe values of the input image so as to maximize this activation value. This will be oursecond example of a low-level gradient descent loop leveraging the GradientTapeobject (the first one was in chapter 2). First, let’s instantiate the Xception model, loaded with weights pretrained on theImageNet dataset.model = keras.applications.xception.Xception( weights=\"imagenet\", include_top=False) We’re interested in the convolutional layers of the model—the Conv2D and Separa-bleConv2D layers. We’ll need to know their names so we can retrieve their outputs.Let’s print their names, in order of depth.for layer in model.layers: if isinstance(layer, (keras.layers.Conv2D, keras.layers.SeparableConv2D)): print(layer.name)You’ll notice that the SeparableConv2D l a y e r s h e r e a r e a l l n a m e d s o m e t h i n g l i k eblock6_sepconv1, block7_sepconv2, etc. Xception is structured into blocks, eachcontaining several convolutional layers. Now, let’s create a second model that returns the output of a specific layer—a fea-ture extractor model. Because our model is a Functional API model, it is inspectable: wecan query the output of one of its layers and reuse it in a new model. No need to copythe entire Xception code. Listing 9.12 Instantiating the Xception convolutional base Listing 9.13 Printing the names of all convolutional layers in XceptionThe classification layers are irrelevant for this use case, so we don’t include the top stage of the model. 269Interpreting what convnets learnlayer_name = \"block3_sepconv1\" layer = model.get_layer(name=layer_name) feature_extractor = keras.Model(inputs=model.input, outputs=layer.output) To use this model, simply call it on some input data (note that Xception requiresinputs to be preprocessed via the keras.applications.xception.preprocess_inputfunction).activation = feature_extractor(keras.applications.xception.preprocess_input(img_tensor))Let’s use our feature extractor model to define a function that returns a scalar valuequantifying how much a given input image “activates” a given filter in the layer. This isthe “loss function” that we’ll maximize during the gradient ascent process:import tensorflow as tf def compute_loss(image, filter_index): activation = feature_extractor(image) filter_activation = activation[:, 2:-2, 2:-2, filter_index] return tf.reduce_mean(filter_activation) Listing 9.14 Creating a feature extractor model Listing 9.15 Using the feature extractor The difference between model.predict(x) and model(x)In the previous chapter, we used predict(x) for feature extraction. Here, we’reusing model(x). What gives?Both y = model.predict(x) and y = model(x) (where x is an array of input data)mean “run the model on x and retrieve the output y.” Yet they aren’t exactly thesame thing.predict() loops over the data in batches (in fact, you can specify the batch size viapredict(x, batch_size=64)), and it extracts the NumPy value of the outputs. It’sschematically equivalent to this:def predict(x): y_batches = [] for x_batch in get_batches(x):You could replace this with the name of anylayer in the Xception convolutional base.This is the layer object we’re interested in.We use model.input and layer.output to create a model that,given an input image, returns the output of our target layer. The loss function takes an image tensor and the index of the filter we are considering (an integer).Note that we avoid border artifacts by only involvingnon-border pixels in the loss; we discard the firsttwo pixels along the sides of the activation.Return the mean of the activation values for the filter. 270CHAPTER 9Advanced deep learning for computer vision Let’s set up the gradient ascent step function, using the GradientTape. Note that we’lluse a @tf.function decorator to speed it up. A non-obvious trick to help the gradient descent process go smoothly is to normal-ize the gradient tensor by dividing it by its L2 norm (the square root of the average ofthe square of the values in the tensor). This ensures that the magnitude of theupdates done to the input image is always within the same range.@tf.function def gradient_ascent_step(image, filter_index, learning_rate): with tf.GradientTape() as tape: tape.watch(image) loss = compute_loss(image, filter_index) grads = tape.gradient(loss, image) grads = tf.math.l2_normalize(grads) image += learning_rate * grads return image Now we have all the pieces. Let’s put them together into a Python function that takesas input a layer name and a filter index, and returns a tensor representing the patternthat maximizes the activation of the specified filter.img_width = 200 img_height = 200 (continued) y_batch = model(x).numpy() y_batches.append(y_batch) return np.concatenate(y_batches)This means that predict() c a l l s c a n s c a l e t o v e r y l a r g e a r r a y s . M e a n w h i l e ,model(x) happens in-memory and doesn’t scale. On the other hand, predict() isnot differentiable: you cannot retrieve its gradient if you call it in a GradientTapescope.You should use model(x) when you need to retrieve the gradients of the model call,and you should use predict() if you just need the output value. In other words,always use predict() unless you’re in the middle of writing a low-level gradientdescent loop (as we are now). Listing 9.16 Loss maximization via stochastic gradient ascent Listing 9.17 Function to generate filter visualizationsExplicitly watch the image tensor, since it isn’t a TensorFlow Variable (only Variables are automatically watched in a gradient tape).Compute the loss scalar, indicating how much the current image activates the filter.Compute the gradients of the loss with respect to the image.Apply the “gradient normalization trick.”Move the image a littlebit in a direction thatactivates our targetfilter more strongly.Return the updated imageso we can run the stepfunction in a loop. 271Interpreting what convnets learndef generate_filter_pattern(filter_index): iterations = 30 learning_rate = 10. image = tf.random.uniform( minval=0.4, maxval=0.6, shape=(1, img_width, img_height, 3)) for i in range(iterations): image = gradient_ascent_step(image, filter_index, learning_rate) return image[0].numpy()The resulting image tensor is a floating-point array of shape (200, 200, 3), with val-ues that may not be integers within [0, 255]. Hence, we need to post-process this ten-sor to turn it into a displayable image. We do so with the following straightforwardutility function.def deprocess_image(image): image -= image.mean() image /= image.std() image *= 64 image += 128 image = np.clip(image, 0, 255).astype(\"uint8\") image = image[25:-25, 25:-25, :] return imageLet’s try it (see figure 9.16):>>> plt.axis(\"off\")>>> plt.imshow(deprocess_image(generate_filter_pattern(filter_index=2)))Listing 9.18 Utility function to convert a tensor into a valid imageNumber of gradient ascent steps to applyAmplitudeof a singlestepInitialize an image tensor with random values (the Xception model expects input values in the [0, 1] range, so here we pick a range centered on 0.5).Repeatedly update the values of the imagetensor so as to maximize our loss function. Normalize image values within the [0, 255] range.Center crop to avoid border artifacts. Figure 9.16 Pattern that the second channel in layer block3_sepconv1 responds to maximally 272CHAPTER 9Advanced deep learning for computer visionIt seems that filter 0 in layer block3_sepconv1 is responsive to a horizontal lines pat-tern, somewhat water-like or fur-like. Now the fun part: you can start visualizing every filter in the layer, and even everyfilter in every layer in the model.all_images = [] for filter_index in range(64): print(f\"Processing filter {filter_index}\") image = deprocess_image( generate_filter_pattern(filter_index) ) all_images.append(image) margin = 5 n = 8 cropped_width = img_width - 25 * 2 cropped_height = img_height - 25 * 2 width = n * cropped_width + (n - 1) * marginheight = n * cropped_height + (n - 1) * marginstitched_filters = np.zeros((width, height, 3)) for i in range(n): for j in range(n): image = all_images[i * n + j] stitched_filters[ row_start = (cropped_width + margin) * i row_end = (cropped_width + margin) * i + cropped_width column_start = (cropped_height + margin) * j column_end = (cropped_height + margin) * j + cropped_height stitched_filters[ row_start: row_end, column_start: column_end, :] = image keras.utils.save_img( f\"filters_for_layer_{layer_name}.png\", stitched_filters)These filter visualizations (see figure 9.17) tell you a lot about how convnet layers seethe world: each layer in a convnet learns a collection of filters such that their inputs canbe expressed as a combination of the filters. This is similar to how the Fourier transformdecomposes signals onto a bank of cosine functions. The filters in these convnet filterbanks get increasingly complex and refined as you go deeper in the model:The filters from the first layers in the model encode simple directional edgesand colors (or colored edges, in some cases).The filters from layers a bit further up the stack, such as block4_sepconv1,encode simple textures made from combinations of edges and colors.The filters in higher layers begin to resemble textures found in natural images:feathers, eyes, leaves, and so on. Listing 9.19 Generating a grid of all filter response patterns in a layerGenerate and save visualizations for the first 64 filters in the layer.Prepare a blank canvas for us to paste filter visualizations on.Fill the picture with the saved filters. Save the canvas to disk. 273Interpreting what convnets learn 9.4.3 Visualizing heatmaps of class activationWe’ll introduce one last visualization technique—one that is useful for understandingwhich parts of a given image led a convnet to its final classification decision. This ishelpful for “debugging” the decision process of a convnet, particularly in the case of aclassification mistake (a problem domain called model interpretability). It can also allowyou to locate specific objects in an image. This general category of techniques is called class activation map (CAM) visualiza-tion, and it consists of producing heatmaps of class activation over input images. AFigure 9.17 Some filter patterns for layers block2_sepconv1, block4_sepconv1, and block8_sepconv1 274CHAPTER 9Advanced deep learning for computer visionclass activation heatmap is a 2D grid of scores associated with a specific output class,computed for every location in any input image, indicating how important each loca-tion is with respect to the class under consideration. For instance, given an image fedinto a dogs-versus-cats convnet, CAM visualization would allow you to generate a heat-map for the class “cat,” indicating how cat-like different parts of the image are, andalso a heatmap for the class “dog,” indicating how dog-like parts of the image are. The specific implementation we’ll use is the one described in an article titled “Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization.”5 Grad-CAM consists of taking the output feature map of a convolution layer, givenan input image, and weighing every channel in that feature map by the gradient ofthe class with respect to the channel. Intuitively, one way to understand this trick is toimagine that you’re weighting a spatial map of “how intensely the input image acti-vates different channels” by “how important each channel is with regard to the class,”resulting in a spatial map of “how intensely the input image activates the class.” Let’s demonstrate this technique using the pretrained Xception model.model = keras.applications.xception.Xception(weights=\"imagenet\") Consider the image of two African elephants shown in figure 9.18, possibly a motherand her calf, strolling on the savanna. Let’s convert this image into something the Xcep-tion model can read: the model was trained on images of size 299 × 299, preprocessedaccording to a few rules that are packaged in the keras.applications.xception.preprocess_input utility function. So we need to load the image, resize it to 299 × 299,convert it to a NumPy float32 tensor, and apply these preprocessing rules.img_path = keras.utils.get_file( fname=\"elephant.jpg\", origin=\"https://img-datasets.s3.amazonaws.com/elephant.jpg\") def get_img_array(img_path, target_size): img = keras.utils.load_img(img_path, target_size=target_size) array = keras.utils.img_to_array(img) array = np.expand_dims(array, axis=0) array = keras.applications.xception.preprocess_input(array) return arrayimg_array = get_img_array(img_path, target_size=(299, 299))5Ramprasaath R. Selvaraju et al., arXiv (2017), https:/ /arxiv.org/abs/1610.02391.Listing 9.20 Loading the Xception network with pretrained weights Listing 9.21 Preprocessing an input image for XceptionNote that we include the densely connected classi-fier on top; in all previous cases, we discarded it. Download the image and store itlocally under the path img_path.Return a Python Imaging Library(PIL) image of size 299 × 299.Returna float32NumPy arrayof shape(299, 299, 3).Add a dimension to transform the array into a batch of size (1, 299, 299, 3).Preprocess the batch (this doeschannel-wise color normalization). 275Interpreting what convnets learn You can now run the pretrained network on the image and decode its prediction vec-tor back to a human-readable format:>>> preds = model.predict(img_array)>>> print(keras.applications.xception.decode_predictions(preds, top=3)[0])[(\"n02504458\", \"African_elephant\", 0.8699266), (\"n01871265\", \"tusker\", 0.076968715), (\"n02504013\", \"Indian_elephant\", 0.02353728)]The top three classes predicted for this image are as follows:African elephant (with 87% probability)Tusker (with 7% probability)Indian elephant (with 2% probability)Figure 9.18 Test picture of African elephants 276CHAPTER 9Advanced deep learning for computer visionThe network has recognized the image as containing an undetermined quantity ofAfrican elephants. The entry in the prediction vector that was maximally activated isthe one corresponding to the “African elephant” class, at index 386:>>> np.argmax(preds[0])386 To visualize which parts of the image are the most African-elephant–like, let’s set upthe Grad-CAM process. First, we create a model that maps the input image to the activations of the lastconvolutional layer.last_conv_layer_name = \"block14_sepconv2_act\" classifier_layer_names = [ \"avg_pool\", \"predictions\",]last_conv_layer = model.get_layer(last_conv_layer_name)last_conv_layer_model = keras.Model(model.inputs, last_conv_layer.output)Second, we create a model that maps the activations of the last convolutional layer tothe final class predictions.classifier_input = keras.Input(shape=last_conv_layer.output.shape[1:])x = classifier_input for layer_name in classifier_layer_names: x = model.get_layer(layer_name)(x)classifier_model = keras.Model(classifier_input, x)Then we compute the gradient of the top predicted class for our input image withrespect to the activations of the last convolution layer.import tensorflow as tf with tf.GradientTape() as tape: last_conv_layer_output = last_conv_layer_model(img_array) tape.watch(last_conv_layer_output) preds = classifier_model(last_conv_layer_output) top_pred_index = tf.argmax(preds[0]) top_class_channel = preds[:, top_pred_index] grads = tape.gradient(top_class_channel, last_conv_layer_output) Listing 9.22 Setting up a model that returns the last convolutional output Listing 9.23 Reapplying the classifier on top of the last convolutional output Listing 9.24 Retrieving the gradients of the top predicted classCompute activations of the last convlayer and make the tape watch it.Retrieve the activation channel corresponding to the top predicted class.This is the gradient of the top predicted class with regardto the output feature map of the last convolutional layer. 277Interpreting what convnets learnNow we apply pooling and importance weighting to the gradient tensor to obtain ourheatmap of class activation.pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2)).numpy() last_conv_layer_output = last_conv_layer_output.numpy()[0]for i in range(pooled_grads.shape[-1]): last_conv_layer_output[:, :, i] *= pooled_grads[i] heatmap = np.mean(last_conv_layer_output, axis=-1) For visualization purposes, we’ll also normalize the heatmap between 0 and 1. Theresult is shown in figure 9.19.heatmap = np.maximum(heatmap, 0)heatmap /= np.max(heatmap)plt.matshow(heatmap) Finally, let’s generate an image that superimposes the original image on the heatmapwe just obtained (see figure 9.20).import matplotlib.cm as cm img = keras.utils.load_img(img_path) img = keras.utils.img_to_array(img) Listing 9.25 Gradient pooling and channel-importance weighting Listing 9.26 Heatmap post-processing Listing 9.27 Superimposing the heatmap on the original pictureThis is a vector where each entry is the mean intensity of thegradient for a given channel. It quantifies the importance ofeach channel with regard to the top predicted class.Multiply each channel in the output of the last convolutional layer by “how important this channel is.”The channel-wise mean of the resulting featuremap is our heatmap of class activation. Figure 9.19 Standalone class activation heatmap Load the original image. 278CHAPTER 9Advanced deep learning for computer visionheatmap = np.uint8(255 * heatmap) jet = cm.get_cmap(\"jet\") jet_colors = jet(np.arange(256))[:, :3] jet_heatmap = jet_colors[heatmap] jet_heatmap = keras.utils.array_to_img(jet_heatmap) jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0])) jet_heatmap = keras.utils.img_to_array(jet_heatmap) superimposed_img = jet_heatmap * 0.4 + img superimposed_img = keras.utils.array_to_img(superimposed_img) save_path = \"elephant_cam.jpg\" superimposed_img.save(save_path) Rescale theheatmap tothe range0–255.Use the \"jet\" colormap to recolorize the heatmap.Create an image that contains the recolorized heatmap.Superimpose the heatmap and the original image, with the heatmap at 40% opacity.Save the superimposed image. Figure 9.20 African elephant class activation heatmap over the test picture 279SummaryThis visualization technique answers two important questions:Why did the network think this image contained an African elephant?Where is the African elephant located in the picture?In particular, it’s interesting to note that the ears of the elephant calf are strongly acti-vated: this is probably how the network can tell the difference between African andIndian elephants. SummaryThere are three essential computer vision tasks you can do with deep learning:image classification, image segmentation, and object detection.Following modern convnet architecture best practices will help you get themost out of your models. Some of these best practices include using residualconnections, batch normalization, and depthwise separable convolutions.The representations that convnets learn are easy to inspect—convnets are theopposite of black boxes!You can generate visualizations of the filters learned by your convnets, as well asheatmaps of class activity. 280Deep learningfor timeseries 10.1 Different kinds of timeseries tasksA timeseries can be any data obtained via measurements at regular intervals, likethe daily price of a stock, the hourly electricity consumption of a city, or theweekly sales of a store. Timeseries are everywhere, whether we’re looking at natu-ral phenomena (like seismic activity, the evolution of fish populations in a river,or the weather at a location) or human activity patterns (like visitors to a website,a country’s GDP, or credit card transactions). Unlike the types of data you’veencountered so far, working with timeseries involves understanding the dynamicsof a system—its periodic cycles, how it trends over time, its regular regime and itssudden spikes.This chapter coversExamples of machine learning tasks that involve timeseries dataUnderstanding recurrent neural networks (RNNs)Applying RNNs to a temperature-forecasting exampleAdvanced RNN usage patterns 281A temperature-forecasting example By far, the most common timeseries-related task is forecasting: predicting what willhappen next in a series. Forecast electricity consumption a few hours in advance soyou can anticipate demand; forecast revenue a few months in advance so you can planyour budget; forecast the weather a few days in advance so you can plan your sched-ule. Forecasting is what this chapter focuses on. But there’s actually a wide range ofother things you can do with timeseries:Classification—Assign one or more categorical labels to a timeseries. For instance,given the timeseries of the activity of a visitor on a website, classify whether thevisitor is a bot or a human.Event detection—Identify the occurrence of a specific expected event within acontinuous data stream. A particularly useful application is “hotword detec-tion,” where a model monitors an audio stream and detects utterances like “OkGoogle” or “Hey Alexa.”Anomaly detection—Detect anything unusual happening within a continuousdatastream. Unusual activity on your corporate network? Might be an attacker.Unusual readings on a manufacturing line? Time for a human to go take a look.Anomaly detection is typically done via unsupervised learning, because youoften don’t know what kind of anomaly you’re looking for, so you can’t train onspecific anomaly examples.When working with timeseries, you’ll encounter a wide range of domain-specific data-representation techniques. For instance, you have likely already heard about the Fou-rier transform, which consists of expressing a series of values in terms of a superpositionof waves of different frequencies. The Fourier transform can be highly valuable whenpreprocessing any data that is primarily characterized by its cycles and oscillations(like sound, the vibrations of the frame of a skyscraper, or your brain waves). In thecontext of deep learning, Fourier analysis (or the related Mel-frequency analysis) andother domain-specific representations can be useful as a form of feature engineering,a way to prepare data before training a model on it, to make the job of the model eas-ier. However, we won’t cover these techniques in these pages; we will instead focus onthe modeling part. In this chapter, you’ll learn about recurrent neural networks (RNNs) and how toapply them to timeseries forecasting.10.2 A temperature-forecasting exampleThroughout this chapter, all of our code examples will target a single problem: pre-dicting the temperature 24 hours in the future, given a timeseries of hourly measure-ments of quantities such as atmospheric pressure and humidity, recorded over therecent past by a set of sensors on the roof of a building. As you will see, it’s a fairly chal-lenging problem! We’ll use this temperature-forecasting task to highlight what makes timeseries datafundamentally different from the kinds of datasets you’ve encountered so far. You’ll 282CHAPTER 10Deep learning for timeseriessee that densely connected networks and convolutional networks aren’t well-equippedto deal with this kind of dataset, while a different kind of machine learning tech-nique—recurrent neural networks (RNNs)—really shines on this type of problem. We’ll work with a weather timeseries dataset recorded at the weather station at theMax Planck Institute for Biogeochemistry in Jena, Germany.1 In this dataset, 14 differ-ent quantities (such as temperature, pressure, humidity, wind direction, and so on)were recorded every 10 minutes over several years. The original data goes back to2003, but the subset of the data we’ll download is limited to 2009–2016. Let’s start by downloading and uncompressing the data:!wget https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip!unzip jena_climate_2009_2016.csv.zipNow let’s look at the data.import osfname = os.path.join(\"jena_climate_2009_2016.csv\") with open(fname) as f: data = f.read() lines = data.split(\"\\n\")header = lines[0].split(\",\")lines = lines[1:] print(header) print(len(lines))This outputs a count of 420,551 lines of data (each line is a timestep: a record of adate and 14 weather-related values), as well as the following header:[\"Date Time\", \"p (mbar)\", \"T (degC)\", \"Tpot (K)\", \"Tdew (degC)\", \"rh (%)\", \"VPmax (mbar)\", \"VPact (mbar)\", \"VPdef (mbar)\", \"sh (g/kg)\", \"H2OC (mmol/mol)\", \"rho (g/m**3)\", \"wv (m/s)\", \"max. wv (m/s)\", \"wd (deg)\"]1Adam Erickson and Olaf Kolle, www.bgc-jena.mpg.de/wetter.Listing 10.1 Inspecting the data of the Jena weather dataset 283A temperature-forecasting exampleNow, convert all 420,551 lines of data into NumPy arrays: one array for the tempera-ture (in degrees Celsius), and another one for the rest of the data—the features wewill use to predict future temperatures. Note that we discard the “Date Time” column.import numpy as nptemperature = np.zeros((len(lines),))raw_data = np.zeros((len(lines), len(header) - 1)) for i, line in enumerate(lines): values = [float(x) for x in line.split(\",\")[1:]] temperature[i] = values[1] raw_data[i, :] = values[:] Figure 10.1 shows the plot of temperature (in degrees Celsius) over time. On this plot,you can clearly see the yearly periodicity of temperature—the data spans 8 years.from matplotlib import pyplot as pltplt.plot(range(len(temperature)), temperature) Figure 10.2 shows a more narrow plot of the first 10 days of temperature data. Becausethe data is recorded every 10 minutes, you get 24 × 6 = 144 data points per day.plt.plot(range(1440), temperature[:1440])Listing 10.2 Parsing the data Listing 10.3 Plotting the temperature timeseries Listing 10.4 Plotting the first 10 days of the temperature timeseriesWe store column 1 in the “temperature” array.We store all columns (including the temperature) in the “raw_data” array. Figure 10.1 Temperature over the full temporal range of the dataset (ºC) 284CHAPTER 10Deep learning for timeseries On this plot, you can see daily periodicity, especially for the last 4 days. Also note thatthis 10-day period must be coming from a fairly cold winter month. With our dataset, if you were trying to predict average temperature for the next monthgiven a few months of past data, the problem would be easy, due to the reliable year-scale periodicity of the data. But looking at the data over a scale of days, the temperaturelooks a lot more chaotic. Is this timeseries predictable at a daily scale? Let’s find out. In all our experiments, we’ll use the first 50% of the data for training, the follow-ing 25% for validation, and the last 25% for testing. When working with timeseriesdata, it’s important to use validation and test data that is more recent than the train-ing data, because you’re trying to predict the future given the past, not the reverse,and your validation/test splits should reflect that. Some problems happen to be con-siderably simpler if you reverse the time axis!>>> num_train_samples = int(0.5 * len(raw_data))>>> num_val_samples = int(0.25 * len(raw_data))>>> num_test_samples = len(raw_data) - num_train_samples - num_val_samplesAlways look for periodicity in your dataPeriodicity over multiple timescales is an important and very common property oftimeseries data. Whether you’re looking at the weather, mall parking occupancy, traf-fic to a website, sales of a grocery store, or steps logged in a fitness tracker, you’llsee daily cycles and yearly cycles (human-generated data also tends to featureweekly cycles). When exploring your data, make sure to look for these patterns. Listing 10.5 Computing the number of samples we’ll use for each data splitFigure 10.2 Temperature over the first 10 days of the dataset (ºC) 285A temperature-forecasting example>>> print(\"num_train_samples:\", num_train_samples)>>> print(\"num_val_samples:\", num_val_samples)>>> print(\"num_test_samples:\", num_test_samples)num_train_samples: 210225 num_val_samples: 105112 num_test_samples: 105114 10.2.1 Preparing the dataThe exact formulation of the problem will be as follows: given data covering the previ-ous five days and sampled once per hour, can we predict the temperature in 24 hours? First, let’s preprocess the data to a format a neural network can ingest. This iseasy: the data is already numerical, so you don’t need to do any vectorization. Buteach timeseries in the data is on a different scale (for example, atmospheric pres-sure, measured in mbar, is around 1,000, while H2OC, measured in millimoles permole, is around 3). We’ll normalize each timeseries independently so that they alltake small values on a similar scale. We’re going to use the first 210,225 timesteps astraining data, so we’ll compute the mean and standard deviation only on this frac-tion of the data.mean = raw_data[:num_train_samples].mean(axis=0)raw_data -= meanstd = raw_data[:num_train_samples].std(axis=0)raw_data /= stdNext, let’s create a Dataset object that yields batches of data from the past five daysalong with a target temperature 24 hours in the future. Because the samples in thedataset are highly redundant (sample N and sample N + 1 will have most of their time-steps in common), it would be wasteful to explicitly allocate memory for every sample.Instead, we’ll generate the samples on the fly while only keeping in memory the origi-nal raw_data and temperature arrays, and nothing more. We could easily write a Python generator to do this, but there’s a built-in datasetutility in Keras that does just that (timeseries_dataset_from_array()), so we cansave ourselves some work by using it. You can generally use it for any kind of timeseriesforecasting task.Listing 10.6 Normalizing the data Understanding timeseries_dataset_from_array()To understand what timeseries_dataset_from_array() does, let’s look at a sim-ple example. The general idea is that you provide an array of timeseries data (thedata argument), and timeseries_dataset_from_array() g i v e s y o u w i n d o w sextracted from the original timeseries (we’ll call them “sequences”).For example, if you use data = [0 1 2 3 4 5 6] a nd sequence_length=3, thentimeseries_dataset_from_array() will generate the following samples: [0 1 2],[1 2 3], [2 3 4], [3 4 5], [4 5 6]. 286CHAPTER 10Deep learning for timeseries We’ll use timeseries_dataset_from_array() to instantiate three datasets: one fortraining, one for validation, and one for testing. We’ll use the following parameter values:sampling_rate = 6—Observations will be sampled at one data point per hour:we will only keep one data point out of 6.sequence_length = 120—Observations will go back 5 days (120 hours).delay = sampling_rate * (sequence_length + 24 - 1)—The target for a sequencewill be the temperature 24 hours after the end of the sequence.When making the training dataset, we’ll pass start_index = 0 a n d end_index =num_train_samples t o o n l y u s e t h e f i r s t 5 0 % o f t h e d a t a . F o r t h e v a l i d a t i o n d a t a s e t ,we’ll pass start_index = num_train_samples and end_index = num_train_samples +num_val_samples to use the next 25% of the data. Finally, for the test dataset, we’ll passstart_index = num_train_samples + num_val_samples to use the remaining samples.(continued)You can also pass a targets a r g u m e n t ( a n a r r a y ) t o timeseries_dataset_from_array(). The first entry of the targets array should match the desired tar-get for the first sequence that will be generated from the data array. So if you’redoing timeseries forecasting, targets should be the same array as data, offsetby some amount.For instance, with data = [0 1 2 3 4 5 6 …] and sequence_length=3, you could createa dataset to predict the next step in the series by passing targets = [3 4 5 6 …]. Let’stry it:import numpy as np from tensorflow import kerasint_sequence = np.arange(10)dummy_dataset = keras.utils.timeseries_dataset_from_array( data=int_sequence[:-3], targets=int_sequence[3:], sequence_length=3, batch_size=2, )for inputs, targets in dummy_dataset: for i in range(inputs.shape[0]): print([int(x) for x in inputs[i]], int(targets[i]))This bit of code prints the following results:[0, 1, 2] 3 [1, 2, 3] 4 [2, 3, 4] 5 [3, 4, 5] 6 [4, 5, 6] 7 Generate an array of sorted integers from 0 to 9.The sequences we generate will be sampled from [0 1 2 3 4 5 6].The target for the sequence that starts at data[N] will be data[N + 3].The sequences will be 3 steps long.The sequences will be batched in batches of size 2. 287A temperature-forecasting examplesampling_rate = 6 sequence_length = 120 delay = sampling_rate * (sequence_length + 24 - 1)batch_size = 256 train_dataset = keras.utils.timeseries_dataset_from_array( raw_data[:-delay], targets=temperature[delay:], sampling_rate=sampling_rate, sequence_length=sequence_length, shuffle=True, batch_size=batch_size, start_index=0, end_index=num_train_samples) val_dataset = keras.utils.timeseries_dataset_from_array( raw_data[:-delay], targets=temperature[delay:], sampling_rate=sampling_rate, sequence_length=sequence_length, shuffle=True, batch_size=batch_size, start_index=num_train_samples, end_index=num_train_samples + num_val_samples) test_dataset = keras.utils.timeseries_dataset_from_array( raw_data[:-delay], targets=temperature[delay:], sampling_rate=sampling_rate, sequence_length=sequence_length, shuffle=True, batch_size=batch_size, start_index=num_train_samples + num_val_samples)Each dataset yields a tuple (samples, targets), where samples is a batch of 256 sam-ples, each containing 120 consecutive hours of input data, and targets is the corre-sponding array of 256 target temperatures. Note that the samples are randomlyshuffled, so two consecutive sequences in a batch (like samples[0] and samples[1])aren’t necessarily temporally close. >>> for samples, targets in train_dataset:>>> print(\"samples shape:\", samples.shape)>>> print(\"targets shape:\", targets.shape)>>> breaksamples shape: (256, 120, 14)targets shape: (256,)Listing 10.7 Instantiating datasets for training, validation, and testing Listing 10.8 Inspecting the output of one of our datasets 288CHAPTER 10Deep learning for timeseries10.2.2 A common-sense, non-machine learning baselineBefore we start using black-box deep learning models to solve the temperature-prediction problem, let’s try a simple, common-sense approach. It will serve as a sanitycheck, and it will establish a baseline that we’ll have to beat in order to demonstratethe usefulness of more-advanced machine learning models. Such common-sense base-lines can be useful when you’re approaching a new problem for which there is noknown solution (yet). A classic example is that of unbalanced classification tasks,where some classes are much more common than others. If your dataset contains 90%instances of class A and 10% instances of class B, then a common-sense approach tothe classification task is to always predict “A” when presented with a new sample. Sucha classifier is 90% accurate overall, and any learning-based approach should thereforebeat this 90% score in order to demonstrate usefulness. Sometimes, such elementarybaselines can prove surprisingly hard to beat. In this case, the temperature timeseries can safely be assumed to be continuous(the temperatures tomorrow are likely to be close to the temperatures today) aswell as periodical with a daily period. Thus a common-sense approach is to alwayspredict that the temperature 24 hours from now will be equal to the temperatureright now. Let’s evaluate this approach, using the mean absolute error (MAE) met-ric, defined as follows:np.mean(np.abs(preds - targets))Here’s the evaluation loop.def evaluate_naive_method(dataset): total_abs_err = 0. samples_seen = 0 for samples, targets in dataset: preds = samples[:, -1, 1] * std[1] + mean[1] total_abs_err += np.sum(np.abs(preds - targets)) samples_seen += samples.shape[0] return total_abs_err / samples_seen print(f\"Validation MAE: {evaluate_naive_method(val_dataset):.2f}\") print(f\"Test MAE: {evaluate_naive_method(test_dataset):.2f}\")This common-sense baseline achieves a validation MAE of 2.44 degrees Celsius and atest MAE of 2.62 degrees Celsius. So if you always assume that the temperature 24hours in the future will be the same as it is now, you will be off by two and a halfdegrees on average. It’s not too bad, but you probably won’t launch a weather fore-casting service based on this heuristic. Now the game is to use your knowledge of deeplearning to do better. Listing 10.9 Computing the common-sense baseline MAE The temperature feature is at column 1, so samples[:, -1, 1] is the last temperature measurement in the input sequence. Recall that we normalized our features, so to retrieve a temperature in degrees Celsius, we need to un-normalize it by multiplying it by the standard deviation and adding back the mean. 289A temperature-forecasting example10.2.3 Let’s try a basic machine learning modelIn the same way that it’s useful to establish a common-sense baseline before tryingmachine learning approaches, it’s useful to try simple, cheap machine learning mod-els (such as small, densely connected networks) before looking into complicated andcomputationally expensive models such as RNNs. This is the best way to make sure anyfurther complexity you throw at the problem is legitimate and delivers real benefits. The following listing shows a fully connected model that starts by flattening thedata and then runs it through two Dense layers. Note the lack of an activation functionon the last Dense layer, which is typical for a regression problem. We use meansquared error (MSE) as the loss, rather than MAE, because unlike MAE, it’s smootharound zero, which is a useful property for gradient descent. We will monitor MAE byadding it as a metric in compile().from tensorflow import keras from tensorflow.keras import layers inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))x = layers.Flatten()(inputs)x = layers.Dense(16, activation=\"relu\")(x)outputs = layers.Dense(1)(x)model = keras.Model(inputs, outputs) callbacks = [ keras.callbacks.ModelCheckpoint(\"jena_dense.keras\", save_best_only=True)] model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])history = model.fit(train_dataset, epochs=10, validation_data=val_dataset, callbacks=callbacks) model = keras.models.load_model(\"jena_dense.keras\")print(f\"Test MAE: {model.evaluate(test_dataset)[1]:.2f}\")Let’s display the loss curves for validation and training (see figure 10.3).import matplotlib.pyplot as pltloss = history.history[\"mae\"]val_loss = history.history[\"val_mae\"]epochs = range(1, len(loss) + 1)plt.figure()plt.plot(epochs, loss, \"bo\", label=\"Training MAE\")plt.plot(epochs, val_loss, \"b\", label=\"Validation MAE\")plt.title(\"Training and validation MAE\")plt.legend()plt.show()Listing 10.10 Training and evaluating a densely connected model Listing 10.11 Plotting resultsWe use a callback to save the best-performing model.Reload the best model and evaluate it on the test data. 290CHAPTER 10Deep learning for timeseries Some of the validation losses are close to the no-learning baseline, but not reliably.This goes to show the merit of having this baseline in the first place: it turns out to benot easy to outperform. Your common sense contains a lot of valuable information towhich a machine learning model doesn’t have access. You may wonder, if a simple, well-performing model exists to go from the data tothe targets (the common-sense baseline), why doesn’t the model you’re training find itand improve on it? Well, the space of models in which you’re searching for a solution—that is, your hypothesis space—is the space of all possible two-layer networks with theconfiguration you defined. The common-sense heuristic is just one model among mil-lions that can be represented in this space. It’s like looking for a needle in a haystack.Just because a good solution technically exists in your hypothesis space doesn’t meanyou’ll be able to find it via gradient descent. T h a t ’ s a p r e t t y s i g n i f i c a n t l i m i t a t i o n o f m a c hi n e l e a r n i n g i n g e n e r a l : u n l es s t h elearning algorithm is hardcoded to look for a specific kind of simple model, it cansometimes fail to find a simple solution to a simple problem. That’s why leveraginggood feature engineering and relevant architecture priors is essential: you need toprecisely tell your model what it should be looking for. 10.2.4 Let’s try a 1D convolutional modelSpeaking of leveraging the right architecture priors, since our input sequences fea-ture daily cycles, perhaps a convolutional model could work. A temporal convnetcould reuse the same representations across different days, much like a spatial conv-net can reuse the same representations across different locations in an image. You already know about the Conv2D and SeparableConv2D layers, which see theirinputs through small windows that swipe across 2D grids. There are also 1D and evenFigure 10.3 Training and validation MAE on the Jena temperature-forecasting task with a simple, densely connected network 291A temperature-forecasting example3D versions of these layers: Conv1D, SeparableConv1D, and Conv3D.2 The Conv1D layerrelies on 1D windows that slide across input sequences, and the Conv3D layer relies oncubic windows that slide across input volumes. You can thus build 1D convnets, strictly analogous to 2D convnets. They’re a greatfit for any sequence data that follows the translation invariance assumption (meaningthat if you slide a window over the sequence, the content of the window should followthe same properties independently of the location of the window). Let’s try one on our temperature-forecasting problem. We’ll pick an initial windowlength of 24, so that we look at 24 hours of data at a time (one cycle). As we downsam-ple the sequences (via MaxPooling1D layers), we’ll reduce the window size accordingly:inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))x = layers.Conv1D(8, 24, activation=\"relu\")(inputs)x = layers.MaxPooling1D(2)(x)x = layers.Conv1D(8, 12, activation=\"relu\")(x)x = layers.MaxPooling1D(2)(x)x = layers.Conv1D(8, 6, activation=\"relu\")(x)x = layers.GlobalAveragePooling1D()(x)outputs = layers.Dense(1)(x)model = keras.Model(inputs, outputs) callbacks = [ keras.callbacks.ModelCheckpoint(\"jena_conv.keras\", save_best_only=True)]model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])history = model.fit(train_dataset, epochs=10, validation_data=val_dataset, callbacks=callbacks) model = keras.models.load_model(\"jena_conv.keras\")print(f\"Test MAE: {model.evaluate(test_dataset)[1]:.2f}\")We get the training and validation curves shown in figure 10.4. As it turns out, this model performs even worse than the densely connected one,only achieving a validation MAE of about 2.9 degrees, far from the common-sensebaseline. What went wrong here? Two things:First, weather data doesn’t quite respect the translation invariance assumption.While the data does feature daily cycles, data from a morning follows differentproperties than data from an evening or from the middle of the night. Weatherdata is only translation-invariant for a very specific timescale.Second, order in our data matters—a lot. The recent past is far more informa-tive for predicting the next day’s temperature than data from five days ago. A1D convnet is not able to leverage this fact. In particular, our max pooling andglobal average pooling layers are largely destroying order information. 2Note that there isn’t a SeparableConv3D layer, not for any theoretical reason, but simply because I haven’timplemented it. 292CHAPTER 10Deep learning for timeseries 10.2.5 A first recurrent baselineNeither the fully connected approach nor the convolutional approach did well, butthat doesn’t mean machine learning isn’t applicable to this problem. The densely con-nected approach first flattened the timeseries, which removed the notion of timefrom the input data. The convolutional approach treated every segment of the data inthe same way, even applying pooling, which destroyed order information. Let’sinstead look at the data as what it is: a sequence, where causality and order matter. There’s a family of neural network architectures designed specifically for this usecase: recurrent neural networks. Among them, the Long Short Term Memory (LSTM)layer has long been very popular. We’ll see in a minute how these models work, butlet’s start by giving the LSTM layer a try.inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))x = layers.LSTM(16)(inputs)outputs = layers.Dense(1)(x)model = keras.Model(inputs, outputs) callbacks = [ keras.callbacks.ModelCheckpoint(\"jena_lstm.keras\", save_best_only=True)]model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])history = model.fit(train_dataset, epochs=10, validation_data=val_dataset, callbacks=callbacks)Listing 10.12 A simple LSTM-based modelFigure 10.4 Training and validation MAE on the Jena temperature-forecasting task with a 1D convnet 293Understanding recurrent neural networksmodel = keras.models.load_model(\"jena_lstm.keras\") print(f\"Test MAE: {model.evaluate(test_dataset)[1]:.2f}\")Figure 10.5 shows the results. Much better! We achieve a validation MAE as low as 2.36degrees and a test MAE of 2.55 degrees. The LSTM-based model can finally beat thecommon-sense baseline (albeit just by a bit, for now), demonstrating the value ofmachine learning on this task. But why did the LSTM model perform markedly better than the densely connectedone or the convnet? And how can we further refine the model? To answer this, let’stake a closer look at recurrent neural networks. 10.3 Understanding recurrent neural networksA major characteristic of all neural networks you’ve seen so far, such as densely con-nected networks and convnets, is that they have no memory. Each input shown tothem is processed independently, with no state kept between inputs. With such net-works, in order to process a sequence or a temporal series of data points, you have toshow the entire sequence to the network at once: turn it into a single data point. Forinstance, this is what we did in the densely connected network example: we flattenedour five days of data into a single large vector and processed it in one go. Such net-works are called feedforward networks. In contrast, as you’re reading the present sentence, you’re processing it word byword—or rather, eye saccade by eye saccade—while keeping memories of what camebefore; this gives you a fluid representation of the meaning conveyed by this sentence.Figure 10.5 Training and validation MAE on the Jena temperature-forecasting task with an LSTM-based model (note that we omit epoch 1 on this graph, because the high training MAE (7.75) at epoch 1 would distort the scale) 294CHAPTER 10Deep learning for timeseriesBiological intelligence processes information incrementally while maintaining aninternal model of what it’s processing, built from past information and constantlyupdated as new information comes in. A recurrent neural network (RNN) adopts the same princi-ple, albeit in an extremely simplified version: it processessequences by iterating through the sequence elements andmaintaining a state that contains information relative to whatit has seen so far. In effect, an RNN is a type of neural net-work that has an internal loop (see figure 10.6). The state of the RNN is reset between processing two dif-ferent, independent sequences (such as two samples in abatch), so you still consider one sequence to be a single datapoint: a single input to the network. What changes is that thisdata point is no longer processed in a single step; rather, thenetwork internally loops over sequence elements. To make these notions of loop and state clear, let’s implement the forward pass of atoy RNN. This RNN takes as input a sequence of vectors, which we’ll encode as a rank-2tensor of size (timesteps, input_features). It loops over timesteps, and at each time-step, it considers its current state at t and the input at t (of shape (input_features,),and combines them to obtain the output at t. We’ll then set the state for the next stepto be this previous output. For the first timestep, the previous output isn’t defined;hence, there is no current state. So we’ll initialize the state as an all-zero vector calledthe initial state of the network. In pseudocode, this is the RNN.state_t = 0for input_t in input_sequence: output_t = f(input_t, state_t) state_t = output_tYou can even flesh out the function f: the transformation of the input and state into anoutput will be parameterized by two matrices, W and U, and a bias vector. It’s similar tothe transformation operated by a densely connected layer in a feedforward network.state_t = 0 for input_t in input_sequence: output_t = activation(dot(W, input_t) + dot(U, state_t) + b) state_t = output_tTo make these notions absolutely unambiguous, let’s write a naive NumPy implemen-tation of the forward pass of the simple RNN.Listing 10.13 Pseudocode RNN Listing 10.14 More-detailed pseudocode for the RNNRNNInputOutputRecurrentconnectionFigure 10.6 A recurrent network: a network with a loop The state at tIterates over sequence elementsThe previous output becomes the state for the next iteration. 295Understanding recurrent neural networksimport numpy as nptimesteps = 100 input_features = 32 output_features = 64 inputs = np.random.random((timesteps, input_features))state_t = np.zeros((output_features,))W=n p . r a n d o m . r a n d o m ( ( o u t p u t _ f e a t u r e s ,i n p u t _ f e a t u r e s ) )U=n p . r a n d o m . r a n d o m ( ( o u t p u t _ f e a t u r e s ,o u t p u t _ f e a t u r e s ) ) b=n p . r a n d o m . r a n d o m ( ( o u t p u t _ f e a t u r e s , ) ) successive_outputs = []for input_t in inputs: output_t = np.tanh(np.dot(W, input_t) + np.dot(U, state_t) + b)successive_outputs.append(output_t)state_t = output_tfinal_output_sequence = np.stack(successive_outputs, axis=0)That’s easy enough. In summary, an RNN is a for loop that reuses quantities com-puted during the previous iteration of the loop, nothing more. Of course, there aremany different RNNs fitting this definition that you could build—this example is oneof the simplest RNN formulations. RNNs are characterized by their step function,such as the following function in this case (see figure 10.7).output_t = np.tanh(np.dot(W, input_t) + np.dot(U, state_t) + b)Listing 10.15 NumPy implementation of a simple RNNNumber of timesteps in the input sequenceDimensionality of the input feature spaceDimensionality of the output feature spaceInput data: random noise for the sake of the exampleInitial state: an all-zero vectorCreates random weight matrices input_t is a vector of shape (input_features,).Combines the input with the current state (the previous output) to obtain the current output. We use tanh to add non-linearity (we could use any other activation function).Stores this output in a listUpdates the state of thenetwork for the next timestepThe final output is a rank-2tensor of shape (timesteps,output_features). ...output t-1 output t output t+1 input t-1 input t input t+1...State tState t+1output_t =activation(Wo • input_t +Uo • state_t +bo)Figure 10.7 A simple RNN, unrolled over time 296CHAPTER 10Deep learning for timeseriesNOTEIn this example, the final output is a rank-2 tensor of shape (time-steps, output_features), where each timestep is the output of the loop attime t. Each timestep t in the output tensor contains information about time-steps 0 to t in the input sequence—about the entire past. For this reason, inmany cases, you don’t need this full sequence of outputs; you just need thelast output (output_t a t t h e e n d o f t h e l o o p ) , b e c a u s e i t a l r e a d y c o n t a i n sinformation about the entire sequence.10.3.1 A recurrent layer in KerasThe process we just naively implemented in NumPy corresponds to an actual Keraslayer—the SimpleRNN layer. There is one minor difference: SimpleRNN processes batches of sequences, like allother Keras layers, not a single sequence as in the NumPy example. This means it takesinputs of shape (batch_size, timesteps, input_features), rather than (timesteps,input_features). When specifying the shape argument of the initial Input(), notethat you can set the timesteps entry to None, which enables your network to processsequences of arbitrary length.num_features = 14 inputs = keras.Input(shape=(None, num_features))outputs = layers.SimpleRNN(16)(inputs)This is especially useful if your model is meant to process sequences of variable length.However, if all of your sequences have the same length, I recommend specifying acomplete input shape, since it enables model.summary() t o d i s p l a y o u t p u t l e n g t hinformation, which is always nice, and it can unlock some performance optimizations(see the “Note on RNN runtime performance” sidebar a little later in this chapter). All recurrent layers in Keras (SimpleRNN, LSTM, and GRU) can be run in two differ-ent modes: they can return either full sequences of successive outputs for each time-step (a rank-3 tensor of shape (batch_size, timesteps, output_features)) orreturn only the last output for each input sequence (a rank-2 tensor of shape (batch_size, output_features)). These two modes are controlled by the return_sequencesconstructor argument. Let’s look at an example that uses SimpleRNN and returns onlythe output at the last timestep.>>> num_features = 14 >>> steps = 120 >>> inputs = keras.Input(shape=(steps, num_features))>>> outputs = layers.SimpleRNN(16, return_sequences=False)(inputs)>>> print(outputs.shape)(None, 16)Listing 10.16 An RNN layer that can process sequences of any length Listing 10.17 An RNN layer that returns only its last output stepNote thatreturn_sequences=Falseis the default. 297Understanding recurrent neural networksThe following example returns the full state sequence.>>> num_features = 14 >>> steps = 120 >>> inputs = keras.Input(shape=(steps, num_features))>>> outputs = layers.SimpleRNN(16, return_sequences=True)(inputs)>>> print(outputs.shape)(120, 16)It’s sometimes useful to stack several recurrent layers one after the other in order toincrease the representational power of a network. In such a setup, you have to get allof the intermediate layers to return a full sequence of outputs.inputs = keras.Input(shape=(steps, num_features))x = layers.SimpleRNN(16, return_sequences=True)(inputs)x = layers.SimpleRNN(16, return_sequences=True)(x)outputs = layers.SimpleRNN(16)(x)In practice, you’ll rarely work with the SimpleRNN layer. It’s generally too simplistic to beof real use. In particular, SimpleRNN has a major issue: although it should theoreticallybe able to retain at time t information about inputs seen many timesteps before, suchlong-term dependencies prove impossible to learn in practice. This is due to the vanish-ing gradient problem, an effect that is similar to what is observed with non-recurrent net-works (feedforward networks) that are many layers deep: as you keep adding layers to anetwork, the network eventually becomes untrainable. The theoretical reasons for thiseffect were studied by Hochreiter, Schmidhuber, and Bengio in the early 1990s.3 Thankfully, SimpleRNN isn’t the only recurrent layer available in Keras. There aretwo others, LSTM and GRU, which were designed to address these issues. Let’s consider the LSTM layer. The underlying Long Short-Term Memory (LSTM)algorithm was developed by Hochreiter and Schmidhuber in 1997;4 it was the culmi-nation of their research on the vanishing gradient problem. This layer is a variant of the SimpleRNN layer you already know about; it adds a wayto carry information across many timesteps. Imagine a conveyor belt running parallelto the sequence you’re processing. Information from the sequence can jump onto theconveyor belt at any point, be transported to a later timestep, and jump off, intact,when you need it. This is essentially what LSTM does: it saves information for later,thus preventing older signals from gradually vanishing during processing. This shouldListing 10.18 An RNN layer that returns its full output sequence Listing 10.19 Stacking RNN layers 3See, for example, Yoshua Bengio, Patrice Simard, and Paolo Frasconi, “Learning Long-Term Dependencieswith Gradient Descent Is Difficult,” IEEE Transactions on Neural Networks 5, no. 2 (1994).4Sepp Hochreiter and Jürgen Schmidhuber, “Long Short-Term Memory,” Neural Computation 9, no. 8 (1997). 298CHAPTER 10Deep learning for timeseriesremind you of residual connections, which you learned about in chapter 9: it’s prettymuch the same idea. To understand this process in detail, let’s start from the SimpleRNN cell (see fig-ure 10.8). Because you’ll have a lot of weight matrices, index the W and U matrices inthe cell, with the letter o (Wo and Uo) for output. Let’s add to this picture an additional data flow that carries information across time-steps. Call its values at different timesteps c_t, where C stands for carry. This informa-tion will have the following impact on the cell: it will be combined with the inputconnection and the recurrent connection (via a dense transformation: a dot productwith a weight matrix followed by a bias add and the application of an activation func-tion), and it will affect the state being sent to the next timestep (via an activation func-tion and a multiplication operation). Conceptually, the carry dataflow is a way tomodulate the next output and the next state (see figure 10.9). Simple so far....output t-1 output t output t+1 input t-1 input t input t+1...State tState t+1output_t =activation(Wo • input_t +Uo • state_t +bo)Figure 10.8 The starting point of an LSTM layer: a SimpleRNN ...output t-1 output t output t+1 input t-1 input t input t+1...State tState t+1Carry track c t+1 c tc t c tc t-1output_t =activation(Wo • input_t +Uo • state_t +Vo • c_t +bo)Figure 10.9 Going from a SimpleRNN to an LSTM: adding a carry track 299Understanding recurrent neural networksNow the subtlety—the way the next value of the carry dataflow is computed. It involvesthree distinct transformations. All three have the form of a SimpleRNN cell:y=a c t i v a t i o n ( d o t ( s t a t e _ t ,U )+d o t ( i n p u t _ t ,W )+b )But all three transformations have their own weight matrices, which we’ll index withthe letters i, f, and k. Here’s what we have so far (it may seem a bit arbitrary, butbear with me).output_t = activation(dot(state_t, Uo) + dot(input_t, Wo) + dot(c_t, Vo) + bo)i_t = activation(dot(state_t, Ui) + dot(input_t, Wi) + bi)f_t = activation(dot(state_t, Uf) + dot(input_t, Wf) + bf)k_t = activation(dot(state_t, Uk) + dot(input_t, Wk) + bk)We obtain the new carry state (the next c_t) by combining i_t, f_t, and k_t.c_t+1 = i_t * k_t + c_t * f_tAdd this as shown in figure 10.10, and that’s it. Not so complicated—merely a tadcomplex. If you want to get philosophical, you can interpret what each of these operations ismeant to do. For instance, you can say that multiplying c_t and f_t is a way to deliber-ately forget irrelevant information in the carry dataflow. Meanwhile, i_t and k_t pro-vide information about the present, updating the carry track with new information.But at the end of the day, these interpretations don’t mean much, because what theseListing 10.20 Pseudocode details of the LSTM architecture (1/2) Listing 10.21 Pseudocode details of the LSTM architecture (2/2) ...output t-1 output t output t+1 input t-1 input t input t+1...State tState t+1Carry track c t+1 c tc t c tc t-1output_t =activation(Wo•input_t +Uo•state_t +Vo•c_t +bo)ComputenewcarryComputenewcarry Figure 10.10 Anatomy of an LSTM 300CHAPTER 10Deep learning for timeseriesoperations actually do is de te rm i ned b y the co nte nt s of the we ight s pa r a me t er iz i ngthem; and the weights are learned in an end-to-end fashion, starting over with eachtraining round, making it impossible to credit this or that operation with a specificpurpose. The specification of an RNN cell (as just described) determines your hypoth-esis space—the space in which you’ll search for a good model configuration duringtraining—but it doesn’t determine what the cell does; that is up to the cell weights.The same cell with different weights can be doing very different things. So the combi-nation of operations making up an RNN cell is better interpreted as a set of constraintson your search, not as a design in an engineering sense. Arguably, the choice of such constraints—the question of how to implement RNNcells—is better left to optimization algorithms (like genetic algorithms or reinforcement-learning processes) than to human engineers. In the future, that’s how we’ll build ourmodels. In summary: you don’t need to understand anything about the specific archi-tecture of an LSTM cell; as a human, it shouldn’t be your job to understand it. Justkeep in mind what the LSTM cell is meant to do: allow past information to be rein-jected at a later time, thus fighting the vanishing-gradient problem. 10.4 Advanced use of recurrent neural networksSo far you’ve learnedWhat RNNs are and how they workWhat LSTM is, and why it works better on long sequences than a naive RNNHow to use Keras RNN layers to process sequence dataNext, we’ll review a number of more advanced features of RNNs, which can help youget the most out of your deep learning sequence models. By the end of the section,you’ll know most of what there is to know about using recurrent networks with Keras. We’ll cover the following:Recurrent dropout—This is a variant of dropout, used to fight overfitting in recur-rent layers.Stacking recurrent layers—This increases the representational power of the model(at the cost of higher computational loads).Bidirectional recurrent layers—These present the same information to a recurrentnetwork in different ways, increasing accuracy and mitigating forgetting issues.We’ll use these techniques to refine our temperature-forecasting RNN.10.4.1 Using recurrent dropout to fight overfittingLet’s go back to the LSTM-based model we used in section 10.2.5—our first modelable to beat the common-sense baseline. If you look at the training and validationcurves (figure 10.5), it’s evident that the model is quickly overfitting, despite only hav-ing very few units: the training and validation losses start to diverge considerably aftera few epochs. You’re already familiar with a classic technique for fighting this phe-nomenon: dropout, which randomly zeros out input units of a layer in order to break 301Advanced use of recurrent neural networkshappenstance correlations in the training data that the layer is exposed to. But how tocorrectly apply dropout in recurrent networks isn’t a trivial question. It has long been known that applying dropout before a recurrent layer hinderslearning rather than helping with regularization. In 2016, Yarin Gal, as part of his PhDthesis on Bayesian deep learning,5 determined the proper way to use dropout with arecurrent network: the same dropout mask (the same pattern of dropped units)should be applied at every timestep, instead of using a dropout mask that varies ran-domly from timestep to timestep. What’s more, in order to regularize the representa-tions formed by the recurrent gates of layers such as GRU a n d LSTM, a temporallyconstant dropout mask should be applied to the inner recurrent activations of thelayer (a recurrent dropout mask). Using the same dropout mask at every timestepallows the network to properly propagate its learning error through time; a tempo-rally random dropout mask would disrupt this error signal and be harmful to thelearning process. Yarin Gal did his research using Keras and helped build this mechanism directlyinto Keras recurrent layers. Every recurrent layer in Keras has two dropout-relatedarguments: dropout, a float specifying the dropout rate for input units of the layer,and recurrent_dropout, specifying the dropout rate of the recurrent units. Let’s addrecurrent dropout to the LSTM layer of our first LSTM example and see how doing soimpacts overfitting. Thanks to dropout, we won’t need to rely as much on network size for regulariza-tion, so we’ll use an LSTM layer with twice as many units, which should, hopefully, bemore expressive (without dropout, this network would have started overfitting rightaway—try it). Because networks being regularized with dropout always take much lon-ger to fully converge, we’ll train the model for five times as many epochs.inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))x = layers.LSTM(32, recurrent_dropout=0.25)(inputs)x = layers.Dropout(0.5)(x)outputs = layers.Dense(1)(x)model = keras.Model(inputs, outputs) callbacks = [ keras.callbacks.ModelCheckpoint(\"jena_lstm_dropout.keras\", save_best_only=True)]model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])history = model.fit(train_dataset, epochs=50, validation_data=val_dataset, callbacks=callbacks) 5See Yarin Gal, “Uncertainty in Deep Learning,” PhD thesis (2016), http:/ /mng.bz/WBq1.Listing 10.22 Training and evaluating a dropout-regularized LSTMTo regularize the Dense layer, we also add a Dropout layer after the LSTM. 302CHAPTER 10Deep learning for timeseries Figure 10.11 shows the results. Success! We’re no longer overfitting during the first 20epochs. We achieve a validation MAE as low as 2.27 degrees (7% improvement overthe no-learning baseline) and a test MAE of 2.45 degrees (6.5% improvement over thebaseline). Not too bad. RNN runtime performanceRecurrent models with very few parameters, like the ones in this chapter, tend tobe significantly faster on a multicore CPU than on GPU, because they only involvesmall matrix multiplications, and the chain of multiplications is not well paralleliz-able due to the presence of a for loop. But larger RNNs can greatly benefit from aGPU runtime.When using a Keras LSTM or GRU layer on GPU with default keyword arguments, yourlayer will be leveraging a cuDNN kernel, a highly optimized, low-level, NVIDIA-providedimplementation of the underlying algorithm (I mentioned these in the previous chap-ter). As usual, cuDNN kernels are a mixed blessing: they’re fast, but inflexible—if youtry to do anything not supported by the default kernel, you will suffer a dramatic slow-down, which more or less forces you to stick to what NVIDIA happens to provide. Forinstance, recurrent dropout isn’t supported by the LSTM and GRU cuDNN kernels, soadding it to your layers forces the runtime to fall back to the regular TensorFlow imple-mentation, which is generally two to five times slower on GPU (even though its com-putational cost is the same).As a way to speed up your RNN layer when you can’t use cuDNN, you can try unrollingit. Unrolling a for loop consists of removing the loop and simply inlining its contentN times. In the case of the for loop of an RNN, unrolling can help TensorFlow opti-mize the underlying computation graph. However, it will also considerably increaseFigure 10.11 Training and validation loss on the Jena temperature-forecasting task with a dropout-regularized LSTM 303Advanced use of recurrent neural networks 10.4.2 Stacking recurrent layersBecause you’re no longer overfitting but seem to have hit a performance bottleneck,you should consider increasing the capacity and expressive power of the network. Recallthe description of the universal machine learning workflow: it’s generally a good idea toincrease the capacity of your model until overfitting becomes the primary obstacle(assuming you’re already taking basic steps to mitigate overfitting, such as using drop-out). As long as you aren’t overfitting too badly, you’re likely under capacity. Increasing network capacity is typically done by increasing the number of units inthe layers or adding more layers. Recurrent layer stacking is a classic way to buildmore-powerful recurrent networks: for instance, not too long ago the Google Trans-late algorithm was powered by a stack of seven large LSTM layers—that’s huge. To stack recurrent layers on top of each other in Keras, all intermediate layers shouldreturn their full sequence of outputs (a rank-3 tensor) rather than their output at the lasttimestep. As you’ve already learned, this is done by specifying return_sequences=True. In the following example, we’ll try a stack of two dropout-regularized recurrent lay-ers. For a change, we’ll use Gated Recurrent Unit (GRU) layers instead of LSTM. GRUis very similar to LSTM—you can think of it as a slightly simpler, streamlined version ofthe LSTM architecture. It was introduced in 2014 by Cho et al. when recurrent networkswere just starting to gain interest anew in the then-tiny research community.6inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))x = layers.GRU(32, recurrent_dropout=0.5, return_sequences=True)(inputs)x = layers.GRU(32, recurrent_dropout=0.5)(x)x = layers.Dropout(0.5)(x)outputs = layers.Dense(1)(x)model = keras.Model(inputs, outputs) callbacks = [ keras.callbacks.ModelCheckpoint(\"jena_stacked_gru_dropout.keras\", save_best_only=True)]the memory consumption of your RNN—as such, it’s only viable for relatively smallsequences (around 100 steps or fewer). Also, note that you can only do this if thenumber of timesteps in the data is known in advance by the model (that is to say, ifyou pass a shape without any None entries to your initial Input()). It works like this:inputs = keras.Input(shape=(sequence_length, num_features))x = layers.LSTM(32, recurrent_dropout=0.2, unroll=True)(inputs) Listing 10.23 Training and evaluating a dropout-regularized, stacked GRU model 6See Cho et al., “On the Properties of Neural Machine Translation: Encoder-Decoder Approaches” (2014),https:/ /arxiv.org/abs/1409.1259.sequence_length cannot be None.Pass unroll=True to enable unrolling. 304CHAPTER 10Deep learning for timeseriesmodel.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])history = model.fit(train_dataset, epochs=50, validation_data=val_dataset, callbacks=callbacks)model = keras.models.load_model(\"jena_stacked_gru_dropout.keras\")print(f\"Test MAE: {model.evaluate(test_dataset)[1]:.2f}\")Figure 10.12 shows the results. We achieve a test MAE of 2.39 degrees (an 8.8%improvement over the baseline). You can see that the added layer does improve theresults a bit, though not dramatically. You may be seeing diminishing returns fromincreasing network capacity at this point. 10.4.3 Using bidirectional RNNsThe last technique we’ll look at in this section is the bidirectional RNN. A bidirectionalRNN is a common RNN variant that can offer greater performance than a regularRNN on certain tasks. It’s frequently used in natural language processing—you couldcall it the Swiss Army knife of deep learning for natural language processing. R N N s a r e n o t a b l y o r d e r - d e p e n d e n t : t h e y p r o c e s s t h e t i m e s t e p s o f t h e i r i n p u tsequences in order, and shuffling or reversing the timesteps can completely changethe representations the RNN extracts from the sequence. This is precisely the reasonthey perform well on problems where order is meaningful, such as the temperature-forecasting problem. A bidirectional RNN exploits the order sensitivity of RNNs: itFigure 10.12 Training and validation loss on the Jena temperature-forecasting task with a stacked GRU network 305Advanced use of recurrent neural networksuses two regular RNNs, such as the GRU and LSTM layers you’re already familiar with,each of which processes the input sequence in one direction (chronologically andantichronologically), and then merges their representations. By processing a sequenceboth ways, a bidirectional RNN can catch patterns that may be overlooked by a unidi-rectional RNN. Remarkably, the fact that the RNN layers in this section have processed sequencesin chronological order (with older timesteps first) may have been an arbitrary decision.At least, it’s a decision we’ve made no attempt to question so far. Could the RNNs haveperformed well enough if they processed input sequences in antichronological order, forinstance (with newer timesteps first)? Let’s try this and see what happens. All you need todo is write a variant of the data generator where the input sequences are reverted alongthe time dimension (replace the last line with yield samples[:, ::-1, :], targets).Training the same LSTM-based model that you used in the first experiment in this sec-tion, you get the results shown in figure 10.13. The reversed-order LSTM strongly underperforms even the common-sense baseline,indicating that in this case, chronological processing is important to the success of theapproach. This makes perfect sense: the underlying LSTM layer will typically be betterat remembering the recent past than the distant past, and naturally the more recentweather data points are more predictive than older data points for the problem (that’swhat makes the common-sense baseline fairly strong). Thus the chronological versionof the layer is bound to outperform the reversed-order version.Figure 10.13 Training and validation loss on the Jena temperature-forecasting task with an LSTM trained on reversed sequences 306CHAPTER 10Deep learning for timeseries However, this isn’t true for many other problems, including natural language: intui-tively, the importance of a word in understanding a sentence isn’t usually dependent onits position in the sentence. On text data, reversed-order processing works just as well aschronological processing—you can read text backwards just fine (try it!). Althoughword order does matter in understanding language, which order you use isn’t crucial. Importantly, an RNN trained on reversed sequences will learn different repre-sentations than one trained on the original sequences, much as you would have dif-ferent mental models if time flowed backward in the real world—if you lived a lifewhere you died on your first day and were born on your last day. In machine learn-ing, representations that are different yet useful are always worth exploiting, and themore they differ, the better: they offer a new angle from which to look at your data,capturing aspects of the data that were missed by other approaches, and thus theycan help boost performance on a task. This is the intuition behind ensembling, a con-cept we’ll explore in chapter 13. A b i d i r e c t i o n a l R N N e x p l o i t s t h i s i d e a t o i m p r o v e o n t h e p e r f o r m a n c e o fchronological-order RNNs. It looks at its input sequence both ways (see figure 10.14),obtaining potentially richer representations and capturing patterns that may havebeen missed by the chronological-order version alone. To instantiate a bidirectional RNN in Keras, you use the Bidirectional layer, whichtakes as its first argument a recurrent layer instance. Bidirectional creates a second,separate instance of this recurrent layer and uses one instance for processing theinput sequences in chronological order and the other instance for processing theinput sequences in reversed order. You can try it on our temperature-forecasting task.inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))x = layers.Bidirectional(layers.LSTM(16))(inputs)outputs = layers.Dense(1)(x)model = keras.Model(inputs, outputs) Listing 10.24 Training and evaluating a bidirectional LSTMMerge (add,concatenate)Input data a, b, c, d, ea, b, c, d, ee, d, c, b, aRNNRNNFigure 10.14 How a bidirectional RNN layer works 307Advanced use of recurrent neural networksmodel.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])history = model.fit(train_dataset, epochs=10, validation_data=val_dataset)You’ll find that it doesn’t perform as well as the plain LSTM layer. It’s easy to under-stand why: all the predictive capacity must come from the chronological half of thenetwork, because the antichronological half is known to be severely underperformingon this task (again, because the recent past matters much more than the distant past,in this case). At the same time, the presence of the antichronological half doubles thenetwork’s capacity and causes it to start overfitting much earlier. However, bidirectional RNNs are a great fit for text data, or any other kind of datawhere order matters, yet where which order you use doesn’t matter. In fact, for a whilein 2016, bidirectional LSTMs were considered the state of the art on many natural lan-guage processing tasks (before the rise of the Transformer architecture, which youwill learn about in the next chapter). 10.4.4 Going even furtherThere are many other things you could try in order to improve performance on thetemperature-forecasting problem:Adjust the number of units in each recurrent layer in the stacked setup, as wellas the amount of dropout. The current choices are largely arbitrary and thusprobably suboptimal.Adjust the learning rate used by the RMSprop o p t i m i z e r , o r t r y a d i f f e r e n toptimizer.Try using a stack of Dense layers as the regressor on top of the recurrent layer,instead of a single Dense layer.Improve the input to the model: try using longer or shorter sequences or a dif-ferent sampling rate, or start doing feature engineering.As always, deep learning is more an art than a science. We can provide guidelines thatsuggest what is likely to work or not work on a given problem, but, ultimately, everydataset is unique; you’ll have to evaluate different strategies empirically. There is cur-rently no theory that will tell you in advance precisely what you should do to optimallysolve a problem. You must iterate. In my experience, improving on the no-learning baseline by about 10% is likelythe best you can do with this dataset. This isn’t so great, but these results make sense:while near-future weather is highly predictable if you have access to data from a widegrid of different locations, it’s not very predictable if you only have measurementsfrom a single location. The evolution of the weather where you are depends on cur-rent weather patterns in surrounding locations. 308CHAPTER 10Deep learning for timeseries SummaryAs you first learned in chapter 5, when approaching a new problem, it’s good tofirst establish common-sense baselines for your metric of choice. If you don’thave a baseline to beat, you can’t tell whether you’re making real progress.Try simple models before expensive ones, to make sure the additional expenseis justified. Sometimes a simple model will turn out to be your best option.When you have data where ordering matters, and in particular for timeseriesdata, recurrent networks are a great fit and easily outperform models that first flat-ten the temporal data. The two essential RNN layers available in Keras are theLSTM layer and the GRU layer.To use dropout with recurrent networks, you should use a time-constant drop-out mask and recurrent dropout mask. These are built into Keras recurrent lay-ers, so all you have to do is use the recurrent_dropout arguments of recurrentlayers.Stacked RNNs provide more representational power than a single RNN layer.They’re also much more expensive and thus not always worth it. Although theyoffer clear gains on complex problems (such as machine translation), they maynot always be relevant to smaller, simpler problems.Markets and machine learningSome readers are bound to want to take the techniques I’ve introduced here and trythem on the problem of forecasting the future price of securities on the stock market(or currency exchange rates, and so on). However, markets have very different statis-tical characteristics than natural phenomena such as weather patterns. When it comesto markets, past performance is not a good predictor of future returns—looking in therear-view mirror is a bad way to drive. Machine learning, on the other hand, is appli-cable to datasets where the past is a good predictor of the future, like weather, elec-tricity consumption, or foot traffic at a store.Always remember that all trading is fundamentally information arbitrage: gaining anadvantage by leveraging data or insights that other market participants are missing.Trying to use well-known machine learning techniques and publicly available data tobeat the markets is effectively a dead end, since you won’t have any informationadvantage compared to everyone else. You’re likely to waste your time and resourceswith nothing to show for it. 309Deep learning for text 11.1 Natural language processing: The bird’s eye viewIn computer science, we refer to human languages, like English or Mandarin, as“natural” languages, to distinguish them from languages that were designed formachines, like Assembly, LISP, or XML. Every machine language was designed: itsstarting point was a human engineer writing down a set of formal rules to describewhat statements you could make in that language and what they meant. Rules camefirst, and people only started using the language once the rule set was complete.With human language, it’s the reverse: usage comes first, rules arise later. Naturallanguage was shaped by an evolution process, much like biological organisms—that’s what makes it “natural.” Its “rules,” like the grammar of English, were formal-ized after the fact and are often ignored or broken by its users. As a result, whileThis chapter coversPreprocessing text data for machine learning applicationsBag-of-words approaches and sequence-modeling approaches for text processingThe Transformer architectureSequence-to-sequence learning 310CHAPTER 11Deep learning for textmachine-readable language is highly structured and rigorous, using precise syntacticrules to weave together exactly defined concepts from a fixed vocabulary, natural lan-guage is messy—ambiguous, chaotic, sprawling, and constantly in flux. C r e a t i n g a l g o r i t h m s t h a t c a n m a k e s e n s e o f n a t u r a l l a n gu a g e i s a b i g d e a l : l a n -guage, and in particular text, underpins most of our communications and our cul-tural production. The internet is mostly text. Language is how we store almost all ofour knowledge. Our very thoughts are largely built upon language. However, the abil-ity to understand natural language has long eluded machines. Some people oncenaively thought that you could simply write down the “rule set of English,” much likeone can write down the rule set of LISP. Early attempts to build natural language pro-cessing (NLP) systems were thus made through the lens of “applied linguistics.” Engi-neers and linguists would handcraft complex sets of rules to perform basic machinetranslation or create simple chatbots—like the famous ELIZA program from the1960s, which used pattern matching to sustain very basic conversation. But language isa rebellious thing: it’s not easily pliable to formalization. After several decades ofeffort, the capabilities of these systems remained disappointing. H a n d c r a f t e d r u l e s h e l d o u t a s t h e d o m in a n t a p p r o a c h w e l l i n t o t h e 1 9 9 0 s . B u tstarting in the late 1980s, faster computers and greater data availability started makinga better alternative viable. When you find yourself building systems that are big pilesof ad hoc rules, as a clever engineer, you’re likely to start asking: “Could I use a corpusof data to automate the process of finding these rules? Could I search for the ruleswithin some kind of rule space, instead of having to come up with them myself?” Andjust like that, you’ve graduated to doing machine learning. And so, in the late 1980s,we started seeing machine learning approaches to natural language processing. Theearliest ones were based on decision trees—the intent was literally to automate thedevelopment of the kind of if/then/else rules of previous systems. Then statisticalapproaches started gaining speed, starting with logistic regression. Over time, learnedparametric models fully took over, and linguistics came to be seen as more of a hin-drance than a useful tool. Frederick Jelinek, an early speech recognition researcher,joked in the 1990s: “Every time I fire a linguist, the performance of the speech recog-nizer goes up.” That’s what modern NLP is about: using machine learning and large datasets togive computers the ability not to understand language, which is a more lofty goal, butto ingest a piece of language as input and return something useful, like predicting thefollowing:“What’s the topic of this text?” (text classification)“Does this text contain abuse?” (content filtering)“Does this text sound positive or negative?” (sentiment analysis)“What should be the next word in this incomplete sentence?” (language modeling)“How would you say this in German?” (translation)“How would you summarize this article in one paragraph?” (summarization)etc. 311Preparing text dataOf course, keep in mind throughout this chapter that the text-processing models youwill train won’t possess a human-like understanding of language; rather, they simplylook for statistical regularities in their input data, which turns out to be sufficient toperform well on many simple tasks. In much the same way that computer vision is pat-tern recognition applied to pixels, NLP is pattern recognition applied to words, sen-tences, and paragraphs. The toolset of NLP—decision trees, logistic regression—only saw slow evolutionfrom the 1990s to the early 2010s. Most of the research focus was on feature engineer-ing. When I won my first NLP competition on Kaggle in 2013, my model was, youguessed it, based on decision trees and logistic regression. However, around 2014–2015, things started changing at last. Multiple researchers began to investigate thelanguage-understanding capabilities of recurrent neural networks, in particular LSTM—a sequence-processing algorithm from the late 1990s that had stayed under the radaruntil then. In early 2015, Keras made available the first open source, easy-to-use implementa-tion of LSTM, just at the start of a massive wave of renewed interest in recurrent neu-ral networks—until then, there had only been “research code” that couldn’t be readilyreused. Then from 2015 to 2017, recurrent neural networks dominated the boomingNLP scene. Bidirectional LSTM models, in particular, set the state of the art on manyimportant tasks, from summarization to question-answering to machine translation. Finally, around 2017–2018, a new architecture rose to replace RNNs: the Trans-former, which you will learn about in the second half of this chapter. Transformersunlocked considerable progress across the field in a short period of time, and todaymost NLP systems are based on them. Let’s dive into the details. This chapter will take you from the very basics to doingmachine translation with a Transformer. 11.2 Preparing text dataDeep learning models, being differentiable functions, can only process numeric ten-sors: they can’t take raw text as input. Vectorizing text is the process of transformingtext into numeric tensors. Text vectorization processes come in many shapes andforms, but they all follow the same template (see figure 11.1):First, you standardize the text to make it easier to process, such as by convertingit to lowercase or removing punctuation.You split the text into units (called tokens), such as characters, words, or groupsof words. This is called tokenization.You convert each such token into a numerical vector. This will usually involvefirst indexing all tokens present in the data.Let’s review each of these steps. 312CHAPTER 11Deep learning for text 11.2.1 Text standardizationConsider these two sentences:“sunset came. i was staring at the Mexico sky. Isnt nature splendid??”“Sunset came; I stared at the México sky. Isn’t nature splendid?”They’re very similar—in fact, they’re almost identical. Yet, if you were to convert themto byte strings, they would end up with very different representations, because “i” and“I” are two different characters, “Mexico” and “México” are two different words, “isnt”isn’t “isn’t,” and so on. A machine learning model doesn’t know a priori that “i” and“I” are the same letter, that “é” is an “e” with an accent, or that “staring” and “stared”are two forms of the same verb. T e x t s t a n d a r d i z a t i o n i s a b a s i c f o r m o f f e a t u r e e n g i n e e r i n g t h a t a i m s t o e r a s eencoding differences that you don’t want your model to have to deal with. It’s notexclusive to machine learning, either—you’d have to do the same thing if you werebuilding a search engine. One of the simplest and most widespread standardization schemes is “convert tolowercase and remove punctuation characters.” Our two sentences would become“sunset came i was staring at the mexico sky isnt nature splendid”“sunset came i stared at the méxico sky isnt nature splendid”the cat sat on the mat\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"3, 26, 65, 9, 3, 133Standardized textTokensToken indicesVectorencodingof indicesTokenizationThe cat sat on the mat.TextStandardization IndexingOne-hot encoding or embedding000100010010010100000100100001000010Figure 11.1 From raw text to vectors 313Preparing text dataMuch closer already. Another common transformation is to convert special charactersto a standard form, such as replacing “é” with “e,” “æ” with “ae,” and so on. Our token“méxico” would then become “mexico”. Lastly, a much more advanced standardization pattern that is more rarely used in amachine learning context is stemming: converting variations of a term (such as differ-ent conjugated forms of a verb) into a single shared representation, like turning“caught” and “been catching” into “[catch]” or “cats” into “[cat]”. With stemming,“was staring” and “stared” would become something like “[stare]”, and our two similarsentences would finally end up with an identical encoding:“sunset came i [stare] at the mexico sky isnt nature splendid”With these standardization techniques, your model will require less training data andwill generalize better—it won’t need abundant examples of both “Sunset” and “sun-set” to learn that they mean the same thing, and it will be able to make sense of “Méx-ico” even if it has only seen “mexico” in its training set. Of course, standardization mayalso erase some amount of information, so always keep the context in mind: forinstance, if you’re writing a model that extracts questions from interview articles, itshould definitely treat “?” as a separate token instead of dropping it, because it’s a use-ful signal for this specific task. 11.2.2 Text splitting (tokenization)Once your text is standardized, you need to break it up into units to be vectorized(tokens), a step called tokenization. You could do this in three different ways:Word-level tokenization—Where tokens are space-separated (or punctuation-separated) substrings. A variant of this is to further split words into subwordswhen applicable—for instance, treating “staring” as “star+ing” or “called” as“call+ed.”N-gram tokenization—Where tokens are groups of N consecutive words. Forinstance, “the cat” or “he was” would be 2-gram tokens (also called bigrams).Character-level tokenization—Where each character is its own token. In practice,this scheme is rarely used, and you only really see it in specialized contexts, liketext generation or speech recognition.In general, you’ll always use either word-level or N-gram tokenization. There are twokinds of text-processing models: those that care about word order, called sequence mod-els, and those that treat input words as a set, discarding their original order, calledbag-of-words models. If you’re building a sequence model, you’ll use word-level tokeni-zation, and if you’re building a bag-of-words model, you’ll use N-gram tokenization.N-grams are a way to artificially inject a small amount of local word order informationinto the model. Throughout this chapter, you’ll learn more about each type of modeland when to use them. 314CHAPTER 11Deep learning for text 11.2.3 Vocabulary indexingOnce your text is split into tokens, you need to encode each token into a numericalrepresentation. You could potentially do this in a stateless way, such as by hashing eachtoken into a fixed binary vector, but in practice, the way you’d go about it is to buildan index of all terms found in the training data (the “vocabulary”), and assign aunique integer to each entry in the vocabulary. Something like this:vocabulary = {} for text in dataset: text = standardize(text) tokens = tokenize(text) for token in tokens: if token not in vocabulary: vocabulary[token] = len(vocabulary)Understanding N-grams and bag-of-wordsWord N-grams are groups of N (or fewer) consecutive words that you can extract froma sentence. The same concept may also be applied to characters instead of words.Here’s a simple example. Consider the sentence “the cat sat on the mat.” It may bedecomposed into the following set of 2-grams:{\"the\", \"the cat\", \"cat\", \"cat sat\", \"sat\", \"sat on\", \"on\", \"on the\", \"the mat\", \"mat\"}It may also be decomposed into the following set of 3-grams:{\"the\", \"the cat\", \"cat\", \"cat sat\", \"the cat sat\", \"sat\", \"sat on\", \"on\", \"cat sat on\", \"on the\", \"sat on the\", \"the mat\", \"mat\", \"on the mat\"}Such a set is called a bag-of-2-grams or bag-of-3-grams, respectively. The term “bag”here refers to the fact that you’re dealing with a set of tokens rather than a list orsequence: the tokens have no specific order. This family of tokenization methods iscalled bag-of-words (or bag-of-N-grams).Because bag-of-words isn’t an order-preserving tokenization method (the tokens gen-erated are understood as a set, not a sequence, and the general structure of the sen-tences is lost), it tends to be used in shallow language-processing models rather thanin deep learning models. Extracting N-grams is a form of feature engineering, anddeep learning sequence models do away with this manual approach, replacing it withhierarchical feature learning. One-dimensional convnets, recurrent neural networks,and Transformers are capable of learning representations for groups of words andcharacters without being explicitly told about the existence of such groups, by lookingat continuous word or character sequences. 315Preparing text dataYou can then convert that integer into a vector encoding that can be processed by aneural network, like a one-hot vector:def one_hot_encode_token(token): vector = np.zeros((len(vocabulary),)) token_index = vocabulary[token] vector[token_index] = 1 return vectorNote that at this step it’s common to restrict the vocabulary to only the top 20,000 or30,000 most common words found in the training data. Any text dataset tends to fea-ture an extremely large number of unique terms, most of which only show up once ortwice—indexing those rare terms would result in an excessively large feature space,where most features would have almost no information content. Remember when you were training your first deep learning models on the IMDBdataset in chapters 4 and 5? The data you were using from keras.datasets.imdb wasalready preprocessed into sequences of integers, where each integer stood for a givenword. Back then, we used the setting num_words=10000, in order to restrict our vocab-ulary to the top 10,000 most common words found in the training data. Now, there’s an important detail here that we shouldn’t overlook: when we lookup a new token in our vocabulary index, it may not necessarily exist. Your trainingdata may not have contained any instance of the word “cherimoya” (or maybe youexcluded it from your index because it was too rare), so doing token_index =vocabulary[\"cherimoya\"] may result in a KeyError. To handle this, you should usean “out of vocabulary” index (abbreviated as OOV index)—a catch-all for any tokenthat wasn’t in the index. It’s usually index 1: you’re actually doing token_index =vocabulary.get(token, 1). When decoding a sequence of integers back into words,you’ll replace 1 with something like “[UNK]” (which you’d call an “OOV token”). “Why use 1 and not 0?” you may ask. That’s because 0 is already taken. There aretwo special tokens that you will commonly use: the OOV token (index 1), and themask token (index 0). While the OOV token means “here was a word we did not recog-nize,” the mask token tells us “ignore me, I’m not a word.” You’d use it in particular topad sequence data: because data batches need to be contiguous, all sequences in abatch of sequence data must have the same length, so shorter sequences should bepadded to the length of the longest sequence. If you want to make a batch of data withthe sequences [5, 7, 124, 4, 89] and [8, 34, 21], it would have to look like this:[[5, 7, 124, 4, 89] [8, 34, 21, 0, 0]]The batches of integer sequences for the IMDB dataset that you worked with in chap-ters 4 and 5 were padded with zeros in this way. 316CHAPTER 11Deep learning for text11.2.4 Using the TextVectorization layerEvery step I’ve introduced so far would be very easy to implement in pure Python.Maybe you could write something like this:import string class Vectorizer: def standardize(self, text): text = text.lower() return \"\".join(char for char in text if char not in string.punctuation) def tokenize(self, text): text = self.standardize(text) return text.split() def make_vocabulary(self, dataset): self.vocabulary = {\"\": 0, \"[UNK]\": 1} for text in dataset: text = self.standardize(text) tokens = self.tokenize(text) for token in tokens: if token not in self.vocabulary: self.vocabulary[token] = len(self.vocabulary) self.inverse_vocabulary = dict( (v, k) for k, v in self.vocabulary.items()) def encode(self, text): text = self.standardize(text) tokens = self.tokenize(text) return [self.vocabulary.get(token, 1) for token in tokens] def decode(self, int_sequence): return \" \".join( self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence) vectorizer = Vectorizer()dataset = [ \"I write, erase, rewrite\", \"Erase again, and then\", \"A poppy blooms.\",]vectorizer.make_vocabulary(dataset)It does the job:>>> test_sentence = \"I write, rewrite, and still rewrite again\" >>> encoded_sentence = vectorizer.encode(test_sentence)>>> print(encoded_sentence)[2, 3, 5, 7, 1, 5, 6]>>> decoded_sentence = vectorizer.decode(encoded_sentence)>>> print(decoded_sentence)\"i write rewrite and [UNK] rewrite again\" Haiku by poet Hokushi 317Preparing text dataHowever, using something like this wouldn’t be very performant. In practice, you’llwork with the Keras TextVectorization layer, which is fast and efficient and can bedropped directly into a tf.data pipeline or a Keras model. This is what the TextVectorization layer looks like:from tensorflow.keras.layers import TextVectorizationtext_vectorization = TextVectorization( output_mode=\"int\", )By default, the TextVectorization layer will use the setting “convert to lowercase andremove punctuation” for text standardization, and “split on whitespace” for tokeniza-tion. But importantly, you can provide custom functions for standardization and toke-nization, which means the layer is flexible enough to handle any use case. Note thatsuch custom functions should operate on tf.string t e n s o r s , n o t r e g u l a r P y t h o nstrings! For instance, the default layer behavior is equivalent to the following:import re import string import tensorflow as tf def custom_standardization_fn(string_tensor): lowercase_string = tf.strings.lower(string_tensor) return tf.strings.regex_replace( lowercase_string, f\"[{re.escape(string.punctuation)}]\", \"\") def custom_split_fn(string_tensor): return tf.strings.split(string_tensor) text_vectorization = TextVectorization( output_mode=\"int\", standardize=custom_standardization_fn, split=custom_split_fn,)To index the vocabulary of a text corpus, just call the adapt() method of the layerwith a Dataset object that yields strings, or just with a list of Python strings:dataset = [ \"I write, erase, rewrite\", \"Erase again, and then\", \"A poppy blooms.\",]text_vectorization.adapt(dataset)Note that you can retrieve the computed vocabulary via get_vocabulary()—this canbe useful if you need to convert text encoded as integer sequences back into words.The first two entries in the vocabulary are the mask token (index 0) and the OOVtoken (index 1). Entries in the vocabulary list are sorted by frequency, so with a real-world dataset, very common words like “the” or “a” would come first.Configures the layer to return sequences of words encoded as integer indices. There are several other output modes available, which you will see in action in a bit. Convertstrings tolowercase.Replace punctuation characters with the empty string.Split strings on whitespace. 318CHAPTER 11Deep learning for text>>> text_vectorization.get_vocabulary()[\"\", \"[UNK]\", \"erase\", \"write\", ...]For a demonstration, let’s try to encode and then decode an example sentence:>>> vocabulary = text_vectorization.get_vocabulary()>>> test_sentence = \"I write, rewrite, and still rewrite again\" >>> encoded_sentence = text_vectorization(test_sentence)>>> print(encoded_sentence)tf.Tensor([ 7 3 5 9 1 5 10], shape=(7,), dtype=int64)>>> inverse_vocab = dict(enumerate(vocabulary))>>> decoded_sentence = \" \".join(inverse_vocab[int(i)] for i in encoded_sentence)>>> print(decoded_sentence)\"i write rewrite and [UNK] rewrite again\" Listing 11.1 Displaying the vocabulary Using the TextVectorization layer in a tf.data pipeline or as part of a modelImportantly, because TextVectorization is mostly a dictionary lookup operation, itcan’t be executed on a GPU (or TPU)—only on a CPU. So if you’re training your modelon a GPU, your TextVectorization layer will run on the CPU before sending its out-put to the GPU. This has important performance implications.There are two ways we could use our TextVectorization layer. The first option isto put it in the tf.data pipeline, like this:int_sequence_dataset = string_dataset.map( text_vectorization, num_parallel_calls=4) The second option is to make it part of the model (after all, it’s a Keras layer), like this:text_input = keras.Input(shape=(), dtype=\"string\") vectorized_text = text_vectorization(text_input) embedded_input = keras.layers.Embedding(...)(vectorized_text) output = ... model = keras.Model(text_input, output) There’s an important difference between the two: if the vectorization step is part ofthe model, it will happen synchronously with the rest of the model. This means thatat each training step, the rest of the model (placed on the GPU) will have to wait forthe output of the TextVectorization layer (placed on the CPU) to be ready in orderto get to work. Meanwhile, putting the layer in the tf.data pipeline enables you tostring_dataset would be a dataset that yields string tensors.The num_parallel_calls argument is used to parallelize the map() call across multiple CPU cores.Create a symbolic inputthat expects strings.Apply the text vectorization layer to it.You can keep chaining new layers on top—just your regular Functional API model. 319Two approaches for representing groups of words: Sets and sequences You’ve now learned everything you need to know about text preprocessing—let’smove on to the modeling stage. 11.3 Two approaches for representing groups of words: Sets and sequencesHow a machine learning model should represent individual words is a relatively uncon-troversial question: they’re categorical features (values from a predefined set), and weknow how to handle those. They should be encoded as dimensions in a feature space,or as category vectors (word vectors in this case). A much more problematic question,however, is how to encode the way words are woven into sentences: word order. The problem of order in natural language is an interesting one: unlike the steps ofa timeseries, words in a sentence don’t have a natural, canonical order. Different lan-guages order similar words in very different ways. For instance, the sentence structureof English is quite different from that of Japanese. Even within a given language, youcan typically say the same thing in different ways by reshuffling the words a bit. Evenfurther, if you fully randomize the words in a short sentence, you can still largely fig-ure out what it was saying—though in many cases significant ambiguity seems to arise.Order is clearly important, but its relationship to meaning isn’t straightforward. How to represent word order is the pivotal question from which different kinds ofNLP architectures spring. The simplest thing you could do is just discard order andtreat text as an unordered set of words—this gives you bag-of-words models. You couldalso decide that words should be processed strictly in the order in which they appear,one at a time, like steps in a timeseries—you could then leverage the recurrent modelsfrom the last chapter. Finally, a hybrid approach is also possible: the Transformerdo asynchronous preprocessing of your data on CPU: while the GPU runs the modelon one batch of vectorized data, the CPU stays busy by vectorizing the next batch ofraw strings.So if you’re training the model on GPU or TPU, you’ll probably want to go with the firstoption to get the best performance. This is what we will do in all practical examplesthroughout this chapter. When training on a CPU, though, synchronous processing isfine: you will get 100% utilization of your cores regardless of which option you go with.Now, if you were to export our model to a production environment, you would want toship a model that accepts raw strings as input, like in the code snippet for the secondoption above—otherwise you would have to reimplement text standardization andtokenization in your production environment (maybe in JavaScript?), and you wouldface the risk of introducing small preprocessing discrepancies that would hurt themodel’s accuracy. Thankfully, the TextVectorization layer enables you to includetext preprocessing right into your model, making it easier to deploy—even if you wereoriginally using the layer as part of a tf.data pipeline. In the sidebar “Exporting amodel that processes raw strings,” you’ll learn how to export an inference-onlytrained model that incorporates text preprocessing. 320CHAPTER 11Deep learning for textarchitecture is technically order-agnostic, yet it injects word-position information intothe representations it processes, which enables it to simultaneously look at differentparts of a sentence (unlike RNNs) while still being order-aware. Because they take intoaccount word order, both RNNs and Transformers are called sequence models. Historically, most early applications of machine learning to NLP just involvedbag-of-words models. Interest in sequence models only started rising in 2015, with therebirth of recurrent neural networks. Today, both approaches remain relevant. Let’ssee how they work, and when to leverage which. We’ll demonstrate each approach on a well-known text classification benchmark:the IMDB movie review sentiment-classification dataset. In chapters 4 and 5, youworked with a prevectorized version of the IMDB dataset; now, let’s process the rawIMDB text data, just like you would do when approaching a new text-classificationproblem in the real world.11.3.1 Preparing the IMDB movie reviews dataLet’s start by downloading the dataset from the Stanford page of Andrew Maas anduncompressing it:!curl -O https:/ /ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz!tar -xf aclImdb_v1.tar.gzYou’re left with a directory named aclImdb, with the following structure:aclImdb/...train/......pos/......neg/...test/......pos/......neg/For instance, the train/pos/ directory contains a set of 12,500 text files, each of whichcontains the text body of a positive-sentiment movie review to be used as training data.The negative-sentiment reviews live in the “neg” directories. In total, there are 25,000text files for training and another 25,000 for testing. T h e r e ’ s a l s o a t r a i n / u n s u p s u b d i r e c t o r y i n t h e r e , w h i c h w e d o n ’ t n e e d . L e t ’ sdelete it:!rm -r aclImdb/train/unsupTake a look at the content of a few of these text files. Whether you’re working withtext data or image data, remember to always inspect what your data looks like beforeyou dive into modeling it. It will ground your intuition about what your model is actu-ally doing:!cat aclImdb/train/pos/4077_10.txt 321Two approaches for representing groups of words: Sets and sequencesNext, let’s prepare a validation set by setting apart 20% of the training text files in anew directory, aclImdb/val:import os, pathlib, shutil, random base_dir = pathlib.Path(\"aclImdb\")val_dir = base_dir / \"val\" train_dir = base_dir / \"train\" for category in (\"neg\", \"pos\"): os.makedirs(val_dir / category) files = os.listdir(train_dir / category) random.Random(1337).shuffle(files) num_val_samples = int(0.2 * len(files)) val_files = files[-num_val_samples:] for fname in val_files: shutil.move(train_dir / category / fname, val_dir / category / fname) Remember how, in chapter 8, we used the image_dataset_from_directory utility tocreate a batched Dataset of images and their labels for a directory structure? You cando the exact same thing for text files using the text_dataset_from_directory utility.Let’s create three Dataset objects for training, validation, and testing:from tensorflow import kerasbatch_size = 32 train_ds = keras.utils.text_dataset_from_directory( \"aclImdb/train\", batch_size=batch_size)val_ds = keras.utils.text_dataset_from_directory( \"aclImdb/val\", batch_size=batch_size)test_ds = keras.utils.text_dataset_from_directory( \"aclImdb/test\", batch_size=batch_size)These datasets yield inputs that are TensorFlow tf.string tensors and targets that areint32 tensors encoding the value “0” or “1.”>>> for inputs, targets in train_ds:>>> print(\"inputs.shape:\", inputs.shape)>>> print(\"inputs.dtype:\", inputs.dtype)>>> print(\"targets.shape:\", targets.shape)>>> print(\"targets.dtype:\", targets.dtype)>>> print(\"inputs[0]:\", inputs[0])>>> print(\"targets[0]:\", targets[0])>>> breakinputs.shape: (32,)inputs.dtype: <dtype: \"string\">targets.shape: (32,)targets.dtype: <dtype: \"int32\">Listing 11.2 Displaying the shapes and dtypes of the first batchShuffle the list of training files using a seed, to ensure we get the same validation set every time we run the code.Take 20% of the training files to use for validation.Move the files to aclImdb/val/neg and aclImdb/val/pos. Running this line should output “Found 20000 files belonging to 2 classes”; if you see “Found 70000 files belonging to 3 classes,” it means you forgot to delete the aclImdb/train/unsup directory. 322CHAPTER 11Deep learning for textinputs[0]: tf.Tensor(b\"This string contains the movie review.\", shape=(), dtype=string)targets[0]: tf.Tensor(1, shape=(), dtype=int32)All set. Now let’s try learning something from this data. 11.3.2 Processing words as a set: The bag-of-words approachThe simplest way to encode a piece of text for processing by a machine learningmodel is to discard order and treat it as a set (a “bag”) of tokens. You could either lookat individual words (unigrams), or try to recover some local order information bylooking at groups of consecutive token (N-grams).SINGLE WORDS (UNIGRAMS) WITH BINARY ENCODINGIf you use a bag of single words, the sentence “the cat sat on the mat” becomes{\"cat\", \"mat\", \"on\", \"sat\", \"the\"}The main advantage of this encoding is that you can represent an entire text as a sin-gle vector, where each entry is a presence indicator for a given word. For instance,using binary encoding (multi-hot), you’d encode a text as a vector with as manydimensions as there are words in your vocabulary—with 0s almost everywhere andsome 1s for dimensions that encode words present in the text. This is what we didwhen we worked with text data in chapters 4 and 5. Let’s try this on our task. First, let’s process our raw text datasets with a TextVectorization layer so thatthey yield multi-hot encoded binary word vectors. Our layer will only look at singlewords (that is to say, unigrams). text_vectorization = TextVectorization( max_tokens=20000, output_mode=\"multi_hot\", )text_only_train_ds = train_ds.map(lambda x, y: x) text_vectorization.adapt(text_only_train_ds) binary_1gram_train_ds = train_ds.map( lambda x, y: (text_vectorization(x), y), num_parallel_calls=4) binary_1gram_val_ds = val_ds.map( lambda x, y: (text_vectorization(x), y), num_parallel_calls=4) binary_1gram_test_ds = test_ds.map( lambda x, y: (text_vectorization(x), y), num_parallel_calls=4) Listing 11.3 Preprocessing our datasets with a TextVectorization layerLimit the vocabulary to the 20,000 most frequent words.Otherwise we’d be indexing every word in the training data—potentially tens of thousands of terms that only occur once ortwice and thus aren’t informative. In general, 20,000 is theright vocabulary size for text classification.Encode the output tokens as multi-hot binary vectors.Prepare a dataset that only yields raw text inputs (no labels).Use that dataset to index the dataset vocabulary via the adapt() method.Prepare processed versions of our training, validation, and test dataset.Make sure to specify num_parallel_calls to leverage multiple CPU cores. 323Two approaches for representing groups of words: Sets and sequencesYou can try to inspect the output of one of these datasets.>>> for inputs, targets in binary_1gram_train_ds:>>> print(\"inputs.shape:\", inputs.shape)>>> print(\"inputs.dtype:\", inputs.dtype)>>> print(\"targets.shape:\", targets.shape)>>> print(\"targets.dtype:\", targets.dtype)>>> print(\"inputs[0]:\", inputs[0])>>> print(\"targets[0]:\", targets[0])>>> breakinputs.shape: (32, 20000) inputs.dtype: <dtype: \"float32\">targets.shape: (32,)targets.dtype: <dtype: \"int32\">inputs[0]: tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32) targets[0]: tf.Tensor(1, shape=(), dtype=int32)Next, let’s write a reusable model-building function that we’ll use in all of our experi-ments in this section.from tensorflow import keras from tensorflow.keras import layers def get_model(max_tokens=20000, hidden_dim=16): inputs = keras.Input(shape=(max_tokens,)) x = layers.Dense(hidden_dim, activation=\"relu\")(inputs) x = layers.Dropout(0.5)(x) outputs = layers.Dense(1, activation=\"sigmoid\")(x) model = keras.Model(inputs, outputs) model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"]) return modelFinally, let’s train and test our model.model = get_model()model.summary()callbacks = [ keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\", save_best_only=True)]model.fit(binary_1gram_train_ds.cache(), validation_data=binary_1gram_val_ds.cache(), epochs=10, callbacks=callbacks)model = keras.models.load_model(\"binary_1gram.keras\") print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")Listing 11.4 Inspecting the output of our binary unigram dataset Listing 11.5 Our model-building utility Listing 11.6 Training and testing the binary unigram modelInputs are batches of 20,000-dimensional vectors.These vectors consistentirely of ones and zeros. We call cache() on the datasets to cache them in memory: this way, we will only do the preprocessing once, during the first epoch, and we’ll reuse the preprocessed texts for the following epochs. This can only be done if the data is small enough to fit in memory. 324CHAPTER 11Deep learning for textThis gets us to a test accuracy of 89.2%: not bad! Note that in this case, since the data-set is a balanced two-class classification dataset (there are as many positive samples asnegative samples), the “naive baseline” we could reach without training an actual modelwould only be 50%. Meanwhile, the best score that can be achieved on this datasetwithout leveraging external data is around 95% test accuracy. BIGRAMS WITH BINARY ENCODINGOf course, discarding word order is very reductive, because even atomic concepts canbe expressed via multiple words: the term “United States” conveys a concept that isquite distinct from the meaning of the words “states” and “united” taken separately.For this reason, you will usually end up re-injecting local order information into yourbag-of-words representation by looking at N-grams rather than single words (mostcommonly, bigrams). With bigrams, our sentence becomes{\"the\", \"the cat\", \"cat\", \"cat sat\", \"sat\", \"sat on\", \"on\", \"on the\", \"the mat\", \"mat\"}The TextVectorization layer can be configured to return arbitrary N-grams: bigrams,trigrams, etc. Just pass an ngrams=N argument as in the following listing.text_vectorization = TextVectorization( ngrams=2, max_tokens=20000, output_mode=\"multi_hot\",)Let’s test how our model performs when trained on such binary-encoded bags ofbigrams.text_vectorization.adapt(text_only_train_ds)binary_2gram_train_ds = train_ds.map( lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)binary_2gram_val_ds = val_ds.map( lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)binary_2gram_test_ds = test_ds.map( lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)model = get_model()model.summary()callbacks = [ keras.callbacks.ModelCheckpoint(\"binary_2gram.keras\", save_best_only=True)]Listing 11.7 Configuring the TextVectorization layer to return bigrams Listing 11.8 Training and testing the binary bigram model 325Two approaches for representing groups of words: Sets and sequencesmodel.fit(binary_2gram_train_ds.cache(), validation_data=binary_2gram_val_ds.cache(), epochs=10, callbacks=callbacks)model = keras.models.load_model(\"binary_2gram.keras\")print(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")We’re now getting 90.4% test accuracy, a marked improvement! Turns out local orderis pretty important. BIGRAMS WITH TF-IDF ENCODINGYou can also add a bit more information to this representation by counting how manytimes each word or N-gram occurs, that is to say, by taking the histogram of the wordsover the text:{\"the\": 2, \"the cat\": 1, \"cat\": 1, \"cat sat\": 1, \"sat\": 1, \"sat on\": 1, \"on\": 1, \"on the\": 1, \"the mat: 1\", \"mat\": 1}If you’re doing text classification, knowing how many times a word occurs in a sampleis critical: any sufficiently long movie review may contain the word “terrible” regard-less of sentiment, but a review that contains many instances of the word “terrible” islikely a negative one. Here’s how you’d count bigram occurrences with the TextVectorization layer.text_vectorization = TextVectorization( ngrams=2, max_tokens=20000, output_mode=\"count\")Now, of course, some words are bound to occur more often than others no matterwhat the text is about. The words “the,” “a,” “is,” and “are” will always dominate yourword count histograms, drowning out other words—despite being pretty much uselessfeatures in a classification context. How could we address this? You already guessed it: via normalization. We could just normalize word counts bysubtracting the mean and dividing by the variance (computed across the entire train-ing dataset). That would make sense. Except most vectorized sentences consist almostentirely of zeros (our previous example features 12 non-zero entries and 19,988 zeroentries), a property called “sparsity.” That’s a great property to have, as it dramaticallyreduces compute load and reduces the risk of overfitting. If we subtracted the meanfrom each feature, we’d wreck sparsity. Thus, whatever normalization scheme we useshould be divide-only. What, then, should we use as the denominator? The best prac-tice is to go with something called TF-IDF normalization—TF-IDF stands for “term fre-quency, inverse document frequency.” TF-IDF is so common that it’s built into the TextVectorization layer. All you needto do to start using it is to switch the output_mode argument to \"tf_idf\".Listing 11.9 Configuring the TextVectorization layer to return token counts 326CHAPTER 11Deep learning for text text_vectorization = TextVectorization( ngrams=2, max_tokens=20000, output_mode=\"tf_idf\",)Let’s train a new model with this scheme.text_vectorization.adapt(text_only_train_ds) tfidf_2gram_train_ds = train_ds.map( lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)tfidf_2gram_val_ds = val_ds.map( lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)tfidf_2gram_test_ds = test_ds.map( lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)model = get_model()model.summary()callbacks = [ keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\", save_best_only=True)]model.fit(tfidf_2gram_train_ds.cache(), validation_data=tfidf_2gram_val_ds.cache(), epochs=10, callbacks=callbacks)model = keras.models.load_model(\"tfidf_2gram.keras\")print(f\"Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")Understanding TF-IDF normalizationThe more a given term appears in a document, the more important that term is forunderstanding what the document is about. At the same time, the frequency at whichthe term appears across all documents in your dataset matters too: terms thatappear in almost every document (like “the” or “a”) aren’t particularly informative,while terms that appear only in a small subset of all texts (like “Herzog”) are very dis-tinctive, and thus important. TF-IDF is a metric that fuses these two ideas. It weightsa given term by taking “term frequency,” how many times the term appears in thecurrent document, and dividing it by a measure of “document frequency,” which esti-mates how often the term comes up across the dataset. You’d compute it as follows:def tfidf(term, document, dataset): term_freq = document.count(term) doc_freq = math.log(sum(doc.count(term) for doc in dataset) + 1) return term_freq / doc_freqListing 11.10 Configuring TextVectorization to return TF-IDF-weighted outputs Listing 11.11 Training and testing the TF-IDF bigram modelThe adapt() call will learn the TF-IDF weights in addition to the vocabulary. 327Two approaches for representing groups of words: Sets and sequencesThis gets us an 89.8% test accuracy on the IMDB classification task: it doesn’t seem tobe particularly helpful in this case. However, for many text-classification datasets, itwould be typical to see a one-percentage-point increase when using TF-IDF comparedto plain binary encoding. 11.3.3 Processing words as a sequence: The sequence model approachThese past few examples clearly show that word order matters: manual engineering oforder-based features, such as bigrams, yields a nice accuracy boost. Now remember: thehistory of deep learning is that of a move away from manual feature engineering, towardletting models learn their own features from exposure to data alone. What if, instead ofmanually crafting order-based features, we exposed the model to raw word sequencesand let it figure out such features on its own? This is what sequence models are about. To implement a sequence model, you’d start by representing your input samples assequences of integer indices (one integer standing for one word). Then, you’d mapeach integer to a vector to obtain vector sequences. Finally, you’d feed thesesequences of vectors into a stack of layers that could cross-correlate features from adja-cent vectors, such as a 1D convnet, a RNN, or a Transformer. For some time around 2016–2017, bidirectional RNNs (in particular, bidirectionalLSTMs) were considered to be the state of the art for sequence modeling. Since you’reExporting a model that processes raw stringsIn the preceding examples, we did our text standardization, splitting, and indexing aspart of the tf.data pipeline. But if we want to export a standalone model indepen-dent of this pipeline, we should make sure that it incorporates its own text prepro-cessing (otherwise, you’d have to reimplement in the production environment, whichcan be challenging or can lead to subtle discrepancies between the training data andthe production data). Thankfully, this is easy.Just create a new model that reuses your TextVectorization layer and adds to itthe model you just trained:inputs = keras.Input(shape=(1,), dtype=\"string\") processed_inputs = text_vectorization(inputs) outputs = model(processed_inputs) inference_model = keras.Model(inputs, outputs) The resulting model can process batches of raw strings:import tensorflow as tfraw_text_data = tf.convert_to_tensor([ [\"That was an excellent movie, I loved it.\"],])predictions = inference_model(raw_text_data) print(f\"{float(predictions[0] * 100):.2f} percent positive\")One input sample would be one string.Apply text preprocessing.Apply the previously trained model.Instantiate the end-to-end model. 328CHAPTER 11Deep learning for textalready familiar with this architecture, this is what we’ll use in our first sequence modelexamples. However, nowadays sequence modeling is almost universally done with Trans-formers, which we will cover shortly. Oddly, one-dimensional convnets were neververy popular in NLP, even though, in my own experience, a residual stack of depth-wise-separable 1D convolutions can often achieve comparable performance to a bidi-rectional LSTM, at a greatly reduced computational cost.A FIRST PRACTICAL EXAMPLELet’s try out a first sequence model in practice. First, let’s prepare datasets that returninteger sequences.from tensorflow.keras import layers max_length = 600 max_tokens = 20000 text_vectorization = layers.TextVectorization( max_tokens=max_tokens, output_mode=\"int\", output_sequence_length=max_length, )text_vectorization.adapt(text_only_train_ds)int_train_ds = train_ds.map( lambda x, y: (text_vectorization(x), y)), num_parallel_calls=4)int_val_ds = val_ds.map( lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)int_test_ds = test_ds.map( lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)Next, let’s make a model. The simplest way to convert our integer sequences to vectorsequences is to one-hot encode the integers (each dimension would represent onepossible term in the vocabulary). On top of these one-hot vectors, we’ll add a simplebidirectional LSTM.import tensorflow as tfinputs = keras.Input(shape=(None,), dtype=\"int64\") embedded = tf.one_hot(inputs, depth=max_tokens) x = layers.Bidirectional(layers.LSTM(32))(embedded) x = layers.Dropout(0.5)(x) outputs = layers.Dense(1, activation=\"sigmoid\")(x) model = keras.Model(inputs, outputs)model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])model.summary()Listing 11.12 Preparing integer sequence datasets Listing 11.13 A sequence model built on one-hot encoded vector sequencesIn order to keep a manageable input size, we’ll truncate the inputs after the first 600 words. This is a reasonable choice, since the average review length is 233 words, and only 5% of reviews are longer than 600 words. One input is a sequence of integers.Encode the integers into binary 20,000-dimensional vectors.Add a bidirectional LSTM.Finally, add aclassificationlayer. 329Two approaches for representing groups of words: Sets and sequencesNow, let’s train our model.callbacks = [ keras.callbacks.ModelCheckpoint(\"one_hot_bidir_lstm.keras\", save_best_only=True)]model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)model = keras.models.load_model(\"one_hot_bidir_lstm.keras\") print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")A first observation: this model trains very slowly, especially compared to the light-weight model of the previous section. This is because our inputs are quite large: eachinput sample is encoded as a matrix of size (600, 20000) ( 6 0 0 w o r d s p e r s a m p l e ,20,000 possible words). That’s 12,000,000 floats for a single movie review. Our bidirec-tional LSTM has a lot of work to do. Second, the model only gets to 87% test accu-racy—it doesn’t perform nearly as well as our (very fast) binary unigram model. Clearly, using one-hot encoding to turn words into vectors, which was the simplestthing we could do, wasn’t a great idea. There’s a better way: word embeddings. UNDERSTANDING WORD EMBEDDINGSCrucially, when you encode something via one-hot encoding, you’re making a feature-engineering decision. You’re injecting into your model a fundamental assumptionabout the structure of your feature space. That assumption is that the different tokensyou’re encoding are all independent from each other: indeed, one-hot vectors are all orthogo-nal to one another. And in the case of words, that assumption is clearly wrong. Wordsform a structured space: they share information with each other. The words “movie”and “film” are interchangeable in most sentences, so the vector that represents“movie” should not be orthogonal to the vector that represents “film”—they should bethe same vector, or close enough. T o g e t a b i t m o r e a b s t r a c t , t h e geometric relationship b e t w e e n t w o w o r d v e c t o r sshould reflect the semantic relationship between these words. For instance, in a reason-able word vector space, you would expect synonyms to be embedded into similar wordvectors, and in general, you would expect the geometric distance (such as the cosinedistance or L2 distance) between any two word vectors to relate to the “semantic dis-tance” between the associated words. Words that mean different things should lie faraway from each other, whereas related words should be closer. Word embeddings are vector representations of words that achieve exactly this: theymap human language into a structured geometric space. Whereas the vectors obtained through one-hot encoding are binary, sparse (mostlymade of zeros), and very high-dimensional (the same dimensionality as the number ofwords in the vocabulary), word embeddings are low-dimensional floating-point vectors(that is, dense vectors, as opposed to sparse vectors); see figure 11.2. It’s common to seeword embeddings that are 256-dimensional, 512-dimensional, or 1,024-dimensionalListing 11.14 Training a first basic sequence model 330CHAPTER 11Deep learning for textwhen dealing with very large vocabularies. On the other hand, one-hot encoding wordsgenerally leads to vectors that are 20,000-dimensional or greater (capturing a vocabu-lary of 20,000 tokens, in this case). So, word embeddings pack more information intofar fewer dimensions. Besides being dense representations, word embeddings are also structured representa-tions, and their structure is learned from data. Similar words get embedded in closelocations, and further, specific directions in the embedding space are meaningful. Tomake this clearer, let’s look at a concrete example. In figure 11.3, four words are embedded on a 2D plane: cat, dog, wolf, and tiger.With the vector representations we chose here, some semantic relationships betweenthese words can be encoded as geometric transformations. For instance, the samevector allows us to go from cat to tiger and from dog to wolf: this vector could be inter-preted as the “from pet to wild animal” vector. Similarly, another vector lets us gofrom dog to cat and from wolf to tiger, which could be interpreted as a “from canineto feline” vector.One-hot word vectors:- Sparse- High-dimensional- HardcodedWord embeddings:- Dense- Lower-dimensional- Learned from dataFigure 11.2 Word representations obtained from one-hot encoding or hashing are sparse, high-dimensional, and hardcoded. Word embeddings are dense, relatively low-dimensional, and learned from data. 1010WolfTigerCatDogXFigure 11.3 A toy example of a word-embedding space 331Two approaches for representing groups of words: Sets and sequencesIn real-world word-embedding spaces, common examples of meaningful geometrictransformations are “gender” vectors and “plural” vectors. For instance, by adding a“female” vector to the vector “king,” we obtain the vector “queen.” By adding a “plu-ral” vector, we obtain “kings.” Word-embedding spaces typically feature thousands ofsuch interpretable and potentially useful vectors. Let’s look at how to use such an embedding space in practice. There are two waysto obtain word embeddings:Learn word embeddings jointly with the main task you care about (such as doc-ument classification or sentiment prediction). In this setup, you start with ran-dom word vectors and then learn word vectors in the same way you learn theweights of a neural network.Load into your model word embeddings that were precomputed using a differ-ent machine learning task than the one you’re trying to solve. These are calledpretrained word embeddings.Let’s review each of these approaches. LEARNING WORD EMBEDDINGS WITH THE EMBEDDING LAYERIs there some ideal word-embedding space that would perfectly map human languageand could be used for any natural language processing task? Possibly, but we have yetto compute anything of the sort. Also, there is no such a thing as human language—there are many different languages, and they aren’t isomorphic to one another,because a language is the reflection of a specific culture and a specific context. Butmore pragmatically, what makes a good word-embedding space depends heavily onyour task: the perfect word-embedding space for an English-language movie-reviewsentiment-analysis model may look different from the perfect embedding space for anEnglish-language legal-document classification model, because the importance of cer-tain semantic relationships varies from task to task. It’s thus reasonable to learn a new embedding space with every new task. Fortu-nately, backpropagation makes this easy, and Keras makes it even easier. It’s aboutlearning the weights of a layer: the Embedding layer.embedding_layer = layers.Embedding(input_dim=max_tokens, output_dim=256) The Embedding l a y e r i s b e s t u n d e r s t o o d a s a d i c t i o n a r y t h a t m a p s i n t e g e r i n d i c e s(which stand for specific words) to dense vectors. It takes integers as input, looks upthese integers in an internal dictionary, and returns the associated vectors. It’s effec-tively a dictionary lookup (see figure 11.4).Listing 11.15 Instantiating an Embedding layerThe Embedding layer takes at least two arguments: the number ofpossible tokens and the dimensionality of the embeddings (here, 256). Word index Embedding layer Corresponding word vectorFigure 11.4 The Embedding layer 332CHAPTER 11Deep learning for textThe Embedding layer takes as input a rank-2 tensor of integers, of shape (batch_size,sequence_length), where each entry is a sequence of integers. The layer then returnsa 3D floating-point tensor of shape (batch_size, sequence_length, embedding_dimensionality). When you instantiate an Embedding l a y e r , i t s w e i g h t s ( i t s i n t e r n a l d i c t i o n a r y o ftoken vectors) are initially random, just as with any other layer. During training, theseword vectors are gradually adjusted via backpropagation, structuring the space intosomething the downstream model can exploit. Once fully trained, the embeddingspace will show a lot of structure—a kind of structure specialized for the specific prob-lem for which you’re training your model. Let’s build a model that includes an Embedding layer and benchmark it on our task.inputs = keras.Input(shape=(None,), dtype=\"int64\")embedded = layers.Embedding(input_dim=max_tokens, output_dim=256)(inputs)x = layers.Bidirectional(layers.LSTM(32))(embedded)x = layers.Dropout(0.5)(x)outputs = layers.Dense(1, activation=\"sigmoid\")(x)model = keras.Model(inputs, outputs)model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])model.summary() callbacks = [ keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru.keras\", save_best_only=True)]model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)model = keras.models.load_model(\"embeddings_bidir_gru.keras\") print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")It trains much faster than the one-hot model (since the LSTM only has to process256-dimensional vectors instead of 20,000-dimensional), and its test accuracy is com-parable (87%). However, we’re still some way off from the results of our basic bigrammodel. Part of the reason why is simply that the model is looking at slightly less data:the bigram model processed full reviews, while our sequence model truncates sequencesafter 600 words. UNDERSTANDING PADDING AND MASKINGOne thing that’s slightly hurting model performance here is that our input sequencesare full of zeros. This comes from our use of the output_sequence_length=max_length option in TextVectorization (with max_length equal to 600): sentences lon-ger than 600 tokens are truncated to a length of 600 tokens, and sentences shorterthan 600 tokens are padded with zeros at the end so that they can be concatenatedtogether with other sequences to form contiguous batches.Listing 11.16 Model that uses an Embedding layer trained from scratch 333Two approaches for representing groups of words: Sets and sequences We’re using a bidirectional RNN: two RNN layers running in parallel, with oneprocessing the tokens in their natural order, and the other processing the sametokens in reverse. The RNN that looks at the tokens in their natural order will spendits last iterations seeing only vectors that encode padding—possibly for several hun-dreds of iterations if the original sentence was short. The information stored in theinternal state of the RNN will gradually fade out as it gets exposed to these meaning-less inputs. We need some way to tell the RNN that it should skip these iterations. There’s anAPI for that: masking. T h e Embedding layer is capable of generating a “mask” that corresponds to itsinput data. This mask is a tensor of ones and zeros (or True/False booleans), of shape(batch_size, sequence_length), where the entry mask[i, t] indicates where time-step t of sample i should be skipped or not (the timestep will be skipped if mask[i, t]is 0 or False, and processed otherwise). By default, this option isn’t active—you can turn it on by passing mask_zero=Trueto your Embedding layer. You can retrieve the mask with the compute_mask() method:>>> embedding_layer = Embedding(input_dim=10, output_dim=256, mask_zero=True)>>> some_input = [... [4, 3, 2, 1, 0, 0, 0],... [5, 4, 3, 2, 1, 0, 0],... [2, 1, 0, 0, 0, 0, 0]]>>> mask = embedding_layer.compute_mask(some_input)<tf.Tensor: shape=(3, 7), dtype=bool, numpy=array([[ True, True, True, True, False, False, False], [ True, True, True, True, True, False, False], [ True, True, False, False, False, False, False]])>In practice, you will almost never have to manage masks by hand. Instead, Keras willautomatically pass on the mask to every layer that is able to process it (as a piece ofmetadata attached to the sequence it represents). This mask will be used by RNN lay-ers to skip masked steps. If your model returns an entire sequence, the mask will alsobe used by the loss function to skip masked steps in the output sequence. Let’s try retraining our model with masking enabled.inputs = keras.Input(shape=(None,), dtype=\"int64\")embedded = layers.Embedding( input_dim=max_tokens, output_dim=256, mask_zero=True)(inputs)x = layers.Bidirectional(layers.LSTM(32))(embedded)x = layers.Dropout(0.5)(x)outputs = layers.Dense(1, activation=\"sigmoid\")(x)model = keras.Model(inputs, outputs)model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])model.summary()Listing 11.17 Using an Embedding layer with masking enabled 334CHAPTER 11Deep learning for textcallbacks = [ keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru_with_masking.keras\", save_best_only=True)]model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)model = keras.models.load_model(\"embeddings_bidir_gru_with_masking.keras\") print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")This time we get to 88% test accuracy—a small but noticeable improvement. USING PRETRAINED WORD EMBEDDINGSSometimes you have so little training data available that you can’t use your data aloneto learn an appropriate task-specific embedding of your vocabulary. In such cases,instead of learning word embeddings jointly with the problem you want to solve, youcan load embedding vectors from a precomputed embedding space that you know ishighly structured and exhibits useful properties—one that captures generic aspects oflanguage structure. The rationale behind using pretrained word embeddings in natu-ral language processing is much the same as for using pretrained convnets in imageclassification: you don’t have enough data available to learn truly powerful features onyour own, but you expect that the features you need are fairly generic—that is, com-mon visual features or semantic features. In this case, it makes sense to reuse featureslearned on a different problem. Such word embeddings are generally computed using word-occurrence statistics(observations about what words co-occur in sentences or documents), using a varietyof techniques, some involving neural networks, others not. The idea of a dense, low-dimensional embedding space for words, computed in an unsupervised way, was ini-tially explored by Bengio et al. in the early 2000s,1 but it only started to take off inresearch and industry applications after the release of one of the most famous andsuccessful word-embedding schemes: the Word2Vec algorithm (https:/ /code.google.com/archive/p/word2vec), developed by Tomas Mikolov at Google in 2013. Word2Vecdimensions capture specific semantic properties, such as gender. There are various precomputed databases of word embeddings that you can down-load and use in a Keras Embedding layer. Word2vec is one of them. Another popularone is called Global Vectors for Word Representation (GloVe, https:/ /nlp.stanford.edu/projects/glove), which was developed by Stanford researchers in 2014. Thisembedding technique is based on factorizing a matrix of word co-occurrence statis-tics. Its developers have made available precomputed embeddings for millions ofEnglish tokens, obtained from Wikipedia data and Common Crawl data. Let’s look at how you can get started using GloVe embeddings in a Keras model.The same method is valid for Word2Vec embeddings or any other word-embeddingdatabase. We’ll start by downloading the GloVe files and parse them. We’ll then loadthe word vectors into a Keras Embedding layer, which we’ll use to build a new model.1Yoshua Bengio et al., “A Neural Probabilistic Language Model,” Journal of Machine Learning Research (2003). 335Two approaches for representing groups of words: Sets and sequences F i r s t , l e t ’ s d o w n l o a d t h e G l o V e w o r d e m b e d d i n g s p r e c o m p u t e d o n t h e 2 0 1 4English Wikipedia dataset. It’s an 822 MB zip file containing 100-dimensional embed-ding vectors for 400,000 words (or non-word tokens).!wget http:/ /nlp.stanford.edu/data/glove.6B.zip!unzip -q glove.6B.zipLet’s parse the unzipped file (a .txt file) to build an index that maps words (as strings)to their vector representation.import numpy as nppath_to_glove_file = \"glove.6B.100d.txt\" embeddings_index = {} with open(path_to_glove_file) as f: for line in f: word, coefs = line.split(maxsplit=1) coefs = np.fromstring(coefs, \"f\", sep=\" \") embeddings_index[word] = coefs print(f\"Found {len(embeddings_index)} word vectors.\")Next, let’s build an embedding matrix that you can load into an Embedding layer. Itmust be a matrix of shape (max_words, embedding_dim), where each entry i containsthe embedding_dim-dimensional vector for the word of index i in the reference wordindex (built during tokenization).embedding_dim = 100 vocabulary = text_vectorization.get_vocabulary() word_index = dict(zip(vocabulary, range(len(vocabulary)))) embedding_matrix = np.zeros((max_tokens, embedding_dim)) for word, i in word_index.items(): if i < max_tokens: embedding_vector = embeddings_index.get(word) if embedding_vector is not None: embedding_matrix[i] = embedding_vector Finally, we use a Constant initializer to load the pretrained embeddings in an Embeddinglayer. So as not to disrupt the pretrained representations during training, we freezethe layer via trainable=False:embedding_layer = layers.Embedding(max_tokens,embedding_dim,embeddings_initializer=keras.initializers.Constant(embedding_matrix),Listing 11.18 Parsing the GloVe word-embeddings file Listing 11.19 Preparing the GloVe word-embeddings matrixRetrieve the vocabulary indexed byour previous TextVectorization layer.Use it to create a mapping from words to their index in the vocabulary.Prepare a matrix that we’ll fill with the GloVe vectors.Fill entry i in the matrix with the word vector for index i. Words not found in the embedding index will be all zeros. 336CHAPTER 11Deep learning for texttrainable=False,mask_zero=True,)We’re now ready to train a new model—identical to our previous model, but leverag-ing the 100-dimensional pretrained GloVe embeddings instead of 128-dimensionallearned embeddings.inputs = keras.Input(shape=(None,), dtype=\"int64\")embedded = embedding_layer(inputs)x = layers.Bidirectional(layers.LSTM(32))(embedded)x = layers.Dropout(0.5)(x)outputs = layers.Dense(1, activation=\"sigmoid\")(x)model = keras.Model(inputs, outputs)model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])model.summary() callbacks = [ keras.callbacks.ModelCheckpoint(\"glove_embeddings_sequence_model.keras\", save_best_only=True)]model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)model = keras.models.load_model(\"glove_embeddings_sequence_model.keras\")print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")You’ll find that on this particular task, pretrained embeddings aren’t very helpful,because the dataset contains enough samples that it is possible to learn a specializedenough embedding space from scratch. However, leveraging pretrained embeddingscan be very helpful when you’re working with a smaller dataset. 11.4 The Transformer architectureStarting in 2017, a new model architecture started overtaking recurrent neural net-works across most natural language processing tasks: the Transformer. Transformers were introduced in the seminal paper “Attention is all you need” byVaswani et al.2 The gist of the paper is right there in the title: as it turned out, a simplemechanism called “neural attention” could be used to build powerful sequence mod-els that didn’t feature any recurrent layers or convolution layers. This finding unleashed nothing short of a revolution in natural language process-ing—and beyond. Neural attention has fast become one of the most influential ideasin deep learning. In this section, you’ll get an in-depth explanation of how it worksand why it has proven so effective for sequence data. We’ll then leverage self-attentionListing 11.20 Model that uses a pretrained Embedding layer 2Ashish Vaswani et al., “Attention is all you need” (2017), https:/ /arxiv.org/abs/1706.03762. 337The Transformer architectureto create a Transformer encoder, one of the basic components of the Transformerarchitecture, and we’ll apply it to the IMDB movie review classification task.11.4.1 Understanding self-attentionAs you’re going through this book, you may be skimming some parts and attentivelyreading others, depending on what your goals or interests are. What if your modelsdid the same? It’s a simple yet powerful idea: not all input information seen by amodel is equally important to the task at hand, so models should “pay more attention”to some features and “pay less attention” to other features. Does that sound familiar? You’ve already encountered a similar concept twice inthis book:Max pooling in convnets looks at a pool of features in a spatial region andselects just one feature to keep. That’s an “all or nothing” form of attention:keep the most important feature and discard the rest.TF-IDF normalization assigns importance scores to tokens based on how muchinformation different tokens are likely to carry. Important tokens get boostedwhile irrelevant tokens get faded out. That’s a continuous form of attention.There are many different forms of attention you could imagine, but they all start by com-puting importance scores for a set of features, with higher scores for more relevant fea-tures and lower scores for less relevant ones (see figure 11.5). How these scores should becomputed, and what you should do with them, will vary from approach to approach. Originalrepresentation Attention scoresNewrepresentation Attentionmechanism Figure 11.5 The general concept of “attention” in deep learning: input features get assigned “attention scores,” which can be used to inform the next representation of the input. 338CHAPTER 11Deep learning for textCrucially, this kind of attention mechanism can be used for more than just highlightingor erasing certain features. It can be used to make features context-aware. You’ve justlearned about word embeddings—vector spaces that capture the “shape” of the semanticrelationships between different words. In an embedding space, a single word has a fixedposition—a fixed set of relationships with every other word in the space. But that’s notquite how language works: the meaning of a word is usually context-specific. When youmark the date, you’re not talking about the same “date” as when you go on a date, nor isit the kind of date you’d buy at the market. When you say, “I’ll see you soon,” the mean-ing of the word “see” is subtly different from the “see” in “I’ll see this project to its end” or“I see what you mean.” And, of course, the meaning of pronouns like “he,” “it,” “in,” etc.,is entirely sentence-specific and can even change multiple times within a single sentence. Clearly, a smart embedding space would provide a different vector representationfor a word depending on the other words surrounding it. That’s where self-attentioncomes in. The purpose of self-attention is to modulate the representation of a tokenby using the representations of related tokens in the sequence. This produces context-aware token representations. Consider an example sentence: “The train left the sta-tion on time.” Now, consider one word in the sentence: station. What kind of stationare we talking about? Could it be a radio station? Maybe the International Space Sta-tion? Let’s figure it out algorithmically via self-attention (see figure 11.6).'the', 'train', 'left', 'the', 'station', 'on', 'time' thetrain timeleftthesai n t toonthetrainleftthestationontime1.00.3 0.1 0.5 0.2 0.1 0.10.31.00.6 0.30.80.1 0.20.1 0.61.00.1 0.6 0.1 0.10.5 0.3 0.11.00.3 0.1 0.20.20.80.6 0.31.00.2 0.20.1 0.1 0.1 0.1 0.20.1 0.2 0.1 0.2 0.21.01.00.50.50.20.80.60.31.00.20.2So c o r e s f r“station”Inputsequence Attention scoresTok envo e c t r s Softmax,scaling, andmultiplicationWeightedtn o k e v e c t o r sContext-awarevectorSumtheta r i nleftstationtheontimeFigure 11.6 Self-attention: attention scores are computed between “station” and every other word in the sequence, and they are then used to weight a sum of word vectors that becomes the new “station” vector. 339The Transformer architectureStep 1 is to compute relevancy scores between the vector for “station” and every otherword in the sentence. These are our “attention scores.” We’re simply going to use thedot product between two word vectors as a measure of the strength of their relation-ship. It’s a very computationally efficient distance function, and it was already the stan-dard way to relate two word embeddings to each other long before Transformers. Inpractice, these scores will also go through a scaling function and a softmax, but fornow, that’s just an implementation detail. Step 2 is to compute the sum of all word vectors in the sentence, weighted by ourrelevancy scores. Words closely related to “station” will contribute more to the sum(including the word “station” itself), while irrelevant words will contribute almostnothing. The resulting vector is our new representation for “station”: a representationthat incorporates the surrounding context. In particular, it includes part of the “train”vector, clarifying that it is, in fact, a “train station.” Y o u ’ d r e p e a t t h i s p r o c e s s f o r e v e r y w o r d i n t h e s e n t e n c e , p r o d u c i n g a n e wsequence of vectors encoding the sentence. Let’s see it in NumPy-like pseudocode:def self_attention(input_sequence): output = np.zeros(shape=input_sequence.shape) for i, pivot_vector in enumerate(input_sequence): scores = np.zeros(shape=(len(input_sequence),)) for j, vector in enumerate(input_sequence): scores[j] = np.dot(pivot_vector, vector.T) scores /= np.sqrt(input_sequence.shape[1]) scores = softmax(scores) new_pivot_representation = np.zeros(shape=pivot_vector.shape) for j, vector in enumerate(input_sequence): new_pivot_representation += vector * scores[j] output[i] = new_pivot_representation return outputOf course, in practice you’d use a vectorized implementation. Keras has a built-inlayer to handle it: the MultiHeadAttention layer. Here’s how you would use it:num_heads = 4 embed_dim = 256 mha_layer = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)outputs = mha_layer(inputs, inputs, inputs)Reading this, you’re probably wonderingWhy are we passing the inputs to the layer three times? That seems redundant.What are these “multiple heads” we’re referring to? That sounds intimidating—do they also grow back if you cut them?Both of these questions have simple answers. Let’s take a look.Iterate over each tokenin the input sequence.Compute the dot product (attention score) between the token and every other token.Scale by a normalization factor, and apply a softmax.Take the sum of all tokens weighted by the attention scores.That sum isour output. 340CHAPTER 11Deep learning for textGENERALIZED SELF-ATTENTION: THE QUERY-KEY-VALUE MODELSo far, we have only considered one input sequence. However, the Transformer archi-tecture was originally developed for machine translation, where you have to deal withtwo input sequences: the source sequence you’re currently translating (such as “How’sthe weather today?”), and the target sequence you’re converting it to (such as “¿Quétiempo hace hoy?”). A Transformer is a sequence-to-sequence model: it was designed toconvert one sequence into another. You’ll learn about sequence-to-sequence modelsin depth later in this chapter. Now let’s take a step back. The self-attention mechanism as we’ve introduced itperforms the following, schematically:This means “for each token in inputs (A), compute how much the token is related toevery token in inputs ( B ) , a n d u s e t h e s e s c o r e s t o w e i g h t a s u m o f t o k e n s f r o minputs (C).” Crucially, there’s nothing that requires A, B, and C to refer to the sameinput sequence. In the general case, you could be doing this with three differentsequences. We’ll call them “query,” “keys,” and “values.” The operation becomes “foreach element in the query, compute how much the element is related to every key,and use these scores to weight a sum of values”:This terminology comes from search engines and recommender systems (see figure11.7). Imagine that you’re typing up a query to retrieve a photo from your collection—“dogs on the beach.” Internally, each of your pictures in the database is described by aset of keywords—“cat,” “dog,” “party,” etc. We’ll call those “keys.” The search engine willstart by comparing your query to the keys in the database. “Dog” yields a match of 1, and“cat” yields a match of 0. It will then rank those keys by strength of match—relevance—and it will return the pictures associated with the top N matches, in order of relevance. Conceptually, this is what Transformer-style attention is doing. You’ve got a refer-ence sequence that describes something you’re looking for: the query. You’ve got abody of knowledge that you’re trying to extract information from: the values. Eachvalue is assigned a key that describes the value in a format that can be readily com-pared to a query. You simply match the query to the keys. Then you return a weightedsum of values. In practice, the keys and the values are often the same sequence. In machine trans-lation, for instance, the query would be the target sequence, and the source sequencewould play the roles of both keys and values: for each element of the target (likeCAoutputs = sum( * pairwise_scores( , ))inputs inputs inputsB outputs = sum( * pairwise_scores( , ))values query keys 341The Transformer architecture “tiempo”), you want to go back to the source (“How’s the weather today?”) and iden-tify the different bits that are related to it (“tiempo” and “weather” should have astrong match). And naturally, if you’re just doing sequence classification, then query,keys, and values are all the same: you’re comparing a sequence to itself, to enrich eachtoken with context from the whole sequence. That explains why we needed to pass inputs three times to our MultiHeadAttentionlayer. But why “multi-head” attention?11.4.2 Multi-head attention“Multi-head attention” is an extra tweak to the self-attention mechanism, introducedin “Attention is all you need.” The “multi-head” moniker refers to the fact that theoutput space of the self-attention layer gets factored into a set of independent sub-spaces, learned separately: the initial query, key, and value are sent through threeindependent sets of dense projections, resulting in three separate vectors. Each vectoris processed via neural attention, and the three outputs are concatenated backtogether into a single output sequence. Each such subspace is called a “head.” The fullpicture is shown in figure 11.8. The presence of the learnable dense projections enables the layer to actually learnsomething, as opposed to being a purely stateless transformation that would requireadditional layers before or after it to be useful. In addition, having independent headshelps the layer learn different groups of features for each token, where features withinone group are correlated with each other but are mostly independent from featuresin a different group. QueryKeys Values“dogs on the beach”match: 0.5match: 1.0match: 0.5 BeachTreeBoatBeachDogTreeDogFigure 11.7 Retrieving images from a database: the “query” is compared to a set of “keys,” and the match scores are used to rank “values” (images). 342CHAPTER 11Deep learning for text This is similar in principle to what makes depthwise separable convolutions work: in adepthwise separable convolution, the output space of the convolution is factored intomany subspaces (one per input channel) that get learned independently. The “Atten-tion is all you need” paper was written at a time when the idea of factoring featurespaces into independent subspaces had been shown to provide great benefits for com-puter vision models—both in the case of depthwise separable convolutions, and in thecase of a closely related approach, grouped convolutions. Multi-head attention is simplythe application of the same idea to self-attention. 11.4.3 The Transformer encoderIf adding extra dense projections is so useful, why don’t we also apply one or two tothe output of the attention mechanism? Actually, that’s a great idea—let’s do that.And our model is starting to do a lot, so we might want to add residual connections tomake sure we don’t destroy any valuable information along the way—you learned inchapter 9 that they’re a must for any sufficiently deep architecture. And there’sanother thing you learned in chapter 9: normalization layers are supposed to helpgradients flow better during backpropagation. Let’s add those too. That’s roughly the thought process that I imagine unfolded in the minds of theinventors of the Transformer architecture at the time. Factoring outputs into multipleindependent spaces, adding residual connections, adding normalization layers—all ofthese are standard architecture patterns that one would be wise to leverage in anycomplex model. Together, these bells and whistles form the Transformer encoder—oneof two critical parts that make up the Transformer architecture (see figure 11.9).DenseDenseDenseAttentionQK VAttentionhead 1 DenseDenseDenseAttentionQK VAttentionhead 2Concatenate Query Key Value Query Key ValueQuery Key ValueOutput Figure 11.8 The MultiHeadAttention layer 343The Transformer architecture The original Transformer architecture consists of two parts: a Transformer encoder thatprocesses the source sequence, and a Transformer decoder that uses the source sequenceto generate a translated version. You’ll learn about about the decoder part in a minute. Crucially, the encoder part can be used for text classification—it’s a very genericmodule that ingests a sequence and learns to turn it into a more useful representa-tion. Let’s implement a Transformer encoder and try it on the movie review sentimentclassification task.import tensorflow as tffrom tensorflow import kerasfrom tensorflow.keras import layers class TransformerEncoder(layers.Layer): def __init__(self, embed_dim, dense_dim, num_heads, **kwargs): super().__init__(**kwargs) self.embed_dim = embed_dim self.dense_dim = dense_dim self.num_heads = num_heads self.attention = layers.MultiHeadAttention( num_heads=num_heads, key_dim=embed_dim) self.dense_proj = keras.Sequential( [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),] )Listing 11.21 Transformer encoder implemented as a subclassed LayerDense projectionResidual connectionLayerNormalizationDense+DenseLayerNormalization+MultiHeadAttentionFigure 11.9 The TransformerEncoder chains a MultiHeadAttention layer with a dense projection and adds normalization as well as residual connections. Size of the input token vectorsSize of the inner dense layerNumber of attention heads 344CHAPTER 11Deep learning for text self.layernorm_1 = layers.LayerNormalization() self.layernorm_2 = layers.LayerNormalization() def call(self, inputs, mask=None): if mask is not None: mask = mask[:, tf.newaxis, :] attention_output = self.attention( inputs, inputs, attention_mask=mask) proj_input = self.layernorm_1(inputs + attention_output) proj_output = self.dense_proj(proj_input) return self.layernorm_2(proj_input + proj_output) def get_config(self): config = super().get_config() config.update({ \"embed_dim\": self.embed_dim, \"num_heads\": self.num_heads, \"dense_dim\": self.dense_dim, }) return config You’ll note that the normalization layers we’re using here aren’t BatchNormalizationlayers like those we’ve used before in image models. That’s because BatchNormalizationdoesn’t work well for sequence data. Instead, we’re using the LayerNormalization layer,which normalizes each sequence independently from other sequences in the batch.Like this, in NumPy-like pseudocode:Saving custom layersWhen you write custom layers, make sure to implement the get_config method: thisenables the layer to be reinstantiated from its config dict, which is useful duringmodel saving and loading. The method should return a Python dict that contains thevalues of the constructor arguments used to create the layer.All Keras layers can be serialized and deserialized as follows:config = layer.get_config()new_layer = layer.__class__.from_config(config) For instance:layer = PositionalEmbedding(sequence_length, input_dim, output_dim)config = layer.get_config()new_layer = PositionalEmbedding.from_config(config)When saving a model that contains custom layers, the savefile will contain these con-fig dicts. When loading the model from the file, you should provide the custom layerclasses to the loading process, so that it can make sense of the config objects:model = keras.models.load_model( filename, custom_objects={\"PositionalEmbedding\": PositionalEmbedding})Computation goes in call().The mask that will be generated by the Embedding layer will be 2D, but the attention layer expects to be 3D or 4D, so we expand its rank.Implement serialization so we can save the model. The config does not contain weight values, so all weights in the layer get initialized from scratch. 345The Transformer architecturedef layer_normalization(batch_of_sequences): mean = np.mean(batch_of_sequences, keepdims=True, axis=-1) variance = np.var(batch_of_sequences, keepdims=True, axis=-1) return (batch_of_sequences - mean) / varianceCompare to BatchNormalization (during training):def batch_normalization(batch_of_images): mean = np.mean(batch_of_images, keepdims=True, axis=(0, 1, 2)) variance = np.var(batch_of_images, keepdims=True, axis=(0, 1, 2)) return (batch_of_images - mean) / varianceWhile BatchNormalization collects information from many samples to obtain accu-rate statistics for the feature means and variances, LayerNormalization pools datawithin each sequence separately, which is more appropriate for sequence data. Now that we’ve implemented our TransformerEncoder, we can use it to assemble atext-classification model similar to the GRU-based one you’ve seen previously.vocab_size = 20000 embed_dim = 256 num_heads = 2 dense_dim = 32 inputs = keras.Input(shape=(None,), dtype=\"int64\")x = layers.Embedding(vocab_size, embed_dim)(inputs)x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)x = layers.GlobalMaxPooling1D()(x) x = layers.Dropout(0.5)(x)outputs = layers.Dense(1, activation=\"sigmoid\")(x)model = keras.Model(inputs, outputs)model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])model.summary()Let’s train it. It gets to 87.5% test accuracy—slightly worse than the GRU model.callbacks = [ keras.callbacks.ModelCheckpoint(\"transformer_encoder.keras\", save_best_only=True)]model.fit(int_train_ds, validation_data=int_val_ds, epochs=20, callbacks=callbacks)Listing 11.22 Using the Transformer encoder for text classification Listing 11.23 Training and evaluating the Transformer encoder based modelInput shape: (batch_size, sequence_length, embedding_dim)To compute mean andvariance, we only pool dataover the last axis (axis -1).Input shape: (batch_size, height, width, channels)Pool data over the batch axis(axis 0), which creates interactionsbetween samples in a batch. Since TransformerEncoder returns full sequences, we need to reduce each sequence to a single vector for classification via a global pooling layer. 346CHAPTER 11Deep learning for textmodel = keras.models.load_model( \"transformer_encoder.keras\", custom_objects={\"TransformerEncoder\": TransformerEncoder}) print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")At this point, you should start to feel a bit uneasy. Something’s off here. Can you tellwhat it is? This section is ostensibly about “sequence models.” I started off by highlightingthe importance of word order. I said that Transformer was a sequence-processingarchitecture, originally developed for machine translation. And yet . . . the Trans-former encoder you just saw in action wasn’t a sequence model at all. Did younotice? It’s composed of dense layers that process sequence tokens independentlyfrom each other, and an attention layer that looks at the tokens as a set. You couldchange the order of the tokens in a sequence, and you’d get the exact same pairwiseattention scores and the exact same context-aware representations. If you were tocompletely scramble the words in every movie review, the model wouldn’t notice,and you’d still get the exact same accuracy. Self-attention is a set-processing mecha-nism, focused on the relationships between pairs of sequence elements (see figure11.10)—it’s blind to whether these elements occur at the beginning, at the end, orin the middle of a sequence. So why do we say that Transformer is a sequencemodel? And how could it possibly be good for machine translation if it doesn’t lookat word order? I hinted at the solution earlier in the chapter: I mentioned in passing that Trans-former was a hybrid approach that is technically order-agnostic, but that manuallyinjects order information in the representations it processes. This is the missing ingre-dient! It’s called positional encoding. Let’s take a look.USING POSITIONAL ENCODING TO RE-INJECT ORDER INFORMATIONThe idea behind positional encoding is very simple: to give the model access to word-order information, we’re going to add the word’s position in the sentence to each wordembedding. Our input word embeddings will have two components: the usual wordProvide the custom TransformerEncoderclass to the model-loading process. Word orderawarenessContextawareness(cross-wordsinteractions)Bag-of-unigramsBag-of-bigramsRNNSelf-attentionTransformerNoVery limitedYesNoYesNoNoNoYesYesFigure 11.10 Features of different types of NLP models 347The Transformer architecturevector, which represents the word independently of any specific context, and a posi-tion vector, which represents the position of the word in the current sentence. Hope-fully, the model will then figure out how to best leverage this additional information. The simplest scheme you could come up with would be to concatenate the word’sposition to its embedding vector. You’d add a “position” axis to the vector and fill itwith 0 for the first word in the sequence, 1 for the second, and so on. That may not be ideal, however, because the positions can potentially be very largeintegers, which will disrupt the range of values in the embedding vector. As you know,neural networks don’t like very large input values, or discrete input distributions. The original “Attention is all you need” paper used an interesting trick to encodeword positions: it added to the word embeddings a vector containing values in therange [-1, 1] that varied cyclically depending on the position (it used cosine func-tions to achieve this). This trick offers a way to uniquely characterize any integer in alarge range via a vector of small values. It’s clever, but it’s not what we’re going to usein our case. We’ll do something simpler and more effective: we’ll learn position-embedding vectors the same way we learn to embed word indices. We’ll then proceedto add our position embeddings to the corresponding word embeddings, to obtain aposition-aware word embedding. This technique is called “positional embedding.”Let’s implement it.class PositionalEmbedding(layers.Layer): def __init__(self, sequence_length, input_dim, output_dim, **kwargs): super().__init__(**kwargs) self.token_embeddings = layers.Embedding( input_dim=input_dim, output_dim=output_dim) self.position_embeddings = layers.Embedding( input_dim=sequence_length, output_dim=output_dim) self.sequence_length = sequence_length self.input_dim = input_dim self.output_dim = output_dim def call(self, inputs): length = tf.shape(inputs)[-1] positions = tf.range(start=0, limit=length, delta=1) embedded_tokens = self.token_embeddings(inputs) embedded_positions = self.position_embeddings(positions) return embedded_tokens + embedded_positions def compute_mask(self, inputs, mask=None): return tf.math.not_equal(inputs, 0) def get_config(self): config = super().get_config() config.update({ \"output_dim\": self.output_dim,Listing 11.24 Implementing positional embedding as a subclassed layerA downside of position embeddings is that the sequencelength needs to be known in advance.Prepare anEmbeddinglayer for thetoken indices.And another one for the token positionsAdd bothembeddingvectorstogether.Like the Embedding layer, this layer should be able to generate a mask so we can ignore padding 0s in the inputs. The compute_mask method will called automatically by the framework, and the mask will get propagated to the next layer.Implementserializationso we cansave themodel. 348CHAPTER 11Deep learning for text \"sequence_length\": self.sequence_length, \"input_dim\": self.input_dim, }) return configYou would use this PositionEmbedding layer just like a regular Embedding layer. Let’ssee it in action!PUTTING IT ALL TOGETHER: A TEXT-CLASSIFICATION TRANSFORMERAll you have to do to start taking word order into account is swap the old Embeddinglayer with our position-aware version.vocab_size = 20000 sequence_length = 600 embed_dim = 256 num_heads = 2 dense_dim = 32 inputs = keras.Input(shape=(None,), dtype=\"int64\")x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs) x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)x = layers.GlobalMaxPooling1D()(x)x = layers.Dropout(0.5)(x)outputs = layers.Dense(1, activation=\"sigmoid\")(x)model = keras.Model(inputs, outputs)model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])model.summary() callbacks = [ keras.callbacks.ModelCheckpoint(\"full_transformer_encoder.keras\", save_best_only=True)] model.fit(int_train_ds, validation_data=int_val_ds, epochs=20, callbacks=callbacks)model = keras.models.load_model( \"full_transformer_encoder.keras\", custom_objects={\"TransformerEncoder\": TransformerEncoder, \"PositionalEmbedding\": PositionalEmbedding}) print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")We get to 88.3% test accuracy, a solid improvement that clearly demonstrates thevalue of word order information for text classification. This is our best sequencemodel so far! However, it’s still one notch below the bag-of-words approach. Listing 11.25 Combining the Transformer encoder with positional embeddingLook here! 349The Transformer architecture11.4.4 When to use sequence models over bag-of-words modelsYou may sometimes hear that bag-of-words methods are outdated, and that Transformer-based sequence models are the way to go, no matter what task or dataset you’re look-ing at. This is definitely not the case: a small stack of Dense layers on top of a bag-of-bigrams remains a perfectly valid and relevant approach in many cases. In fact, amongthe various techniques that we’ve tried on the IMDB dataset throughout this chapter,the best performing so far was the bag-of-bigrams! So, when should you prefer one approach over the other? In 2017, my team and I ran a systematic analysis of the performance of various text-classification techniques across many different types of text datasets, and we discov-ered a remarkable and surprising rule of thumb for deciding whether to go with abag-of-words model or a sequence model (http:/ /mng.bz/AOzK)—a golden constantof sorts. It turns out that when approaching a new text-classification task, you should payclose attention to the ratio between the number of samples in your training data andthe mean number of words per sample (see figure 11.11). If that ratio is small—lessthan 1,500—then the bag-of-bigrams model will perform better (and as a bonus, it willbe much faster to train and to iterate on too). If that ratio is higher than 1,500, thenyou should go with a sequence model. In other words, sequence models work bestwhen lots of training data is available and when each sample is relatively short. So if you’re classifying 1,000-word long documents, and you have 100,000 of them (aratio of 100), you should go with a bigram model. If you’re classifying tweets that are40 words long on average, and you have 50,000 of them (a ratio of 1,250), you shouldalso go with a bigram model. But if you increase your dataset size to 500,000 tweets (aratio of 12,500), go with a Transformer encoder. What about the IMDB movie reviewclassification task? We had 20,000 training samples and an average word count of 233,so our rule of thumb points toward a bigram model, which confirms what we foundin practice. This intuitively makes sense: the input of a sequence model represents a richerand more complex space, and thus it takes more data to map out that space; mean-while, a plain set of terms is a space so simple that you can train a logistic regressionon top using just a few hundreds or thousands of samples. In addition, the shorter asample is, the less the model can afford to discard any of the information it contains—in particular, word order becomes more important, and discarding it can create ambi-guity. The sentences “this movie is the bomb” and “this movie was a bomb” have veryNumber of samplesMean sample length> 1500< 1500Sequence modelBag-of-bigramsFigure 11.11 A simple heuristic for selecting a text-classification model: the ratio between the number of training samples and the mean number of words per sample 350CHAPTER 11Deep learning for textclose unigram representations, which could confuse a bag-of-words model, but asequence model could tell which one is negative and which one is positive. With a lon-ger sample, word statistics would become more reliable and the topic or sentimentwould be more apparent from the word histogram alone. Now, keep in mind that this heuristic rule was developed specifically for text classi-fication. It may not necessarily hold for other NLP tasks—when it comes to machinetranslation, for instance, Transformer shines especially for very long sequences, com-pared to RNNs. Our heuristic is also just a rule of thumb, rather than a scientific law,so expect it to work most of the time, but not necessarily every time. 11.5 Beyond text classification: Sequence-to-sequence learningYou now possess all of the tools you will need to tackle most natural language process-ing tasks. However, you’ve only seen these tools in action on a single problem: textclassification. This is an extremely popular use case, but there’s a lot more to NLPthan classification. In this section, you’ll deepen your expertise by learning aboutsequence-to-sequence models. A s e q u e n c e - t o - s e q u e n c e m o d e l t a k e s a s e q u e n c e a s i n p u t ( o f t e n a s e n t e n c e o rparagraph) and translates it into a different sequence. This is the task at the heart ofmany of the most successful applications of NLP:Machine translation—Convert a paragraph in a source language to its equivalentin a target language.Text summarization—Convert a long document to a shorter version that retainsthe most important information.Question answering—Convert an input question into its answer.Chatbots—Convert a dialogue prompt into a reply to this prompt, or convert thehistory of a conversation into the next reply in the conversation.Text generation—Convert a text prompt into a paragraph that completes the prompt.Etc.The general template behind sequence-to-sequence models is described in figure 11.12.During training,An encoder model turns the source sequence into an intermediate representation.A decoder is trained to predict the next token i in the target sequence by lookingat both previous tokens (0 to i - 1) and the encoded source sequence.During inference, we don’t have access to the target sequence—we’re trying to pre-dict it from scratch. We’ll have to generate it one token at a time:1We obtain the encoded source sequence from the encoder.2The decoder starts by looking at the encoded source sequence as well as an ini-tial “seed” token (such as the string \"[start]\"), and uses them to predict thefirst real token in the sequence. 351Beyond text classification: Sequence-to-sequence learning 3The predicted sequence so far is fed back into the decoder, which generates thenext token, and so on, until it generates a stop token (such as the string\"[end]\").Everything you’ve learned so far can be repurposed to build this new kind of model.Let’s dive in.11.5.1 A machine translation exampleWe’ll demonstrate sequence-to-sequence modeling on a machine translation task.Machine translation is precisely what Transformer was developed for! We’ll start with arecurrent sequence model, and we’ll follow up with the full Transformer architecture. W e ’ l l b e w o r k i n g w i t h a n E n g l i s h - t o - S p a n i s h t r a n s l a t i o n d a t a s e t a v a i l a b l e a twww.manythings.org/anki/. Let’s download it:!wget http:/ /storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip!unzip -q spa-eng.zipThe text file contains one example per line: an English sentence, followed by a tabcharacter, followed by the corresponding Spanish sentence. Let’s parse this file.text_file = \"spa-eng/spa.txt\" with open(text_file) as f: lines = f.read().split(\"\\n\")[:-1]text_pairs = [] how, is, the, weather, todayqué, tiempo, hace, hoy, [end] [start], qué, tiempo, hace, hoy how, is, the, weather, todayqué [start]Training phase Inference phaseDecoderEncoder DecoderEncoderFigure 11.12 Sequence-to-sequence learning: the source sequence is processed by the encoder and is then sent to the decoder. The decoder looks at the target sequence so far and predicts the target sequence offset by one step in the future. During inference, we generate one target token at a time and feed it back into the decoder. 352CHAPTER 11Deep learning for textfor line in lines: english, spanish = line.split(\"\\t\") spanish = \"[start] \" + spanish + \" [end]\" text_pairs.append((english, spanish))Our text_pairs look like this:>>> import random>>> print(random.choice(text_pairs))(\"Soccer is more popular than tennis.\", \"[start] El fútbol es más popular que el tenis. [end]\")Let’s shuffle them and split them into the usual training, validation, and test sets:import randomrandom.shuffle(text_pairs)num_val_samples = int(0.15 * len(text_pairs))num_train_samples = len(text_pairs) - 2 * num_val_samplestrain_pairs = text_pairs[:num_train_samples]val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]test_pairs = text_pairs[num_train_samples + num_val_samples:]Next, let’s prepare two separate TextVectorization layers: one for English and onefor Spanish. We’re going to need to customize the way strings are preprocessed:We need to preserve the \"[start]\" and \"[end]\" tokens that we’ve inserted. Bydefault, the characters [ and ] would be stripped, but we want to keep themaround so we can tell apart the word “start” and the start token \"[start]\".Punctuation is different from language to language! In the Spanish Text-Vectorization layer, if we’re going to strip punctuation characters, we need toalso strip the character ¿.Note that for a non-toy translation model, we would treat punctuation characters as sep-arate tokens rather than stripping them, since we would want to be able to generate cor-rectly punctuated sentences. In our case, for simplicity, we’ll get rid of all punctuation.import tensorflow as tf import stringimport re strip_chars = string.punctuation + \"¿\" strip_chars = strip_chars.replace(\"[\", \"\") strip_chars = strip_chars.replace(\"]\", \"\") def custom_standardization(input_string): lowercase = tf.strings.lower(input_string) return tf.strings.regex_replace( lowercase, f\"[{re.escape(strip_chars)}]\", \"\")Listing 11.26 Vectorizing the English and Spanish text pairsIterate over the lines in the file.Each line contains an English phrase and its Spanish translation, tab-separated.We prepend \"[start]\" and append \"[end]\" to the Spanishsentence, to match the template from figure 11.12. Prepare a custom string standardization function for the Spanish TextVectorization layer: it preserves [ and ] but strips ¿ (as well as all other characters from strings.punctuation). 353Beyond text classification: Sequence-to-sequence learningvocab_size = 15000 sequence_length = 20 source_vectorization = layers.TextVectorization( max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length,)target_vectorization = layers.TextVectorization( max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length + 1, standardize=custom_standardization,)train_english_texts = [pair[0] for pair in train_pairs]train_spanish_texts = [pair[1] for pair in train_pairs]source_vectorization.adapt(train_english_texts) target_vectorization.adapt(train_spanish_texts) Finally, we can turn our data into a tf.data pipeline. We want it to return a tuple(inputs, target) where inputs is a dict with two keys, “encoder_inputs” (the Englishsentence) and “decoder_inputs” (the Spanish sentence), and target is the Spanishsentence offset by one step ahead.batch_size = 64 def format_dataset(eng, spa): eng = source_vectorization(eng) spa = target_vectorization(spa) return ({ \"english\": eng, \"spanish\": spa[:, :-1], }, spa[:, 1:]) def make_dataset(pairs): eng_texts, spa_texts = zip(*pairs) eng_texts = list(eng_texts) spa_texts = list(spa_texts) dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts)) dataset = dataset.batch(batch_size) dataset = dataset.map(format_dataset, num_parallel_calls=4) return dataset.shuffle(2048).prefetch(16).cache() train_ds = make_dataset(train_pairs)val_ds = make_dataset(val_pairs)Here’s what our dataset outputs look like:>>> for inputs, targets in train_ds.take(1):>>> print(f\"inputs['english'].shape: {inputs['english'].shape}\")>>> print(f\"inputs['spanish'].shape: {inputs['spanish'].shape}\")Listing 11.27 Preparing datasets for the translation taskTo keep things simple, we’ll only look at the top 15,000 words in each language, and we’ll restrict sentences to 20 words.The English layerThe Spanish layerGenerate Spanish sentences that have one extra token, since we’ll need to offset the sentence by one step during training.Learn the vocabulary of each language. The input Spanish sentence doesn’t include the last token to keep inputs and targets at the same length.The target Spanish sentence is one step ahead. Both are still the same length (20 words).Use in-memory caching to speed up preprocessing. 354CHAPTER 11Deep learning for text>>> print(f\"targets.shape: {targets.shape}\")inputs[\"encoder_inputs\"].shape: (64, 20)inputs[\"decoder_inputs\"].shape: (64, 20)targets.shape: (64, 20)The data is now ready—time to build some models. We’ll start with a recurrentsequence-to-sequence model before moving on to a Transformer. 11.5.2 Sequence-to-sequence learning with RNNsRecurrent neural networks dominated sequence-to-sequence learning from 2015–2017 before being overtaken by Transformer. They were the basis for many real-world machine-translation systems—as mentioned in chapter 10, Google Translatecirca 2017 was powered by a stack of seven large LSTM layers. It’s still worth learningabout this approach today, as it provides an easy entry point to understandingsequence-to-sequence models. The simplest, naive way to use RNNs to turn a sequence into another sequence isto keep the output of the RNN at each time step. In Keras, it would look like this:inputs = keras.Input(shape=(sequence_length,), dtype=\"int64\")x = layers.Embedding(input_dim=vocab_size, output_dim=128)(inputs)x = layers.LSTM(32, return_sequences=True)(x)outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)model = keras.Model(inputs, outputs)However, there are two major issues with this approach:The target sequence must always be the same length as the source sequence.In practice, this is rarely the case. Technically, this isn’t critical, as you couldalways pad either the source sequence or the target sequence to make theirlengths match.Due to the step-by-step nature of RNNs, the model will only be looking attokens 0…N in the source sequence in order to predict token N in the targetsequence. This constraint makes this setup unsuitable for most tasks, andparticularly translation. Consider translating “The weather is nice today” toFrench—that would be “Il fait beau aujourd’hui.” You’d need to be ableto predict “Il” from just “The,” “Il fait” from just “The weather,” etc., which issimply impossible.If you’re a human translator, you’d start by reading the entire source sentence beforestarting to translate it. This is especially important if you’re dealing with languagesthat have wildly different word ordering, like English and Japanese. And that’s exactlywhat standard sequence-to-sequence models do. In a proper sequence-to-sequence setup (see figure 11.13), you would first use anRNN (the encoder) to turn the entire source sequence into a single vector (or set ofvectors). This could be the last output of the RNN, or alternatively, its final internalstate vectors. Then you would use this vector (or vectors) as the initial state of another 355Beyond text classification: Sequence-to-sequence learning RNN (the decoder), which would look at elements 0…N in the target sequence, andtry to predict step N+1 in the target sequence. Let’s implement this in Keras with GRU-based encoders and decoders. The choiceof GRU rather than LSTM makes things a bit simpler, since GRU only has a singlestate vector, whereas LSTM has multiple. Let’s start with the encoder.from tensorflow import keras from tensorflow.keras import layers embed_dim = 256 latent_dim = 1024 source = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\") x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(source) encoded_source = layers.Bidirectional( layers.GRU(latent_dim), merge_mode=\"sum\")(x) Next, let’s add the decoder—a simple GRU layer that takes as its initial state theencoded source sentence. On top of it, we add a Dense layer that produces for eachoutput step a probability distribution over the Spanish vocabulary.past_target = keras.Input(shape=(None,), dtype=\"int64\", name=\"spanish\") x=l a y e r s . E m b e d d i n g ( v o c a b _ s i z e ,e m b e d _ d i m ,m a s k _ z e r o =True)(past_target) decoder_gru = layers.GRU(latent_dim, return_sequences=True)x=d e c o d e r _ g r u ( x ,i n i t i a l _ s t a t e = e n c o d e d _ s o u r c e ) x = layers.Dropout(0.5)(x)target_next_step = layers.Dense(vocab_size, activation=\"softmax\")(x) seq2seq_rnn = keras.Model([source, past_target], target_next_step) During training, the decoder takes as input the entire target sequence, but thanks tothe step-by-step nature of RNNs, it only looks at tokens 0…N in the input to predictListing 11.28 GRU-based encoder Listing 11.29 GRU-based decoder and the end-to-end modelhow, is, the, weather, todayqué, tiempo, hace, hoy, [end] [start], qué, tiempo, hace, hoyFinal output vector,or ﬁnal internal stateInitial stateSequence outputDecoder RNNEncoder RNNFigure 11.13 A sequence-to-sequence RNN: an RNN encoder is used to produce a vector that encodes the entire source sequence, which is used as the initial state for an RNN decoder. The English source sentence goes here.Specifying the name of the input enablesus to fit() the model with a dict of inputs.Don’t forget masking:it’s critical in this setup.Our encoded source sentence is the last output of a bidirectional GRU.The Spanish target sentence goes here.Don’t forget masking. The encoded source sentence serves as the initial state of the decoder GRU.Predicts thenext tokenEnd-to-end model: maps the sourcesentence and the target sentence to thetarget sentence one step in the future 356CHAPTER 11Deep learning for texttoken N in the output (which corresponds to the next token in the sequence, sincethe output is intended to be offset by one step). This means we only use informationfrom the past to predict the future, as we should; otherwise we’d be cheating, and ourmodel would not work at inference time. Let’s start training.seq2seq_rnn.compile( optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])seq2seq_rnn.fit(train_ds, epochs=15, validation_data=val_ds)We picked accuracy as a crude way to monitor validation-set performance duringtraining. We get to 64% accuracy: on average, the model predicts the next word in theSpanish sentence correctly 64% of the time. However, in practice, next-token accuracyisn’t a great metric for machine translation models, in particular because it makes theassumption that the correct target tokens from 0 to N are already known when pre-dicting token N+1. In reality, during inference, you’re generating the target sentencefrom scratch, and you can’t rely on previously generated tokens being 100% correct.If you work on a real-world machine translation system, you will likely use “BLEUscores” to evaluate your models—a metric that looks at entire generated sequencesand that seems to correlate well with human perception of translation quality. At last, let’s use our model for inference. We’ll pick a few sentences in the test setand check how our model translates them. We’ll start from the seed token, \"[start]\",and feed it into the decoder model, together with the encoded English source sen-tence. We’ll retrieve a next-token prediction, and we’ll re-inject it into the decoderrepeatedly, sampling one new target token at each iteration, until we get to \"[end]\"or reach the maximum sentence length.import numpy as npspa_vocab = target_vectorization.get_vocabulary() spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab)) max_decoded_sentence_length = 20def decode_sequence(input_sentence): tokenized_input_sentence = source_vectorization([input_sentence]) decoded_sentence = \"[start]\" for i in range(max_decoded_sentence_length): tokenized_target_sentence = target_vectorization([decoded_sentence]) next_token_predictions = seq2seq_rnn.predict( [tokenized_input_sentence, tokenized_target_sentence]) sampled_token_index = np.argmax(next_token_predictions[0, i, :]) Listing 11.30 Training our recurrent sequence-to-sequence model Listing 11.31 Translating new sentences with our RNN encoder and decoderPrepare a dict to convert tokenindex predictions to string tokens.SeedtokenSample thenext token. 357Beyond text classification: Sequence-to-sequence learningsampled_token = spa_index_lookup[sampled_token_index] decoded_sentence += \" \" + sampled_token if sampled_token == \"[end]\": break return decoded_sentence test_eng_texts = [pair[0] for pair in test_pairs] for _ in range(20): input_sentence = random.choice(test_eng_texts) print(\"-\") print(input_sentence) print(decode_sequence(input_sentence))Note that this inference setup, while very simple, is rather inefficient, since we repro-cess the entire source sentence and the entire generated target sentence every timewe sample a new word. In a practical application, you’d factor the encoder and thedecoder as two separate models, and your decoder would only run a single step ateach token-sampling iteration, reusing its previous internal state. Here are our translation results. Our model works decently well for a toy model,though it still makes many basic mistakes.Who is in this room?[start] quién está en esta habitación [end]-That doesn't sound too dangerous.[start] eso no es muy difícil [end]-No one will stop me.[start] nadie me va a hacer [end]-Tom is friendly.[start] tom es un buen [UNK] [end]There are many ways this toy model could be improved: We could use a deep stack ofrecurrent layers for both the encoder and the decoder (note that for the decoder, thismakes state management a bit more involved). We could use an LSTM instead of a GRU.And so on. Beyond such tweaks, however, the RNN approach to sequence-to-sequencelearning has a few fundamental limitations:The source sequence representation has to be held entirely in the encoder statevector(s), which puts significant limitations on the size and complexity of thesentences you can translate. It’s a bit as if a human were translating a sentenceentirely from memory, without looking twice at the source sentence while pro-ducing the translation.RNNs have trouble dealing with very long sequences, since they tend to pro-gressively forget about the past—by the time you’ve reached the 100th token ineither sequence, little information remains about the start of the sequence.Listing 11.32 Some sample results from the recurrent translation modelConvert the next token prediction to a string and append it to the generated sentence.Exit condition: either hit max length or sample a stop character 358CHAPTER 11Deep learning for textThat means RNN-based models can’t hold onto long-term context, which canbe essential for translating long documents.These limitations are what has led the machine learning community to embrace theTransformer architecture for sequence-to-sequence problems. Let’s take a look. 11.5.3 Sequence-to-sequence learning with TransformerSequence-to-sequence learning is the task where Transformer really shines. Neuralattention enables Transformer models to successfully process sequences that are con-siderably longer and more complex than those RNNs can handle. As a human translating English to Spanish, you’re not going to read the Englishsentence one word at a time, keep its meaning in memory, and then generate theSpanish sentence one word at a time. That may work for a five-word sentence, butit’s unlikely to work for an entire paragraph. Instead, you’ll probably want to goback and forth between the source sentence and your translation in progress, andpay attention to different words in the source as you’re writing down different partsof your translation. T h a t ’ s e x a c t l y w h a t y o u c a n a c h i e v e w i t h n e u r a l a t t e n t i o n a n d T r a n s f o r m e r s .You’re already familiar with the Transformer encoder, which uses self-attention to pro-duce context-aware representations of each token in an input sequence. In asequence-to-sequence Transformer, the Transformer encoder would naturally play therole of the encoder, which reads the source sequence and produces an encoded rep-resentation of it. Unlike our previous RNN encoder, though, the Transformerencoder keeps the encoded representation in a sequence format: it’s a sequence ofcontext-aware embedding vectors. The second half of the model is the Transformer decoder. Just like the RNN decoder,it reads tokens 0…N in the target sequence and tries to predict token N+1. Crucially,while doing this, it uses neural attention to identify which tokens in the encoded sourcesentence are most closely related to the target token it’s currently trying to predict—perhaps not unlike what a human translator would do. Recall the query-key-valuemodel: in a Transformer decoder, the target sequence serves as an attention “query”that is used to to pay closer attention to different parts of the source sequence (thesource sequence plays the roles of both keys and values).THE TRANSFORMER DECODERFigure 11.14 shows the full sequence-to-sequence Transformer. Look at the decoderinternals: you’ll recognize that it looks very similar to the Transformer encoder, exceptthat an extra attention block is inserted between the self-attention block applied tothe target sequence and the dense layers of the exit block. Let’s implement it. Like for the TransformerEncoder, we’ll use a Layer subclass.Before we focus on the call(), method, where the action happens, let’s start by defin-ing the class constructor, containing the layers we’re going to need. 359Beyond text classification: Sequence-to-sequence learning class TransformerDecoder(layers.Layer): def __init__(self, embed_dim, dense_dim, num_heads, **kwargs): super().__init__(**kwargs) self.embed_dim = embed_dim self.dense_dim = dense_dim self.num_heads = num_heads self.attention_1 = layers.MultiHeadAttention( num_heads=num_heads, key_dim=embed_dim) self.attention_2 = layers.MultiHeadAttention( num_heads=num_heads, key_dim=embed_dim) self.dense_proj = keras.Sequential(Listing 11.33 The TransformerDecoderTransformerEncoderTransformerDecoderEncoded sourceLayerNormalizationDense+DenseLayerNormalization+MultiHeadAttentionLayerNormalizationDense+DenseLayerNormalization+MultiHeadAttentionLayerNormalization+MultiHeadAttentionSource TargetTarget oﬀset by one step Figure 11.14 The TransformerDecoder is similar to the TransformerEncoder, except it features an additional attention block where the keys and values are the source sequence encoded by the TransformerEncoder. Together, the encoder and the decoder form an end-to-end Transformer. 360CHAPTER 11Deep learning for text [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),] ) self.layernorm_1 = layers.LayerNormalization() self.layernorm_2 = layers.LayerNormalization() self.layernorm_3 = layers.LayerNormalization() self.supports_masking = True def get_config(self): config = super().get_config() config.update({ \"embed_dim\": self.embed_dim, \"num_heads\": self.num_heads, \"dense_dim\": self.dense_dim, }) return configThe call() method is almost a straightforward rendering of the connectivity dia-gram from figure 11.14. But there’s an additional detail we need to take intoaccount: causal padding. Causal padding is absolutely critical to successfully traininga sequence-to-sequence Transformer. Unlike an RNN, which looks at its input onestep at a time, and thus will only have access to steps 0...N to generate output step N(which is token N+1 in the target sequence), the TransformerDecoder is order-agnos-tic: it looks at the entire target sequence at once. If it were allowed to use its entireinput, it would simply learn to copy input step N+1 to location N in the output. Themodel would thus achieve perfect training accuracy, but of course, when runninginference, it would be completely useless, since input steps beyond N aren’t available. The fix is simple: we’ll mask the upper half of the pairwise attention matrix to pre-vent the model from paying any attention to information from the future—only infor-mation from tokens 0...N i n t h e t a r g e t s e q u e n c e s h o u l d b e u s e d w h e n g e n e r a t i n gtarget token N+1. To do this, we’ll add a get_causal_attention_mask(self, inputs)method to our TransformerDecoder to retrieve an attention mask that we can pass toour MultiHeadAttention layers. def get_causal_attention_mask(self, inputs): input_shape = tf.shape(inputs) batch_size, sequence_length = input_shape[0], input_shape[1] i = tf.range(sequence_length)[:, tf.newaxis] j = tf.range(sequence_length) mask = tf.cast(i >= j, dtype=\"int32\") mask = tf.reshape(mask, (1, input_shape[1], input_shape[1])) mult = tf.concat( [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], axis=0) return tf.tile(mask, mult) Listing 11.34TransformerDecoder method that generates a causal maskThis attribute ensures that the layer will propagate its input mask to its outputs; masking in Keras is explicitly opt-in. If you pass a mask to a layer that doesn’t implement compute_mask() and that doesn’t expose this supports_masking attribute, that’s an error. Generate matrix of shape (sequence_length, sequence_length) with 1s in one half and 0s in the other.Replicate it along thebatch axis to get a matrixof shape (batch_size,sequence_length,sequence_length). 361Beyond text classification: Sequence-to-sequence learningNow we can write down the full call() method implementing the forward pass of thedecoder. def call(self, inputs, encoder_outputs, mask=None): causal_mask = self.get_causal_attention_mask(inputs) if mask is not None: padding_mask = tf.cast( mask[:, tf.newaxis, :], dtype=\"int32\") padding_mask = tf.minimum(padding_mask, causal_mask) attention_output_1 = self.attention_1( query=inputs, value=inputs, key=inputs, attention_mask=causal_mask) attention_output_1 = self.layernorm_1(inputs + attention_output_1) attention_output_2 = self.attention_2( query=attention_output_1, value=encoder_outputs, key=encoder_outputs, attention_mask=padding_mask, ) attention_output_2 = self.layernorm_2( attention_output_1 + attention_output_2) proj_output = self.dense_proj(attention_output_2) return self.layernorm_3(attention_output_2 + proj_output)PUTTING IT ALL TOGETHER: A TRANSFORMER FOR MACHINE TRANSLATIONThe end-to-end Transformer is the model we’ll be training. It maps the sourcesequence and the target sequence to the target sequence one step in the future. Itstraightforwardly combines the pieces we’ve built so far: PositionalEmbedding layers,the TransformerEncoder, and the TransformerDecoder. Note that both the Trans-formerEncoder and the TransformerDecoder a r e s h a p e - i n v a r i a n t , s o y o u c o u l d b estacking many of them to create a more powerful encoder or decoder. In our exam-ple, we’ll stick to a single instance of each.embed_dim = 256 dense_dim = 2048 num_heads = 8 encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x) decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"spanish\")x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs) x = layers.Dropout(0.5)(x)Listing 11.35 The forward pass of the TransformerDecoder Listing 11.36 End-to-end TransformerRetrievethe causalmask.Prepare the input mask (that describes padding locations in the target sequence).Merge thetwo maskstogether.Pass the causal mask to the first attention layer, which performs self-attention over the target sequence.Pass the combined mask to the second attention layer, which relates the source sequence to the target sequence. Encode thesource sentence. Encode the target sentence and combineit with the encoded source sentence. 362CHAPTER 11Deep learning for textdecoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x) transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)We’re now ready to train our model—we get to 67% accuracy, a good deal above theGRU-based model.transformer.compile( optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])transformer.fit(train_ds, epochs=30, validation_data=val_ds)Finally, let’s try using our model to translate never-seen-before English sentences fromthe test set. The setup is identical to what we used for the sequence-to-sequence RNNmodel.import numpy as npspa_vocab = target_vectorization.get_vocabulary()spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))max_decoded_sentence_length = 20 def decode_sequence(input_sentence): tokenized_input_sentence = source_vectorization([input_sentence]) decoded_sentence = \"[start]\" for i in range(max_decoded_sentence_length): tokenized_target_sentence = target_vectorization( [decoded_sentence])[:, :-1] predictions = transformer( [tokenized_input_sentence, tokenized_target_sentence]) sampled_token_index = np.argmax(predictions[0, i, :]) sampled_token = spa_index_lookup[sampled_token_index] decoded_sentence += \" \" + sampled_token if sampled_token == \"[end]\": break return decoded_sentence test_eng_texts = [pair[0] for pair in test_pairs] for _ in range(20): input_sentence = random.choice(test_eng_texts) print(\"-\") print(input_sentence) print(decode_sequence(input_sentence))Subjectively, the Transformer seems to perform significantly better than the GRU-based translation model. It’s still a toy model, but it’s a better toy model. Listing 11.37 Training the sequence-to-sequence Transformer Listing 11.38 Translating new sentences with our Transformer modelPredict a word for each output position. Sample thenext token.Convert the next token prediction to a string, and append it to the generated sentence.Exit condition 363SummaryThis is a song I learned when I was a kid.[start] esta es una canción que aprendí cuando era chico [end] -She can play the piano.[start] ella puede tocar piano [end]-I'm not who you think I am.[start] no soy la persona que tú creo que soy [end]-It may have rained a little last night.[start] puede que llueve un poco el pasado [end]That concludes this chapter on natural language processing—you just went from thevery basics to a fully fledged Transformer that can translate from English to Spanish.Teaching machines to make sense of language is the latest superpower you can add toyour collection. SummaryThere are two kinds of NLP models: bag-of-words models that process sets of wordsor N-grams without taking into account their order, and sequence models that pro-cess word order. A bag-of-words model is made of Dense layers, while a sequencemodel could be an RNN, a 1D convnet, or a Transformer.When it comes to text classification, the ratio between the number of samplesin your training data and the mean number of words per sample can help youdetermine whether you should use a bag-of-words model or a sequence model.Word embeddings are vector spaces where semantic relationships between words aremodeled as distance relationships between vectors that represent those words.Sequence-to-sequence learning is a generic, powerful learning framework that can beapplied to solve many NLP problems, including machine translation. A sequence-to-sequence model is made of an encoder, which processes a source sequence,and a decoder, which tries to predict future tokens in target sequence by lookingat past tokens, with the help of the encoder-processed source sequence.Neural attention is a way to create context-aware word representations. It’s thebasis for the Transformer architecture.The Transformer a r c h i t e c t u r e , w h i c h c o n s i s t s o f a TransformerEncoder and aTransformerDecoder, yields excellent results on sequence-to-sequence tasks.The first half, the TransformerEncoder, can also be used for text classificationor any sort of single-input NLP task.Listing 11.39 Some sample results from the Transformer translation modelWhile the source sentence wasn’tgendered, this translation assumesa male speaker. Keep in mind thattranslation models will often makeunwarranted assumptions abouttheir input data, which leads toalgorithmic bias. In the worstcases, a model might hallucinatememorized information that hasnothing to do with the data it’scurrently processing. 364Generative deep learning The potential of artificial intelligence to emulate human thought processes goesbeyond passive tasks such as object recognition and mostly reactive tasks such asdriving a car. It extends well into creative activities. When I first made the claim thatin a not-so-distant future, most of the cultural content that we consume will be cre-ated with substantial help from AIs, I was met with utter disbelief, even from long-time machine learning practitioners. That was in 2014. Fast-forward a few years,and the disbelief had receded at an incredible speed. In the summer of 2015, wewere entertained by Google’s DeepDream algorithm turning an image into a psy-chedelic mess of dog eyes and pareidolic artifacts; in 2016, we started using smart-phone applications to turn photos into paintings of various styles. In the summer of2016, an experimental short movie, Sunspring, was directed using a script written byThis chapter coversText generationDeepDreamNeural style transferVariational autoencodersGenerative adversarial networks 365a Long Short-Term Memory. Maybe you’ve recently listened to music that was tenta-tively generated by a neural network. Granted, the artistic productions we’ve seen from AI so far have been fairly lowquality. AI isn’t anywhere close to rivaling human screenwriters, painters, and compos-ers. But replacing humans was always beside the point: artificial intelligence isn’tabout replacing our own intelligence with something else, it’s about bringing into ourlives and work more intelligence—intelligence of a different kind. In many fields, butespecially in creative ones, AI will be used by humans as a tool to augment their owncapabilities: more augmented intelligence than artificial intelligence. A large part of artistic creation consists of simple pattern recognition and technicalskill. And that’s precisely the part of the process that many find less attractive or evendispensable. That’s where AI comes in. Our perceptual modalities, our language, andour artwork all have statistical structure. Learning this structure is what deep learningalgorithms excel at. Machine learning models can learn the statistical latent space ofimages, music, and stories, and they can then sample from this space, creating new art-works with characteristics similar to those the model has seen in its training data.Naturally, such sampling is hardly an act of artistic creation in itself. It’s a mere mathe-matical operation: the algorithm has no grounding in human life, human emotions,or our experience of the world; instead, it learns from an experience that has little incommon with ours. It’s only our interpretation, as human spectators, that will givemeaning to what the model generates. But in the hands of a skilled artist, algorithmicgeneration can be steered to become meaningful—and beautiful. Latent space sam-pling can become a brush that empowers the artist, augments our creative affor-dances, and expands the space of what we can imagine. What’s more, it can makeartistic creation more accessible by eliminating the need for technical skill and prac-tice—setting up a new medium of pure expression, factoring art apart from craft. Iannis Xenakis, a visionary pioneer of electronic and algorithmic music, beauti-fully expressed this same idea in the 1960s, in the context of the application of auto-mation technology to music composition:1Freed from tedious calculations, the composer is able to devote himself to the generalproblems that the new musical form poses and to explore the nooks and crannies of thisform while modifying the values of the input data. For example, he may test allinstrumental combinations from soloists, to chamber orchestras, to large orchestras. Withthe aid of electronic computers the composer becomes a sort of pilot: he presses the buttons,introduces coordinates, and supervises the controls of a cosmic vessel sailing in the spaceof sound, across sonic constellations and galaxies that he could formerly glimpse only asa distant dream.In this chapter, we’ll explore from various angles the potential of deep learning toaugment artistic creation. We’ll review sequence data generation (which can be1Iannis Xenakis, “Musiques formelles: nouveaux principes formels de composition musicale,” special issue ofLa Revue musicale, nos. 253–254 (1963). 366CHAPTER 12Generative deep learningused to generate text or music), DeepDream, and image generation using both vari-ational autoencoders and generative adversarial networks. We’ll get your computerto dream up content never seen before; and maybe we’ll get you to dream, too,about the fantastic possibilities that lie at the intersection of technology and art.Let’s get started.12.1 Text generationIn this section, we’ll explore how recurrent neural networks can be used to generatesequence data. We’ll use text generation as an example, but the exact same tech-niques can be generalized to any kind of sequence data: you could apply it tosequences of musical notes in order to generate new music, to timeseries of brush-stroke data (perhaps recorded while an artist paints on an iPad) to generate paintingsstroke by stroke, and so on. Sequence data generation is in no way limited to artistic content generation. Ithas been successfully applied to speech synthesis and to dialogue generation forchatbots. The Smart Reply feature that Google released in 2016, capable of automat-ically generating a selection of quick replies to emails or text messages, is poweredby similar techniques.12.1.1 A brief history of generative deep learning for sequence generationIn late 2014, few people had ever seen the initials LSTM, even in the machine learn-ing community. Successful applications of sequence data generation with recurrentnetworks only began to appear in the mainstream in 2016. But these techniques havea fairly long history, starting with the development of the LSTM algorithm in 1997(discussed in chapter 10). This new algorithm was used early on to generate text char-acter by character. In 2002, Douglas Eck, then at Schmidhuber’s lab in Switzerland, applied LSTM tomusic generation for the first time, with promising results. Eck is now a researcher atGoogle Brain, and in 2016 he started a new research group there, called Magenta,focused on applying modern deep learning techniques to produce engaging music.Sometimes good ideas take 15 years to get started. In the late 2000s and early 2010s, Alex Graves did important pioneering work onusing recurrent networks for sequence data generation. In particular, his 2013 workon applying recurrent mixture density networks to generate human-like handwritingusing timeseries of pen positions is seen by some as a turning point.2 This s pe ci fi capplication of neural networks at that specific moment in time captured for me thenotion of machines that dream a n d w a s a s i g n i f i c a n t i n s p i r a t i o n a r o u n d t h e t i m e Istarted developing Keras. Graves left a similar commented-out remark hidden in a2013 LaTeX file uploaded to the preprint server arXiv: “Generating sequential data is2Alex Graves, “Generating Sequences With Recurrent Neural Networks,” arXiv (2013), https:/ /arxiv.org/abs/1308.0850. 367Text generationthe closest computers get to dreaming.” Several years later, we take a lot of these devel-opments for granted, but at the time it was difficult to watch Graves’s demonstrationsand not walk away awe-inspired by the possibilities. Between 2015 and 2017, recurrentneural networks were successfully used for text and dialogue generation, music gener-ation, and speech synthesis. Then around 2017–2018, the Transformer architecture started taking over recur-rent neural networks, not just for supervised natural language processing tasks, butalso for generative sequence models—in particular language modeling (word-level textgeneration). The best-known example of a generative Transformer would be GPT-3, a175 billion parameter text-generation model trained by the startup OpenAI on anastoundingly large text corpus, including most digitally available books, Wikipedia,and a large fraction of a crawl of the entire internet. GPT-3 made headlines in 2020due to its capability to generate plausible-sounding text paragraphs on virtually anytopic, a prowess that has fed a short-lived hype wave worthy of the most torrid AIsummer. 12.1.2 How do you generate sequence data?The universal way to generate sequence data in deep learning is to train a model (usu-ally a Transformer or an RNN) to predict the next token or next few tokens in asequence, using the previous tokens as input. For instance, given the input “the cat is onthe,” the model is trained to predict the target “mat,” the next word. As usual whenworking with text data, tokens are typically words or characters, and any network thatcan model the probability of the next token given the previous ones is called a languagemodel. A language model captures the latent space of language: its statistical structure. Once you have such a trained language model, you can sample from it (generatenew sequences): you feed it an initial string of text (called conditioning data), ask it togenerate the next character or the next word (you can even generate several tokens atonce), add the generated output back to the input data, and repeat the process manytimes (see figure 12.1). This loop allows you to generate sequences of arbitrary lengththat reflect the structure of the data on which the model was trained: sequences thatlook almost like human-written sentences. LanguagemodelInitial textProbabilitydistribution overnext word Initial textSampled nextwordSamplingstrategymat The cat sat on theLanguagemodel...Samplingstrategywhich The cat sat on the matFigure 12.1 The process of word-by-word text generation using a language model 368CHAPTER 12Generative deep learning12.1.3 The importance of the sampling strategyWhen generating text, the way you choose the next token is crucially important. Anaive approach is greedy sampling, consisting of always choosing the most likely nextcharacter. But such an approach results in repetitive, predictable strings that don’tlook like coherent language. A more interesting approach makes slightly more sur-prising choices: it introduces randomness in the sampling process by sampling fromthe probability distribution for the next character. This is called stochastic sampling(recall that stochasticity is what we call randomness in this field). In such a setup, if aword has probability 0.3 of being next in the sentence according to the model, you’llchoose it 30% of the time. Note that greedy sampling can also be cast as samplingfrom a probability distribution: one where a certain word has probability 1 and all oth-ers have probability 0. Sampling probabilistically from the softmax output of the model is neat: it allowseven unlikely words to be sampled some of the time, generating more interesting-looking sentences and sometimes showing creativity by coming up with new, realistic-sounding sentences that didn’t occur in the training data. But there’s one issue withthis strategy: it doesn’t offer a way to control the amount of randomness in the samplingprocess. Why would you want more or less randomness? Consider an extreme case: purerandom sampling, where you draw the next word from a uniform probability distribu-tion, and every word is equally likely. This scheme has maximum randomness; inother words, this probability distribution has maximum entropy. Naturally, it won’tproduce anything interesting. At the other extreme, greedy sampling doesn’t produceanything interesting, either, and has no randomness: the corresponding probabilitydistribution has minimum entropy. Sampling from the “real” probability distribu-tion—the distribution that is output by the model’s softmax function—constitutes anintermediate point between these two extremes. But there are many other intermedi-ate points of higher or lower entropy that you may want to explore. Less entropy willgive the generated sequences a more predictable structure (and thus they will poten-tially be more realistic looking), whereas more entropy will result in more surprisingand creative sequences. When sampling from generative models, it’s always good toexplore different amounts of randomness in the generation process. Because we—humans—are the ultimate judges of how interesting the generated data is, interesting-ness is highly subjective, and there’s no telling in advance where the point of optimalentropy lies. In order to control the amount of stochasticity in the sampling process, we’ll intro-duce a parameter called the softmax temperature, which characterizes the entropy of theprobability distribution used for sampling: it characterizes how surprising or predict-able the choice of the next word will be. Given a temperature value, a new probabilitydistribution is computed from the original one (the softmax output of the model) byreweighting it in the following way. 369Text generationimport numpy as np def reweight_distribution(original_distribution, temperature=0.5): distribution = np.log(original_distribution) / temperature distribution = np.exp(distribution) return distribution / np.sum(distribution) Higher temperatures result in sampling distributions of higher entropy that will gener-ate more surprising and unstructured generated data, whereas a lower temperature willresult in less randomness and much more predictable generated data (see figure 12.2). 12.1.4 Implementing text generation with KerasLet’s put these ideas into practice in a Keras implementation. The first thing you needis a lot of text data that you can use to learn a language model. You can use any suffi-ciently large text file or set of text files—Wikipedia, The Lord of the Rings, and so on. In this example, we’ll keep working with the IMDB movie review dataset from thelast chapter, and we’ll learn to generate never-read-before movie reviews. As such, ourlanguage model will be a model of the style and topics of these movie reviews specifi-cally, rather than a general model of the English language.Listing 12.1 Reweighting a probability distribution to a different temperatureoriginal_distribution is a 1D NumPy array of probabilityvalues that must sum to 1. temperature is a factorquantifying the entropy of the output distribution.Returns a reweighted version of the original distribution. The sum of the distributionmay no longer be 1, so you divide it by its sum to obtain the new distribution. temperature = 0.01 temperature = 0.2 temperature = 0.4 temperature = 0.6Elements (words)Probability of sampling elementtemperature = 0.8 temperature = 1.0 Figure 12.2 Different reweightings of one probability distribution. Low temperature = more deterministic, high temperature = more random. 370CHAPTER 12Generative deep learningPREPARING THE DATAJust like in the previous chapter, let’s download and uncompress the IMDB moviereviews dataset.!wget https:/ /ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz!tar -xf aclImdb_v1.tar.gzYou’re already familiar with the structure of the data: we get a folder named aclImdbcontaining two subfolders, one for negative-sentiment movie reviews, and one forpositive-sentiment reviews. There’s one text file per review. We’ll call text_dataset_from_directory with label_mode=None to create a dataset that reads from these filesand yields the text content of each file.import tensorflow as tf from tensorflow import kerasdataset = keras.utils.text_dataset_from_directory( directory=\"aclImdb\", label_mode=None, batch_size=256)dataset = dataset.map(lambda x: tf.strings.regex_replace(x, \"<br />\", \" \")) Now let’s use a TextVectorization layer to compute the vocabulary we’ll be workingwith. We’ll only use the first sequence_length words of each review: our TextVector-ization layer will cut off anything beyond that when vectorizing a text.from tensorflow.keras.layers import TextVectorization sequence_length = 100 vocab_size = 15000 text_vectorization = TextVectorization( max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length, )text_vectorization.adapt(dataset)Let’s use the layer to create a language modeling dataset where input samples are vec-torized texts, and corresponding targets are the same texts offset by one word.def prepare_lm_dataset(text_batch): vectorized_sequences = text_vectorization(text_batch) Listing 12.2 Downloading and uncompressing the IMDB movie reviews dataset Listing 12.3 Creating a dataset from text files (one file = one sample) Listing 12.4 Preparing a TextVectorization layer Listing 12.5 Setting up a language modeling datasetStrip the <br /> HTML tag that occurs in many of thereviews. This did not matter much for text classification,but we wouldn’t want to generate <br />tags in this example! We’ll only consider the top 15,000 most common words—anything else will be treated as the out-of-vocabulary token, \"[UNK]\".We want to return integer word index sequences.We’ll work with inputs and targets of length 100 (but since we’ll offset the targets by 1, the model will actually see sequences of length 99).Convert a batch of texts (strings)to a batch of integer sequences. 371Text generation x = vectorized_sequences[:, :-1] y = vectorized_sequences[:, 1:] return x, y lm_dataset = dataset.map(prepare_lm_dataset, num_parallel_calls=4)A TRANSFORMER-BASED SEQUENCE-TO-SEQUENCE MODELWe’ll train a model to predict a probability distribution over the next word in a sen-tence, given a number of initial words. When the model is trained, we’ll feed it with aprompt, sample the next word, add that word back to the prompt, and repeat, untilwe’ve generated a short paragraph. Like we did for temperature forecasting in chapter 10, we could train a model thattakes as input a sequence of N words and simply predicts word N+1. However, thereare several issues with this setup in the context of sequence generation. First, the model would only learn to produce predictions when N words were avail-able, but it would be useful to be able to start predicting with fewer than N words.Otherwise we’d be constrained to only use relatively long prompts (in our implemen-tation, N=100 words). We didn’t have this need in chapter 10. Second, many of our training sequences will be mostly overlapping. Consider N = 4.The text “A complete sentence must have, at minimum, three things: a subject, verb,and an object” would be used to generate the following training sequences:“A complete sentence must”“complete sentence must have”“sentence must have at”and so on, until “verb and an object”A model that treats each such sequence as an independent sample would have to do alot of redundant work, re-encoding multiple times subsequences that it has largelyseen before. In chapter 10, this wasn’t much of a problem, because we didn’t have thatmany training samples in the first place, and we needed to benchmark dense and con-volutional models, for which redoing the work every time is the only option. We couldtry to alleviate this redundancy problem by using strides to sample our sequences—skipping a few words between two consecutive samples. But that would reduce ournumber of training samples while only providing a partial solution. To address these two issues, we’ll use a sequence-to-sequence model: we’ll feed sequencesof N words (indexed from 0 to N) into our model, and we’ll predict the sequence offsetby one (from 1 to N+1). We’ll use causal masking to make sure that, for any i, the modelwill only be using words from 0 to i in order to predict the word i + 1. This means thatwe’re simultaneously training the model to solve N m o s t l y o v e r l a p p i n g b u t d i f f e r e n tproblems: predicting the next words given a sequence of 1 <= i <= N prior words (see fig-ure 12.3). At generation time, even if you only prompt the model with a single word, itwill be able to give you a probability distribution for the next possible words.Create inputs by cutting off the last word of the sequences.Create targets by offsetting the sequences by 1. 372CHAPTER 12Generative deep learning Note that we could have used a similar sequence-to-sequence setup on our tempera-ture forecasting problem in chapter 10: given a sequence of 120 hourly data points,learn to generate a sequence of 120 temperatures offset by 24 hours in the future.You’d be not only solving the initial problem, but also solving the 119 related prob-lems of forecasting temperature in 24 hours, given 1 <= i < 120 p r i o r h o u r l y d a t apoints. If you try to retrain the RNNs from chapter 10 in a sequence-to-sequencesetup, you’ll find that you get similar but incrementally worse results, because the con-straint of solving these additional 119 related problems with the same model inter-feres slightly with the task we actually do care about. In the previous chapter, you learned about the setup you can use for sequence-to-sequence learning in the general case: feed the source sequence into an encoder, andthen feed both the encoded sequence and the target sequence into a decoder that triesto predict the same target sequence offset by one step. When you’re doing text genera-tion, there is no source sequence: you’re just trying to predict the next tokens in the tar-get sequence given past tokens, which we can do using only the decoder. And thanks tocausal padding, the decoder will only look at words 0…N to predict the word N+1. Let’s implement our model—we’re going to reuse the building blocks we createdin chapter 11: PositionalEmbedding and TransformerDecoder.from tensorflow.keras import layersembed_dim = 256 latent_dim = 2048 num_heads = 2 inputs = keras.Input(shape=(None,), dtype=\"int64\")x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, x)outputs = layers.Dense(vocab_size, activation=\"softmax\")(x) model = keras.Model(inputs, outputs)model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\")12.1.5 A text-generation callback with variable-temperature samplingWe’ll use a callback to generate text using a range of different temperatures afterevery epoch. This allows you to see how the generated text evolves as the modelbegins to converge, as well as the impact of temperature in the sampling strategy. ToListing 12.6 A simple Transformer-based language modelNext word pred ct on - i iSe uenc -t -sequence q e omod li g e nthe cat sat on thematthecat sat on the matthe catsat on the matthe cat saton the matthe cat sat onthe matthe cat sat on thematFigure 12.3 Compared to plain next-word prediction, sequence-to-sequence modeling simultaneously optimizes for multiple prediction problems. Softmax over possiblevocabulary words, com-puted for each outputsequence timestep. 373Text generationseed text generation, we’ll use the prompt “this movie”: all of our generated textswill start with this.import numpy as np tokens_index = dict(enumerate(text_vectorization.get_vocabulary())) def sample_next(predictions, temperature=1.0): predictions = np.asarray(predictions).astype(\"float64\") predictions = np.log(predictions) / temperature exp_preds = np.exp(predictions) predictions = exp_preds / np.sum(exp_preds) probas = np.random.multinomial(1, predictions, 1) return np.argmax(probas) class TextGenerator(keras.callbacks.Callback): def __init__(self, prompt, generate_length, model_input_length, temperatures=(1.,), print_freq=1): self.prompt = prompt self.generate_length = generate_length self.model_input_length = model_input_length self.temperatures = temperatures self.print_freq = print_freq def on_epoch_end(self, epoch, logs=None): if (epoch + 1) % self.print_freq != 0: return for temperature in self.temperatures: print(\"== Generating with temperature\", temperature) sentence = self.prompt for i in range(self.generate_length): tokenized_sentence = text_vectorization([sentence]) predictions = self.model(tokenized_sentence) next_token = sample_next(predictions[0, i, :]) sampled_token = tokens_index[next_token] sentence += \" \" + sampled_token print(sentence) prompt = \"This movie\" text_gen_callback = TextGenerator( prompt, generate_length=50, model_input_length=sequence_length, temperatures=(0.2, 0.5, 0.7, 1., 1.5)) Let’s fit() this thing.model.fit(lm_dataset, epochs=200, callbacks=[text_gen_callback])Listing 12.7 The text-generation callback Listing 12.8 Fitting the language modelDict that maps word indices back tostrings, to be used for text decodingImplements variable-temperature sampling from a probability distributionPrompt thatwe use to seedtext generationHow many words to generateRange of temperatures to use for samplingWhen generatingtext, we start fromour prompt.Feed the currentsequence into our model.Retrieve the predictions for the last timestep, and use them to sample a new word.Append the new wordto the current sequenceand repeat.We’ll use a diverse range of temperatures to sample text, to demonstrate the effect of temperature on text generation. 374CHAPTER 12Generative deep learningHere are some cherrypicked examples of what we’re able to generate after 200 epochsof training. Note that punctuation isn’t part of our vocabulary, so none of our gener-ated text has any punctuation:With temperature=0.2–“ t h i s m o v i e i s a [ U N K ] o f t h e o r i g i n a l m o v i e a n d t h e f i r s t h a l f h o u r o f t h emovie is pretty good but it is a very good movie it is a good movie for the timeperiod”–“ t h i s m o v i e i s a [ U N K ] o f t h e m o v i e i t i s a m o v i e t h a t i s s o b a d t h a t i t i s a[UNK] movie it is a movie that is so bad that it makes you laugh and cry atthe same time it is not a movie i dont think ive ever seen”With temperature=0.5– “this movie is a [UNK] of the best genre movies of all time and it is not agood movie it is the only good thing about this movie i have seen it for thefirst time and i still remember it being a [UNK] movie i saw a lot of years”–“ t h i s m o v i e i s a w a s t e o f t i m e a n d m o n e y i h a v e t o s a y t h a t t h i s m o v i e w a s acomplete waste of time i was surprised to see that the movie was made up of agood movie and the movie was not very good but it was a waste of time and”With temperature=0.7–“ t h i s m o v i e i s f u n t o w a t c h a n d i t i s r e a l l y f u n n y t o w a t c h a l l t h e c h a r a c t e r sare extremely hilarious also the cat is a bit like a [UNK] [UNK] and a hat[UNK] the rules of the movie can be told in another scene saves it frombeing in the back of”–“ t h i s m o v i e i s a b o u t [ U N K ] a n d a c o u p l e o f y o u n g p e o p l e u p o n a s m a l l b o a tin the middle of nowhere one might find themselves being exposed to a[UNK] dentist they are killed by [UNK] i was a huge fan of the book and ihavent seen the original so it”With temperature=1.0– “this movie was entertaining i felt the plot line was loud and touching but ona whole watch a stark contrast to the artistic of the original we watched theoriginal version of england however whereas arc was a bit of a little too ordi-nary the [UNK] were the present parent [UNK]”–“ t h i s m o v i e w a s a m a s t e r p i e c e a w a y f r o m t h e s t o r y l i n e b u t t h i s m o v i e w a s s i m -ply exciting and frustrating it really entertains friends like this the actors inthis movie try to go straight from the sub thats image and they make it areally good tv show”With temperature=1.5–“ t h i s m o v i e w a s p o s s i b l y t h e w o r s t f i l m a b o u t t h a t 8 0 w o m e n i t s a s w e i r dinsightful actors like barker movies but in great buddies yes no decoratedshield even [UNK] land dinosaur ralph ian was must make a play happenedfalls after miscast [UNK] bach not really not wrestlemania seriously samdidnt exist” 375Text generation–“ t h i s m o v i e c o u l d b e s o u n b e l i e v a b l y l u c a s h i m s e l f b r i n g i n g o u r c o u n t r ywildly funny things has is for the garish serious and strong performancescolin writing more detailed dominated but before and that images gearsburning the plate patriotism we you expected dyan bosses devotion to mustdo your own duty and another”As you can see, a low temperature value results in very boring and repetitive text andcan sometimes cause the generation process to get stuck in a loop. With higher tem-peratures, the generated text becomes more interesting, surprising, even creative.With a very high temperature, the local structure starts to break down, and the outputlooks largely random. Here, a good generation temperature would seem to be about0.7. Always experiment with multiple sampling strategies! A clever balance betweenlearned structure and randomness is what makes generation interesting. Note that by training a bigger model, longer, on more data, you can achieve gen-erated samples that look far more coherent and realistic than this one—the outputof a model like GPT-3 is a good example of what can be done with language models(GPT-3 is effectively the same thing as what we trained in this example, but with adeep stack of Transformer decoders, and a much bigger training corpus). But don’texpect to ever generate any meaningful text, other than through random chanceand the magic of your own interpretation: all you’re doing is sampling data from astatistical model of which words come after which words. Language models are allform and no substance. Natural language is many things: a communication channel, a way to act on theworld, a social lubricant, a way to formulate, store, and retrieve your own thoughts . . .These uses of languages are where its meaning originates. A deep learning “languagemodel,” despite its name, captures effectively none of these fundamental aspects oflanguage. It cannot communicate (it has nothing to communicate about and no oneto communicate with), it cannot act on the world (it has no agency and no intent), itcannot be social, and it doesn’t have any thoughts to process with the help of words.Language is the operating system of the mind, and so, for language to be meaningful,it needs a mind to leverage it. What a language model does is capture the statistical structure of the observableartifacts—books, online movie reviews, tweets—that we generate as we use language tolive our lives. The fact that these artifacts have a statistical structure at all is a sideeffect of how humans implement language. Here’s a thought experiment: what if ourlanguages did a better job of compressing communications, much like computers dowith most digital communications? Language would be no less meaningful and couldstill fulfill its many purposes, but it would lack any intrinsic statistical structure, thusmaking it impossible to model as you just did. 376CHAPTER 12Generative deep learning12.1.6 Wrapping upYou can generate discrete sequence data by training a model to predict the nexttoken(s), given previous tokens.In the case of text, such a model is called a language model. It can be based oneither words or characters.Sampling the next token requires a balance between adhering to what themodel judges likely, and introducing randomness.One way to handle this is the notion of softmax temperature. Always experi-ment with different temperatures to find the right one. 12.2 DeepDreamDeepDream i s a n a r t i s t i c i m a g e - m o d i f i c a t i o n t e c h n i q u e t h a t u s e s t h e r e p r e s e n t a t i o n slearned by convolutional neural networks. It was first released by Google in the sum-mer of 2015 as an implementation written using the Caffe deep learning library (thiswas several months before the first public release of TensorFlow).3 It quickly becamean internet sensation thanks to the trippy pictures it could generate (see, for example,figure 12.4), full of algorithmic pareidolia artifacts, bird feathers, and dog eyes—abyproduct of the fact that the DeepDream convnet was trained on ImageNet, wheredog breeds and bird species are vastly overrepresented. 3Alexander Mordvintsev, Christopher Olah, and Mike Tyka, “DeepDream: A Code Example for VisualizingNeural Networks,” Google Research Blog, July 1, 2015, http:/ /mng.bz/xXlM.Figure 12.4 Example of a DeepDream output image 377DeepDreamThe DeepDream algorithm is almost identical to the convnet filter-visualization tech-nique introduced in chapter 9, consisting of running a convnet in reverse: doing gra-dient ascent on the input to the convnet in order to maximize the activation of aspecific filter in an upper layer of the convnet. DeepDream uses this same idea, with afew simple differences:With DeepDream, you try to maximize the activation of entire layers ratherthan that of a specific filter, thus mixing together visualizations of large num-bers of features at once.You start not from blank, slightly noisy input, but rather from an existingimage—thus the resulting effects latch on to preexisting visual patterns, distort-ing elements of the image in a somewhat artistic fashion.The input images are processed at different scales (called octaves), whichimproves the quality of the visualizations.Let’s make some DeepDreams.12.2.1 Implementing DeepDream in KerasLet’s start by retrieving a test image to dream with. We’ll use a view of the ruggedNorthern California coast in the winter (figure 12.5).from tensorflow import keras import matplotlib.pyplot as plt base_image_path = keras.utils.get_file( \"coast.jpg\", origin=\"https:/ /img-datasets.s3.amazonaws.com/coast.jpg\") plt.axis(\"off\")plt.imshow(keras.utils.load_img(base_image_path))Listing 12.9 Fetching the test image Figure 12.5 Our test image 378CHAPTER 12Generative deep learningNext, we need a pretrained convnet. In Keras, many such convnets are available:VGG16, VGG19, Xception, ResNet50, and so on, all available with weights pretrainedon ImageNet. You can implement DeepDream with any of them, but your base modelof choice will naturally affect your visualizations, because different architectures resultin different learned features. The convnet used in the original DeepDream releasewas an Inception model, and in practice, Inception is known to produce nice-lookingDeepDreams, so we’ll use the Inception V3 model that comes with Keras.from tensorflow.keras.applications import inception_v3model = inception_v3.InceptionV3(weights=\"imagenet\", include_top=False)We’ll use our pretrained convnet to create a feature exactor model that returns theactivations of the various intermediate layers, listed in the following code. For eachlayer, we pick a scalar score that weights the contribution of the layer to the loss we willseek to maximize during the gradient ascent process. If you want a complete list oflayer names that you can use to pick new layers to play with, just use model.summary().layer_settings = { \"mixed4\": 1.0, \"mixed5\": 1.5, \"mixed6\": 2.0, \"mixed7\": 2.5,}outputs_dict = dict( [ (layer.name, layer.output) for layer in [model.get_layer(name) for name in layer_settings.keys()] ])feature_extractor = keras.Model(inputs=model.inputs, outputs=outputs_dict) Next, we’ll compute the loss: the quantity we’ll seek to maximize during the gradient-ascent process at each processing scale. In chapter 9, for filter visualization, we tried tomaximize the value of a specific filter in a specific layer. Here, we’ll simultaneouslymaximize the activation of all filters in a number of layers. Specifically, we’ll maximizea weighted mean of the L2 norm of the activations of a set of high-level layers. Theexact set of layers we choose (as well as their contribution to the final loss) has a majorinfluence on the visuals we’ll be able to produce, so we want to make these parameterseasily configurable. Lower layers result in geometric patterns, whereas higher layersresult in visuals in which you can recognize some classes from ImageNet (for example,birds or dogs). We’ll start from a somewhat arbitrary configuration involving four lay-ers—but you’ll definitely want to explore many different configurations later.Listing 12.10 Instantiating a pretrained InceptionV3 model Listing 12.11 Configuring the contribution of each layer to the DeepDream lossLayers for which we try to maximize activation, as well as their weight in the total loss. You can tweak these setting to obtain new visual effects.Symbolic outputs of each layerModel that returnsthe activation valuesfor every targetlayer (as a dict) 379DeepDreamdef compute_loss(input_image): features = feature_extractor(input_image) loss = tf.zeros(shape=()) for name in features.keys(): coeff = layer_settings[name] activation = features[name] loss += coeff * tf.reduce_mean(tf.square(activation[:, 2:-2, 2:-2, :])) return lossNow let’s set up the gradient ascent process that we will run at each octave. You’ll rec-ognize that it’s the same thing as the filter-visualization technique from chapter 9! TheDeepDream algorithm is simply a multiscale form of filter visualization.import tensorflow as tf @tf.function def gradient_ascent_step(image, learning_rate): with tf.GradientTape() as tape: tape.watch(image) loss = compute_loss(image) grads = tape.gradient(loss, image) grads = tf.math.l2_normalize(grads) image += learning_rate * grads return loss, image def gradient_ascent_loop(image, iterations, learning_rate, max_loss=None): for i in range(iterations): loss, image = gradient_ascent_step(image, learning_rate) if max_loss is not None and loss > max_loss: break print(f\"... Loss value at step {i}: {loss:.2f}\") return imageFinally, the outer loop of the DeepDream algorithm. First, we’ll define a list of scales(also called octaves) at which to process the images. We’ll process our image overthree different such “octaves.” For each successive octave, from the smallest to thelargest, we’ll run 20 gradient ascent steps via gradient_ascent_loop() to maximizethe loss we previously defined. Between each octave, we’ll upscale the image by 40%(1.4x): we’ll start by processing a small image and then increasingly scale it up (seefigure 12.6). We define the parameters of this process in the following code. Tweaking theseparameters will allow you to achieve new effects!Listing 12.12 The DeepDream loss Listing 12.13 The DeepDream gradient ascent processExtractactivations.Initialize theloss to 0.We avoid border artifacts byonly involving non-borderpixels in the loss. We make the training step fast by compiling it as a tf.function.Compute gradients of DeepDream loss with respect to the current image.Normalize gradients (the same trick we used in chapter 9).This runsgradientascentfor a givenimage scale(octave).Repeatedly update the image in a way that increases the DeepDream loss.Break out if the loss crosses a certainthreshold (over-optimizing wouldcreate unwanted image artifacts). 380CHAPTER 12Generative deep learning step = 20. num_octave = 3 octave_scale = 1.4 iterations = 30 max_loss = 15. We’re also going to need a couple of utility functions to load and save images.import numpy as np def preprocess_image(image_path): img = keras.utils.load_img(image_path) img = keras.utils.img_to_array(img) img = np.expand_dims(img, axis=0) img = keras.applications.inception_v3.preprocess_input(img) return img def deprocess_image(img): img = img.reshape((img.shape[1], img.shape[2], 3)) img /= 2.0 img += 0.5 img *= 255. img = np.clip(img, 0, 255).astype(\"uint8\") return imgThis is the outer loop. To avoid losing a lot of image detail after each successive scale-up (resulting in increasingly blurry or pixelated images), we can use a simple trick:Listing 12.14 Image processing utilities DreamOctave 1Octave 2Octave 3UpscaleDetailreinjectionDetailreinjectionDream Upscale Dream Figure 12.6 The DeepDream process: successive scales of spatial processing (octaves) and detail re-injection upon upscalingGradient ascent step sizeNumber of scales at which to run gradient ascentSize ratio between successive scalesNumber of gradient ascent steps per scaleWe’ll stop the gradient ascent process fora scale if the loss gets higher than this.Util function to open, resize, and format pictures into appropriate arraysUtil function to convert a NumPy array into a valid imageUndo inception v3 preprocessing.Convert to uint8 and clip to the valid range [0, 255]. 381DeepDreamafter each scale-up, we’ll re-inject the lost details back into the image, which is possiblebecause we know what the original image should look like at the larger scale. Given asmall image size S and a larger image size L, we can compute the difference betweenthe original image resized to size L and the original resized to size S—this differencequantifies the details lost when going from S to L.original_img = preprocess_image(base_image_path) original_shape = original_img.shape[1:3] successive_shapes = [original_shape] for i in range(1, num_octave): shape = tuple([int(dim / (octave_scale ** i)) for dim in original_shape]) successive_shapes.append(shape) successive_shapes = successive_shapes[::-1] shrunk_original_img = tf.image.resize(original_img, successive_shapes[0]) img = tf.identity(original_img) for i, shape in enumerate(successive_shapes): print(f\"Processing octave {i} with shape {shape}\") img = tf.image.resize(img, shape) img = gradient_ascent_loop( img, iterations=iterations, learning_rate=step, max_loss=max_loss ) upscaled_shrunk_original_img = tf.image.resize(shrunk_original_img, shape) same_size_original = tf.image.resize(original_img, shape) lost_detail = same_size_original - upscaled_shrunk_original_img img += lost_detail shrunk_original_img = tf.image.resize(original_img, shape) keras.utils.save_img(\"dream.png\", deprocess_image(img.numpy())) NOTEBecause the original Inception V3 network was trained to recognize con-cepts in images of size 299 × 299, and given that the process involves scaling theimages down by a reasonable factor, the DeepDream implementation producesmuch better results on images that are somewhere between 300 × 300 and 400 ×400. Regardless, you can run the same code on images of any size and any ratio.On a GPU, it only takes a few seconds to run the whole thing. Figure 12.7 shows theresult of our dream configuration on the test image. I strongly suggest that you explore what you can do by adjusting which layers youuse in your loss. Layers that are lower in the network contain more-local, less-abstractrepresentations and lead to dream patterns that look more geometric. Layers that arehigher up lead to more-recognizable visual patterns based on the most common objectsListing 12.15 Running gradient ascent over multiple successive \"octaves\"Loadthe testimage.Compute the targetshape of the image atdifferent octaves.Make a copy of the image (we need to keep the original around).Iterateover thedifferentoctaves.Scale upthe dreamimage.Run gradient ascent, altering the dream. Scale up the smaller version of the original image: it will be pixellated.Compute the high-quality version of the original image at this size.The difference between the two is thedetail that was lost when scaling up.Re-inject lost detail into the dream.Save the final result. 382CHAPTER 12Generative deep learning found in ImageNet, such as dog eyes, bird feathers, and so on. You can use random gen-eration of the parameters in the layer_settings dictionary to quickly explore manydifferent layer combinations. Figure 12.8 shows a range of results obtained on an imageof a delicious homemade pastry using different layer configurations. Figure 12.7 Running the DeepDream code on the test image Figure 12.8 Trying a range of DeepDream configurations on an example image 383Neural style transfer12.2.2 Wrapping upDeepDream consists of running a convnet in reverse to generate inputs basedon the representations learned by the network.The results produced are fun and somewhat similar to the visual artifactsinduced in humans by the disruption of the visual cortex via psychedelics.Note that the process isn’t specific to image models or even to convnets. It canbe done for speech, music, and more. 12.3 Neural style transferIn addition to DeepDream, another major development in deep-learning-drivenimage modification is neural style transfer, introduced by Leon Gatys et al. in the sum-mer of 2015.4 The neural style transfer algorithm has undergone many refinementsand spawned many variations since its original introduction, and it has made its wayinto many smartphone photo apps. For simplicity, this section focuses on the formula-tion described in the original paper. Neural style transfer consists of applying the style of a reference image to a targetimage while conserving the content of the target image. Figure 12.9 shows an example. In this context, style e s s e n t i a l l y m e a n s t e x t u r e s , c o l o r s , a n d v i s u a l p a t t e r n s i n t h eimage, at various spatial scales, and the content is the higher-level macrostructure ofthe image. For instance, blue-and-yellow circular brushstrokes are considered to bethe style in figure 12.9 (using Starry Night by Vincent Van Gogh), and the buildings inthe Tübingen photograph are considered to be the content. The idea of style transfer, which is tightly related to that of texture generation, hashad a long history in the image-processing community prior to the development ofneural style transfer in 2015. But as it turns out, the deep-learning-based implementa-tions of style transfer offer results unparalleled by what had been previously achievedwith classical computer vision techniques, and they triggered an amazing renaissancein creative applications of computer vision.4Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge, “A Neural Algorithm of Artistic Style,” arXiv (2015),https:/ /arxiv.org/abs/1508.06576. Content target Combination image Style reference Figure 12.9 A style transfer example 384CHAPTER 12Generative deep learning The key notion behind implementing style transfer is the same idea that’s centralto all deep learning algorithms: you define a loss function to specify what you want toachieve, and you minimize this loss. We know what we want to achieve: conserving thecontent of the original image while adopting the style of the reference image. If wewere able to mathematically define content and style, then an appropriate loss functionto minimize would be the following:loss = (distance(style(reference_image) - style(combination_image)) + distance(content(original_image) - content(combination_image)))Here, distance is a norm function such as the L2 norm, content is a function thattakes an image and computes a representation of its content, and style is a functionthat takes an image and computes a representation of its style. Minimizing this losscauses style(combination_image) t o b e c l o s e t o style(reference_image), andcontent(combination_image) is close to content(original_image), thus achievingstyle transfer as we defined it. A fundamental observation made by Gatys et al. was that deep convolutional neu-ral networks offer a way to mathematically define the style and content functions.Let’s see how.12.3.1 The content lossAs you already know, activations from earlier layers in a network contain local informa-tion about the image, whereas activations from higher layers contain increasinglyglobal, abstract information. Formulated in a different way, the activations of the dif-ferent layers of a convnet provide a decomposition of the contents of an image overdifferent spatial scales. Therefore, you’d expect the content of an image, which ismore global and abstract, to be captured by the representations of the upper layers ina convnet. A good candidate for content loss is thus the L2 norm between the activations ofan upper layer in a pretrained convnet, computed over the target image, and the acti-vations of the same layer computed over the generated image. This guarantees that, asseen from the upper layer, the generated image will look similar to the original targetimage. Assuming that what the upper layers of a convnet see is really the content oftheir input images, this works as a way to preserve image content. 12.3.2 The style lossThe content loss only uses a single upper layer, but the style loss as defined by Gatyset al. uses multiple layers of a convnet: you try to capture the appearance of the style-reference image at all spatial scales extracted by the convnet, not just a single scale.For the style loss, Gatys et al. use the Gram matrix of a layer’s activations: the innerproduct of the feature maps of a given layer. This inner product can be understoodas representing a map of the correlations between the layer’s features. These feature 385Neural style transfercorrelations capture the statistics of the patterns of a particular spatial scale, whichempirically correspond to the appearance of the textures found at this scale. Hence, the style loss aims to preserve similar internal correlations within the activa-tions of different layers, across the style-reference image and the generated image. Inturn, this guarantees that the textures found at different spatial scales look similaracross the style-reference image and the generated image. In short, you can use a pretrained convnet to define a loss that will do the following:Preserve content by maintaining similar high-level layer activations between theoriginal image and the generated image. The convnet should “see” both theoriginal image and the generated image as containing the same things.Preserve style by maintaining similar correlations within activations for both low-level layers and high-level layers. Feature correlations capture textures: the gen-erated image and the style-reference image should share the same textures atdifferent spatial scales.Now let’s look at a Keras implementation of the original 2015 neural style transferalgorithm. As you’ll see, it shares many similarities with the DeepDream implementa-tion we developed in the previous section. 12.3.3 Neural style transfer in KerasNeural style transfer can be implemented using any pretrained convnet. Here, we’lluse the VGG19 network used by Gatys et al. VGG19 is a simple variant of the VGG16network introduced in chapter 5, with three more convolutional layers. Here’s the general process:Set up a network that computes VGG19 layer activations for the style-referenceimage, the base image, and the generated image at the same time.Use the layer activations computed over these three images to define the lossfunction described earlier, which we’ll minimize in order to achieve styletransfer.Set up a gradient-descent process to minimize this loss function.Let’s start by defining the paths to the style-reference image and the base image. Tomake sure that the processed images are a similar size (widely different sizes makestyle transfer more difficult), we’ll later resize them all to a shared height of 400 px.from tensorflow import keras base_image_path = keras.utils.get_file( \"sf.jpg\", origin=\"https:/ /img-datasets.s3.amazonaws.com/sf.jpg\")style_reference_image_path = keras.utils.get_file( \"starry_night.jpg\", origin=\"https:/ /img-datasets.s3.amazonaws.com/starry_night.jpg\") Listing 12.16 Getting the style and content imagesPath to the image we want to transformPath to the style image 386CHAPTER 12Generative deep learningoriginal_width, original_height = keras.utils.load_img(base_image_path).sizeimg_height = 400 img_width = round(original_width * img_height / original_height) Our content image is shown in figure 12.10, and figure 12.11 shows our style image.Dimensions of the generated picture Figure 12.10 Content image: San Francisco from Nob Hill Figure 12.11 Style image: Starry Night by Van Gogh 387Neural style transferWe also need some auxiliary functions for loading, preprocessing, and postprocessingthe images that go in and out of the VGG19 convnet.import numpy as np def preprocess_image(image_path): img = keras.utils.load_img( image_path, target_size=(img_height, img_width)) img = keras.utils.img_to_array(img) img = np.expand_dims(img, axis=0) img = keras.applications.vgg19.preprocess_input(img) return img def deprocess_image(img): img = img.reshape((img_height, img_width, 3)) img[:, :, 0] += 103.939 img[:, :, 1] += 116.779 img[:, :, 2] += 123.68 img = img[:, :, ::-1] img = np.clip(img, 0, 255).astype(\"uint8\") return imgLet’s set up the VGG19 network. Like in the DeepDream example, we’ll use the pre-trained convnet to create a feature exactor model that returns the activations of inter-mediate layers—all layers in the model this time.model = keras.applications.vgg19.VGG19(weights=\"imagenet\", include_top=False) outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])feature_extractor = keras.Model(inputs=model.inputs, outputs=outputs_dict) Let’s define the content loss, which will make sure the top layer of the VGG19 convnethas a similar view of the style image and the combination image.def content_loss(base_img, combination_img): return tf.reduce_sum(tf.square(combination_img - base_img))Next is the style loss. It uses an auxiliary function to compute the Gram matrix of aninput matrix: a map of the correlations found in the original feature matrix. Listing 12.17 Auxiliary functions Listing 12.18 Using a pretrained VGG19 model to create a feature extractor Listing 12.19 Content lossUtil function to open, resize, and format pictures into appropriate arraysUtil function to convert a NumPy array into a valid imageZero-centering by removing the mean pixel value from ImageNet. This reverses a transformation done by vgg19.preprocess_input.Converts images from 'BGR' to 'RGB'. This is also part of the reversal of vgg19.preprocess_input. Build a VGG19 model loaded withpretrained ImageNet weights.Model that returns the activation values forevery target layer (as a dict) 388CHAPTER 12Generative deep learningdef gram_matrix(x): x = tf.transpose(x, (2, 0, 1)) features = tf.reshape(x, (tf.shape(x)[0], -1)) gram = tf.matmul(features, tf.transpose(features)) return gram def style_loss(style_img, combination_img): S = gram_matrix(style_img) C = gram_matrix(combination_img) channels = 3 size = img_height * img_width return tf.reduce_sum(tf.square(S - C)) / (4.0 * (channels ** 2) * (size ** 2))To these two loss components, you add a third: the total variation loss, which operateson the pixels of the generated combination image. It encourages spatial continuity inthe generated image, thus avoiding overly pixelated results. You can interpret it as aregularization loss.def total_variation_loss(x): a = tf.square( x[:, : img_height - 1, : img_width - 1, :] - x[:, 1:, : img_width - 1, :] ) b = tf.square( x[:, : img_height - 1, : img_width - 1, :] - x[:, : img_height - 1, 1:, :] ) return tf.reduce_sum(tf.pow(a + b, 1.25))The loss that you minimize is a weighted average of these three losses. To compute thecontent loss, you use only one upper layer—the block5_conv2 layer—whereas for thestyle loss, you use a list of layers that spans both low-level and high-level layers. You addthe total variation loss at the end. Depending on the style-reference image and content image you’re using, you’lllikely want to tune the content_weight coefficient (the contribution of the contentloss to the total loss). A higher content_weight means the target content will be morerecognizable in the generated image.style_layer_names = [ \"block1_conv1\", \"block2_conv1\", \"block3_conv1\", \"block4_conv1\", \"block5_conv1\",]content_layer_name = \"block5_conv2\" total_variation_weight = 1e-6 Listing 12.20 Style loss Listing 12.21 Total variation loss Listing 12.22 Defining the final loss that you’ll minimizeList of layers to use for the style lossThe layer to use forthe content lossContribution weight of the total variation loss 389Neural style transferstyle_weight = 1e-6 content_weight = 2.5e-8 def compute_loss(combination_image, base_image, style_reference_image): input_tensor = tf.concat( [base_image, style_reference_image, combination_image], axis=0) features = feature_extractor(input_tensor) loss = tf.zeros(shape=()) layer_features = features[content_layer_name] base_image_features = layer_features[0, :, :, :] combination_features = layer_features[2, :, :, :] loss = loss + content_weight * content_loss( base_image_features, combination_features ) for layer_name in style_layer_names: layer_features = features[layer_name] style_reference_features = layer_features[1, :, :, :] combination_features = layer_features[2, :, :, :] style_loss_value = style_loss( style_reference_features, combination_features) loss += (style_weight / len(style_layer_names)) * style_loss_value loss += total_variation_weight * total_variation_loss(combination_image) return lossFinally, let’s set up the gradient-descent process. In the original Gatys et al. paper, opti-mization is performed using the L-BFGS algorithm, but that’s not available in Tensor-Flow, so we’ll just do mini-batch gradient descent with the SGD optimizer instead. We’llleverage an optimizer feature you haven’t seen before: a learning-rate schedule. We’lluse it to gradually decrease the learning rate from a very high value (100) to a muchsmaller final value (about 20). That way, we’ll make fast progress in the early stages oftraining and then proceed more cautiously as we get closer to the loss minimum.import tensorflow as tf @tf.function def compute_loss_and_grads( combination_image, base_image, style_reference_image): with tf.GradientTape() as tape: loss = compute_loss( combination_image, base_image, style_reference_image) grads = tape.gradient(loss, combination_image) return loss, grads optimizer = keras.optimizers.SGD( keras.optimizers.schedules.ExponentialDecay( initial_learning_rate=100.0, decay_steps=100, decay_rate=0.96 )) Listing 12.23 Setting up the gradient-descent processContribution weight of the style lossContributionweight of thecontent lossInitializethe lossto 0.Add the content loss.Add thestyleloss.Add the totalvariation loss. We make the training step fast by compiling it as a tf.function.We’ll start with a learning rate of 100 and decrease it by 4% every 100 steps. 390CHAPTER 12Generative deep learningbase_image = preprocess_image(base_image_path)style_reference_image = preprocess_image(style_reference_image_path)combination_image = tf.Variable(preprocess_image(base_image_path)) iterations = 4000 for i in range(1, iterations + 1): loss, grads = compute_loss_and_grads( combination_image, base_image, style_reference_image ) optimizer.apply_gradients([(grads, combination_image)]) if i % 100 == 0: print(f\"Iteration {i}: loss={loss:.2f}\") img = deprocess_image(combination_image.numpy()) fname = f\"combination_image_at_iteration_{i}.png\" keras.utils.save_img(fname, img) Figure 12.12 shows what you get. Keep in mind that what this technique achieves ismerely a form of image retexturing, or texture transfer. It works best with style-referenceimages that are strongly textured and highly self-similar, and with content targets thatdon’t require high levels of detail in order to be recognizable. It typically can’t achievefairly abstract feats such as transferring the style of one portrait to another. The algo-rithm is closer to classical signal processing than to AI, so don’t expect it to work likemagic! Additionally, note that this style-transfer algorithm is slow to run. But the transforma-tion operated by the setup is simple enough that it can be learned by a small, fastUse a Variable to store thecombination image since we’ll beupdating it during training.Update the combination image in a direction that reduces the style transfer loss.Save the combinationimage at regular intervals. Figure 12.12 Style transfer result 391Generating images with variational autoencodersfeedforward convnet as well—as long as you have appropriate training data available.Fast style transfer can thus be achieved by first spending a lot of compute cycles togenerate input-output training examples for a fixed style-reference image, using themethod outlined here, and then training a simple convnet to learn this style-specifictransformation. Once that’s done, stylizing a given image is instantaneous: it’s just aforward pass of this small convnet. 12.3.4 Wrapping upStyle transfer consists of creating a new image that preserves the contents of atarget image while also capturing the style of a reference image.Content can be captured by the high-level activations of a convnet.Style can be captured by the internal correlations of the activations of differentlayers of a convnet.Hence, deep learning allows style transfer to be formulated as an optimizationprocess using a loss defined with a pretrained convnet.Starting from this basic idea, many variants and refinements are possible. 12.4 Generating images with variational autoencodersThe most popular and successful application of creative AI today is image generation:learning latent visual spaces and sampling from them to create entirely new picturesinterpolated from real ones—pictures of imaginary people, imaginary places, imagi-nary cats and dogs, and so on. In this section and the next, we’ll review some high-level concepts pertaining toimage generation, alongside implementation details relative to the two main tech-niques in this domain: variational autoencoders (VAEs) and generative adversarial networks(GANs). Note that the techniques I’ll present here aren’t specific to images—youcould develop latent spaces of sound, music, or even text, using GANs and VAEs—butin practice, the most interesting results have been obtained with pictures, and that’swhat we’ll focus on here.12.4.1 Sampling from latent spaces of imagesThe key idea of image generation is to develop a low-dimensional latent space of repre-sentations (which, like everything else in deep learning, is a vector space), where anypoint can be mapped to a “valid” image: an image that looks like the real thing. Themodule capable of realizing this mapping, taking as input a latent point and output-ting an image (a grid of pixels), is called a generator (in the case of GANs) or a decoder(in the case of VAEs). Once such a latent space has been learned, you can samplepoints from it, and, by mapping them back to image space, generate images that havenever been seen before (see figure 12.13). These new images are the in-betweens ofthe training images. G A N s a n d V A E s a r e t w o d i f f e r e n t s t r a t e g i e s f o r l e a r n i n g s u c h l a t e n t s p a c e s o fimage representations, each with its own characteristics. VAEs are great for learning 392CHAPTER 12Generative deep learning latent spaces that are well structured, where specific directions encode a meaningfulaxis of variation in the data (see figure 12.14). GANs generate images that can poten-tially be highly realistic, but the latent space they come from may not have as muchstructure and continuity. Generator / Decoder Training data Latent spaceof images(a vector space)Latent spacevectorArtiﬁcialimageLearningprocess? Figure 12.13 Learning a latent vector space of images and using it to sample new images Figure 12.14 A continuous space of faces generated by Tom White using VAEs 393Generating images with variational autoencoders12.4.2 Concept vectors for image editingWe already hinted at the idea of a concept vector when we covered word embeddings inchapter 11. The idea is still the same: given a latent space of representations, or anembedding space, certain directions in the space may encode interesting axes of vari-ation in the original data. In a latent space of images of faces, for instance, there maybe a smile vector, such that if latent point z is the embedded representation of a certainface, then latent point z + s is the embedded representation of the same face, smiling.Once you’ve identified such a vector, it then becomes possible to edit images by pro-jecting them into the latent space, moving their representation in a meaningful way,and then decoding them back to image space. There are concept vectors for essen-tially any independent dimension of variation in image space—in the case of faces,you may discover vectors for adding sunglasses to a face, removing glasses, turning amale face into a female face, and so on. Figure 12.15 is an example of a smile vector, aconcept vector discovered by Tom White, from the Victoria University School ofDesign in New Zealand, using VAEs trained on a dataset of faces of celebrities (theCelebA dataset). 12.4.3 Variational autoencodersVariational autoencoders, simultaneously discovered by Kingma and Welling inDecember 20135 and Rezende, Mohamed, and Wierstra in January 2014,6 are a kindof generative model that’s especially appropriate for the task of image editing via con-cept vectors. They’re a modern take on autoencoders (a type of network that aims toencode an input to a low-dimensional latent space and then decode it back) thatmixes ideas from deep learning with Bayesian inference.5Diederik P. Kingma and Max Welling, “Auto-Encoding Variational Bayes,” arXiv (2013), https:/ /arxiv.org/abs/1312.6114.6Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra, “Stochastic Backpropagation and Approxi-mate Inference in Deep Generative Models,” arXiv (2014), https:/ /arxiv.org/abs/1401.4082.Figure 12.15 The smile vector 394CHAPTER 12Generative deep learning A classical image autoencoder takes an image, maps it to a latent vector space viaan encoder module, and then decodes it back to an output with the same dimensionsas the original image, via a decoder module (see figure 12.16). It’s then trained byusing as target data the same images as the input images, meaning the autoencoderlearns to reconstruct the original inputs. By imposing various constraints on thecode (the output of the encoder), you can get the autoencoder to learn more- orless-interesting latent representations of the data. Most commonly, you’ll constrainthe code to be low-dimensional and sparse (mostly zeros), in which case the encoderacts as a way to compress the input data into fewer bits of information. In practice, such classical autoencoders don’t lead to particularly useful or nicelystructured latent spaces. They’re not much good at compression, either. For these rea-sons, they have largely fallen out of fashion. VAEs, however, augment autoencoderswith a little bit of statistical magic that forces them to learn continuous, highly struc-tured latent spaces. They have turned out to be a powerful tool for image generation. A VAE, instead of compressing its input image into a fixed code in the latentspace, turns the image into the parameters of a statistical distribution: a mean and avariance. Essentially, this means we’re assuming the input image has been generatedby a statistical process, and that the randomness of this process should be taken intoaccount during encoding and decoding. The VAE then uses the mean and varianceparameters to randomly sample one element of the distribution, and decodes thatelement back to the original input (see figure 12.17). The stochasticity of this pro-cess improves robustness and forces the latent space to encode meaningful repre-sentations everywhere: every point sampled in the latent space is decoded to a validoutput. In technical terms, here’s how a VAE works:1An encoder module turns the input sample, input_img, into two parameters ina latent space of representations, z_mean and z_log_variance.2You randomly sample a point z f r o m t h e l a t e n t n o r m a l d i s t r i b u t i o n t h a t ’ sassumed to generate the input image, via z = z_mean + exp(z_log_variance) *epsilon, where epsilon is a random tensor of small values.3A decoder module maps this point in the latent space back to the originalinput image. OriginalinputCompressedrepresentationReconstructedinput EncoderDecoderFigure 12.16 An autoencoder mapping an input x to a compressed representation and then decoding it back as x' 395Generating images with variational autoencoders Because epsilon is random, the process ensures that every point that’s close to thelatent location where you encoded input_img (z-mean) can be decoded to somethingsimilar to input_img, thus forcing the latent space to be continuously meaningful. Anytwo close points in the latent space will decode to highly similar images. Continuity,combined with the low dimensionality of the latent space, forces every direction in thelatent space to encode a meaningful axis of variation of the data, making the latentspace very structured and thus highly suitable to manipulation via concept vectors. The parameters of a VAE are trained via two loss functions: a reconstruction loss thatforces the decoded samples to match the initial inputs, and a regularization loss thathelps learn well-rounded latent distributions and reduces overfitting to the trainingdata. Schematically, the process looks like this:z_mean, z_log_variance = encoder(input_img) z = z_mean + exp(z_log_variance) * epsilon reconstructed_img = decoder(z) model = Model(input_img, reconstructed_img) You can then train the model using the reconstruction loss and the regularization loss.For the regularization loss, we typically use an expression (the Kullback–Leibler diver-gence) meant to nudge the distribution of the encoder output toward a well-roundednormal distribution centered around 0. This provides the encoder with a sensibleassumption about the structure of the latent space it’s modeling. Now let’s see what implementing a VAE looks like in practice! Input imageReconstructedimageDistribution over latentspace deﬁned by z_meanand z_log_var Point randomlysampled fromthe distribution EncoderDecoderFigure 12.17 A VAE maps an image to two vectors, z_mean and z_log_sigma, which define a probability distribution over the latent space, used to sample a latent point to decode. Encodes the input into meanand variance parametersDraws a latent point using a small random epsilonDecodes z back to an imageInstantiates the autoencoder model, whichmaps an input image to its reconstruction 396CHAPTER 12Generative deep learning12.4.4 Implementing a VAE with KerasWe’re going to be implementing a VAE that can generate MNIST digits. It’s going tohave three parts:An encoder network that turns a real image into a mean and a variance in thelatent spaceA sampling layer that takes such a mean and variance, and uses them to samplea random point from the latent spaceA decoder network that turns points from the latent space back into imagesThe following listing shows the encoder network we’ll use, mapping images to theparameters of a probability distribution over the latent space. It’s a simple convnetthat maps the input image x to two vectors, z_mean and z_log_var. One importantdetail is that we use strides for downsampling feature maps instead of max pooling.The last time we did this was in the image segmentation example in chapter 9. Recallthat, in general, strides are preferable to max pooling for any model that cares aboutinformation location—that is to say, where stuff is in the image—and this one does, sinceit will have to produce an image encoding that can be used to reconstruct a validimage.from tensorflow import keras from tensorflow.keras import layers latent_dim = 2 encoder_inputs = keras.Input(shape=(28, 28, 1))x = layers.Conv2D( 32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)x = layers.Flatten()(x)x = layers.Dense(16, activation=\"relu\")(x)z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x) z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x) encoder = keras.Model(encoder_inputs, [z_mean, z_log_var], name=\"encoder\")Its summary looks like this:>>> encoder.summary()Model: \"encoder\" __________________________________________________________________________________________________Layer (type) Output Shape Param # Connected to==================================================================================================input_1 (InputLayer) [(None, 28, 28, 1)] 0 __________________________________________________________________________________________________conv2d (Conv2D) (None, 14, 14, 32) 320 input_1[0][0]__________________________________________________________________________________________________conv2d_1 (Conv2D) (None, 7, 7, 64) 18496 conv2d[0][0]__________________________________________________________________________________________________Listing 12.24 VAE encoder networkDimensionality of the latent space: a 2D planeThe input image ends up being encoded into these two parameters. 397Generating images with variational autoencodersflatten (Flatten) (None, 3136) 0 conv2d_1[0][0]__________________________________________________________________________________________________dense (Dense) (None, 16) 50192 flatten[0][0]__________________________________________________________________________________________________z_mean (Dense) (None, 2) 34 dense[0][0]__________________________________________________________________________________________________z_log_var (Dense) (None, 2) 34 dense[0][0]==================================================================================================Total params: 69,076 Trainable params: 69,076 Non-trainable params: 0 __________________________________________________________________________________________________Next is the code for using z_mean and z_log_var, the parameters of the statistical dis-tribution assumed to have produced input_img, to generate a latent space point z.import tensorflow as tf class Sampler(layers.Layer): def call(self, z_mean, z_log_var): batch_size = tf.shape(z_mean)[0] z_size = tf.shape(z_mean)[1] epsilon = tf.random.normal(shape=(batch_size, z_size)) return z_mean + tf.exp(0.5 * z_log_var) * epsilon The following listing shows the decoder implementation. We reshape the vector z tothe dimensions of an image and then use a few convolution layers to obtain a finalimage output that has the same dimensions as the original input_img.latent_inputs = keras.Input(shape=(latent_dim,)) x = layers.Dense(7 * 7 * 64, activation=\"relu\")(latent_inputs) x = layers.Reshape((7, 7, 64))(x) x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)decoder_outputs = layers.Conv2D(1, 3, activation=\"sigmoid\", padding=\"same\")(x) decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")Its summary looks like this:>>> decoder.summary()Model: \"decoder\" _________________________________________________________________Layer (type) Output Shape Param # =================================================================input_2 (InputLayer) [(None, 2)] 0 _________________________________________________________________dense_1 (Dense) (None, 3136) 9408 Listing 12.25 Latent-space-sampling layer Listing 12.26 VAE decoder network, mapping latent space points to imagesDraw a batch ofrandom normalvectors.Apply the VAEsamplingformula. Input where we’ll feed zProduce the same number of coefficients that wehad at the level of the Flatten layer in the encoder.Revert the Flatten layer of the encoder.Revert theConv2D layersof the encoder.The output ends up with shape (28, 28, 1). 398CHAPTER 12Generative deep learning_________________________________________________________________reshape (Reshape) (None, 7, 7, 64) 0 _________________________________________________________________conv2d_transpose (Conv2DTran (None, 14, 14, 64) 36928 _________________________________________________________________conv2d_transpose_1 (Conv2DTr (None, 28, 28, 32) 18464 _________________________________________________________________conv2d_2 (Conv2D) (None, 28, 28, 1) 289 =================================================================Total params: 65,089 Trainable params: 65,089 Non-trainable params: 0 _________________________________________________________________Now let’s create the VAE model itself. This is your first example of a model that isn’tdoing supervised learning (an autoencoder is an example of self-supervised learning,because it uses its inputs as targets). Whenever you depart from classic supervisedlearning, it’s common to subclass the Model c l a s s a n d i m p l e m e n t a c u s t o m train_step() to specify the new training logic, a workflow you learned about in chapter 7.That’s what we’ll do here.class VAE(keras.Model): def __init__(self, encoder, decoder, **kwargs): super().__init__(**kwargs) self.encoder = encoder self.decoder = decoder self.sampler = Sampler() self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\") self.reconstruction_loss_tracker = keras.metrics.Mean( name=\"reconstruction_loss\") self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\") @property def metrics(self): return [self.total_loss_tracker, self.reconstruction_loss_tracker, self.kl_loss_tracker] def train_step(self, data): with tf.GradientTape() as tape: z_mean, z_log_var = self.encoder(data) z = self.sampler(z_mean, z_log_var) reconstruction = decoder(z) reconstruction_loss = tf.reduce_mean( tf.reduce_sum( keras.losses.binary_crossentropy(data, reconstruction), axis=(1, 2) ) ) kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)) total_loss = reconstruction_loss + tf.reduce_mean(kl_loss) Listing 12.27 VAE model with custom train_step()We use these metrics to keeptrack of the loss averagesover each epoch.We list the metrics in the metrics property to enable the model to reset them after each epoch (or between multiple calls to fit()/evaluate()).We sum the reconstruction loss over the spatial dimensions (axes 1 and 2) and take its mean over the batch dimension.Add the regularizationterm (Kullback–Leiblerdivergence). 399Generating images with variational autoencoders grads = tape.gradient(total_loss, self.trainable_weights) self.optimizer.apply_gradients(zip(grads, self.trainable_weights)) self.total_loss_tracker.update_state(total_loss) self.reconstruction_loss_tracker.update_state(reconstruction_loss) self.kl_loss_tracker.update_state(kl_loss) return { \"total_loss\": self.total_loss_tracker.result(), \"reconstruction_loss\": self.reconstruction_loss_tracker.result(), \"kl_loss\": self.kl_loss_tracker.result(), }Finally, we’re ready to instantiate and train the model on MNIST digits. Because theloss is taken care of in the custom layer, we don’t specify an external loss at compiletime (loss=None), which in turn means we won’t pass target data during training (asyou can see, we only pass x_train to the model in fit()).import numpy as np (x_train, _), (x_test, _) = keras.datasets.mnist.load_data()mnist_digits = np.concatenate([x_train, x_test], axis=0) mnist_digits = np.expand_dims(mnist_digits, -1).astype(\"float32\") / 255 vae = VAE(encoder, decoder)vae.compile(optimizer=keras.optimizers.Adam(), run_eagerly=True) vae.fit(mnist_digits, epochs=30, batch_size=128) Once the model is trained, we can use the decoder network to turn arbitrary latentspace vectors into images.import matplotlib.pyplot as plt n = 30 digit_size = 28 figure = np.zeros((digit_size * n, digit_size * n)) grid_x = np.linspace(-1, 1, n) grid_y = np.linspace(-1, 1, n)[::-1] for i, yi in enumerate(grid_y): for j, xi in enumerate(grid_x): z_sample = np.array([[xi, yi]]) x_decoded = vae.decoder.predict(z_sample) digit = x_decoded[0].reshape(digit_size, digit_size) figure[ i * digit_size : (i + 1) * digit_size,Listing 12.28 Training the VAE Listing 12.29 Sampling a grid of images from the 2D latent spaceWe train on all MNIST digits, so we concatenate the training and test samples.Note that we don’t pass a lossargument in compile(), since the lossis already part of the train_step().Note that we don’t pass targets in fit(), since train_step() doesn’t expect any. We’ll display a grid of 30 × 30 digits (900 digits total).Sample points linearly on a 2D grid.Iterate over grid locations.For each location, sample a digit and add it to our figure. 400CHAPTER 12Generative deep learning j * digit_size : (j + 1) * digit_size, ] = digit plt.figure(figsize=(15, 15))start_range = digit_size // 2 end_range = n * digit_size + start_rangepixel_range = np.arange(start_range, end_range, digit_size)sample_range_x = np.round(grid_x, 1)sample_range_y = np.round(grid_y, 1)plt.xticks(pixel_range, sample_range_x)plt.yticks(pixel_range, sample_range_y)plt.xlabel(\"z[0]\")plt.ylabel(\"z[1]\")plt.axis(\"off\")plt.imshow(figure, cmap=\"Greys_r\")The grid of sampled digits (see figure 12.18) shows a completely continuous distribu-tion of the different digit classes, with one digit morphing into another as you follow apath through latent space. Specific directions in this space have a meaning: for exam-ple, there are directions for “five-ness,” “one-ness,” and so on. Figure 12.18 Grid of digits decoded from the latent space 401Introduction to generative adversarial networksIn the next section, we’ll cover in detail the other major tool for generating artificialimages: generative adversarial networks (GANs). 12.4.5 Wrapping upImage generation with deep learning is done by learning latent spaces that cap-ture statistical information about a dataset of images. By sampling and decod-ing points from the latent space, you can generate never-before-seen images.There are two major tools to do this: VAEs and GANs.VAEs result in highly structured, continuous latent representations. For this rea-son, they work well for doing all sorts of image editing in latent space: faceswapping, turning a frowning face into a smiling face, and so on. They also worknicely for doing latent-space-based animations, such as animating a walk along across section of the latent space or showing a starting image slowly morphinginto different images in a continuous way.GANs enable the generation of realistic single-frame images but may not inducelatent spaces with solid structure and high continuity.Most successful practical applications I have seen with images rely on VAEs, but GANshave enjoyed enduring popularity in the world of academic research. You’ll find outhow they work and how to implement one in the next section. 12.5 Introduction to generative adversarial networksGenerative adversarial networks (GANs), introduced in 2014 by Goodfellow et al.,7 arean alternative to VAEs for learning latent spaces of images. They enable the genera-tion of fairly realistic synthetic images by forcing the generated images to be statisti-cally almost indistinguishable from real ones. An intuitive way to understand GANs is to imagine a forger trying to create a fakePicasso painting. At first, the forger is pretty bad at the task. He mixes some of hisfakes with authentic Picassos and shows them all to an art dealer. The art dealer makesan authenticity assessment for each painting and gives the forger feedback about whatmakes a Picasso look like a Picasso. The forger goes back to his studio to prepare somenew fakes. As time goes on, the forger becomes increasingly competent at imitatingthe style of Picasso, and the art dealer becomes increasingly expert at spotting fakes.In the end, they have on their hands some excellent fake Picassos. That’s what a GAN is: a forger network and an expert network, each being trainedto best the other. As such, a GAN is made of two parts:Generator network—Takes as input a random vector (a random point in thelatent space), and decodes it into a synthetic imageDiscriminator network (or adversary)—Takes as input an image (real or synthetic),and predicts whether the image came from the training set or was created bythe generator network7Ian Goodfellow et al., “Generative Adversarial Networks,” arXiv (2014), https:/ /arxiv.org/abs/1406.2661. 402CHAPTER 12Generative deep learningThe generator network is trained to be able to fool the discriminator network, andthus it evolves toward generating increasingly realistic images as training goes on: arti-ficial images that look indistinguishable from real ones, to the extent that it’s impossi-ble for the discriminator network to tell the two apart (see figure 12.19). Meanwhile,the discriminator is constantly adapting to the gradually improving capabilities of thegenerator, setting a high bar of realism for the generated images. Once training isover, the generator is capable of turning any point in its input space into a believableimage. Unlike VAEs, this latent space has fewer explicit guarantees of meaningfulstructure; in particular, it isn’t continuous. Remarkably, a GAN is a system where the optimization minimum isn’t fixed, unlike inany other training setup you’ve encountered in this book. Normally, gradient descentconsists of rolling down hills in a static loss landscape. But with a GAN, every steptaken down the hill changes the entire landscape a little. It’s a dynamic system wherethe optimization process is seeking not a minimum, but an equilibrium between twoforces. For this reason, GANs are notoriously difficult to train—getting a GAN to workrequires lots of careful tuning of the model architecture and training parameters.12.5.1 A schematic GAN implementationIn this section, we’ll explain how to implement a GAN in Keras in its barest form.GANs are advanced, so diving deeply into the technical details of architectures likethat of the StyleGAN2 that generated the images in figure 12.20 would be out of scopefor this book. The specific implementation we’ll use in this demonstration is a deepconvolutional GAN (DCGAN): a very basic GAN where the generator and discriminatorare deep convnets.Generator (decoder) Discriminator“Real”, “Fake” Randomlatent spacevectorGenerated(decoded)image Mix of realand fake imagesTrainingfeedback Figure 12.19 A generator transforms random latent vectors into images, and a discriminator seeks to tell real images from generated ones. The generator is trained to fool the discriminator. 403Introduction to generative adversarial networks We’ll train our GAN on images from the Large-scale CelebFaces Attributes dataset(known as CelebA), a dataset of 200,000 faces of celebrities (http:/ /mmlab.ie.cuhk.edu.hk/projects/CelebA.html) To speed up training, we’ll resize the images to 64 × 64,so we’ll be learning to generate 64 × 64 images of human faces. Schematically, the GAN looks like this:A generator network maps vectors of shape (latent_dim,) to images of shape(64, 64, 3).A discriminator network maps images of shape (64, 64, 3) to a binary scoreestimating the probability that the image is real.A gan network chains the generator and the discriminator together: gan(x) =discriminator(generator(x)). Thus, this gan network maps latent space vec-tors to the discriminator’s assessment of the realism of these latent vectors asdecoded by the generator.We train the discriminator using examples of real and fake images along with“real”/“fake” labels, just as we train any regular image-classification model.To train the generator, we use the gradients of the generator’s weights withregard to the loss of the gan model. This means that at every step, we move theweights of the generator in a direction that makes the discriminator more likelyto classify as “real” the images decoded by the generator. In other words, wetrain the generator to fool the discriminator. 12.5.2 A bag of tricksThe process of training GANs and tuning GAN implementations is notoriously diffi-cult. There are a number of known tricks you should keep in mind. Like most thingsin deep learning, it’s more alchemy than science: these tricks are heuristics, not theory-backed guidelines. They’re supported by a level of intuitive understanding of the phe-nomenon at hand, and they’re known to work well empirically, although not necessar-ily in every context.Figure 12.20 Latent space dwellers. Images generated by https:/ /thispersondoesnotexist.com using a StyleGAN2 model. (Image credit: Phillip Wang is the website author. The model used is the StyleGAN2 model from Karras et al., https:/ /arxiv.org/abs/1912.04958.) 404CHAPTER 12Generative deep learning Here are a few of the tricks used in the implementation of the GAN generator anddiscriminator in this section. It isn’t an exhaustive list of GAN-related tips; you’ll findmany more across the GAN literature:We use strides instead of pooling for downsampling feature maps in the dis-criminator, just like we did in our VAE encoder.We sample points from the latent space using a normal distribution (Gaussian dis-tribution), not a uniform distribution.Stochasticity is good for inducing robustness. Because GAN training results in adynamic equilibrium, GANs are likely to get stuck in all sorts of ways. Introduc-ing randomness during training helps prevent this. We introduce randomnessby adding random noise to the labels for the discriminator.Sparse gradients can hinder GAN training. In deep learning, sparsity is oftena desirable property, but not in GANs. Two things can induce gradient spar-sity: max pooling operations and relu activations. Instead of max pooling,we recommend using strided convolutions for downsampling, and we recom-mend using a LeakyReLU l a y e r i n s t e a d o f a relu a c t i v a t i o n . I t ’ s s i m i l a r t orelu, but it relaxes sparsity constraints by allowing small negative activationvalues.In generated images, it’s common to see checkerboard artifacts caused byunequal coverage of the pixel space in the generator (see figure 12.21). To fixthis, we use a kernel size that’s divisible by the stride size whenever we use astrided Conv2DTranspose or Conv2D in both the generator and the discriminator. 12.5.3 Getting our hands on the CelebA datasetYou can download the dataset manually from the website: http:/ /mmlab.ie.cuhk.edu.hk/projects/CelebA.html. If you’re using Colab, you can run the following to down-load the data from Google Drive and uncompress it. Figure 12.21 Checkerboard artifacts caused by mismatching strides and kernel sizes, resulting in unequal pixel-space coverage: one of the many gotchas of GANs 405Introduction to generative adversarial networks!mkdir celeba_gan !gdown --id 1O7m1010EJjLE5QxLZiM9Fpjs7Oj6e684 -O celeba_gan/data.zip !unzip -qq celeba_gan/data.zip -d celeba_gan Once you’ve got the uncompressed images in a directory, you can use image_data-set_from_directory to turn it into a dataset. Since we just need the images—thereare no labels—we’ll specify label_mode=None.from tensorflow import kerasdataset = keras.utils_dataset_from_directory( \"celeba_gan\", label_mode=None, image_size=(64, 64), batch_size=32, smart_resize=True) Finally, let’s rescale the images to the [0-1] range.dataset = dataset.map(lambda x: x / 255.)You can use the following code to display a sample image. import matplotlib.pyplot as plt for x in dataset: plt.axis(\"off\") plt.imshow((x.numpy() * 255).astype(\"int32\")[0]) break12.5.4 The discriminatorFirst, we’ll develop a discriminator model that takes as input a candidate image(real or synthetic) and classifies it into one of two classes: “generated image” or “realimage that comes from the training set.” One of the many issues that commonlyarise with GANs is that the generator gets stuck with generated images that look likenoise. A possible solution is to use dropout in the discriminator, so that’s what wewill do here. Listing 12.30 Getting the CelebA data Listing 12.31 Creating a dataset from a directory of images Listing 12.32 Rescaling the imagesListing 12.33 Displaying the first imageCreate a working directory.Download the compressed datausing gdown (available by defaultin Colab; install it otherwise).Uncompress the data. Only the images will be returned—no labels.We will resize the images to 64 × 64 by using a smart combination of cropping and resizing to preserve aspect ratio. We don’t want face proportions to get distorted! 406CHAPTER 12Generative deep learningfrom tensorflow.keras import layers discriminator = keras.Sequential( [ keras.Input(shape=(64, 64, 3)), layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"), layers.LeakyReLU(alpha=0.2), layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"), layers.LeakyReLU(alpha=0.2), layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"), layers.LeakyReLU(alpha=0.2), layers.Flatten(), layers.Dropout(0.2), layers.Dense(1, activation=\"sigmoid\"), ], name=\"discriminator\",)Here’s the discriminator model summary:>>> discriminator.summary()Model: \"discriminator\" _________________________________________________________________Layer (type) Output Shape Param # =================================================================conv2d (Conv2D) (None, 32, 32, 64) 3136 _________________________________________________________________leaky_re_lu (LeakyReLU) (None, 32, 32, 64) 0 _________________________________________________________________conv2d_1 (Conv2D) (None, 16, 16, 128) 131200 _________________________________________________________________leaky_re_lu_1 (LeakyReLU) (None, 16, 16, 128) 0 _________________________________________________________________conv2d_2 (Conv2D) (None, 8, 8, 128) 262272 _________________________________________________________________leaky_re_lu_2 (LeakyReLU) (None, 8, 8, 128) 0 _________________________________________________________________flatten (Flatten) (None, 8192) 0 _________________________________________________________________dropout (Dropout) (None, 8192) 0 _________________________________________________________________dense (Dense) (None, 1) 8193 =================================================================Total params: 404,801 Trainable params: 404,801 Non-trainable params: 0 _________________________________________________________________Listing 12.34 The GAN discriminator network One dropout layer: an important trick! 407Introduction to generative adversarial networks12.5.5 The generatorNext, let’s develop a generator model that turns a vector (from the latent space—during training it will be sampled at random) into a candidate image.latent_dim = 128 generator = keras.Sequential( [ keras.Input(shape=(latent_dim,)), layers.Dense(8 * 8 * 128), layers.Reshape((8, 8, 128)), layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"), layers.LeakyReLU(alpha=0.2), layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"), layers.LeakyReLU(alpha=0.2), layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"), layers.LeakyReLU(alpha=0.2), layers.Conv2D(3, kernel_size=5, padding=\"same\", activation=\"sigmoid\"), ], name=\"generator\",)This is the generator model summary:>>> generator.summary()Model: \"generator\" _________________________________________________________________Layer (type) Output Shape Param # =================================================================dense_1 (Dense) (None, 8192) 1056768 _________________________________________________________________reshape (Reshape) (None, 8, 8, 128) 0 _________________________________________________________________conv2d_transpose (Conv2DTran (None, 16, 16, 128) 262272 _________________________________________________________________leaky_re_lu_3 (LeakyReLU) (None, 16, 16, 128) 0 _________________________________________________________________conv2d_transpose_1 (Conv2DTr (None, 32, 32, 256) 524544 _________________________________________________________________leaky_re_lu_4 (LeakyReLU) (None, 32, 32, 256) 0 _________________________________________________________________conv2d_transpose_2 (Conv2DTr (None, 64, 64, 512) 2097664 _________________________________________________________________leaky_re_lu_5 (LeakyReLU) (None, 64, 64, 512) 0 _________________________________________________________________conv2d_3 (Conv2D) (None, 64, 64, 3) 38403 =================================================================Total params: 3,979,651 Trainable params: 3,979,651 Non-trainable params: 0 _________________________________________________________________Listing 12.35 GAN generator networkThe latent space will be made of 128-dimensional vectors.Produce the same number of coefficients we had at the level of the Flatten layer in the encoder.Revert theFlatten layer ofthe encoder.Revert theConv2D layersof the encoder.We use LeakyReLUas our activation.The output endsup with shape(28, 28, 1). 408CHAPTER 12Generative deep learning12.5.6 The adversarial networkFinally, we’ll set up the GAN, which chains the generator and the discriminator.When trained, this model will move the generator in a direction that improves itsability to fool the discriminator. This model turns latent-space points into a classifi-cation decision—“fake” or “real”—and it’s meant to be trained with labels that arealways “these are real images.” So training gan will update the weights of generatorin a way that makes discriminator more likely to predict “real” when looking atfake images. To recapitulate, this is what the training loop looks like schematically. For eachepoch, you do the following:1Draw random points in the latent space (random noise).2Generate images with generator using this random noise.3Mix the generated images with real ones.4Train discriminator u s i n g t h e s e m i x e d i m a g e s , w i t h c o r r e s p o n d i n g t a r g e t s :either “real” (for the real images) or “fake” (for the generated images).5Draw new random points in the latent space.6Train generator using these random vectors, with targets that all say “these arereal images.” This updates the weights of the generator to move them towardgetting the discriminator to predict “these are real images” for generatedimages: this trains the generator to fool the discriminator.Let’s implement it. Like in our VAE example, we’ll use a Model subclass with a cus-tom train_step(). Note that we’ll use two optimizers (one for the generator andone for the discriminator), so we will also override compile() to allow for passingtwo optimizers.import tensorflow as tfclass GAN(keras.Model): def __init__(self, discriminator, generator, latent_dim): super().__init__() self.discriminator = discriminator self.generator = generator self.latent_dim = latent_dim self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\") self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\") def compile(self, d_optimizer, g_optimizer, loss_fn): super(GAN, self).compile() self.d_optimizer = d_optimizer self.g_optimizer = g_optimizer self.loss_fn = loss_fn @property def metrics(self): return [self.d_loss_metric, self.g_loss_metric]Listing 12.36 The GAN Model Sets up metrics to track the two losses over each training epoch 409Introduction to generative adversarial networks def train_step(self, real_images): batch_size = tf.shape(real_images)[0] random_latent_vectors = tf.random.normal( shape=(batch_size, self.latent_dim)) generated_images = self.generator(random_latent_vectors) combined_images = tf.concat([generated_images, real_images], axis=0) labels = tf.concat( [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0 ) labels += 0.05 * tf.random.uniform(tf.shape(labels)) with tf.GradientTape() as tape: predictions = self.discriminator(combined_images) d_loss = self.loss_fn(labels, predictions) grads = tape.gradient(d_loss, self.discriminator.trainable_weights) self.d_optimizer.apply_gradients( zip(grads, self.discriminator.trainable_weights) ) random_latent_vectors = tf.random.normal( shape=(batch_size, self.latent_dim)) misleading_labels = tf.zeros((batch_size, 1)) with tf.GradientTape() as tape: predictions = self.discriminator( self.generator(random_latent_vectors)) g_loss = self.loss_fn(misleading_labels, predictions) grads = tape.gradient(g_loss, self.generator.trainable_weights) self.g_optimizer.apply_gradients( zip(grads, self.generator.trainable_weights)) self.d_loss_metric.update_state(d_loss) self.g_loss_metric.update_state(g_loss) return {\"d_loss\": self.d_loss_metric.result(), \"g_loss\": self.g_loss_metric.result()}Before we start training, let’s also set up a callback to monitor our results: it will usethe generator to create and save a number of fake images at the end of each epoch.class GANMonitor(keras.callbacks.Callback): def __init__(self, num_img=3, latent_dim=128): self.num_img = num_img self.latent_dim = latent_dim def on_epoch_end(self, epoch, logs=None): random_latent_vectors = tf.random.normal( shape=(self.num_img, self.latent_dim)) generated_images = self.model.generator(random_latent_vectors) generated_images *= 255 generated_images.numpy()Listing 12.37 A callback that samples generated images during trainingSamples random points in the latent spaceDecodesthem tofake imagesCombinesthem withreal imagesAssembles labels, discriminating real from fake imagesAdds random noise to the labels—an important trick!Trains thediscriminatorSamplesrandompoints in thelatent spaceAssembles labels that say “these are all real images” (it’s a lie!)Trains thegenerator 410CHAPTER 12Generative deep learning for i in range(self.num_img): img = keras.utils.array_to_img(generated_images[i]) img.save(f\"generated_img_{epoch:03d}_{i}.png\")Finally, we can start training.epochs = 100 gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)gan.compile( d_optimizer=keras.optimizers.Adam(learning_rate=0.0001), g_optimizer=keras.optimizers.Adam(learning_rate=0.0001), loss_fn=keras.losses.BinaryCrossentropy(),) gan.fit( dataset, epochs=epochs, callbacks=[GANMonitor(num_img=10, latent_dim=latent_dim)])When training, you may see the adversarial loss begin to increase considerably, whilethe discriminative loss tends to zero—the discriminator may end up dominating thegenerator. If that’s the case, try reducing the discriminator learning rate, and increasethe dropout rate of the discriminator. Figure 12.22 shows what our GAN is capable of generating after 30 epochs of training. 12.5.7 Wrapping upA GAN consists of a generator network coupled with a discriminator network.The discriminator is trained to differentiate between the output of the genera-tor and real images from a training dataset, and the generator is trained to foolListing 12.38 Compiling and training the GANYou’ll start getting interesting results after epoch 20. Figure 12.22 Some generated images around epoch 30 411Summarythe discriminator. Remarkably, the generator never sees images from the trainingset directly; the information it has about the data comes from the discriminator.GANs are difficult to train, because training a GAN is a dynamic process ratherthan a simple gradient descent process with a fixed loss landscape. Getting aGAN to train correctly requires using a number of heuristic tricks, as well asextensive tuning.GANs can potentially produce highly realistic images. But unlike VAEs, thelatent space they learn doesn’t have a neat continuous structure and thus maynot be suited for certain practical applications, such as image editing via latent-space concept vectors.These few techniques cover only the basics of this fast-expanding field. There’s a lotmore to discover out there—generative deep learning is deserving of an entire bookof its own. SummaryYou can use a sequence-to-sequence model to generate sequence data, one stepat a time. This is applicable to text generation, but also to note-by-note musicgeneration or any other type of timeseries data.DeepDream works by maximizing convnet layer activations through gradientascent in input space.In the style-transfer algorithm, a content image and a style image are combinedtogether via gradient descent to produce an image with the high-level featuresof the content image and the local characteristics of the style image.VAEs and GANs are models that learn a latent space of images and can thendream up entirely new images by sampling from the latent space. Concept vectorsin the latent space can even be used for image editing. 412Best practicesfor the real world You’ve come far since the beginning of this book. You can now train image classifi-cation models, image segmentation models, models for classification or regressionon vector data, timeseries forecasting models, text-classification models, sequence-to-sequence models, and even generative models for text and images. You’ve got allthe bases covered. However, your models so far have all been trained at a small scale—on smalldatasets, with a single GPU—and they generally haven’t reached the best achiev-able performance on each dataset we looked at. This book is, after all, an introduc-tory book. If you are to go out in the real world and achieve state-of-the-art resultson brand new problems, there’s still a bit of a chasm that you’ll need to cross. T h i s p e n u l t i m a t e c h a p t e r i s a b o u t b r i dgi ng that gap and givi ng you the bestpractices you’ll need as you go from machine learning student to fully fledgedThis chapter coversHyperparameter tuningModel ensemblingMixed-precision trainingTraining Keras models on multiple GPUs or on a TPU 413Getting the most out of your modelsmachine learning engineer. We’ll review essential techniques for systematically improv-ing model performance: hyperparameter tuning and model ensembling. Then we’lllook at how you can speed up and scale up model training, with multi-GPU and TPUtraining, mixed precision, and leveraging remote computing resources in the cloud.13.1 Getting the most out of your modelsBlindly trying out different architecture configurations works well enough if youjust need something that works okay. In this section, we’ll go beyond “works okay” to“works great and wins machine learning competitions” via a set of must-know tech-niques for building state-of-the-art deep learning models.13.1.1 Hyperparameter optimizationWhen building a deep learning model, you have to make many seemingly arbitrarydecisions: How many layers should you stack? How many units or filters should go ineach layer? Should you use relu as activation, or a different function? Should you useBatchNormalization after a given layer? How much dropout should you use? And soon. These architecture-level parameters are called hyperparameters to distinguish themfrom the parameters of a model, which are trained via backpropagation. In practice, experienced machine learning engineers and researchers build intu-ition over time as to what works and what doesn’t when it comes to these choices—they develop hyperparameter-tuning skills. But there are no formal rules. If you wantto get to the very limit of what can be achieved on a given task, you can’t be contentwith such arbitrary choices. Your initial decisions are almost always suboptimal, even ifyou have very good intuition. You can refine your choices by tweaking them by handand retraining the model repeatedly—that’s what machine learning engineers andresearchers spend most of their time doing. But it shouldn’t be your job as a human tofiddle with hyperparameters all day—that is better left to a machine. Thus you need to explore the space of possible decisions automatically, systemati-cally, in a principled way. You need to search the architecture space and find the best-performing architectures empirically. That’s what the field of automatic hyperparame-ter optimization is about: it’s an entire field of research, and an important one. The process of optimizing hyperparameters typically looks like this:1Choose a set of hyperparameters (automatically).2Build the corresponding model.3Fit it to your training data, and measure performance on the validation data.4Choose the next set of hyperparameters to try (automatically).5Repeat.6Eventually, measure performance on your test data.The key to this process is the algorithm that analyzes the relationship between vali-dation performance and various hyperparameter values to choose the next set of 414CHAPTER 13Best practices for the real worldhyperparameters to evaluate. Many different techniques are possible: Bayesian optimi-zation, genetic algorithms, simple random search, and so on. Training the weights of a model is relatively easy: you compute a loss function on amini-batch of data and then use backpropagation to move the weights in the rightdirection. Updating hyperparameters, on the other hand, presents unique challenges.Consider these points:The hyperparameter space is typically made up of discrete decisions and thusisn’t continuous or differentiable. Hence, you typically can’t do gradient descentin hyperparameter space. Instead, you must rely on gradient-free optimizationtechniques, which naturally are far less efficient than gradient descent.Computing the feedback signal of this optimization process (does this set ofhyperparameters lead to a high-performing model on this task?) can be extremelyexpensive: it requires creating and training a new model from scratch on yourdataset.The feedback signal may be noisy: if a training run performs 0.2% better, is thatbecause of a better model configuration, or because you got lucky with the ini-tial weight values?Thankfully, there’s a tool that makes hyperparameter tuning simpler: KerasTuner.Let’s check it out.USING KERASTUNERLet’s start by installing KerasTuner:!pip install keras-tuner -qKerasTuner lets you replace hard-coded hyperparameter values, such as units=32,with a range of possible choices, such as Int(name=\"units\", min_value=16,max_value=64, step=16). This set of choices in a given model is called the search spaceof the hyperparameter tuning process. To specify a search space, define a model-building function (see the next listing).It takes an hp argument, from which you can sample hyperparameter ranges, and itreturns a compiled Keras model.from tensorflow import keras from tensorflow.keras import layers def build_model(hp): units = hp.Int(name=\"units\", min_value=16, max_value=64, step=16) model = keras.Sequential([ layers.Dense(units, activation=\"relu\"), layers.Dense(10, activation=\"softmax\") ]) optimizer = hp.Choice(name=\"optimizer\", values=[\"rmsprop\", \"adam\"]) model.compile(Listing 13.1 A KerasTuner model-building functionSample hyperparameter values from thehp object. After sampling, these values(such as the \"units\" variable here) arejust regular Python constants.Different kinds of hyperpa-rameters are available: Int,Float, Boolean, Choice. 415Getting the most out of your models optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]) return model If you want to adopt a more modular and configurable approach to model-building,you can also subclass the HyperModel class and define a build method, as follows.import kerastuner as kt class SimpleMLP(kt.HyperModel): def __init__(self, num_classes): self.num_classes = num_classes def build(self, hp): units = hp.Int(name=\"units\", min_value=16, max_value=64, step=16) model = keras.Sequential([ layers.Dense(units, activation=\"relu\"), layers.Dense(self.num_classes, activation=\"softmax\") ]) optimizer = hp.Choice(name=\"optimizer\", values=[\"rmsprop\", \"adam\"]) model.compile( optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]) return model hypermodel = SimpleMLP(num_classes=10)The next step is to define a “tuner.” Schematically, you can think of a tuner as a forloop that will repeatedlyPick a set of hyperparameter valuesCall the model-building function with these values to create a modelTrain the model and record its metricsKerasTuner has several built-in tuners available—RandomSearch, BayesianOptimiza-tion, and Hyperband. Let’s try BayesianOptimization, a tuner that attempts to makesmart predictions for which new hyperparameter values are likely to perform bestgiven the outcomes of previous choices:tuner = kt.BayesianOptimization( build_model, objective=\"val_accuracy\", max_trials=100, Listing 13.2 A KerasTuner HyperModelThe function returns a compiled model. Thanks to the object-oriented approach, we canconfigure model constantsas constructor arguments(instead of hardcoding themin the model-buildingfunction).The build() method is identical to our prior build_model() standalone function. Specify the model-build-ing function (or hyper-model instance).Specify the metric that the tuner will seek to optimize. Always specify validation metrics, since the goal of the search process is to find models that generalize!Maximum number of different model configurations (“trials”) to try before ending the search. 416CHAPTER 13Best practices for the real world executions_per_trial=2, directory=\"mnist_kt_test\", overwrite=True, )You can display an overview of the search space via search_space_summary():>>> tuner.search_space_summary()Search space summaryDefault search space size: 2 units (Int){\"default\": None, \"conditions\": [], \"min_value\": 128, \"max_value\": 1024, \"step\": 128, \"sampling\": None}optimizer (Choice){\"default\": \"rmsprop\", \"conditions\": [], \"values\": [\"rmsprop\", \"adam\"], \"ordered\": False} Finally, let’s launch the search. Don’t forget to pass validation data, and make sure notto use your test set as validation data—otherwise you’d quickly start overfitting to yourtest data, and you wouldn’t be able to trust your test metrics anymore:(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()x_train = x_train.reshape((-1, 28 * 28)).astype(\"float32\") / 255 x_test = x_test.reshape((-1, 28 * 28)).astype(\"float32\") / 255 x_train_full = x_train[:] y_train_full = y_train[:] Objective maximization and minimizationFor built-in metrics (like accuracy, in our case), the direction of the metric (accuracyshould be maximized, but a loss should be minimized) is inferred by KerasTuner.However, for a custom metric, you should specify it yourself, like this:objective = kt.Objective( name=\"val_accuracy\", direction=\"max\") tuner = kt.BayesianOptimization( build_model, objective=objective, ...)To reduce metrics variance, you can train the same model multiple times and average the results. executions_per_trial is how many training rounds (executions) to run for each model configuration (trial).Where to store search logsWhether to overwrite data in directory to start a new search. Set this to True if you’ve modified the model-building function, or to False to resume a previously started search with the same model-building function. The metric’s name, as found in epoch logsThe metric’s desired direction: \"min\" or \"max\" Reserve these for later. 417Getting the most out of your modelsnum_val_samples = 10000 x_train, x_val = x_train[:-num_val_samples], x_train[-num_val_samples:] y_train, y_val = y_train[:-num_val_samples], y_train[-num_val_samples:] callbacks = [ keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5), ]tuner.search( x_train, y_train, batch_size=128, epochs=100, validation_data=(x_val, y_val), callbacks=callbacks, verbose=2,)The preceding example will run in just a few minutes, since we’re only looking at afew possible choices and we’re training on MNIST. However, with a typical searchspace and dataset, you’ll often find yourself letting the hyperparameter search runovernight or even over several days. If your search process crashes, you can alwaysrestart it—just specify overwrite=False in the tuner so that it can resume from thetrial logs stored on disk. Once the search is complete, you can query the best hyperparameter configura-tions, which you can use to create high-performing models that you can then retrain.top_n = 4 best_hps = tuner.get_best_hyperparameters(top_n) Usually, when retraining these models, you may want to include the validation data aspart of the training data, since you won’t be making any further hyperparameterchanges, and thus you will no longer be evaluating performance on the validationdata. In our example, we’d train these final models on the totality of the originalMNIST training data, without reserving a validation set. Before we can train on the full training data, though, there’s one last parameter weneed to settle: the optimal number of epochs to train for. Typically, you’ll want to trainthe new models for longer than you did during the search: using an aggressivepatience value in the EarlyStopping callback saves time during the search, but it maylead to under-fit models. Just use the validation set to find the best epoch:def get_best_epoch(hp): model = build_model(hp) callbacks=[ keras.callbacks.EarlyStopping( monitor=\"val_loss\", mode=\"min\", patience=10) ]Listing 13.3 Querying the best hyperparameter configurationsSet theseaside as avalidation set. Use a large number of epochs (you don’t know in advance how many epochs each model will need), and use an EarlyStopping callback to stop training when you start overfitting.This takes the same arguments as fit() (it simply passes them down to fit() for each new model). Returns a list of HyperParameter objects, which you can pass to the model-building function Note the very high patience value. 418CHAPTER 13Best practices for the real world history = model.fit( x_train, y_train, validation_data=(x_val, y_val), epochs=100, batch_size=128, callbacks=callbacks) val_loss_per_epoch = history.history[\"val_loss\"] best_epoch = val_loss_per_epoch.index(min(val_loss_per_epoch)) + 1 print(f\"Best epoch: {best_epoch}\") return best_epochFinally, train on the full dataset for just a bit longer than this epoch count, sinceyou’re training on more data; 20% more in this case:def get_best_trained_model(hp): best_epoch = get_best_epoch(hp) model.fit( x_train_full, y_train_full, batch_size=128, epochs=int(best_epoch * 1.2)) return model best_models = []for hp in best_hps: model = get_best_trained_model(hp) model.evaluate(x_test, y_test) best_models.append(model)Note that if you’re not worried about slightly underperforming, there’s a shortcut youcan take: just use the tuner to reload the top-performing models with the best weightssaved during the hyperparameter search, without retraining new models from scratch:best_models = tuner.get_best_models(top_n)NOTEOne important issue to think about when doing automatic hyper-parameter optimization at scale is validation-set overfitting. Because you’reupdating hyperparameters based on a signal that is computed using your vali-dation data, you’re effectively training them on the validation data, and thusthey will quickly overfit to the validation data. Always keep this in mind. THE ART OF CRAFTING THE RIGHT SEARCH SPACEOverall, hyperparameter optimization is a powerful technique that is an absoluterequirement for getting to state-of-the-art models on any task or to win machine learn-ing competitions. Think about it: once upon a time, people handcrafted the featuresthat went into shallow machine learning models. That was very much suboptimal.Now, deep learning automates the task of hierarchical feature engineering—featuresare learned using a feedback signal, not hand-tuned, and that’s the way it should be.In the same way, you shouldn’t handcraft your model architectures; you should opti-mize them in a principled way. However, doing hyperparameter tuning is not a replacement for being familiarwith model architecture best practices. Search spaces grow combinatorially with the 419Getting the most out of your modelsnumber of choices, so it would be far too expensive to turn everything into a hyper-parameter and let the tuner sort it out. You need to be smart about designing theright search space. Hyperparameter tuning is automation, not magic: you use it toautomate experiments that you would otherwise have run by hand, but you stillneed to handpick experiment configurations that have the potential to yield goodmetrics. T h e g o o d n e w s i s t h a t b y l e v e r a g i n g h y p e r p a r a m e t e r t u n i n g , t h e c o n f i g u r a t i o ndecisions you have to make graduate from micro-decisions (what number of units do Ipick for this layer?) to higher-level architecture decisions (should I use residual con-nections throughout this model?). And while micro-decisions are specific to a certainmodel and a certain dataset, higher-level decisions generalize better across differenttasks and datasets. For instance, pretty much every image classification problem canbe solved via the same sort of search-space template. Following this logic, KerasTuner attempts to provide premade search spaces that arerelevant to broad categories of problems, such as image classification. Just add data,run the search, and get a pretty good model. You can try the hypermodels kt.appli-cations.HyperXception a n d kt.applications.HyperResNet, which are effectivelytunable versions of Keras Applications models. THE FUTURE OF HYPERPARAMETER TUNING: AUTOMATED MACHINE LEARNINGCurrently, most of your job as a deep learning engineer consists of munging data withPython scripts and then tuning the architecture and hyperparameters of a deep net-work at length to get a working model, or even to get a state-of-the-art model, if youare that ambitious. Needless to say, that isn’t an optimal setup. But automation canhelp, and it won’t stop merely at hyperparameter tuning. Searching over a set of possible learning rates or possible layer sizes is just the firststep. We can also be far more ambitious and attempt to generate the model architectureitself from scratch, with as few constraints as possible, such as via reinforcement learn-ing or genetic algorithms. In the future, entire end-to-end machine learning pipelineswill be automatically generated, rather than be handcrafted by engineer-artisans. Thisis called automated machine learning, or AutoML. You can already leverage librarieslike AutoKeras (https:/ /github.com/keras-team/autokeras) to solve basic machinelearning problems with very little involvement on your part. Today, AutoML is still in its early days, and it doesn’t scale to large problems. Butwhen AutoML becomes mature enough for widespread adoption, the jobs of machinelearning engineers won’t disappear—rather, engineers will move up the value-creationchain. They will begin to put much more effort into data curation, crafting complex lossfunctions that truly reflect business goals, as well as understanding how their modelsimpact the digital ecosystems in which they’re deployed (such as the users who consumethe model’s predictions and generate the model’s training data). These are problemsthat only the largest companies can afford to consider at present. Always look at the big picture, focus on understanding the fundamentals, and keepin mind that the highly specialized tedium will eventually be automated away. See it as 420CHAPTER 13Best practices for the real worlda gift—greater productivity for your workflows—and not as a threat to your own rele-vance. It shouldn’t be your job to tune knobs endlessly. 13.1.2 Model ensemblingAnother powerful technique for obtaining the best possible results on a task is modelensembling. Ensembling consists of pooling together the predictions of a set of differ-ent models to produce better predictions. If you look at machine learning competi-tions, in particular on Kaggle, you’ll see that the winners use very large ensembles ofmodels that inevitably beat any single model, no matter how good. E n s e m b l i n g r e l i e s o n t h e a s s u m p t i o n t h a t d i f f e r e n t w e l l - p e r f o r m i n g m o d e l strained independently are likely to be good for different reasons: each model looks atslightly different aspects of the data to make its predictions, getting part of the “truth”but not all of it. You may be familiar with the ancient parable of the blind men and theelephant: a group of blind men come across an elephant for the first time and try tounderstand what the elephant is by touching it. Each man touches a different part ofthe elephant’s body—just one part, such as the trunk or a leg. Then the men describeto each other what an elephant is: “It’s like a snake,” “Like a pillar or a tree,” and soon. The blind men are essentially machine learning models trying to understand themanifold of the training data, each from their own perspective, using their ownassumptions (provided by the unique architecture of the model and the unique ran-dom weight initialization). Each of them gets part of the truth of the data, but not thewhole truth. By pooling their perspectives together, you can get a far more accuratedescription of the data. The elephant is a combination of parts: not any single blindman gets it quite right, but, interviewed together, they can tell a fairly accurate story. Let’s use classification as an example. The easiest way to pool the predictions of a setof classifiers (to ensemble the classifiers) is to average their predictions at inference time:preds_a = model_a.predict(x_val) preds_b = model_b.predict(x_val) preds_c = model_c.predict(x_val) preds_d = model_d.predict(x_val)final_preds =0.25*( p r e d s _ a+p r e d s _ b+p r e d s _ c+p r e d s _ d ) However, this will work only if the classifiers are more or less equally good. If one ofthem is significantly worse than the others, the final predictions may not be as good asthe best classifier of the group. A s m a r t e r w a y t o e n s e m b l e c l a s s i f i e r s i s t o d o a w e i g h t e d a v e r a g e , w h e r e t h eweights are learned on the validation data—typically, the better classifiers are given ahigher weight, and the worse classifiers are given a lower weight. To search for a goodset of ensembling weights, you can use random search or a simple optimization algo-rithm, such as the Nelder-Mead algorithm:preds_a = model_a.predict(x_val)preds_b = model_b.predict(x_val)preds_c = model_c.predict(x_val)Use four different models to compute initial predictions.This new prediction array should be more accurate than any of the initial ones. 421Scaling-up model trainingpreds_d = model_d.predict(x_val)final_preds =0.5*p r e d s _ a+0.25*p r e d s _ b+0.1*p r e d s _ c+0.15*p r e d s _ d There are many possible variants: you can do an average of an exponential of the pre-dictions, for instance. In general, a simple weighted average with weights optimizedon the validation data provides a very strong baseline. The key to making ensembling work is the diversity of the set of classifiers. Diversityis strength. If all the blind men only touched the elephant’s trunk, they would agreethat elephants are like snakes, and they would forever stay ignorant of the truth of theelephant. Diversity is what makes ensembling work. In machine learning terms, if allof your models are biased in the same way, your ensemble will retain this same bias. Ifyour models are biased in different ways, the biases will cancel each other out, and theensemble will be more robust and more accurate. For this reason, you should ensemble models that are as good as possible while beingas different as possible. This typically means using very different architectures or evendifferent brands of machine learning approaches. One thing that is largely not worthdoing is ensembling the same network trained several times independently, from dif-ferent random initializations. If the only difference between your models is their ran-dom initialization and the order in which they were exposed to the training data, thenyour ensemble will be low-diversity and will provide only a tiny improvement over anysingle model. One thing I have found to work well in practice—but that doesn’t generalize toevery problem domain—is using an ensemble of tree-based methods (such as randomforests or gradient-boosted trees) and deep neural networks. In 2014, Andrey Kolevand I took fourth place in the Higgs Boson decay detection challenge on Kaggle(www.kaggle.com/c/higgs-boson) using an ensemble of various tree models and deepneural networks. Remarkably, one of the models in the ensemble originated from adifferent method than the others (it was a regularized greedy forest), and it had a sig-nificantly worse score than the others. Unsurprisingly, it was assigned a small weight inthe ensemble. But to our surprise, it turned out to improve the overall ensemble by alarge factor, because it was so different from every other model: it provided informa-tion that the other models didn’t have access to. That’s precisely the point of ensem-bling. It’s not so much about how good your best model is; it’s about the diversity ofyour set of candidate models. 13.2 Scaling-up model trainingRecall the “loop of progress” concept we introduced in chapter 7: the quality of yourideas is a function of how many refinement cycles they’ve been through (see figure 13.1).And the speed at which you can iterate on an idea is a function of how fast you can setup an experiment, how fast you can run that experiment, and finally, how well you cananalyze the resulting data.These weights (0.5, 0.25, 0.1, 0.15) are assumed to be learned empirically. 422CHAPTER 13Best practices for the real world As you develop your expertise with the Keras API, how fast you can code up your deeplearning experiments will cease to be the bottleneck of this progress cycle. The nextbottleneck will become the speed at which you can train your models. Fast traininginfrastructure means that you can get your results back in 10–15 minutes, and hence,that you can go through dozens of iterations every day. Faster training directly improvesthe quality of your deep learning solutions. In this section, you’ll learn about three ways you can train your models faster:Mixed-precision training, which you can use even with a single GPUTraining on multiple GPUsTraining on TPUsLet’s go.13.2.1 Speeding up training on GPU with mixed precisionWhat if I told you there’s a simple technique you can use to speed up the training ofalmost any model by up to 3X, basically for free? It seems too good to but true, andyet, such a trick does exist. That’s mixed-precision training. To understand how it works,we first need to take a look at the notion of “precision” in computer science.UNDERSTANDING FLOATING-POINT PRECISIONPrecision is to numbers what resolution is to images. Because computers can only pro-cess ones and zeros, any number seen by a computer has to be encoded as a binarystring. For instance, you may be familiar with uint8 i n t e g e r s , w h i c h a r e i n t e g e r sencoded on eight bits: 00000000 represents 0 in uint8, and 11111111 represents 255.To represent integers beyond 255, you’d need to add more bits—eight isn’t enough.Most integers are stored on 32 bits, with which you can represent signed integers rang-ing from –2147483648 to 2147483647. Floating-point numbers are the same. In mathematics, real numbers form a con-tinuous axis: there’s an infinite number of points in between any two numbers. Youcan always zoom in on the axis of reals. In computer science, this isn’t true: there’s afinite number of intermediate points between 3 and 4, for instance. How many? Well,it depends on the precision you’re working with—the number of bits you’re using tostore a number. You can only zoom up to a certain resolution.IdeaVisualizationframework:TensorBoardDeep learningframework:KerasGPUs, TPUsResults ExperimentFigure 13.1 The loop of progress 423Scaling-up model training There are three of levels of precision you’d typically use:Half precision, or float16, where numbers are stored on 16 bitsSingle precision, or float32, where numbers are stored on 32 bitsDouble precision, or float64, where numbers are stored on 64 bits The way to think about the resolution of floating-point numbers is in terms of thesmallest distance between two arbitrary numbers that you’ll be able to safely process.In single precision, that’s around 1e-7. In double precision, that’s around 1e-16. Andin half precision, it’s only 1e-3. E v e r y m o d e l y o u ’ v e s e e n i n t h i s b o o k s o f a r u s e d s i n g l e - p r e c i s i o n n u m b e r s : i tstored its state as float32 w e i g h t v a r i a b l e s a n d r a n i t s c o m p u t a t i o n s o n float32inputs. That’s enough precision to run the forward and backwards pass of a modelwithout losing any information—particularly when it comes to small gradient updatesA note on floating-point encodingA counterintuitive fact about floating-point numbers is that representable numbersare not uniformly distributed. Larger numbers have lower precision: there are thesame number of representable values between 2 ** N and 2 ** (N + 1) as there arebetween 1 and 2, for any N.That’s because floating-point numbers are encoded in three parts—the sign, the sig-nificant value (called the \"mantissa\"), and the exponent, in the form{sign} * (2 ** ({exponent} - 127)) * 1.{mantissa}For example, here’s how you would encode the closest float32 value approximating Pi: For this reason, the numerical error incurred when converting a number to its floating-point representation can vary wildly depending on the exact value considered, and theerror tends to get larger for numbers with a large absolute value.01000000010010010000111111011011Sign Exponent Mantissa +1 128 57079637050628661 bit 8 bits 23 bits value = +1 * (2 ** (128 - 127)) * 1.5707963705062866value = 3.1415927410125732The number Pi encoded in single precision via a sign bit, an integer exponent, and an integer mantissa 424CHAPTER 13Best practices for the real world(recall that the typical learning rate is 1e-3, and it’s pretty common to see weightupdates on the order of 1e-6). You could also use float64, though that would be wasteful—operations like matrixmultiplication or addition are much more expensive in double precision, so you’d bedoing twice as much work for no clear benefits. But you could not do the same withfloat16 w e i g h t s a n d c o m p u t a t i o n ; t h e g r a d i e n t d e s c e n t p r o c e s s w o u l d n ’ t r u nsmoothly, since you couldn’t represent small gradient updates of around 1e-5 or 1e-6. You can, however, use a hybrid approach: that’s what mixed precision is about. Theidea is to leverage 16-bit computations in places where precision isn’t an issue, and towork with 32-bit values in other places to maintain numerical stability. Modern GPUsand TPUs feature specialized hardware that can run 16-bit operations much faster anduse less memory than equivalent 32-bits operations. By using these lower-precisionoperations whenever possible, you can speed up training on those devices by a signifi-cant factor. Meanwhile, by maintaining the precision-sensitive parts of the model insingle precision, you can get these benefits without meaningfully impacting modelquality. And those benefits are considerable: on modern NVIDIA GPUs, mixed precisioncan speed up training by up to 3X. It’s also beneficial when training on a TPU (a sub-ject we’ll get to in a bit), where it can speed up training by up to 60%.Beware of dtype defaultsSingle precision is the default floating-point type throughout Keras and TensorFlow:any tensor or variable you create will be in float32 unless you specify otherwise. ForNumPy arrays, however, the default is float64!Converting a default NumPy array to a TensorFlow tensor will result in a float64 ten-sor, which may not be what you want:>>> import tensorflow as tf>>> import numpy as np>>> np_array = np.zeros((2, 2))>>> tf_tensor = tf.convert_to_tensor(np_array)>>> tf_tensor.dtypetf.float64Remember to be explicit about data types when converting NumPy arrays:>>> np_array = np.zeros((2, 2))>>> tf_tensor = tf.convert_to_tensor(np_array, dtype=\"float32\") >>> tf_tensor.dtypetf.float32Note that when you call the Keras fit() method with NumPy data, it will do this con-version for you. Specify the dtype explicitly. 425Scaling-up model trainingMIXED-PRECISION TRAINING IN PRACTICEWhen training on a GPU, you can turn on mixed precision like this:from tensorflow import keraskeras.mixed_precision.set_global_policy(\"mixed_float16\")Typically, most of the forward pass of the model will be done in float16 (with theexception of numerically unstable operations like softmax), while the weights of themodel will be stored and updated in float32. Keras layers have a variable_dtype and a compute_dtype attribute. By default, bothof these are set to float32. When you turn on mixed precision, the compute_dtype ofmost layers switches to float16, and those layers will cast their inputs to float16 and willperform their computations in float16 ( u s i n g h a l f - p r e c i s i o n c o p i e s o f t h e w e i g h t s ) .However, since their variable_dtype is still float32, their weights will be able to receiveaccurate float32 updates from the optimizer, as opposed to half-precision updates. Note that some operations may be numerically unstable in float16 (in particular,softmax and crossentropy). If you need to opt out of mixed precision for a specificlayer, just pass the argument dtype=\"float32\" to the constructor of this layer. 13.2.2 Multi-GPU trainingWhile GPUs are getting more powerful every year, deep learning models are gettingincreasingly larger, requiring ever more computational resources. Training on a singleGPU puts a hard bound on how fast you can move. The solution? You could simplyadd more GPUs and start doing multi-GPU distributed training. There are two ways to distribute computation across multiple devices: data parallel-ism and model parallelism. With data parallelism, a single model is replicated on multiple devices or multiplemachines. Each of the model replicas processes different batches of data, and thenthey merge their results. With model parallelism, different parts of a single model run on different devices,processing a single batch of data together at the same time. This works best with modelsthat have a naturally parallel architecture, such as models that feature multiple branches. In practice, model parallelism is only used for models that are too large to fit onany single device: it isn’t used as a way to speed up training of regular models, but as away to train larger models. We won’t cover model parallelism in these pages; insteadwe’ll focus on what you’ll be using most of the time: data parallelism. Let’s take a lookat how it works.GETTING YOUR HANDS ON TWO OR MORE GPUSFirst, you need to get access to several GPUs. As of now, Google Colab only lets you usea single GPU, so you will need to do one of two things:Acquire 2–4 GPUs, mount them on a single machine (it will require a beefypower supply), and install CUDA drivers, cuDNN, etc. For most people, thisisn’t the best option. 426CHAPTER 13Best practices for the real worldRent a multi-GPU Virtual Machine (VM) on Google Cloud, Azure, or AWS.You’ll be able to use VM images with preinstalled drivers and software, andyou’ll have very little setup overhead. This is likely the best option for anyonewho isn’t training models 24/7.We won’t cover the details of how to spin up multi-GPU cloud VMs, because such instruc-tions would be relatively short-lived, and this information is readily available online. A n d i f y o u d o n ’ t w a n t t o d e a l w i t h t h e o v e r h e a d o f m a n a g i n g y o u r o w n V Minstances, you can use TensorFlow Cloud (https:/ /github.com/tensorflow/cloud), apackage that my team and I have recently released—it enables you to start training onmultiple GPUs by just adding one line of code at the start of a Colab notebook. Ifyou’re looking for a seamless transition from debugging your model in Colab to train-ing it as fast as possible on as many GPUs as you want, check it out. SINGLE-HOST, MULTI-DEVICE SYNCHRONOUS TRAININGOnce you’re able to import tensorflow on a machine with multiple GPUs, you’re sec-onds away from training a distributed model. It works like this:strategy = tf.distribute.MirroredStrategy() print(f\"Number of devices: {strategy.num_replicas_in_sync}\") with strategy.scope(): model = get_compiled_model() model.fit( train_dataset, epochs=100, validation_data=val_dataset, callbacks=callbacks)These few lines implement the most common training setup: single-host, multi-devicesynchronous training, also known in TensorFlow as the “mirrored distribution strategy.”“Single host” means that the different GPUs considered are all on a single machine(as opposed to a cluster of many machines, each with its own GPU, communicatingover a network). “Synchronous training” means that the state of the per-GPU modelreplicas stays the same at all times—there are variants of distributed training wherethis isn’t the case. When you open a MirroredStrategy scope and build your model within it, theMirroredStrategy object will create one model copy (replica) on each available GPU.Then, each step of training unfolds in the following way (see figure 13.2):1A batch of data (called global batch) is drawn from the dataset.2It gets split into four different sub-batches (called local batches). For instance, ifthe global batch has 512 samples, each of the four local batches will have 128samples. Because you want local batches to be large enough to keep the GPUbusy, the global batch size typically needs to be very large.Create a “distribution strategy” object. MirroredStrategy should be your go-to solution. Use it to open a “strategy scope.”Everything that creates variables should be under the strategy scope. In general, this is only model construction and compile().Train the model on all available devices. 427Scaling-up model training3Each of the four replicas processes one local batch, independently, on its owndevice: they run a forward pass, and then a backward pass. Each replica outputsa “weight delta” describing by how much to update each weight variable in themodel, given the gradient of the previous weights with respect to the loss of themodel on the local batch.4The weight deltas originating from local gradients are efficiently merged acrossthe four replicas to obtain a global delta, which is applied to all replicas. Becausethis is done at the end of every step, the replicas always stay in sync: theirweights are always equal. In an ideal world, training on N GPUs would result in a speedup of factor N. In prac-tice, however, distribution introduces some overhead—in particular, merging theweight deltas originating from different devices takes some time. The effectivespeedup you get is a function of the number of GPUs used:With two GPUs, the speedup stays close to 2x.With four, the speedup is around 3.8x.With eight, it’s around 7.3x.This assumes that you’re using a large enough global batch size to keep each GPU uti-lized at full capacity. If your batch size is too small, the local batch size won’t beenough to keep your GPUs busy. tf.data performance tipsWhen doing distributed training, always provide your data as a tf.data.Datasetobject to guarantee best performance. (Passing your data as NumPy arrays alsoworks, since those get converted to Dataset objects by fit()). You should alsomake sure you leverage data prefetching: before passing the dataset to fit(), calldataset.prefetch(buffer_size). If you aren’t sure what buffer size to pick, try thedataset.prefetch(tf.data.AUTOTUNE) option, which will pick a buffer size for you.Modelreplica 0LocalbatchGlobalbatchLocalweightupdatesAveragedweightupdatesDatasetModelreplica 1Figure 13.2 One step of MirroredStrategy training: each model replica computes local weight updates, which are then merged and used to update the state of all replicas. 428CHAPTER 13Best practices for the real world13.2.3 TPU trainingBeyond just GPUs, there is a trend in the deep learning world toward moving work-flows to increasingly specialized hardware designed specifically for deep learningworkflows (such single-purpose chips are known as ASICs, application-specific inte-grated circuits). Various companies big and small are working on new chips, but todaythe most prominent effort along these lines is Google’s Tensor Processing Unit(TPU), which is available on Google Cloud and via Google Colab. Training on a TPU does involve jumping through some hoops, but it’s worth theextra work: TPUs are really, really fast. Training on a TPU V2 will typically be 15xfaster than training an NVIDIA P100 GPU. For most models, TPU training ends upbeing 3x more cost-effective than GPU training on average.USING A TPU VIA GOOGLE COLABYou can actually use an 8-core TPU for free in Colab. In the Colab menu, under theRuntime tab, in the Change Runtime Type option, you’ll notice that you have accessto a TPU runtime in addition to the GPU runtime. When you’re using the GPU runtime, your models have direct access to the GPUwithout you needing to do anything special. This isn’t true for the TPU runtime;there’s an extra step you need to take before you can start building a model: you needto connect to the TPU cluster. It works like this:import tensorflow as tftpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()print(\"Device:\", tpu.master())You don’t have to worry too much about what this does—it’s just a little incantationthat connects your notebook runtime to the device. Open Sesame. Much like in the case of multi-GPU training, using the TPU requires you to open adistribution strategy scope—in this case, a TPUStrategy scope. TPUStrategy followsthe same distribution template as MirroredStrategy—the model is replicated onceper TPU core, and the replicas are kept in sync. Here’s a simple example.from tensorflow import keras from tensorflow.keras import layers strategy = tf.distribute.TPUStrategy(tpu) print(f\"Number of replicas: {strategy.num_replicas_in_sync}\") def build_model(input_size): inputs = keras.Input((input_size, input_size, 3)) x = keras.applications.resnet.preprocess_input(inputs) x = keras.applications.resnet.ResNet50( weights=None, include_top=False, pooling=\"max\")(x)Listing 13.4 Building a model in a TPUStrategy scope 429Scaling-up model training outputs = layers.Dense(10, activation=\"softmax\")(x) model = keras.Model(inputs, outputs) model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]) return model with strategy.scope(): model = build_model(input_size=32)We’re almost ready to start training. But there’s something a bit curious about TPUsin Colab: it’s a two-VM setup, meaning that the VM that hosts your notebook runtimeisn’t the same VM that the TPU lives in. Because of this, you won’t be able to trainfrom files stored on the local disk (that is to say, on the disk linked to the VM thathosts the notebook). The TPU runtime can’t read from there. You have two optionsfor data loading:Train from data that lives in the memory of the VM (not on disk). If your data isin a NumPy array, this is what you’re already doing.Store the data in a Google Cloud Storage (GCS) bucket, and create a datasetthat reads the data directly from the bucket, without downloading locally. TheTPU runtime can read data from GCS. This is your only option for datasets thatare too large to live entirely in memory.In our case, let’s train from NumPy arrays in memory—the CIFAR10 dataset:(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()model.fit(x_train, y_train, batch_size=1024) You’ll notice that the first epoch takes a while to start—that’s because your model isgetting compiled to something that the TPU can execute. Once that step is done, thetraining itself is blazing fast.Beware of I/O bottlenecksBecause TPUs can process batches of data extremely quickly, the speed at which youcan read data from GCS can easily become a bottleneck.If your dataset is small enough, you should keep it in the memory of the VM.You can do so by calling dataset.cache() on your dataset. That way, thedata will only be read from GCS once.If your dataset is too large to fit in memory, make sure to store it as TFRecordfiles—an efficient binary storage format that can be loaded very quickly. Onkeras.io, you’ll find a code example demonstrating how to format your data asTFRecord files (https://keras.io/examples/keras_recipes/creating_tfrecords/). Note that TPU training, much like multi-GPU training, requires large batch sizes to make sure the device stays well-utilized. 430CHAPTER 13Best practices for the real worldLEVERAGING STEP FUSING TO IMPROVE TPU UTILIZATIONBecause a TPU has a lot of compute power available, you need to train with very largebatches to keep the TPU cores busy. For small models, the batch size required can getextraordinarily large—upwards of 10,000 samples per batch. When working with enor-mous batches, you should make sure to increase your optimizer learning rate accord-ingly; you’re going to be making fewer updates to your weights, but each update willbe more accurate (since the gradients are computed using more data points), so youshould move the weights by a greater magnitude with each update. There is, however, a simple trick you can leverage to keep reasonably sized batcheswhile maintaining full TPU utilization: step fusing. The idea is to run multiple steps oftraining during each TPU execution step. Basically, do more work in between tworound trips from the VM memory to the TPU. To do this, simply specify the steps_per_execution a r g u m e n t i n compile()—for instance, steps_per_execution=8 t orun eight steps of training during each TPU execution. For small models that areunderutilizing the TPU, this can result in a dramatic speedup. SummaryYou can leverage hyperparameter tuning and KerasTuner to automate thetedium out of finding the best model configuration. But be mindful of validation-set overfitting!An ensemble of diverse models can often significantly improve the quality ofyour predictions.You can speed up model training on GPU by turning on mixed precision—you’ll generally get a nice speed boost at virtually no cost.To further scale your workflows, you can use the tf.distribute.Mirrored-Strategy API to train models on multiple GPUs.You can even train on Google’s TPUs (available on Colab) by using the TPU-Strategy API. If your model is small, make sure to leverage step fusing (viathe compile(…, steps_per_execution=N) argument) in order to fully utilizethe TPU cores. 431Conclusions You’ve almost reached the end of this book. This last chapter will summarize andreview core concepts while also expanding your horizons beyond what you’velearned so far. Becoming an effective AI practitioner is a journey, and finishing thisbook is merely your first step on it. I want to make sure you realize this and areproperly equipped to take the next steps of this journey on your own. We’ll start with a bird’s-eye view of what you should take away from this book.This should refresh your memory regarding some of the concepts you’ve learned.Next, I’ll present an overview of some key limitations of deep learning. To use atool appropriately, you should not only understand what it can d o b u t a l s o b eaware of what it can’t do. Finally, I’ll offer some speculative thoughts about thefuture evolution of deep learning, machine learning, and AI. This should beThis chapter coversImportant takeaways from this bookThe limitations of deep learningPossible future directions for deep learning, machine learning, and AIResources for further learning and applying your skills in practice 432CHAPTER 14Conclusionsespecially interesting to you if you’d like to get into fundamental research. The chap-ter ends with a short list of resources and strategies for further learning aboutmachine learning and staying up to date with new advances.14.1 Key concepts in reviewThis section briefly synthesizes key takeaways from this book. If you ever need a quickrefresher to help you recall what you’ve learned, you can read these few pages.14.1.1 Various approaches to AIFirst of all, deep learning isn’t synonymous with AI, or even with machine learning:Artificial intelligence (AI) is an ancient, broad field that can generally be under-stood as “all attempts to automate human cognitive processes.” This can rangefrom the very basic, such as an Excel spreadsheet, to the very advanced, like ahumanoid robot that can walk and talk.Machine learning is a specific subfield of AI that aims at automatically developingprograms (called models) purely from exposure to training data. This process ofturning data into a program is called learning. Although machine learning hasbeen around for a long time, it only started to take off in the 1990s, beforebecoming the dominant form of AI in the 2000s.Deep learning is one of many branches of machine learning, where the modelsare long chains of geometric transformations, applied one after the other.These operations are structured into modules called layers: deep learning mod-els are typically stacks of layers—or, more generally, graphs of layers. These lay-ers are parameterized by weights, which are the parameters learned duringtraining. The knowledge of a model is stored in its weights, and the process oflearning consists of finding “good values” for these weights—values that mini-mize a loss function. Because the chain of geometric transformations consideredis differentiable, updating the weights to minimize the loss function is done effi-ciently via gradient descent.Even though deep learning is just one among many approaches to machine learn-ing, it isn’t on an equal footing with the others. Deep learning is a breakout success.Here’s why. 14.1.2 What makes deep learning special within the field of machine learningIn the span of only a few years, deep learning has achieved tremendous breakthroughsacross a wide range of tasks that have been historically perceived as extremely difficultfor computers, especially in the area of machine perception: extracting useful infor-mation from images, videos, sound, and more. Given sufficient training data (in par-ticular, training data appropriately labeled by humans), deep learning makes itpossible to extract from perceptual data almost anything a human could. Hence, it’s 433Key concepts in reviewsometimes said that deep learning has “solved perception”—although that’s true onlyfor a fairly narrow definition of perception. Due to its unprecedented technical successes, deep learning has singlehandedlybrought about the third and by far the largest AI summer: a period of intense interest,investment, and hype in the field of AI. As this book is being written, we’re in the mid-dle of it. Whether this period will end in the near future, and what happens after itends, are topics of debate. One thing is certain: in stark contrast with previous AI sum-mers, deep learning has provided enormous business value to both large and smalltechnology companies, enabling human-level speech recognition, smart assistants,human-level image classification, vastly improved machine translation, and more. Thehype may (and likely will) recede, but the sustained economic and technologicalimpact of deep learning will remain. In that sense, deep learning could be analogousto the internet: it may be overly hyped up for a few years, but in the longer term it willstill be a major revolution that will transform our economy and our lives. I’m particularly optimistic about deep learning, because even if we were to makeno further technological progress in the next decade, deploying existing algorithmsto every applicable problem would be a game changer for most industries. Deeplearning is nothing short of a revolution, and progress is currently happening at anincredibly fast rate, due to an exponential investment in resources and headcount.From where I stand, the future looks bright, although short-term expectations aresomewhat overoptimistic; deploying deep learning to the full extent of its potentialwill likely take multiple decades. 14.1.3 How to think about deep learningThe most surprising thing about deep learning is how simple it is. Ten years ago, noone expected that we would achieve such amazing results on machine-perceptionproblems by using simple parametric models trained with gradient descent. Now, itturns out that all you need is sufficiently large parametric models trained with gradi-ent descent on sufficiently many examples. As Feynman once said about the universe,“It’s not complicated, it’s just a lot of it.”1 In deep learning, everything is a vector—that is to say, everything is a point in a geo-metric space. Model inputs (text, images, and so on) and targets are first vectorized—turned into an initial input vector space and target vector space. Each layer in a deeplearning model operates one simple geometric transformation on the data that goesthrough it. Together, the chain of layers in the model forms one complex geometrictransformation, broken down into a series of simple ones. This complex transforma-tion attempts to map the input space to the target space, one point at a time. Thistransformation is parameterized by the weights of the layers, which are iterativelyupdated based on how well the model is currently performing. A key characteristic ofthis geometric transformation is that it must be differentiable, which is required in order1Richard Feynman, interview, “The World from Another Point of View,” Yorkshire Television, 1972. 434CHAPTER 14Conclusionsfor us to be able to learn its parameters via gradient descent. Intuitively, this meansthe geometric morphing from inputs to outputs must be smooth and continuous—asignificant constraint. T h e e n t i r e p r o c e s s o f a p p l y i n g t h i s c o m p l e x g e o m e t r i c t r a n s f o r m a t i o n t o t h einput data can be visualized in 3D by imagining a person trying to uncrumple apaper ball: the crumpled paper ball is the manifold of the input data that the modelstarts with. Each movement operated by the person on the paper ball is similar to asimple geometric transformation operated by one layer. The full uncrumpling ges-ture sequence is the complex transformation of the entire model. Deep learningmodels are mathematical machines for uncrumpling complicated manifolds of high-dimensional data. That’s the magic of deep learning: turning meaning into vectors, then into geo-metric spaces, and then incrementally learning complex geometric transformationsthat map one space to another. All you need are spaces of sufficiently high dimension-ality in order to capture the full scope of the relationships found in the original data. The whole process hinges on a single core idea: that meaning is derived from the pair-wise relationship between things (between words in a language, between pixels in an image,and so on) and that these relationships can be captured by a distance function. But note thatwhether the brain also implements meaning via geometric spaces is an entirely separatequestion. Vector spaces are efficient to work with from a computational standpoint, butdifferent data structures for intelligence can easily be envisioned—in particular, graphs.Neural networks initially emerged from the idea of using graphs as a way to encodemeaning, which is why they’re named neural networks; the surrounding field ofresearch used to be called connectionism. Nowadays the name “neural network” existspurely for historical reasons—it’s an extremely misleading name because they’re nei-ther neural nor networks. In particular, neural networks have hardly anything to dowith the brain. A more appropriate name would have been layered representations learn-ing o r hierarchical representations learning, or maybe even deep differentiable models orchained geometric transforms, to emphasize the fact that continuous geometric spacemanipulation is at their core. 14.1.4 Key enabling technologiesThe technological revolution that’s currently unfolding didn’t start with any singlebreakthrough invention. Rather, like any other revolution, it’s the product of a vastaccumulation of enabling factors—gradual at first, and then sudden. In the case ofdeep learning, we can point out the following key factors:Incremental algorithmic innovations—These first began appearing slowly over thespan of two decades (starting with backpropagation), and then were devel-oped increasingly faster as more research effort was poured into deep learn-ing after 2012.The availability of large amounts of perceptual data—This was a requirement inorder to realize that sufficiently large models trained on sufficiently large data 435Key concepts in revieware all we need. This is, in turn, a byproduct of the rise of the consumer inter-net and Moore’s law applied to storage media.The availability of fast, highly parallel computation hardware at a low price—Especiallythe GPUs produced by NVIDIA—first gaming GPUs and then chips designedfrom the ground up for deep learning. Early on, NVIDIA CEO Jensen Huangtook note of the deep learning boom and decided to bet the company’s futureon it, which paid off in a big way.A complex stack of software layers that makes this computational power available tohumans—The CUDA language, frameworks like TensorFlow that do auto-matic differentiation, and Keras, which makes deep learning accessible tomost people.In the future, deep learning will not be used only by specialists—researchers, graduatestudents, and engineers with an academic profile—it will be a tool in the toolbox ofevery developer, much like web technology today. Everyone needs to build intelligentapps: just as every business today needs a website, every product will need to intelli-gently make sense of user-generated data. Bringing about this future will require us tobuild tools that make deep learning radically easy to use and accessible to anyone withbasic coding abilities. Keras has been the first major step in that direction. 14.1.5 The universal machine learning workflowHaving access to an extremely powerful tool for creating models that map any inputspace to any target space is great, but the difficult part of the machine learning work-flow is often everything that comes before designing and training such models (and,for production models, what comes after, as well). Understanding the problem domainso as to be able to determine what to attempt to predict, given what data, and how tomeasure success, is a prerequisite for any successful application of machine learning,and it isn’t something that advanced tools like Keras and TensorFlow can help youwith. As a reminder, here’s a quick summary of the typical machine learning workflowas described in chapter 6:1Define the problem: What data is available, and what are you trying to predict?Will you need to collect more data or hire people to manually label a dataset?2Identify a way to reliably measure success on your goal. For simple tasks this maybe prediction accuracy, but in many cases it will require sophisticated, domain-specific metrics.3Prepare the validation process that you’ll use to evaluate your models. In partic-ular, you should define a training set, a validation set, and a test set. The valida-tion- and test-set labels shouldn’t leak into the training data: for instance, withtemporal prediction, the validation and test data should be posterior to thetraining data.4Vectorize the data by turning it into vectors and preprocessing it in a way thatmakes it more easily approachable by a neural network (normalization and so on). 436CHAPTER 14Conclusions5Develop a first model that beats a trivial common-sense baseline, thus demon-strating that machine learning can work on your problem. This may not alwaysbe the case!6Gradually refine your model architecture by tuning hyperparameters and add-ing regularization. Make changes based on performance on the validation dataonly, not the test data or the training data. Remember that you should get yourmodel to overfit (thus identifying a model capacity level that’s greater than youneed) and only then begin to add regularization or downsize your model.Beware of validation-set overfitting when tuning hyperparameters—the factthat your hyperparameters may end up being overspecialized to the validationset. Avoiding this is the purpose of having a separate test set.7Deploy your final model in production—as a web API, as part of a JavaScript orC++ application, on an embedded device, etc. Keep monitoring its perfor-mance on real-world data, and use your findings to refine the next iteration ofthe model!14.1.6 Key network architecturesThe four families of network architectures that you should be familiar with are denselyconnected networks, convolutional networks, recurrent networks, and Transformers. Each typeof model is meant for a specific input modality. A network architecture encodesassumptions about the structure of the data: a hypothesis space within which the searchfor a good model will proceed. Whether a given architecture will work on a givenproblem depends entirely on the match between the structure of the data and theassumptions of the network architecture. T h e s e d i f f e r e n t n e t w o r k t y p e s c a n e a s i l y b e c o m b i n e d t o a c h i e v e l a r g e r m u l t i -modal models, much as you combine LEGO bricks. In a way, deep learning layers areLEGO bricks for information processing. Here’s a quick overview of the mappingbetween input modalities and appropriate network architectures:Vector data—Densely connected models (Dense layers).Image data—2D convnets.Sequence data—RNNs for timeseries, or Transformers for discrete sequences(such as sequences of words). 1D convnets can also be used for translation-invariant, continuous sequence data, such as birdsong waveforms.Video data—Either 3D convnets (if you need to capture motion effects), or acombination of a frame-level 2D convnet for feature extraction followed by asequence-processing model.Volumetric data—3D convnets.Now, let’s quickly review the specificities of each network architecture. 437Key concepts in reviewDENSELY CONNECTED NETWORKSA densely connected network is a stack of Dense layers meant to process vector data(where each sample is a vector of numerical or categorical attributes). Such networksassume no specific structure in the input features: they’re called densely connectedbecause the units of a Dense l a y e r a r e c o n n e c t e d t o e v e r y o t h e r u n i t . T h e l a y e rattempts to map relationships between any two input features; this is unlike a 2D con-volution layer, for instance, which only looks at local relationships. Densely connected networks are most commonly used for categorical data (forexample, where the input features are lists of attributes), such as the Boston HousingPrice dataset used in chapter 4. They’re also used as the final classification or regres-sion stage of most networks. For instance, the convnets covered in chapter 8 typicallyend with one or two Dense layers, and so do the recurrent networks in chapter 10. Remember, to perform binary classification, end your stack of layers with a Denselayer with a single unit and a sigmoid activation, and use binary_crossentropy as theloss. Your targets should be either 0 or 1:from tensorflow import keras from tensorflow.kerasimportlayersinputs = keras.Input(shape=(num_input_features,))x = layers.Dense(32,a c t i v a t i o n =\"relu\")(inputs)x = layers.Dense(32,a c t i v a t i o n =\"relu\")(x)outputs = layers.Dense(1,a c t i v a t i o n =\"sigmoid\")(x)model = keras.Model(inputs, outputs)model.compile(optimizer=\"rmsprop\",l o s s =\"binary_crossentropy\")To perform single-label categorical classification (where each sample has exactly one class,no more), end your stack of layers with a Dense layer with a number of units equal to thenumber of classes, and a softmax activation. If your targets are one-hot encoded, usecategorical_crossentropy as the loss; if they’re integers, use sparse_categorical_crossentropy:inputs = keras.Input(shape=(num_input_features,))x = layers.Dense(32,a c t i v a t i o n =\"relu\")(inputs)x = layers.Dense(32,a c t i v a t i o n =\"relu\")(x)outputs = layers.Dense(num_classes, activation=\"softmax\")(x)model = keras.Model(inputs, outputs)model.compile(optimizer=\"rmsprop\",l o s s =\"categorical_crossentropy\")To perform multilabel categorical classification ( w h e r e e a c h s a m p l e c a n h a v e s e v e r a lclasses), end your stack of layers with a Dense layer with a number of units equal to thenumber of classes, and a sigmoid a c t i v a t i o n , a n d u s e binary_crossentropy a s t h eloss. Your targets should be multi-hot encoded:inputs = keras.Input(shape=(num_input_features,))x = layers.Dense(32,a c t i v a t i o n =\"relu\")(inputs)x = layers.Dense(32,a c t i v a t i o n =\"relu\")(x)outputs = layers.Dense(num_classes, activation=\"sigmoid\")(x)model = keras.Model(inputs, outputs)model.compile(optimizer=\"rmsprop\",l o s s =\"binary_crossentropy\") 438CHAPTER 14ConclusionsTo perform regression toward a vector of continuous values, end your stack of layerswith a Dense layer with a number of units equal to the number of values you’re tryingto predict (often a single one, such as the price of a house), and no activation. Variouslosses can be used for regression—most commonly mean_squared_error (MSE):inputs = keras.Input(shape=(num_input_features,))x = layers.Dense(32,a c t i v a t i o n =\"relu\")(inputs)x = layers.Dense(32,a c t i v a t i o n =\"relu\")(x)outputs layers.Dense(num_values)(x)model = keras.Model(inputs, outputs)model.compile(optimizer=\"rmsprop\",l o s s =\"mse\")CONVNETSConvolution layers look at spatially local patterns by applying the same geometrictransformation to different spatial locations (patches) in an input tensor. This resultsin representations that are translation invariant, making convolution layers highly dataefficient and modular. This idea is applicable to spaces of any dimensionality: 1D(continuous sequences), 2D (images), 3D (volumes), and so on. You can use theConv1D layer to process sequences, the Conv2D layer to process images, and the Conv3Dlayers to process volumes. As a leaner, more efficient alternative to convolution layers,you can also use depthwise separable convolution layers, such as SeparableConv2D. Convnets, or convolutional networks, consist of stacks of convolution and max-poolinglayers. The pooling layers let you spatially downsample the data, which is required tokeep feature maps to a reasonable size as the number of features grows, and to allow sub-sequent convolution layers to “see” a greater spatial extent of the inputs. Convnets areoften ended with either a Flatten operation or a global pooling layer, turning spatial fea-ture maps into vectors, followed by Dense layers to achieve classification or regression. H e r e ’ s a t y p i c a l i m a g e - c l a s s i f i c a t i o n n e t w o r k ( c a t e g o r i c a l c l a s s i f i c a t i o n , i n t h i scase), leveraging SeparableConv2D layers:inputs = keras.Input(shape=(height, width, channels))x = layers.SeparableConv2D(32,3,a c t i v a t i o n =\"relu\")(inputs)x = layers.SeparableConv2D(64,3,a c t i v a t i o n =\"relu\")(x)x = layers.MaxPooling2D(2)(x)x = layers.SeparableConv2D(64,3,a c t i v a t i o n =\"relu\")(x)x = layers.SeparableConv2D(128,3,a c t i v a t i o n =\"relu\")(x)x = layers.MaxPooling2D(2)(x)x = layers.SeparableConv2D(64,3,a c t i v a t i o n =\"relu\")(x)x = layers.SeparableConv2D(128,3,a c t i v a t i o n =\"relu\")(x)x = layers.GlobalAveragePooling2D()(x)x = layers.Dense(32,a c t i v a t i o n =\"relu\")(x)outputs = layers.Dense(num_classes, activation=\"softmax\")(x)model = keras.Model(inputs, outputs)model.compile(optimizer=\"rmsprop\",l o s s =\"categorical_crossentropy\")When building a very deep convnet, it’s common to add batch normalization layers aswell as residual connections—two architecture patterns that help gradient informationflow smoothly through the network. 439Key concepts in reviewRNNSRecurrent neural networks (RNNs) work by processing sequences of inputs one timestepat a time, and maintaining a state throughout (a state is typically a vector or set of vec-tors). They should be used preferentially over 1D convnets in the case of sequenceswhere patterns of interest aren’t invariant by temporal translation (for instance,timeseries data where the recent past is more important than the distant past). Three RNN layers are available in Keras: SimpleRNN, GRU, and LSTM. For most practi-cal purposes, you should use either GRU or LSTM. LSTM is the more powerful of the twobut is also more expensive; you can think of GRU as a simpler, cheaper alternative to it. In order to stack multiple RNN layers on top of each other, each layer prior to thelast layer in the stack should return the full sequence of its outputs (each input time-step will correspond to an output timestep). If you aren’t stacking any further RNNlayers, it’s common to return only the last output, which contains information aboutthe entire sequence. Following is a single RNN layer for binary classification of vector sequences:inputs = keras.Input(shape=(num_timesteps, num_features))x = layers.LSTM(32)(inputs)outputs = layers.Dense(num_classes, activation=\"sigmoid\")(x)model = keras.Model(inputs, outputs)model.compile(optimizer=\"rmsprop\",l o s s =\"binary_crossentropy\")And this is a stacked RNN for binary classification of vector sequences:inputs = keras.Input(shape=(num_timesteps, num_features))x = layers.LSTM(32,r e t u r n _ s e q u e n c e s =True)(inputs)x = layers.LSTM(32,r e t u r n _ s e q u e n c e s =True)(x)x = layers.LSTM(32)(x)outputs = layers.Dense(num_classes, activation=\"sigmoid\")(x)model = keras.Model(inputs, outputs)model.compile(optimizer=\"rmsprop\",l o s s =\"binary_crossentropy\")TRANSFORMERSA Transformer looks at a set of vectors (such as word vectors), and leverages neuralattention to transform each vector into a representation that is aware of the context pro-vided by the other vectors in the set. When the set in question is an ordered sequence,you can also leverage positional encoding t o c r e a t e T r a n s f o r m e r s t h a t c a n t a k e i n t oaccount both global context and word order, capable of processing long text para-graphs much more effectively than RNNs or 1D convnets. T r a n s f o r m e r s c a n b e u s e d f o r a n y s e t - p r o c e s s i n g o r s e q u e n c e - p r o c e s s i n g t a s k ,including text classification, but they excel especially at sequence-to-sequence learning,such as translating paragraphs in a source language into a target language. A sequence-to-sequence Transformer is made up of two parts:A TransformerEncoder t h a t t u r n s a n i n p u t v e c t o r s e q u e n c e i n t o a c o n t e x t -aware, order-aware output vector sequence 440CHAPTER 14ConclusionsA TransformerDecoder that takes the output of the TransformerEncoder, as wellas a target sequence, and predicts what should come next in the target sequenceIf you’re only processing a single sequence (or set) of vectors, you’d be only using theTransformerEncoder. Following is a sequence-to-sequence Transformer for mapping a source sequenceto a target sequence (this setup could be used for machine translation or questionanswering, for instance):encoder_inputs = keras.Input(shape=(sequence_length,), dtype=\"int64\") x = PositionalEmbedding( sequence_length, vocab_size, embed_dim)(encoder_inputs)encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\") x = PositionalEmbedding( sequence_length, vocab_size, embed_dim)(decoder_inputs)x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x) transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)transformer.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")And this is a lone TransformerEncoder for binary classification of integer sequences:inputs = keras.Input(shape=(sequence_length,), dtype=\"int64\")x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)x = layers.GlobalMaxPooling1D()(x)outputs = layers.Dense(1, activation=\"sigmoid\")(x)model = keras.Model(inputs, outputs)model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\")Full implementations of the TransformerEncoder, the TransformerDecoder, and thePositionalEmbedding layer are provided in chapter 11. 14.1.7 The space of possibilitiesWhat will you build with these techniques? Remember, building deep learning modelsis like playing with LEGO bricks: layers can be plugged together to map essentiallyanything to anything, given that you have appropriate training data available and thatthe mapping is achievable via a continuous geometric transformation of reasonablecomplexity. The space of possibilities is infinite. This section offers a few examples toinspire you to think beyond the basic classification and regression tasks that have tra-ditionally been the bread and butter of machine learning. I’ve sorted my suggested applications by input and output modalities in the follow-ing list. Note that quite a few of them stretch the limits of what is possible—although amodel could be trained on all of these tasks, in some cases such a model probablywouldn’t generalize far from its training data. Sections 14.2 through 14.4 will addresshow these limitations could be lifted in the future:Source sequenceTargetsequenceso farTarget sequence one step in the future 441Key concepts in reviewMapping vector data to vector data:–Predictive healthcare—Mapping patient medical records to predictions ofpatient outcomes–Behavioral targeting—Mapping a set of website attributes with data on howlong a user will spend on the website–Product quality control—Mapping a set of attributes relative to an instance ofa manufactured product with the probability that the product will fail bynext yearMapping image data to vector data:–Medical assistant—Mapping slides of medical images to a prediction aboutthe presence of a tumor–Self-driving vehicle—Mapping car dashcam video frames to steering wheelangle commands and gas and braking commands–Board game AI—Mapping Go or chess boards to the next player move–Diet helper—Mapping pictures of a dish to its calorie count–Age prediction—Mapping selfies to the age of the personMapping timeseries data to vector data:–Weather prediction—Mapping timeseries of weather data in a grid of locationsto the temperature in a specific place one week later–Brain-computer interfaces—Mapping timeseries of magnetoencephalogram(MEG) data to computer commands–Behavioral targeting—Mapping timeseries of user interactions on a website tothe probability that a user will buy somethingMapping text to text:–Machine translation— Mapping a paragraph in one language to a translatedversion in a different language–Smart reply—Mapping emails to possible one-line replies–Question answering—Mapping general-knowledge questions to answers–Summarization—Mapping a long article to a short summary of the articleMapping images to text:–Text transcription— Mapping images that contain a text element to the corre-sponding text string–Captioning—Mapping images to short captions describing the contents ofthe imagesMapping text to images:–Conditioned image generation—Mapping a short text description to imagesmatching the description–Logo generation/selection—Mapping the name and description of a companyto a logo suggestion 442CHAPTER 14ConclusionsMapping images to images:–Super-resolution—Mapping downsized images to higher-resolution versions ofthe same images–Visual depth sensing—Mapping images of indoor environments to maps ofdepth predictionsMapping images and text to text:–Visual QA—Mapping images and natural language questions about the con-tents of images to natural language answersMapping video and text to text:–Video QA—Mapping short videos and natural language questions about thecontents of videos to natural language answersAlmost anything is possible, but not quite anything. You’ll see in the next section whatwe can’t do with deep learning. 14.2 The limitations of deep learningThe space of applications that can be implemented with deep learning is infinite. Andyet, many applications remain completely out of reach for current deep learning tech-niques—even given vast amounts of human-annotated data. Say, for instance, that youcould assemble a dataset of hundreds of thousands—even millions—of English-languagedescriptions of the features of a software product, written by a product manager, as wellas the corresponding source code developed by a team of engineers to meet theserequirements. Even with this data, you could not train a deep learning model to read aproduct description and generate the appropriate codebase. That’s just one exampleamong many. In general, anything that requires reasoning—like programming or apply-ing the scientific method—long-term planning, and algorithmic data manipulation is outof reach for deep learning models, no matter how much data you throw at them. Evenlearning a simple sorting algorithm with a deep neural network is tremendously difficult. This is because a deep learning model is just a chain of simple, continuous geometrictransformations mapping one vector space into another. All it can do is map one datamanifold X into another manifold Y, assuming the existence of a learnable continu-ous transform from X to Y. A deep learning model can be interpreted as a kind ofprogram, but, inversely, most programs can’t be expressed as deep learning models. Formost tasks, either there exists no corresponding neural network of reasonable sizethat solves the task or, even if one exists, it may not be learnable: the correspondinggeometric transform may be far too complex, or there may not be appropriate dataavailable to learn it. Scaling up current deep learning techniques by stacking more layers and usingmore training data can only superficially palliate some of these issues. It won’t solvethe more fundamental problems that deep learning models are limited in what theycan represent and that most of the programs you may wish to learn can’t be expressedas a continuous geometric morphing of a data manifold. 443The limitations of deep learning14.2.1 The risk of anthropomorphizing machine learning modelsOne real risk with contemporary AI is misinterpreting what deep learning modelsdo and overestimating their abilities. A fundamental feature of humans is our theoryof mind: our tendency to project intentions, beliefs, and knowledge on the thingsaround us. Drawing a smiley face on a rock suddenly makes it “happy” in our minds.Applied to deep learning, this means that, for instance, when we’re able to some-what successfully train a model to generate captions to describe pictures, we’re ledto believe that the model “understands” the contents of the pictures and the cap-tions it generates. Then we’re surprised when any slight departure from the sort ofimages present in the training data causes the model to generate completely absurdcaptions (see figure 14.1). In particular, this is highlighted by adversarial examples, which are samples fed to adeep learning network that are designed to trick the model into misclassifying them.You’re already aware that, for instance, it’s possible to do gradient ascent in inputspace to generate inputs that maximize the activation of some convnet filter—this isthe basis of the filter-visualization technique introduced in chapter 9, as well as theDeepDream algorithm from chapter 12. Similarly, through gradient ascent, you canslightly modify an image in order to maximize the class prediction for a given class. Bytaking a picture of a panda and adding to it a gibbon gradient, we can get a neuralnetwork to classify the panda as a gibbon (see figure 14.2). This evidences both thebrittleness of these models and the deep difference between their input-to-outputmapping and our human perception. In short, deep learning models don’t have any understanding of their input—atleast, not in a human sense. Our own understanding of images, sounds, and languageis grounded in our sensorimotor experience as humans. Machine learning modelshave no access to such experiences and thus can’t understand their inputs in ahuman-relatable way. By annotating large numbers of training examples to feed intoour models, we get them to learn a geometric transform that maps data to humanconcepts on a specific set of examples, but this mapping is a simplistic sketch of the The boy is holding a baseball bat.Figure 14.1 Failure of an image-captioning system based on deep learning 444CHAPTER 14Conclusions original model in our minds—the one developed from our experience as embodiedagents. It’s like a dim image in a mirror (see figure 14.3). The models you create willtake any shortcut available to fit their training data. For instance, image models tendto rely more on local textures than on a global understanding of the input images—amodel trained on a dataset that features both leopards and sofas is likely to classify aleopard-pattern sofa as an actual leopard. Gibbonclass gradient PandaPandaf(x) Adversarial exampleGibbon!f(x) Figure 14.2 An adversarial example: imperceptible changes in an image can upend a model’s classification of an image. Real worldEmbodiedhuman experienceAbstract conceptsin human mindLabeled dataexemplifyingthese concepts May not alwaystransfer well tothe real worldDoesn’t match thehuman mental modelit came fromMatches thetraining dataMachine learningmodelf(x) Figure 14.3 Current machine learning models: like a dim image in a mirror 445The limitations of deep learningAs a machine learning practitioner, always be mindful of this, and never fall into thetrap of believing that neural networks understand the tasks they perform—they don’t,at least not in a way that would make sense to us. They were trained on a different, farnarrower task than the one we wanted to teach them: that of mapping training inputsto training targets, point by point. Show them anything that deviates from their train-ing data, and they will break in absurd ways. 14.2.2 Automatons vs. intelligent agentsThere are fundamental differences between the straightforward geometric morphingfrom input to output that deep learning models do and the way humans think andlearn. It isn’t just the fact that humans learn by themselves from embodied experienceinstead of being presented with explicit training examples. The human brain is anentirely different beast compared to a differentiable parametric function. Let’s zoom out a little bit and ask, “what’s the purpose of intelligence?” Why did itarise in the first place? We can only speculate, but we can make fairly informed specu-lations. We can start by looking at brains—the organ that produces intelligence.Brains are an evolutionary adaption—a mechanism developed incrementally over hun-dreds of millions of years, via random trial-and-error guided by natural selection—thatdramatically expanded the ability of organisms to adapt to their environment. Brainsoriginally appeared more than half a billion years ago as a way to store and execute behav-ioral programs. “Behavioral programs” are just sets of instructions that make an organ-ism reactive to its environment: “if this happens, then do that.” They link theorganism’s sensory inputs to its motor controls. In the beginning, brains would haveserved to hardcode behavioral programs (as neural connectivity patterns), whichwould allow an organism to react appropriately to its sensory input. This is the wayinsect brains still work—flies, ants, C. elegans (see figure 14.4), etc. Because the origi-nal “source code” of these programs was DNA, which would get decoded as neuralconnectivity patterns, evolution was suddenly able to search over behavior space i n alargely unbounded way—a major evolutionary shift.Evolution was the programmer, and brains were computers carefully executing thecode evolution gave them. Because neural connectivity is a very general computingsubstrate, the sensorimotor space of all brain-enabled species could suddenly startundergoing a dramatic expansion. Eyes, ears, mandibles, 4 legs, 24 legs—as long asyou have a brain, evolution will kindly figure out for you behavioral programs thatmake good use of these. Brains can handle any modality—or combination of modali-ties—you throw at them. Now, mind you, these early brains weren’t exactly intelligent per se. They were verymuch automatons: they would merely execute behavioral programs hardcoded in theorganism’s DNA. They could only be described as intelligent in the same sense that athermostat is “intelligent.” Or a list-sorting program. Or . . . a trained deep neural net-work (of the artificial kind). This is an important distinction, so let’s look at it care-fully: what’s the difference between automatons and actual intelligent agents? 446CHAPTER 14Conclusions 14.2.3 Local generalization vs. extreme generalizationSeventeenth century French philosopher and scientist René Descartes wrote in 1637an illuminating comment that perfectly captures this distinction—long before the riseof AI, and in fact, before the first mechanical computer (which his colleague Pascalwould create five years later). Descartes tells us, in reference to automatons,Even though such machines might do some things as well as we do them, or perhaps evenbetter, they would inevitably fail in others, which would reveal they were acting notthrough understanding, but only from the disposition of their organs.—René Descartes, Discourse on the Method (1637)There it is. Intelligence is characterized by understanding, and understanding is evi-denced by generalization—the ability to handle whatever novel situation may arise.How do you tell the difference between a student that has memorized the past threeyears of exam questions but has no understanding of the subject, and a student thatFigure 14.4 The brain network of the C. elegans worm: a behavioral automaton “programmed” by natural evolution. Figure created by Emma Towlson (from Yan et al., “Network control principles predict neuron function in the Caenorhabditis elegans connectome,” Nature, Oct. 2017). 447The limitations of deep learningactually understands the material? You give them a brand new problem. An automa-ton is static, crafted to accomplish specific things in a specific context—“if this, thenthat”—while an intelligent agent can adapt on the fly to novel, unexpected situations.When an automaton is exposed to something that doesn’t match what it is “pro-grammed” to do (whether we’re talking about human-written programs, evolution-generated programs, or the implicit programming process of fitting a model on atraining data set), it will fail. Meanwhile, intelligent agents, like humans, will use theirunderstanding to find a way forward. Humans are capable of far more than mapping immediate stimuli to immediateresponses, as a deep net, or an insect, would. We maintain complex, abstract modelsof our current situation, of ourselves, and of other people, and we can use these mod-els to anticipate different possible futures and perform long-term planning. You canmerge together known concepts to represent something you’ve never experiencedbefore—like imagining what you’d do if you won the lottery, or picturing how yourfriend would react if you discreetly replaced her keys with exact copies made of elasticrubber. This ability to handle novelty and what-ifs, to expand our mental model spacefar beyond what we can experience directly—to leverage abstraction and reasoning—is the defining characteristic of human cognition. I call it extreme generalization: anability to adapt to novel, never-before-experienced situations using little data oreven no new data at all. This capability is key to the intelligence displayed byhumans and advanced animals. T h i s s t a n d s i n s h a r p c o n t r a s t w i t h w h a t a u t o m a t o n - l i k e s y s t e m s d o . A v e r y r i g i dautomaton wouldn’t feature any generalization at all—it would be incapable of han-dling anything that it wasn’t precisely told about in advance. A Python dict or a basicquestion-answering program implemented as hardcoded if-then-else statementswould fall into this category. Deep nets do slightly better: they can successfully processinputs that deviate a bit from what they’re familiar with—which is precisely whatmakes them useful. Our cats vs. dogs model from chapter 8 could classify cat or dogpictures it had not seen before, as long as they were close enough to what it wastrained on. However, deep nets are limited to what I call local generalization (see fig-ure 14.5): the mapping from inputs to outputs performed by a deep net quicklystops making sense as inputs start deviating from what the net saw at training time.Deep nets can only generalize to known unknowns—to factors of variation that wereanticipated during model development and that are extensively featured in thetraining data, such as different camera angles or lighting conditions for pet pic-tures. That’s because deep nets generalize via interpolation on a manifold (remem-ber chapter 5): any factor of variation in their input space needs to be captured bythe manifold they learn. That’s why basic data augmentation is so helpful in improv-ing deep net generalization. Unlike humans, these models have no ability to impro-vise in the face of situations for which little or no data is available (like winning thelottery or being handed rubber keys) that only share abstract commonalities withpast situations. 448CHAPTER 14Conclusions Consider, for instance, the problem of learning the appropriate launch parameters toget a rocket to land on the moon. If you used a deep net for this task and trained itusing supervised learning or reinforcement learning, you’d have to feed it tens ofthousands or even millions of launch trials: you’d need to expose it to a dense samplingof the input space, in order for it to learn a reliable mapping from input space to out-put space. In contrast, as humans, we can use our power of abstraction to come upwith physical models—rocket science—and derive an exact solution that will land therocket on the moon in one or a few trials. Similarly, if you developed a deep net con-trolling a human body, and you wanted it to learn to safely navigate a city withoutgetting hit by cars, the net would have to die many thousands of times in various situa-tions until it could infer that cars are dangerous and develop appropriate avoidancebehaviors. Dropped into a new city, the net would have to relearn most of what itknows. On the other hand, humans are able to learn safe behaviors without having todie even once—again, thanks to our power of abstract modeling of novel situations. 14.2.4 The purpose of intelligenceThis distinction between highly adaptable intelligent agents and rigid automatonsleads us back to brain evolution. Why did brains—originally a mere medium for natu-ral evolution to develop behavioral automatons—eventually turn intelligent? Likeevery significant evolutionary milestone, it happened because natural selection con-straints encouraged it to happen. Brains are responsible for behavior generation. If the set of situations an organismhad to face was mostly static and known in advance, behavior generation would be aneasy problem: evolution would just figure out the correct behaviors via random trialand error and hardcode them into the organism’s DNA. This first stage of brain evo-lution—brains as automatons—would already be optimal. However, crucially, asorganism complexity—and alongside it, environmental complexity—kept increas-ing, the situations that animals had to deal with became much more dynamic andKnownsituationsSituations spaceOperationalareaLower-intelligence system:lower informationconversion ratioHigher-intelligence system:higher informationconversion ratioFigure 14.5 Local generalization vs. extreme generalization 449The limitations of deep learningmore unpredictable. A day in your life, if you look closely, is unlike any day you’veever experienced, and unlike any day ever experienced by any of your evolutionaryancestors. You need to be able to face unknown and surprising situations—constantly.There is no way for evolution to find and hardcode as DNA the sequence of behaviorsyou’ve been executing to successfully navigate your day since you woke up a few hoursago. It has to be generated on the fly—every day. The brain, as a good behavior-generation engine, simply adapted to fit this need. Itoptimized for adaptability and generality, rather than merely optimizing for fitness toa fixed set of situations. This shift likely occurred multiple times throughout evolu-tionary history, resulting in highly intelligent animals in very distant evolutionarybranches—apes, octopuses, ravens, and more. Intelligence is an answer to challengespresented by complex, dynamic ecosystems. That’s the nature of intelligence: it is the ability to efficiently leverage the informa-tion at your disposal in order to produce successful behavior in the face of an uncer-tain, ever-changing future. What Descartes calls “understanding” is the key to thisremarkable capability: the power to mine your past experience to develop modular,reusable abstractions that can be quickly repurposed to handle novel situations andachieve extreme generalization. 14.2.5 Climbing the spectrum of generalizationAs a crude caricature, you could summarize the evolutionary history of biologicalintelligence as a slow climb up the spectrum of generalization. It started with automaton-like brains that could only perform local generalization. Over time, evolution startedproducing organisms capable of increasingly broader generalization that could thrivein ever-more complex and variable environments. Eventually, in the past few millionsof years—an instant in evolutionary terms—certain hominin species started trendingtoward an implementation of biological intelligence capable of extreme generaliza-tion, precipitating the start of the Anthropocene and forever changing the history oflife on earth. The progress of AI over the past 70 years bears striking similarities to this evolu-tion. Early AI systems were pure automatons, like the ELIZA chat program from the1960s, or SHRDLU,2 a 1970 AI capable of manipulating simple objects from naturallanguage commands. In the 1990s and 2000s, we saw the rise of machine learning sys-tems capable of local generalization, which could deal with some level of uncertaintyand novelty. In the 2010s, deep learning further expanded the local-generalizationpower of these systems by enabling engineers to leverage much larger datasets andmuch more expressive models. Today, we may be on the cusp of the next evolutionary step. There is increasinginterest in systems that could achieve broad generalization, which I define as the ability2Terry Winograd, “Procedures as a Representation for Data in a Computer Program for Understanding NaturalLanguage” (1971). 450CHAPTER 14Conclusionsto deal with unknown unknowns within a single broad domain of tasks (including situa-tions the system was not trained to handle and that its creators could not have antici-pated). For instance, a self-driving car capable of safely dealing with any situation youthrow at it, or a domestic robot that could pass the “Woz test of intelligence”—enter-ing a random kitchen and making a cup of coffee.3 By combining deep learning andpainstakingly handcrafted abstract models of the world, we’re already making visibleprogress toward these goals. However, for the time being, AI remains limited to cognitive automation: the “intel-ligence” label in “Artificial Intelligence” is a category error. It would be more accurate tocall our field “Artificial Cognition,” with “Cognitive Automation” and “Artificial Intelli-gence” being two nearly independent subfields within it. In this subdivision, “Artifi-cial Intelligence” would be a greenfield where almost everything remains to bediscovered. Now, I don’t mean to diminish the achievements of deep learning. Cognitive auto-mation is incredibly useful, and the way deep learning models are capable of automat-ing tasks from exposure to data alone represents an especially powerful form ofcognitive automation, far more practical and versatile than explicit programming.Doing this well is a game-changer for essentially every industry. But it’s still a long wayfrom human (or animal) intelligence. Our models, so far, can only perform local gen-eralization: they map space X to space Y via a smooth geometric transform learnedfrom a dense sampling of X-to-Y data points, and any disruption within spaces X or Yinvalidates this mapping. They can only generalize to new situations that stay similarto past data, whereas human cognition is capable of extreme generalization, quicklyadapting to radically novel situations and planning for long-term future situations. 14.3 Setting the course toward greater generality in AITo lift some of the limitations we have discussed and create AI that can compete withhuman brains, we need to move away from straightforward input-to-output mappingsand on to reasoning and abstraction. In the following couple of sections, we’ll take alook at what the road ahead may look like.14.3.1 On the importance of setting the right objective: The shortcut ruleBiological intelligence was the answer to a question asked by nature. Likewise, if we wantto develop true artificial intelligence, first, we need to be asking the right questions. An effect you see constantly in systems design is the shortcut rule: if you focus onoptimizing one success metric, you will achieve your goal, but at the expense of every-thing in the system that wasn’t covered by your success metric. You end up takingevery available shortcut toward the goal. Your creations are shaped by the incentivesyou give yourself.3Fast Company, “Wozniak: Could a Computer Make a Cup of Coffee?” (March 2010), http:/ /mng.bz/pJMP. 451Setting the course toward greater generality in AI You see this often in machine learning competitions. In 2009, Netflix ran a challengethat promised a $1 million prize to the team that achieved the highest score on a movierecommendation task. It ended up never using the system created by the winning team,because it was way too complex and compute-intensive. The winners had optimized forprediction accuracy alone—what they were incentivized to achieve—at the expense ofevery other desirable characteristic of the system: inference cost, maintainability, andexplainability. The shortcut rule holds true in most Kaggle competitions as well—themodels produced by Kaggle winners can rarely, if ever, be used in production. T h e s h o r t c u t r u l e h a s b e e n e v e r y w h e r e i n A I o v e r t h e p a s t f e w d e c a d e s . I n t h e1970s, psychologist and computer science pioneer Allen Newell, concerned that hisfield wasn’t making any meaningful progress toward a proper theory of cognition,proposed a new grand goal for AI: chess-playing. The rationale was that playing chess,in humans, seemed to involve—perhaps even require—capabilities such as percep-tion, reasoning and analysis, memory, study from books, and so on. Surely, if we couldbuild a chess-playing machine, it would have to feature these attributes as well. Right? Over two decades later, the dream came true: in 1997, IBM’s Deep Blue beat GaryKasparov, the best chess player in the world. Researchers had then to contend with thefact that creating a chess-champion AI had taught them little about human intelli-gence. The Alpha–Beta algorithm at the heart of Deep Blue wasn’t a model of thehuman brain and couldn’t generalize to tasks other than similar board games. Itturned out it was easier to build an AI that could only play chess than to build an arti-ficial mind—so that’s the shortcut researchers took. So far, the driving success metric of the field of AI has been to solve specifictasks, from chess to Go, from MNIST classification to ImageNet, from Atari Arcadegames to StarCraft and DotA 2. Consequently, the history of the field has beendefined by a series of “successes” where we figured out how to solve these tasks with-out featuring any intelligence. If that sounds like a surprising statement, keep in mind that human-like intelli-gence isn’t characterized by skill at any particular task—rather, it is the ability to adaptto novelty, to efficiently acquire new skills and master never-seen-before tasks. By fix-ing the task, you make it possible to provide an arbitrarily precise description of whatneeds to be done—either via hardcoding human-provided knowledge or by supplyinghumongous amounts of data. You make it possible for engineers to “buy” more skillfor their AI by just adding data or adding hardcoded knowledge, without increasingthe generalization power of the AI (see figure 14.6). If you have near-infinite trainingdata, even a very crude algorithm like nearest-neighbor search can play video gameswith superhuman skill. Likewise if you have a near-infinite amount of human-writtenif-then-else statements. That is, until you make a small change to the rules of the game—the kind a human could adapt to instantly—that will require the non-intelligent systemto be retrained or rebuilt from scratch. In short, by fixing the task, you remove the need to handle uncertainty and novelty,and since the nature of intelligence is the ability to handle uncertainty and novelty, 452CHAPTER 14Conclusions you’re effectively removing the need for intelligence. And because it’s always easier tofind a non-intelligent solution to a specific task than to solve the general problem ofintelligence, that’s the shortcut you will take 100% of the time. Humans can use theirgeneral intelligence to acquire skills at any new task, but in reverse, there is no pathfrom a collection of task-specific skills to general intelligence. 14.3.2 A new targetTo make artificial intelligence actually intelligent, and give it the ability to deal with theincredible variability and ever-changing nature of the real world, we first need to moveaway from seeking to achieve task-specific skill, and instead, start targeting generalizationpower itself. We need new metrics of progress that will help us develop increasinglyintelligent systems. Metrics that will point in the right direction and that will give us anactionable feedback signal. As long as we set our goal to be “create a model that solvestask X,” the shortcut rule will apply, and we’ll end up with a model that does X, period. In my view, intelligence can be precisely quantified as an efficiency ratio: the conver-sion ratio between the amount of relevant information you have available about the world(which could be either past experience or innate prior knowledge) and your future operatingarea, the set of novel situations where you will be able to produce appropriate behav-ior (you can view this as your skillset). A more intelligent agent will be able to handle abroader set of future tasks and situations using a smaller amount of past experience.To measure such a ratio, you just need to fix the information available to your system—its experience and its prior knowledge—and measure its performance on a set of ref-erence situations or tasks that are known to be sufficiently different from what the sys-tem has had access to. Trying to maximize this ratio should lead you towardintelligence. Crucially, to avoid cheating, you’re going to need to make sure you testthe system only on tasks it wasn’t programmed or trained to handle—in fact, you needtasks that the creators of the system could not have anticipated.Figure 14.6 A low-generalization system can achieve arbitrary skill at a fixed task given unlimited task-specific information. 453Setting the course toward greater generality in AI In 2018 and 2019, I developed a benchmark dataset called the Abstraction and Rea-soning Corpus (ARC)4 that seeks to capture this definition of intelligence. ARC is meantto be approachable by both machines and humans, and it looks very similar to humanIQ tests, such as Raven’s progressive matrices. At test time, you’ll see a series of “tasks.”Each task is explained via three or four “examples” that take the form of an input gridand a corresponding output grid (see figure 14.7). You’ll then be given a brand newinput grid, and you’ll have three tries to produce the correct output grid before mov-ing on to the next task. Compared to IQ tests, two things are unique about ARC. First, ARC seeks to measuregeneralization power, by only testing you on tasks you’ve never seen before. Thatmeans that ARC is a game you can’t practice for, at least in theory: the tasks you will gettested on will have their own unique logic that you will have to understand on the fly.You can’t just memorize specific strategies from past tasks. In addition, ARC tries to control for the prior knowledge that you bring to the test.You never approach a new problem entirely from scratch—you bring to it preexistingskills and information. ARC makes the assumption that all test takers should startfrom the set of knowledge priors, called “Core Knowledge priors,” that represent the“knowledge systems” that humans are born with. Unlike an IQ test, ARC tasks willnever involve acquired knowledge, like English sentences, for instance. U n s u r p r i s i n g l y , d e e p - l e a r n i n g - b a s e d m e t h o d s ( i n c l u d i n g m o d e l s t r a i n e d o nextremely large amounts of external data, like GPT-3) have proven entirely unable tosolve ARC tasks, because these tasks are non-interpolative, and thus are a poor fit forcurve-fitting. Meanwhile, average humans have no issue solving these tasks on the firsttry, without any practice. When you see a situation like this, where humans as young as4François Chollet, “On the Measure of Intelligence” (2019), https:/ / arxiv.org/abs/1911.01547.Figure 14.7 An ARC task: the nature of the task is demonstrated by a couple of input-output pair examples. Provided with a new input, you must construct the corresponding output. 454CHAPTER 14Conclusionsfive are able to naturally perform something that seems to be completely out of reachfor modern AI technology, that’s a clear signal that something interesting is goingon—that we’re missing something. What would it take to solve ARC? Hopefully, this challenge will get you thinking.That’s the entire point of ARC: to give you a goal of a different kind that will nudgeyou in a new direction—hopefully a productive direction. Now, let’s take a quick lookat the key ingredients you’re going to need if you want to answer the call. 14.4 Implementing intelligence: The missing ingredientsSo far, you’ve learned that there’s a lot more to intelligence than the sort of latentmanifold interpolation that deep learning does. But what, then, do we need to startbuilding real intelligence? What are the core pieces that are currently eluding us?14.4.1 Intelligence as sensitivity to abstract analogiesIntelligence is the ability to use your past experience (and innate prior knowledge) toface novel, unexpected future situations. Now, if the future you had to face was trulynovel—sharing no common ground with anything you’ve seen before—you’d beunable to react to it, no matter how intelligent you were. I n t e l l i g e n c e w o r k s b e c a u s e n o t h i n g i s e v e r t r u l y w i t h o u t p r e c e d e n t . W h e n w eencounter something new, we’re able to make sense of it by drawing analogies to ourpast experience, by articulating it in terms of the abstract concepts we’ve collectedover time. A person from the 17th century seeing a jet plane for the first time mightdescribe it as a large, loud metal bird that doesn’t flap its wings. A car? That’s a horse-less carriage. If you’re trying to teach physics to a grade schooler, you can explain howelectricity is like water in a pipe, or how space-time is like a rubber sheet getting dis-torted by heavy objects. Besides such clear-cut, explicit analogies, we’re constantly making smaller, implicitanalogies—every second, with every thought. Analogies are how we navigate life.Going to a new supermarket? You’ll find your way by relating it to similar stores you’vebeen to. Talking to someone new? They’ll remind you of a few people you’ve metbefore. Even seemingly random patterns, like the shape of clouds, instantly evoke inus vivid images—an elephant, a ship, a fish. These analogies aren’t just in our minds, either: physical reality itself is full of isomor-phisms. Electromagnetism is analogous to gravity. Animals are all structurally similar toeach other, due to shared origins. Silica crystals are similar to ice crystals. And so on. I call this the kaleidoscope hypothesis: our experience of the world seems to featureincredible complexity and never-ending novelty, but everything in this sea of complex-ity is similar to everything else. The number of unique atoms of meaning that you needto describe the universe you live in is relatively small, and everything around you is arecombination of these atoms. A few seeds, endless variation—much like what goes oninside a kaleidoscope, where a few glass beads are reflected by a system of mirrors toproduce rich, seemingly ever-changing patterns (see figure 14.8). 455Implementing intelligence: The missing ingredients Generalization power—intelligence—is the ability to mine your experience to identifythese atoms of meaning that can seemingly be reused across many different situations.Once extracted, they’re called abstractions. Whenever you encounter a new situation,you make sense of it via your accumulated collection of abstractions. How do youidentify reusable atoms of meaning? Simply by noticing when two things are similar—by noticing analogies. If something is repeated twice, then both instances must have asingle origin, like in a kaleidoscope. Abstraction is the engine of intelligence, andanalogy-making is the engine that produces abstraction. In short, intelligence is literally sensitivity to abstract analogies, and that’s in fact allthere is to it. If you have a high sensitivity to analogies, you will extract powerfulabstractions from little experience, and you will be able to use these abstractions tooperate in a maximally large area of future experience space. You will be maximallyefficient in converting past experience into the ability to handle future novelty. 14.4.2 The two poles of abstractionIf intelligence is sensitivity to analogies, then developing artificial intelligence shouldstart with spelling out a step-by-step algorithm for analogy-making. Analogy-makingstarts with comparing things to one other. Crucially, there are two distinct ways to comparethings, from which arise two different kinds of abstraction, two modes of thinking,each better suited to a different kind of problem. Together, these two poles of abstrac-tion form the basis for all of our thoughts. The first way to relate things to each other is similarity comparison, which gives riseto value-centric analogies. The second way is exact structural match, which gives rise toprogram-centric analogies (or structure-centric analogies). In both cases, you start frominstances of a thing, and you merge together related instances to produce an abstractionthat captures the common elements of the underlying instances. What varies is how Figure 14.8 A kaleidoscope produces rich (yet repetitive) patterns from just a few beads of colored glass. 456CHAPTER 14Conclusionsyou tell that two instances are related, and how you merge instances into abstractions.Let’s take a close look at each type.VALUE-CENTRIC ANALOGYLet’s say you come across a number of different beetles in your backyard, belonging tomultiple species. You’ll notice similarities between them. Some will be more similar toone another, and some will be less similar: the notion of similarity is implicitly asmooth, continuous distance function t h a t d e f i n e s a l a t e n t m a n i f o l d w h e r e y o u rinstances live. Once you’ve seen enough beetles, you can start clustering more similarinstances together and merging them into a set of prototypes that captures the sharedvisual features of each cluster (see figure 14.9). This prototype is abstract: it doesn’tlook like any specific instance you’ve seen, though it encodes properties that are com-mon across all of them. When you encounter a new beetle, you won’t need to compareit to every single beetle you’ve seen before in order to know what to do with it. You cansimply compare it to your handful of prototypes, so as to find the closest prototype—thebeetle’s category—and use it to make useful predictions: is the beetle likely to bite you?Will it eat your apples? Does this sound familiar? It’s pretty much a description of what unsupervised machinelearning (such as the K-means clustering algorithm) does. In general, all of modernmachine learning, unsupervised or not, works by learning latent manifolds that describea space of instances encoded via prototypes. (Remember the convnet features you visu-alized in chapter 9? They were visual prototypes.) Value-centric analogy is the kind ofanalogy-making that enables deep learning models to perform local generalization. It’s also what many of your own cognitive abilities run on. As a human, you per-form value-centric analogies all the time. It’s the type of abstraction that underliespattern recognition, perception, and intuition. If you can do a task without thinking about it, Instances in the wild Similarity clustering Abstract prototypes Figure 14.9 Value-centric analogy relates instances via a continuous notion of similarity to obtain abstract prototypes. 457Implementing intelligence: The missing ingredientsyou’re relying heavily on value-centric analogies. If you’re watching a movie and youstart subconsciously categorizing the different characters into “types,” that’s value-centric abstraction. PROGRAM-CENTRIC ANALOGYCrucially, there’s more to cognition than the kind of immediate, approximative,intuitive categorization that value-centric analogy enables. There’s another type ofabstraction-generation mechanism that’s slower, exact, deliberate: program-centric(or structure-centric) analogy. In software engineering, you often write different functions or classes that seem tohave a lot in common. When you notice these redundancies, you start asking, “couldthere be a more abstract function that performs the same job, that could be reusedtwice? Could there be an abstract base class that both of my classes could inherit from?”The definition of abstraction you’re using here corresponds to program-centric anal-ogy. You’re not trying to compare your classes and functions by how similar they look,the way you’d compare two human faces, via an implicit distance function. Rather,you’re interested in whether there are parts of them that have exactly the same structure.You’re looking for what is called a subgraph isomorphism (see figure 14.10): programscan be represented as graphs of operators, and you’re trying to find subgraphs (pro-gram subsets) that are exactly shared across your different programs. This kind of analogy-making via exact structural match within different discrete struc-tures isn’t at all exclusive to specialized fields like computer science or mathematics—you’re constantly using it without noticing. It underlies reasoning, planning, and theInstance Instance Shared abstractionInstance Instance Shared ab tr c ion s a tdefcompute_mean(ls):total = 0num_elems = 0foreinls:ifeis not None:total += enum_elems += 1returntotal / num_elemsls = obj.as_list()ls_sum = 0ls_entries = 0forninls:ifnis not None:ls_sum += nls_entries += 1avg = ls sum / ls entriesprint('avg:', avg)my_list = get_data()total = 0num_elems = 0foreinmy_list:ifeis not None:total += enum_elems += 1mean = total / num_elemsupdate_mean(mean) Figure 14.10 Program-centric analogy identifies and isolates isomorphic substructures across different instances 458CHAPTER 14Conclusionsgeneral concept of rigor ( a s o p p o s e d t o i n t u i t i o n ) . A n y t i m e y o u ’ r e t h i n k i n g a b o u tobjects connected to each other by a discrete network of relationships (rather than acontinuous similarity function), you’re leveraging program-centric analogies. COGNITION AS A COMBINATION OF BOTH KINDS OF ABSTRACTIONLet’s compare these two poles of abstraction side by side (see table 14.1). Everything we do, everything we think, is a combination of these two types of abstrac-tion. You’d be hard-pressed to find tasks that only involve one of the two. Even a seem-ingly “pure perception” task, like recognizing objects in a scene, involves a fairamount of implicit reasoning about the relationships between the objects you’re look-ing at. And even a seemingly “pure reasoning” task, like finding the proof of a mathe-matical theorem, involves a good amount of intuition. When a mathematician putstheir pen to the paper, they’ve already got a fuzzy vision of the direction in whichthey’re going. The discrete reasoning steps they take to get to the destination areguided by high-level intuition. T h e s e t w o p o l e s a r e c o m p l e m e n t a r y , a n d i t ’ s t h e i r i n t e r l e a v i n g t h a t e n a b l e sextreme generalization. No mind could be complete without both of them. 14.4.3 The missing half of the pictureBy this point, you should start seeing what’s missing from modern deep learning: it’svery good at encoding value-centric abstraction, but it has basically no ability to generateprogram-centric abstraction. Human-like intelligence is a tight interleaving of bothtypes, so we’re literally missing half of what we need—arguably the most important half. Now, here’s a caveat. So far, I’ve presented each type of abstraction as entirely sep-arate from the other—opposite, even. In practice, however, they’re more of a spec-trum: to an extent, you could do reasoning by embedding discrete programs incontinuous manifolds—just like you may fit a polynomial function through any set ofdiscrete points, as long as you have enough coefficients. And inversely, you could usediscrete programs to emulate continuous distance functions—after all, when you’reTable 14.1 The two poles of abstractionValue-centric abstractionProgram-centric abstractionRelates things by distance Relates things by exact structural matchContinuous, grounded in geometry Discrete, grounded in topologyProduces abstractions by “averaging” instances into “prototypes”Produces abstractions by isolating isomorphic substructures across instancesUnderlies perception and intuition Underlies reasoning and planningImmediate, fuzzy, approximative Slow, exact, rigorousRequires a lot of experience to produce reliable resultsExperience-efficient, can operate on as few as two instances 459The future of deep learningdoing linear algebra on a computer, you’re working with continuous spaces, entirelyvia discrete programs that operate on ones and zeros. However, there are clearly types of problems that are better suited to one or theother. Try to train a deep learning model to sort a list of five numbers, for instance.With the right architecture, it’s not impossible, but it’s an exercise in frustration.You’ll need a massive amount of training data to make it happen—and even then, themodel will still make occasional mistakes when presented with new numbers. And ifyou want to start sorting lists of 10 numbers instead, you’ll need to completely retrainthe model on even more data. Meanwhile, writing a sorting algorithm in Python takesjust a few lines—and the resulting program, once validated on a couple more exam-ples, will work every time on lists of any size. That’s pretty strong generalization: goingfrom a couple of demonstration examples and test examples to a program that cansuccessfully process literally any list of numbers. In reverse, perception problems are a terrible fit for discrete reasoning processes.Try to write a pure-Python program to classify MNIST digits without using anymachine learning technique: you’re in for a ride. You’ll find yourself painstakinglycoding functions that can detect the number of closed loops in a digit, the coordi-nates of the center of mass of a digit, and so on. After thousands of lines of code, youmight achieve . . . 90% test accuracy. In this case, fitting a parametric model is muchsimpler; it can better utilize the large amount of data that’s available, and it achievesmuch more robust results. If you have lots of data and you’re faced with a problemwhere the manifold hypothesis applies, go with deep learning. F o r t h i s r e a s o n , i t ’ s u n l i k e l y t h a t w e ’ l l s e e t h e r i s e o f a n a p p r o a c h t h a t w o u l dreduce reasoning problems to manifold interpolation, or that would reduce percep-tion problems to discrete reasoning. The way forward in AI is to develop a unifiedframework that incorporates both t y p e s o f a b s t r a c t a n a l o g y - m a k i n g . L e t ’ s e x a m i n ewhat that might look like. 14.5 The future of deep learningGiven what we know of how deep nets work, their limitations, and what they’re cur-rently missing, can we predict where things are headed in the medium term? Follow-ing are some purely personal thoughts. Note that I don’t have a crystal ball, so a lot ofwhat I anticipate may fail to become reality. I’m sharing these predictions not becauseI expect them to be proven completely right in the future, but because they’re inter-esting and actionable in the present. At a high level, these are the main directions in which I see promise:Models closer to general-purpose computer programs, built on top of far richer primitivesthan the current differentiable layers. This is how we’ll get to reasoning andabstraction, the lack of which is the fundamental weakness of current models.A fusion between deep learning and discrete search over program spaces, with the formerproviding perception and intuition capabilities, and the latter providing reason-ing and planning capabilities. 460CHAPTER 14ConclusionsGreater, systematic reuse of previously learned features and architectures, such as meta-learning systems using reusable and modular program subroutines.Additionally, note that these considerations aren’t specific to the sort of supervisedlearning that has been the bread and butter of deep learning so far—rather, they’reapplicable to any form of machine learning, including unsupervised, self-supervised,and reinforcement learning. It isn’t fundamentally important where your labels comefrom or what your training loop looks like; these different branches of machine learn-ing are different facets of the same construct. Let’s dive in.14.5.1 Models as programsAs noted in the previous section, a necessary transformational development that wecan expect in the field of machine learning is a move away from models that performpurely pattern recognition and can only achieve local generalization, toward models capa-ble of abstraction and reasoning that can achieve extreme generalization. Current AI pro-grams that are capable of basic forms of reasoning are all hardcoded by humanprogrammers: for instance, software that relies on search algorithms, graph manipula-tion, and formal logic. That may be about to change, thanks to program synthesis—a field that is very nichetoday, but I expect to take off in a big way over the next few decades. Program synthesisconsists of automatically generating simple programs by using a search algorithm (possi-bly genetic search, as in genetic programming) to explore a large space of possible pro-grams (see figure 14.11). The search stops when a program is found that matches therequired specifications, often provided as a set of input-output pairs. This is highly rem-iniscent of machine learning: given training data provided as input-output pairs, we finda program that matches inputs to outputs and can generalize to new inputs. The differ-ence is that instead of learning parameter values in a hardcoded program (a neural net-work), we generate source code via a discrete search process (see table 14.2).Input:[3, 5, 1, 2, 7]Output:[1, 2, 3, 5, 7]Input:[8, 5, 2, 9, 1, 13]Output:[1, 2, 5, 8, 9, 13]+=i f= =fo - else r*+ =- * = =Sc s e a r h p r o c e sVocabulary ofbuilding blocksProgramspecification CandidateprogramsFigure 14.11 A schematic view of program synthesis: given a program specification and a set of building blocks, a search process assembles the building blocks into candidate programs, which are then tested against the specification. The search continues until a valid program is found. 461The future of deep learning Program synthesis is how we’re going to add program-centric abstraction capabilitiesto our AI systems. It’s the missing piece of the puzzle. I mentioned earlier that deeplearning techniques were entirely unusable on ARC, a reasoning-focused intelligencetest. Meanwhile, very crude program-synthesis approaches are already producing verypromising results on this benchmark. 14.5.2 Blending together deep learning and program synthesisOf course, deep learning isn’t going anywhere. Program synthesis isn’t its replace-ment; it is its complement. It’s the hemisphere that has been so far missing from ourartificial brains. We’re going to be leveraging both, in combination. There are twomajor ways this will take place:1Developing systems that integrate both deep learning modules and discretealgorithmic modules2Using deep learning to make the program search process itself more efficientLet’s review each of these possible avenues.INTEGRATING DEEP LEARNING MODULES AND ALGORITHMIC MODULES INTO HYBRID SYSTEMSToday, the most powerful AI systems are hybrid: they leverage both deep learningmodels and handcrafted symbol-manipulation programs. In DeepMind’s AlphaGo,for example, most of the intelligence on display is designed and hardcoded by humanprogrammers (such as Monte Carlo Tree Search). Learning from data happens onlyin specialized submodules (value networks and policy networks). Or consider autono-mous vehicles: a self-driving car is able to handle a large variety of situations because itmaintains a model of the world around it—a literal 3D model—full of assumptionshardcoded by human engineers. This model is constantly updated via deep learningperception modules that interface it with the surroundings of the car. For both of these systems—AlphaGo and self-driving vehicles—the combination ofhuman-created discrete programs and learned continuous models is what unlocks alevel of performance that would be impossible with either approach in isolation, suchas an end-to-end deep net or a piece of software without ML elements. So far, the dis-crete algorithmic elements of such hybrid systems are painstakingly hardcoded byhuman engineers. But in the future, such systems may be fully learned, with nohuman involvement.Table 14.2 Machine learning vs. program synthesisMachine learningProgram synthesisModel: differentiable parametric function Model: graph of operators from a programming languageEngine: gradient descent Engine: discrete search (such as genetic search)Requires a lot of data to produce reliable results Data-efficient, can work with a couple of training examples 462CHAPTER 14Conclusions What will this look like? Consider a well-known type of network: RNNs. It’s import-ant to note that RNNs have slightly fewer limitations than feedforward networks.That’s because RNNs are a bit more than mere geometric transformations: they’regeometric transformations repeatedly applied inside a for loop. The temporal for loop isitself hardcoded by human developers: it’s a built-in assumption of the network. Nat-urally, RNNs are still extremely limited in what they can represent, primarilybecause each step they perform is a differentiable geometric transformation, andthey carry information from step to step via points in a continuous geometric space(state vectors). Now imagine a neural network that’s augmented in a similar waywith programming primitives, but instead of a single hardcoded for loop with hard-coded continuous-space memory, the network includes a large set of programmingprimitives that the model is free to manipulate to expand its processing function, suchas if branches, while statements, variable creation, disk storage for long-term mem-ory, sorting operators, advanced data structures (such as lists, graphs, and hashtables), and many more. The space of programs that such a network could representwould be far broader than what can be represented with current deep learning mod-els, and some of these programs could achieve superior generalization power. Impor-tantly, such programs will not be differentiable end-to-end, though specific moduleswill remain differentiable and thus will need to be generated via a combination of dis-crete program search and gradient descent. We’ll move away from having, on one hand, hardcoded algorithmic intelligence(handcrafted software) and, on the other hand, learned geometric intelligence (deeplearning). Instead, we’ll have a blend of formal algorithmic modules that provide rea-soning and abstraction capabilities, and geometric modules that provide informalintuition and pattern-recognition capabilities (see figure 14.12). The entire systemwill be learned with little or no human involvement. This should dramatically expandthe scope of problems that can be solved with machine learning—the space of pro-grams that we can generate automatically, given appropriate training data. Systemslike AlphaGo—or even RNNs—can be seen as a prehistoric ancestor of such hybridalgorithmic-geometric models. Modular task-level programlearned on the ﬂy to solvea speciﬁc taskData andfeedbackActionsGeometricsubroutineAlgorithmicsubroutineGeometricsubroutineAlgorithmicsubroutineTask #002456Figure 14.12 A learned program relying on both geometric primitives (pattern recognition, intuition) and algorithmic primitives (reasoning, search, memory) 463The future of deep learningUSING DEEP LEARNING TO GUIDE PROGRAM SEARCHToday, program synthesis faces a major obstacle: it’s tremendously inefficient. To cari-cature, program synthesis works by trying every possible program in a search spaceuntil it finds one that matches the specification provided. As the complexity of theprogram specification increases, or as the vocabulary of primitives used to write pro-grams expands, the program search process runs into what’s known as combinatorialexplosion, where the set of possible programs to consider grows very fast; in fact, muchfaster than merely exponentially fast. As a result, today, program synthesis can only beused to generate very short programs. You’re not going to be generating a new OS foryour computer anytime soon. T o m o v e f o r w a r d , w e ’ r e g o i n g t o n e e d t o m a k e p r o g r a m s y n t h e s i s e f f i c i e n t b ybringing it closer to the way humans write software. When you open your editor tocode up a script, you’re not thinking about every possible program you could poten-tially write. You only have in mind a handful of possible approaches: you can use yourunderstanding of the problem and your past experience to drastically cut through thespace of possible options to consider. D e e p l ea r n i n g c a n h e l p p r o g r a m s y n t h e s i s d o t h e s a m e : a l t h o u gh ea c h s pe ci fi cprogram we’d like to generate might be a fundamentally discrete object that performsnon-interpolative data manipulation, evidence so far indicates that the space of all usefulprograms may look a lot like a continuous manifold. That means that a deep learningmodel that has been trained on millions of successful program-generation episodesmight start to develop solid intuition a b o u t t h e path through program space t h a t t h esearch process should take to go from a specification to the corresponding program—just like a software engineer might have immediate intuition about the overall archi-tecture of the script they’re about to write, about the intermediate functions andclasses they should use as stepping stones on the way to the goal. Remember that human reasoning is heavily guided by value-centric abstraction,that is to say, by pattern recognition and intuition. Program synthesis should be too. Iexpect the general approach of guiding program search via learned heuristics to seeincreasing research interest over the next ten to twenty years. 14.5.3 Lifelong learning and modular subroutine reuseIf models become more complex and are built on top of richer algorithmic primitives,this increased complexity will require higher reuse between tasks, rather than training anew model from scratch every time we have a new task or a new dataset. Many datasetsdon’t contain enough information for us to develop a new, complex model from scratch,and it will be necessary to use information from previously encountered datasets (muchas you don’t learn English from scratch every time you open a new book—that would beimpossible). Training models from scratch on every new task is also inefficient due to thelarge overlap between the current tasks and previously encountered tasks. A remarkable observation has been made repeatedly in recent years: training thesame model to do several loosely connected tasks at the same time results in a model 464CHAPTER 14Conclusionsthat’s better at each task. For instance, training the same neural machine-translationmodel to perform both English-to-German translation and French-to-Italian transla-tion will result in a model that’s better at each language pair. Similarly, training animage-classification model jointly with an image-segmentation model, sharing thesame convolutional base, results in a model that’s better at both tasks. This is fairlyintuitive: there’s always some i n f o r m a t i o n o v e r l a p b e t w e e n s e e m i n g l y d i s c o n n e c t e dtasks, and a joint model has access to a greater amount of information about eachindividual task than a model trained on that specific task only. Currently, when it comes to model reuse across tasks, we use pretrained weights formodels that perform common functions, such as visual feature extraction. You sawthis in action in chapter 9. In the future, I expect a generalized version of this to becommonplace: we’ll use not only previously learned features (submodel weights) butalso model architectures and training procedures. As models become more like pro-grams, we’ll begin to reuse program subroutines like the functions and classes found inhuman programming languages. Think of the process of software development today: once an engineer solves a spe-cific problem (HTTP queries in Python, for instance), they package it as an abstract,reusable library. Engineers who face a similar problem in the future will be able tosearch for existing libraries, download one, and use it in their own project. In a similarway, in the future, meta-learning systems will be able to assemble new programs by sift-ing through a global library of high-level reusable blocks. When the system finds itselfdeveloping similar program subroutines for several different tasks, it can come upwith an abstract, reusable version of the subroutine and store it in the global library(see figure 14.13). These subroutines can be either geometric (deep learning modules Modular task-level programlearned on the ﬂy to solvea speciﬁc taskData andfeedbackActionsGeometricsubroutineAlgorithmicsubroutineGeometricsubroutineAlgorithmicsubroutineGlobal library ofabstract subroutinesGeometricsubroutineAlgorithmicsubroutineAlgorithmicsubroutineGeometricsubroutineAlgorithmicsubroutineAlgorithmicsubroutineGeometricsubroutineAlgorithmicsubroutineAlgorithmicsubroutinePerpetual meta-learnercapable of quickly growinga task-level modelacross a variety of tasksPushreusablesubroutinesData andfeedbackDesignchoicesFetchrelevantsubroutines Task #002456Task #002455Task #002454Task #002453 Figure 14.13 A meta-learner capable of quickly developing task-specific models using reusable primitives (both algorithmic and geometric), thus achieving extreme generalization 465The future of deep learningwith pretrained representations) or algorithmic (closer to the libraries that contem-porary software engineers manipulate). 14.5.4 The long-term visionIn short, here’s my long-term vision for machine learning:Models will be more like programs and will have capabilities that go far beyondthe continuous geometric transformations of the input data we currently workwith. These programs will arguably be much closer to the abstract mental mod-els that humans maintain about their surroundings and themselves, and theywill be capable of stronger generalization due to their rich algorithmic nature.In particular, models will blend algorithmic modules providing formal reasoning,search, and abstraction capabilities with geometric modules p r o v i d i n g i n f o r m a lintuition and pattern-recognition capabilities. This will achieve a blend ofvalue-centric and program-centric abstraction. AlphaGo or self-driving cars (sys-tems that required a lot of manual software engineering and human-madedesign decisions) provide an early example of what such a blend of symbolicand geometric AI could look like.Such models will be grown automatically rather than hardcoded by human engi-neers, using modular parts stored in a global library of reusable subroutines—alibrary evolved by learning high-performing models on thousands of previoustasks and datasets. As frequent problem-solving patterns are identified by themeta-learning system, they will be turned into reusable subroutines—much likefunctions and classes in software engineering—and added to the global library.The process that searches over possible combinations of subroutines to grownew models will be a discrete search process (program synthesis), but it will beheavily guided by a form of program-space intuition provided by deep learning.This global subroutine library and associated model-growing system will be ableto achieve some form of human-like extreme generalization: given a new task or sit-uation, the system will be able to assemble a new working model appropriatefor the task using very little data, thanks to rich program-like primitives thatgeneralize well and extensive experience with similar tasks. In the same way,humans can quickly learn to play a complex new video game if they have expe-rience with many previous games, because the models derived from this previ-ous experience are abstract and program-like, rather than a basic mappingbetween stimuli and action.As such, this perpetually learning model-growing system can be interpreted asan artificial general intelligence (AGI). But don’t expect any singularitarian robotapocalypse to ensue: that’s pure fantasy, coming from a long series of profoundmisunderstandings of both intelligence and technology. Such a critique, how-ever, doesn’t belong in this book. 466CHAPTER 14Conclusions14.6 Staying up to date in a fast-moving fieldAs final parting words, I want to give you some pointers about how to keep learningand updating your knowledge and skills after you’ve turned the last page of this book.The field of modern deep learning, as we know it today, is only a few years old, despitea long, slow prehistory stretching back decades. With an exponential increase infinancial resources and research headcount since 2013, the field as a whole is nowmoving at a frenetic pace. What you’ve learned in this book won’t stay relevant for-ever, and it isn’t all you’ll need for the rest of your career. Fortunately, there are plenty of free online resources that you can use to stay up todate and expand your horizons. Here are a few.14.6.1 Practice on real-world problems using KaggleAn effective way to acquire real-world experience is to try your hand at machine learningcompetitions on Kaggle (https:/ / kaggle.com). The only real way to learn is through prac-tice and actual coding—that’s the philosophy of this book, and Kaggle competitions arethe natural continuation of this. On Kaggle, you’ll find an array of constantly reneweddata science competitions, many of which involve deep learning, prepared by companiesinterested in obtaining novel solutions to some of their most challenging machine learn-ing problems. Fairly large monetary prizes are offered to top entrants. Most competitions are won using either the XGBoost library (for shallow machinelearning) or Keras (for deep learning). So you’ll fit right in! By participating in a fewcompetitions, maybe as part of a team, you’ll become more familiar with the practicalside of some of the advanced best practices described in this book, especially hyperpa-rameter tuning, avoiding validation-set overfitting, and model ensembling. 14.6.2 Read about the latest developments on arXivDeep learning research, in contrast with some other scientific fields, takes places com-pletely in the open. Papers are made publicly and freely accessible as soon as they’refinalized, and a lot of related software is open source. arXiv (https:/ /arxiv.org)—pro-nounced “archive” (the X stands for the Greek chi)—is an open-access preprint serverfor physics, mathematics, and computer science research papers. It has become thede facto way to stay up to date on the bleeding edge of machine learning and deeplearning. The large majority of deep learning researchers upload any paper they writeto arXiv shortly after completion. This allows them to plant a flag and claim a specificfinding without waiting for a conference acceptance (which takes months), which isnecessary given the fast pace of research and the intense competition in the field. Italso allows the field to move extremely fast: all new findings are immediately availablefor all to see and to build on. An important downside is that the sheer quantity of new papers posted every dayon arXiv makes it impossible to even skim them all, and the fact that they aren’tpeer-reviewed makes it difficult to identify those that are both important and highquality. It’s challenging, and becoming increasingly more so, to find the signal in 467Final wordsthe noise. But some tools can help: in particular, you can use Google Scholar(https:/ / scholar.google.com) to keep track of publications by your favorite authors. 14.6.3 Explore the Keras ecosystemWith over one million users as of late 2021 and still growing, Keras has a large ecosys-tem of tutorials, guides, and related open source projects:Your main reference for working with Keras is the online documentation athttps:/ / keras.io. In particular, you’ll find extensive developer guides athttps:/ / keras.io/guides, and you’ll find dozens of high-quality Keras code exam-ples at https:/ /keras.io/examples. Make sure to check them out!The Keras source code can be found at https:/ /github.com/keras-team/keras.You can ask for help and join deep learning discussions on the Keras mailinglist: keras-users@googlegroups.com.You can follow me on Twitter: @fchollet. Final wordsThis is the end of Deep Learning with Python, second edition. I hope you’ve learned athing or two about machine learning, deep learning, Keras, and maybe even cognitionin general. Learning is a lifelong journey, especially in the field of AI, where we have farmore unknowns on our hands than certitudes. So please go on learning, question-ing, and researching. Never stop! Because even given the progress made so far, mostof the fundamental questions in AI remain unanswered. Many haven’t even beenproperly asked yet. 469indexSymbols@tf.function decorator198, 270Aablation studies251abstraction, poles of455–458cognition as combination of both kinds of abstraction458program-centric analogy457–458value-centric analogy456–457activation function63adapt() method of preprocess-ing layers317add() method of Sequential class174adversarial examples443affine transform46AGI (artificial general intelligence)465AI (artificial intelligence)1, 432greater generality in450–454new target for 452–454shortcut rule 450–452overview of2–3promise of12–13various approaches to432See also deep learningAI summer433AI winter12algorithmic modules465Analytical Engine3annotations96anomaly detection281anthropomorphizing machine-learning models443–445ARC (Abstraction & Reasoning Corpus) dataset453arrow of time137arXiv466–467assign() method of Variable class77automated hyperparameter tun-ing software165automatic differentiation60automatic shape inference85–87automatons445AutoML (automated machine learning)419–420average presence211BBackpropagation algorithm10, 56backward pass57bag-of-2-grams/bag-of-3-grams314bag-of-words models313batch() method of Dataset class218batch axis35batch dimension35batches96batch gradient descent53batch normalization255–257, 438BatchNormalization layer194, 256, 344batch training82BayesianOptimization tuner415bias vector256Bidirectional layer306bidirectional recurrent layers300bidirectional RNNs304–307bigramswith binary encoding324–325with TF-IDF encoding325–327binary classification97–106, 437building model99–102generating predictions on new data105IMDB dataset97preparing data98–99validating approach102–105binary_crossentropy102, 437binary encodingbigrams with324–325single words (unigrams) with322–324BLAS (Basic Linear Algebra Subprograms)39Boston Housing Price dataset114broadcasting40–41broad generalization449build() method of Layer class84, 174, 415 INDEX470Ccache() method of Dataset class429__call__() method of Layer class63, 86call() method of Layer class84, 182, 358callbacks187–189EarlyStopping and Model-Checkpoint callbacks188–189text generation372–375writing189–190CAM (class activation map) visualization273categorical_crossentropy108, 437categorical encoding107categories27causal padding360CelebA dataset404–405chain rule55–56channels-first convention37channels-last convention37character-level tokenization313chatbots350classes27, 96classification281binary classification97–106building model 99–102generating predictions on new data 105preparing data 98–99validating approach102–105multiclass classification106–113building model 108–109generating predictions on new data 111–112handling labels and loss 112preparing data 107–108sufficiently large intermedi-ate layers 112–113validating approach109–111cognitive automation450Colaboratory73–75first steps with73installing packages with pip74using GPU runtime75columns32combinatorial explosion463compilation step29compile() method of Model class89, 185, 408, 430computation graphs56–60, 197compute_dtype attribute of Layer class425compute_mask() method of Layer class333computer visionconvnets202–211convolution operation 204–209interpreting what convnets learn 261–279max-pooling operation 209–211modern architecture patterns 248–261training from scratch on small datasets 211–224essential tasks238–240image segmentation example240–248pretrained models224–237feature extraction 225–233fine-tuning 234–237concept drift158concept vectors, for image editing393Constant initializer335content function384Conv1D (1D convolution) layer84, 291, 438Conv2D (2D convolution) layer84, 202, 245, 290, 438Conv3D (3D convolution) layer291, 438convnets202–211, 438convolution operation204–209border effects and padding 207–208convolution strides208–209interpreting what convnets learn261–279visualizing convnet filters 268–272visualizing heatmaps of class activation 273–279visualizing intermediate activations 262–268max-pooling operation209–211modern architecture patterns248–261batch normalization255–257depthwise separable convolutions 257–259mini Xception-like model 259–261modularity, hierarchy, and reuse 249–251residual connections251–255training from scratch on small datasets211–224data augmentation221–224data preprocessing217–221downloading data 212–215model building 215–217relevance of deep learning for small-data problems 212convolutional base225convolutional networks201, 436convolution kernel206convolution operation204–209border effects and padding207–208convolution strides208–209correlations385cost function9cuDNN kernel258, 302Ddata-annotation infrastructure157–158data augmentation211feature extraction with231–233feature extraction without229–231training convnets from scratch on small datasets221–224data distillation28data parallelism425data preparationclassificationbinary classification 98–99multiclass classification 107–108model development161–162 INDEX471data preparation (continued)natural language processing311–319text splitting (tokenization) 313–314text standardization312–313TextVectorization layer 316–319vocabulary indexing314–315regression114–115text generation370–371timeseries285–287data representativeness137Dataset class92, 218, 285, 317dataset curation142data type33DCGAN (deep convolutional GAN)402decision trees15–16decoders350, 399DeepDream376–383define task154Dense layer28, 63, 76, 99, 203, 289, 355, 437densely connected layers84densely connected networks437–438dense representations330dense sampling142, 448deploy model154depth axis (channels axis)205depthwise separable convolution layer438depthwise separable convolutions257–259derivativeschaining55–61automatic differentiation with computation graphs 56–60chain rule 55–56gradient tape in TensorFlow 60–61defined49–50of tensor operations51–52develop model154dimensionality31dimensions31discriminator model405–406discriminator network401distance function384, 456dot operation55dot product41dropoutrecurrent, to fight overfitting300regularizing models150–152Dropout layer151, 194, 223dtype attribute of Layer class33Eeager execution78early stopping144EarlyStopping callback144, 188–189, 417efficiency ratio452element-wise operations38–40Embedding layer331–332encoder350evaluate() method of Model class92, 183event detection281exact structural match455expert systems3extreme generalization130, 447Ffailure modes166feature engineering17, 143–144feature extraction181, 225–233with data augmentation231–233without data augmentation229–231feature extractor model268feature maps205feature normalization194features axis35feature selection126feedforward networks293filters206fit() method of Model class29, 62, 76, 183, 218, 424core Keras APIs91leveraging with custom train-ing loops198–200Flatten layer203, 438floating-point precision422–424forecasting281for loops183, 218, 302, 415, 462forward pass57Fourier transform281freezing layers231fully connected layers84Functional API173access to layer connectivity180–181example of176–177multi-input, multi-output modelsoverview of 178–179training 179Ggan model403GANs (generative adversarial networks)391, 401–411adversarial network408–410CelebA dataset404–405discriminator model405–406generator model407implementation tricks403–404schematic implementation402–403generalization122, 142–152, 446dataset curation142early stopping144feature engineering143–144local vs. extreme446–448nature of in deep learning127–133interpolation 129–130manifold hypothesis128–129training data 132–133why deep learning works 130–132regularizing models145–152adding dropout 150–152adding weight regularization 148–149reducing network size145–148spectrum of449–450underfitting and overfitting122–127ambiguous features 124noisy training data 123rare features and spurious correlations 125–127generative deep learningDeepDream376–383generative adversarial networks401–411adversarial network 408–410bag of tricks 403–404 INDEX472generative deep learning (continued)CelebA dataset 404–405discriminator 405–406generator 407schematic GAN implementation402–403image generation with varia-tional autoencoders391–401concept vectors for image editing 393overview of 393–395sampling from latent spaces of images 391–392with Keras 396–401neural style transfer383–391content loss 384style loss 384–385with Keras 385–391text generation366–376callback with variable-temperature sampling372–375generating sequence data 367history of 366–367sampling strategy 368–369with Keras 369–372generative learning193generator model407generator networks401, 403geometric interpretationof deep learning47–48of tensor operations44–47geometric modules465geometric relationships329get_config() method of Layer class344get_vocabulary() method of TextVectorization layer317Google Colab428–429GPUsGPU runtime75multi-GPU training425–427acquiring two or more GPUs 425–426single-host, multi-device synchronous training426–427with mixed precision422–425floating-point precision 422–424mixed-precision training 425gradient-based optimization48–61Backpropagation algorithm55–61automatic differentiation with computation graphs 56–60chain rule 55–56gradient tape in TensorFlow 60–61derivativesdefined 49–50of tensor operations 51–52stochastic gradient descent52–55gradient boosting machines15–16gradient descent49, 268, 432gradient descent parameters138–139gradient propagation22GradientTape class60–61, 65, 76, 78–79, 196, 268GradientTape.gradient() method79GradientTape.watch() method79Gram matrix384greedy sampling368ground-truth96grouped convolutions342GRU (Gated Recurrent Unit)303GRU layer297, 439Hhierarchical representations learning7History class91hold-out validation134–135house price prediction example113–120Boston Housing Price dataset114building model115generating predictions on new data119preparing data114–115validating approach using K-fold validation115–119HSV (hue-saturation-value) format5Hyperband tuner415HyperModel class415hyperparameter optimization413–420automated machine learning419–420crafting the right search space418–419KerasTuner414–418hypothesis space7, 87, 101, 248, 436Iif branches462if statements156image classification239image data37image_dataset_from_directory utility function217, 321image segmentation239–248IMDB dataset97, 320–322import tensorflow426inferencecore Keras APIs93training vs.194information arbitrage308information-distillation8information distillation pipeline267information leaks134information location396information shortcut252initial state355Input class175instance segmentation240interpolation129–130iterated K-fold validation with shuffling136JJavaScript library168Jupyter notebooks72–73KKaggle466kaggle package212kaleidoscope hypothesis454Kerascore APIs84–93configuring learning process 88–90inference 93layers 84–87loss functions 90 INDEX473Keras (continued)models 87–88monitoring loss and met-rics on validation data 91–92DeepDream with377–382deep-learning workspaces71–75Colaboratory 73–75Jupyter notebooks 72–73exploring ecosystem467history of71image generation with varia-tional autoencoders396–401model building APIs173–185choosing 185Functional API 176–181mixing and matching differ-ent components 184model subclassing 182–184Sequential model 174–176neural style transfer with385–391overview of69–71recurrent layers in296–300text generation with369–372preparing data 370–371Transformer-based sequence-to-sequence model 371–372training and evaluation loopsbuilt-in 185–192callbacks 187–189complete 195–197fit() method 198–200low-level usage of metrics 195monitoring and visualiza-tion with TensorBoard 190–192tf.function 197–198training vs. inference 194writing 192–200writing metrics 186–187workflows173keras.applications module226keras.applications.xception.pre-process_input function274keras.callbacks.Callback class189keras.callbacks module188keras.callbacks.TensorBoard callback192keras.datasets154keras.datasets.imdb315keras.metrics.Mean metric195keras.metrics.Metric class186keras.metrics module186KerasTuner414–418kernel function15kernel methods14–15kernel trick15key attributes32–34K-fold validationiterated with shuffling136model evaluation135–136regression115–119LL1 regularization148L2 regularization148labels27, 96language modeling367latent space367Layer class84–85, 87, 173layer compatibility85layered representations learning7LayerNormalization layer344layers28, 84–87, 432access to layer connectivity180–181automatic shape inference85–87base Layer class84–85Embedding layer331–332recurrent layers in Keras296–300stacking recurrent layers303–304sufficiently large intermediate layers112–113TextVectorization layer316–319Layer subclass183, 358LeakyReLU layer404learning5, 133learning rate53LeNet14lifelong learning463–465LightGBM library19linear classifiers79–84linear transformations46, 101local generalization130, 447local slope51logistic regression13, 113loss functions9, 29, 88, 90, 432loss value96LSTM (Long Short Term Mem-ory) algorithm20, 292LSTM layer84, 297, 439Mmachine learning13–20, 432automated419–420decision trees15–16generalization121–133, 142–152dataset curation 142early stopping 144feature engineering143–144nature of in deep learning127–133regularizing models145–152underfitting and overfitting 122–127gradient boosting machines15–16improving model fit138–141increasing model capacity140–141leveraging better architec-ture priors 139–140tuning key gradient descent parameters 138–139kernel methods14–15learning rules and representa-tions from data4–7model evaluation133–137common-sense baseline136–137training, validation, and test sets 133–136modern landscape of18–20neural networks14, 16–17overview of3–4probabilistic modeling13–14random forests15–16See also deep learningmachine learning workflowmodel deployment165–170deploying model as REST API 166–167deploying model in browsers 168deploying model on devices 167–168explaining work to stake-holders and setting expectations 165–166 INDEX474machine learning workflow (continued)maintaining model 170monitoring model 169–170optimization 169shipping inference model 166–169model development161–165beating baselines 163–164choosing evaluation protocol 162–163developing model that overfits 164preparing data 161–162regularizing and tuning model 165task definition155–160choosing measure of success 160collecting dataset 156–159framing problem 155–156understanding data 160universal workflow435–436machine translation350MAE (mean absolute error)115, 288manifold hypothesis128–129manifolds47map() method of Dataset class218–219masking332–334mask token315matmul operation76matrices32maximal presence211maximizing margin15MaxPooling2D layer202, 245max-pooling operation209–211meaningfully transform data5mean squared error115metrics89low-level usage of195monitoring loss and metrics on validation data91–92writing186–187metrics property198MHR (modularity-hierarchy-reuse) formula249mini-batches96mini-batch SGD (mini-batch sto-chastic gradient descent)53MirroredStrategy object426missing values162Model() constructor179model architecture419ModelCheckpoint callback188–189, 219Model class87, 398model ensembling420–421model evaluation133–137common-sense baseline136–137Keras training and evaluation loopsbuilt-in 185–192callbacks 187–189complete 195–197fit() method 198–200low-level usage of metrics 195monitoring and visualiza-tion with TensorBoard 190–192tf.function 197–198training vs. inference 194writing 192–200writing metrics 186–187training, validation, and test sets133–136hold-out validation 134–135iterated K-fold validation with shuffling 136K-fold validation 135–136Model.fit() method102, 187Model.layers property181model parallelism425Model.save() method189Model subclass182, 408model subclassing182–184example of182–183responsibility for model logic183–184Model.summary() method244, 296, 378model training421–430GPU with mixed precision422–425floating-point precision 422–424mixed-precision training 425importance of training data132–133Keras training and evaluation loopsbuilt-in 185–192callbacks 187–189complete 195–197fit() method 198–200low-level usage of metrics 195monitoring and visualiza-tion with TensorBoard 190–192tf.function 197–198training vs. inference 194writing 192–200writing metrics 186–187multi-GPU training425–427acquiring two or more GPUs 425–426single-host, multi-device synchronous training 426–427multi-input, multi-output models179noisy training data123pretrained models for com-puter vision224–237feature extraction 225–233fine-tuning 234–237TPU training428–430Google Colab 428–429step fusing 430training convnets from scratch on small datasets211–224data augmentation221–224data preprocessing217–221downloading data 212–215model building 215–217relevance of deep learning for small-data problems 212modular subroutine reuse463–465momentum54monitoringloss and metrics on validation data91–92model deployment169–170with TensorBoard190–192movie review classification example97–106building model99–102generating predictions on new data105IMDB dataset97preparing data98–99validating approach102–105MSE (mean squared error)119, 289, 438 INDEX475mse function105multiclass classification96, 106–113building model108–109generating predictions on new data111–112handling labels and loss112preparing data107–108sufficiently large intermediate layers112–113validating approach109–111multi-head attention341–342MultiHeadAttention layer339multi-hot encode98multi-input, multi-output mod-elsoverview of178–179training179multilabel categorical classification437multilabel classification96multilabel multiclass classification106NNaiveDense class63, 84NaiveDense layer86NaiveSequential class63ndim attribute31, 33neural networks7, 16–17, 434early14example of27–30, 61–66evaluating model 66full training loop 65–66reimplementing 63–64running one training step 64–65gradient-based optimization48–61Backpropagation algorithm 55–61derivatives 49–52stochastic gradient descent 52–55tensor operations38–48broadcasting 40–41element-wise operations 38–40geometric interpretation of 44–47geometric interpretation of deep learning 47–48tensor product 41–43tensor reshaping 43–44tensors31–38data batches 35higher-rank tensors 32image data 37key attributes 32–34manipulating in NumPy 34matrices 32real-world examples of 35scalars 31timeseries data 36vector data 35–36vectors 31video data 37–38neural style transfer383–391content loss384style loss384–385with Keras385–391newswire classification example106–113building model108–109generating predictions on new data111–112handling labels and loss112preparing data107–108Reuters dataset106–107sufficiently large intermediate layers112–113validating approach109–111N-gram tokenization313NLP (natural language processing)310IMDB dataset320–322overview of309–311preparing data311–319text splitting (tokenization)313–314text standardization312–313TextVectorization layer 316–319vocabulary indexing314–315sequences327–336example of 328–329padding and masking332–334when to use 349–350word embeddings 329–332, 334–336sequence-to-sequence learning350–363machine translation example 351–354with RNNs 354–358with Transformer 358–363sets322–327bigrams with binary encoding 324–325bigrams with TF-IDF encoding 325–327single words (unigrams) with binary encoding322–324when to use 349–350Transformer architecture336–350multi-head attention341–342self-attention 337–341Transformer encoder342–348non-representative data158–159non-trainable weights194notebooks72number of axes32NumPy, manipulating tensors in34numpy.dot() function41NumPy library32Oobject detection239objective function9Occam’s razor principle148on_batch_* methods of Callback class190one-hot encoding107on_epoch_* methods of Call-back class190optimization50, 54, 122optimizer10, 29, 89output feature map205overfitting30, 104, 122–127ambiguous features124noisy training data123rare features and spurious correlations125–127using recurrent dropout to fight300Ppaddingconvolution operation207–208sequences332–334pattern recognition456pip, installing packages with74plot_model() utility180 INDEX476PositionalEmbedding layer361, 440positional encoding344, 346–348, 439predict() method of Model class93, 111, 119, 183, 229, 269–270prediction96prefetch() method of Dataset class218pretrained word embeddings331probabilistic modeling13–14probability distribution108program-centric analogies455program-space intuition465program subroutines464program synthesis460–463integrating deep-learning modules and algorithmic modules into hybrid systems461–462using deep learning to guide program search463progressive disclosure of complexity173prototypes456Qquery-key-value model340–341Rrandom forests15–16randomized A/B testing169RandomSearch tuner415reconstruction loss395recurrent dropout300recurrent layer84recurrent networks436redundancy in data137regression113–120, 438building model115generating predictions on new data119preparing data114–115validating approach115–119regularization133regularization loss395regularizing models145–152adding dropout150–152adding weight regularization148–149reducing network size145–148reinforcement learning193ReLU activation99, 404relu function100representativity163Rescaling layer215reset_state() method of Metric class187reset_state() method of Model class195residual connections251–255, 298, 438response map206result() method of Metric class187Reuters dataset106–107RGB (red-green-blue) format5rmsprop optimizer62RNNs (recurrent neural networks)281, 293–300, 439bidirectional304–307recurrent layers in Keras296–300stacking recurrent layers303–304using recurrent dropout to fight overfitting300ROC AUC (receiver operating characteristic curve)160rows32rules4Ssamples27, 96samples axis35scalar regression96scalars31scales379scaling46scaling-up model training421–430GPU with mixed precision422–425multi-GPU training425–427TPU training428–430Scikit-learn library19search space414search_space_summary() method of Tuner class416second-order gradients79segmentation masks240self-attention337–341self-supervised learning193, 398semantic relationships329semantic segmentation240SeparableConv1D layer291SeparableConv2D layer259, 290, 438sequence data35sequence models320sequences327–336example of328–329padding and masking332–334when to use349–350word embeddings329–331learning with Embedding layer 331–332pretrained 334–336sequence-to-sequence learning439sequence-to-sequence model340, 350–363, 371machine translation example351–354text generation371–372with RNNs354–358with Transformer358–363Sequential model63–64, 76, 173–176, 221sets322–327bigrams with binary encoding324–325bigrams with TF-IDF encoding325–327single words (unigrams) with binary encoding322–324when to use349–350SGD optimizer389shallow learning7shortcut rule450–452shuffle() method of Dataset class218sigmoid activation106, 215, 437similarity comparison455simple model148SimpleRNN layer296, 439single-label categorical classification437single-label multiclass classification106single words (unigrams)322–324smile vector393softmax activation108, 437softmax classification layer28softmax operation55softmax temperature parameter368 INDEX477sparse_categorical_crossentropy function112, 203spatial location245stacking recurrent layers300step fusing430stochastic gradient descent52–55stochastic sampling368structured representations330style function384subgraph isomorphism457summary() method of Model class174supervised learning192SVM (Support Vector Machine)14symbolic AI3symbolic tensor177Ttape.gradient() function79tape.watch() function79target leaking160targets96, 192targets array286temperature-forecasting example281–2931D convolutional model290–291basic machine-learning model289–290first recurrent baseline292–293non-machine-learning baseline288preparing data285–287TensorBoard, monitoring and visualization with190–192TensorFlowdeep-learning workspaces71–75Colaboratory 73–75Jupyter notebooks 72–73first steps with75–84constant tensors and variables 76–78GradientTape API 78–79linear classifier in TensorFlow 79–84tensor operations 78gradient tape in60–61history of71overview of69TensorFlow Serving166tensor operations26, 38–48, 78broadcasting40–41derivatives of51–52element-wise operations38–40geometric interpretation of44–47geometric interpretation of deep learning47–48tensor product41–43tensor reshaping43–44tensor product41–43tensor reshaping43–44tensors31–38constant76–78data batches35higher-rank tensors32key attributes32–34manipulating in NumPy34matrices32real-world examples of35image data 37timeseries data 36vector data 35–36video data 37–38scalars31vectors31tensor slicing34test_step() method of Model class196text_dataset_from_directory function321, 370text generation350, 366–376callback with variable-temperature sampling372–375generating sequence data367history of366–367sampling strategy368–369with Keras369–372preparing data 370–371Transformer-based sequence-to-sequence model 371–372text splitting (tokenization)313–314text standardization312–313text summarization350textures385TextVectorization layer316–319, 370tf.data.Dataset class196, 217, 427tf.function197–198TF-IDF encoding, bigrams with325–327TF-IDF normalization325tf.string tensors317tf.Variable class60, 77theory of mind443timeserieskinds of tasks280–281recurrent neural networks293–300bidirectional 304–307recurrent layers in Keras 296–300stacking recurrent layers 303–304using recurrent dropout to fight overfitting 300temperature-forecasting example281–2931D convolutional model 290–291basic machine-learning model 289–290first recurrent baseline 292–293non-machine-learning baseline 288preparing data 285–287timeseries data (sequence data)36timeseries data35tokenization311total variation loss388TPU (Tensor Processing Unit)21, 428TPUStrategy scope428TPU training428–430Google Colab428–429step fusing430trainable parameters48trainable variables79trainable weights194training loop10, 48train_step() method of Model class198, 398Transformer architecture336–350, 439–440multi-head attention341–342self-attention337–341sequence-to-sequence learning358–363sequence-to-sequence model371–372Transformer decoder358–361Transformer encoder342–348TransformerDecoder440TransformerEncoder439 INDEX478Transformers436translating a vector45Turing test3Uunderfitting122–127ambiguous features124noisy training data123rare features and spurious correlations125–127update_state() method of Metric class186VVAEs (variational autoencod-ers), image generation with391–401concept vectors for image editing393overview of393–395sampling from latent spaces of images391–392with Keras396–401validationbinary classification102–105hold-out validation134–135K-fold validation115–119, 135–136monitoring loss and metrics on validation data91–92multiclass classification109–111validation data92validation metrics162value-centric analogies455value normalization161vanishing gradients251, 297Variable class76variable_dtype attribute of Layer class425variable-temperature sampling372–375vector data35vectorized targets433vectorizing text311vector regression96vectors31concept vectors for image editing393TextVectorization layer316–319vector data35–36vectorization161video data37–38vocabulary indexing314–315Wweight decay148weight pruning169weight quantization169weight regularization148–149weights9, 48, 84, 432word embeddings329–331learning with Embedding layer331–332pretrained334–336word-level tokenization313XXGBoost library19, 466 François Chollet ISBN:/uni00A0978-1-61729-686-4 Recent innovations in deep learning unlock exciting new software capabilities like automated language translation, image recognition, and more. Deep learning is quickly becoming essential knowledge for every software developer, and modern tools like Keras and TensorFlow put it within your reach—even if you have no background in mathematics or data science. /T_h is book shows you how to get started.Deep Learning with Python, Second Edition introduces the /f_i eld of deep learning using Python and the powerful Keras library. In this revised and expanded new edition, Keras creator François Chollet oﬀ ers insights for both novice and experi-enced machine learning practitioners. As you move through this book, you’ll build your understanding through intuitive explanations, crisp illustrations, and clear examples. You’ll quickly pick up the skills you need to start developing deep-learning applications. What’s Inside/uni25CF Deep learning from /f_i rst principles/uni25CF Image classi/f_i cation and image segmentation/uni25CF Time series forecasting/uni25CF Text classi/f_i cation and machine translation/uni25CF Text generation, neural style transfer, and image generationFor readers with intermediate Python skills. No previous experience with Keras, TensorFlow, or machine learning is required.François Chollet is a software engineer at Google and creator of the Keras deep-learning library.Register this print book to get free access to all ebook formats. Visit https:/ /www.manning.com/freebook$59.99 / Can $79.99 [INCLUDING eBOOK] Deep Learning with Python Second EditionPYTHON/DEEP LEARNING MANNING“Chollet is a master of pedagogy and explains complex concepts with minimal fuss, cutting through the math with practical Python code. He is also an experienced ML researcher and his insights on various model architectures or training tips are a joy to read.” —Martin Görner, Google“Immerse yourself into this exciting introduction to the topic with lots of real-world examples. A must read for every deep learning practitioner.” —Sayak Paul, Carted“/T_h e modern classic just got better.” —Edmon BegoliOak Ridge National Laboratory“T ruly the bible of deep learning.”—Yiannis ParaskevopoulosUniversity of West AtticaSee first page"
}